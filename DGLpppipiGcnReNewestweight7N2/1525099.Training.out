0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 02:36:44 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   36C    P0    45W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ad1910c9910> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m9.005s
user	0m2.627s
sys	0m1.146s
[02:36:55] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.7312],
        [-1.1434],
        [-0.5667],
        ...,
        [-0.1905],
        [ 2.2512],
        [-0.3281]], device='cuda:0', requires_grad=True) 
node features sum: tensor(116.0011, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0927, -0.0569,  0.0293, -0.1225, -0.0055, -0.0914,  0.1093, -0.0903,
         -0.1324, -0.1124,  0.1426, -0.0890,  0.0745,  0.0055,  0.1245,  0.1478,
          0.0956, -0.1150,  0.0602, -0.1476, -0.0311, -0.1101,  0.1173,  0.0464,
          0.0409,  0.1396, -0.1434, -0.0941,  0.0889, -0.0674, -0.0918, -0.0049,
          0.0539,  0.0580,  0.1239, -0.0505, -0.0825,  0.1180,  0.1307,  0.1005,
          0.0278,  0.0030,  0.0740,  0.1131,  0.0934,  0.1509,  0.1139, -0.0758,
         -0.0913,  0.0567, -0.1226,  0.0617, -0.1411,  0.1076,  0.0627,  0.1207,
          0.0927, -0.0243, -0.1243,  0.1443, -0.1060,  0.0552,  0.1023, -0.1011,
          0.1513,  0.1416, -0.0326, -0.1499, -0.0377, -0.1128, -0.0093, -0.0672,
          0.0670,  0.0710,  0.0084,  0.1020, -0.0591,  0.1389, -0.0555, -0.0817,
          0.0082, -0.0479, -0.0963,  0.0398,  0.1076, -0.0342, -0.0996,  0.0452,
         -0.0905,  0.1094, -0.0423, -0.1354,  0.0950,  0.0281, -0.0577, -0.0823,
         -0.0651, -0.0647, -0.0102,  0.0561,  0.0204, -0.1517,  0.0649, -0.0174,
         -0.0252,  0.0979, -0.1259,  0.1062, -0.1450, -0.1010,  0.0547,  0.1291,
         -0.0775, -0.0895,  0.0290,  0.1285,  0.1350,  0.1385, -0.0869, -0.0349,
          0.0758, -0.1020,  0.0045,  0.0884, -0.1455, -0.1161, -0.1432, -0.1244,
          0.1253,  0.0928, -0.1078,  0.0274, -0.0931, -0.0830,  0.1012,  0.0163,
          0.0317,  0.1373, -0.1437,  0.0538,  0.0181, -0.0283,  0.0375, -0.0654,
          0.0675, -0.1405, -0.1089, -0.0745,  0.0376, -0.0872, -0.1416, -0.1135,
          0.0557, -0.0140, -0.1379, -0.0642, -0.1010,  0.1516, -0.0465, -0.1506,
          0.0448, -0.0552,  0.0409, -0.0796,  0.0003,  0.0438,  0.1023, -0.0914,
          0.0719, -0.0509, -0.1096, -0.1255,  0.0165, -0.1329,  0.0311, -0.0780,
         -0.1293, -0.1377, -0.0716, -0.0044,  0.0524,  0.0708, -0.0652,  0.1388,
          0.0096,  0.0055, -0.1521,  0.0296,  0.0759, -0.0893,  0.0842, -0.1062,
         -0.0474,  0.0819, -0.0282, -0.1042, -0.1231, -0.0521,  0.0377,  0.0824,
          0.0725,  0.1431,  0.0719, -0.1425,  0.0681,  0.1113,  0.1447,  0.1063,
         -0.0755,  0.1355, -0.0309, -0.1036,  0.1248,  0.0059,  0.1167,  0.0673,
          0.0329, -0.0981,  0.0652,  0.0663,  0.1247,  0.0525, -0.0599,  0.0164,
          0.0910,  0.0470, -0.0279, -0.0813, -0.0604, -0.0961,  0.1409, -0.1127,
          0.0080,  0.1136,  0.0020,  0.0934,  0.1260,  0.1017,  0.0108, -0.0939,
         -0.0216, -0.0487, -0.1422,  0.1237,  0.0940,  0.1352,  0.0172, -0.0892,
          0.0437,  0.0456, -0.0552,  0.0023,  0.1337,  0.0860,  0.0833,  0.0164]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0927, -0.0569,  0.0293, -0.1225, -0.0055, -0.0914,  0.1093, -0.0903,
         -0.1324, -0.1124,  0.1426, -0.0890,  0.0745,  0.0055,  0.1245,  0.1478,
          0.0956, -0.1150,  0.0602, -0.1476, -0.0311, -0.1101,  0.1173,  0.0464,
          0.0409,  0.1396, -0.1434, -0.0941,  0.0889, -0.0674, -0.0918, -0.0049,
          0.0539,  0.0580,  0.1239, -0.0505, -0.0825,  0.1180,  0.1307,  0.1005,
          0.0278,  0.0030,  0.0740,  0.1131,  0.0934,  0.1509,  0.1139, -0.0758,
         -0.0913,  0.0567, -0.1226,  0.0617, -0.1411,  0.1076,  0.0627,  0.1207,
          0.0927, -0.0243, -0.1243,  0.1443, -0.1060,  0.0552,  0.1023, -0.1011,
          0.1513,  0.1416, -0.0326, -0.1499, -0.0377, -0.1128, -0.0093, -0.0672,
          0.0670,  0.0710,  0.0084,  0.1020, -0.0591,  0.1389, -0.0555, -0.0817,
          0.0082, -0.0479, -0.0963,  0.0398,  0.1076, -0.0342, -0.0996,  0.0452,
         -0.0905,  0.1094, -0.0423, -0.1354,  0.0950,  0.0281, -0.0577, -0.0823,
         -0.0651, -0.0647, -0.0102,  0.0561,  0.0204, -0.1517,  0.0649, -0.0174,
         -0.0252,  0.0979, -0.1259,  0.1062, -0.1450, -0.1010,  0.0547,  0.1291,
         -0.0775, -0.0895,  0.0290,  0.1285,  0.1350,  0.1385, -0.0869, -0.0349,
          0.0758, -0.1020,  0.0045,  0.0884, -0.1455, -0.1161, -0.1432, -0.1244,
          0.1253,  0.0928, -0.1078,  0.0274, -0.0931, -0.0830,  0.1012,  0.0163,
          0.0317,  0.1373, -0.1437,  0.0538,  0.0181, -0.0283,  0.0375, -0.0654,
          0.0675, -0.1405, -0.1089, -0.0745,  0.0376, -0.0872, -0.1416, -0.1135,
          0.0557, -0.0140, -0.1379, -0.0642, -0.1010,  0.1516, -0.0465, -0.1506,
          0.0448, -0.0552,  0.0409, -0.0796,  0.0003,  0.0438,  0.1023, -0.0914,
          0.0719, -0.0509, -0.1096, -0.1255,  0.0165, -0.1329,  0.0311, -0.0780,
         -0.1293, -0.1377, -0.0716, -0.0044,  0.0524,  0.0708, -0.0652,  0.1388,
          0.0096,  0.0055, -0.1521,  0.0296,  0.0759, -0.0893,  0.0842, -0.1062,
         -0.0474,  0.0819, -0.0282, -0.1042, -0.1231, -0.0521,  0.0377,  0.0824,
          0.0725,  0.1431,  0.0719, -0.1425,  0.0681,  0.1113,  0.1447,  0.1063,
         -0.0755,  0.1355, -0.0309, -0.1036,  0.1248,  0.0059,  0.1167,  0.0673,
          0.0329, -0.0981,  0.0652,  0.0663,  0.1247,  0.0525, -0.0599,  0.0164,
          0.0910,  0.0470, -0.0279, -0.0813, -0.0604, -0.0961,  0.1409, -0.1127,
          0.0080,  0.1136,  0.0020,  0.0934,  0.1260,  0.1017,  0.0108, -0.0939,
         -0.0216, -0.0487, -0.1422,  0.1237,  0.0940,  0.1352,  0.0172, -0.0892,
          0.0437,  0.0456, -0.0552,  0.0023,  0.1337,  0.0860,  0.0833,  0.0164]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0893,  0.0473,  0.0432,  ...,  0.1143, -0.1090,  0.1160],
        [-0.0790,  0.1106,  0.0857,  ...,  0.0963, -0.0724, -0.0965],
        [ 0.0908,  0.0343,  0.1113,  ..., -0.0578,  0.0876,  0.0396],
        ...,
        [ 0.1231,  0.0336, -0.1095,  ...,  0.0291, -0.0566,  0.0777],
        [ 0.1106, -0.0875, -0.1035,  ...,  0.0004,  0.0802,  0.0301],
        [-0.0326, -0.0608,  0.0722,  ...,  0.1012, -0.0617,  0.0141]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0893,  0.0473,  0.0432,  ...,  0.1143, -0.1090,  0.1160],
        [-0.0790,  0.1106,  0.0857,  ...,  0.0963, -0.0724, -0.0965],
        [ 0.0908,  0.0343,  0.1113,  ..., -0.0578,  0.0876,  0.0396],
        ...,
        [ 0.1231,  0.0336, -0.1095,  ...,  0.0291, -0.0566,  0.0777],
        [ 0.1106, -0.0875, -0.1035,  ...,  0.0004,  0.0802,  0.0301],
        [-0.0326, -0.0608,  0.0722,  ...,  0.1012, -0.0617,  0.0141]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0622, -0.0767, -0.1413,  ...,  0.1359,  0.0212,  0.1566],
        [ 0.0335,  0.0994, -0.1121,  ..., -0.0283,  0.1359,  0.0204],
        [ 0.0970, -0.0022,  0.1481,  ...,  0.0615,  0.1123,  0.0946],
        ...,
        [-0.0479,  0.0531, -0.1181,  ...,  0.1657, -0.1599, -0.1478],
        [-0.0605, -0.0524, -0.0966,  ...,  0.1710, -0.0803,  0.1619],
        [ 0.0065,  0.1589, -0.0368,  ...,  0.0402,  0.0878, -0.0937]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0622, -0.0767, -0.1413,  ...,  0.1359,  0.0212,  0.1566],
        [ 0.0335,  0.0994, -0.1121,  ..., -0.0283,  0.1359,  0.0204],
        [ 0.0970, -0.0022,  0.1481,  ...,  0.0615,  0.1123,  0.0946],
        ...,
        [-0.0479,  0.0531, -0.1181,  ...,  0.1657, -0.1599, -0.1478],
        [-0.0605, -0.0524, -0.0966,  ...,  0.1710, -0.0803,  0.1619],
        [ 0.0065,  0.1589, -0.0368,  ...,  0.0402,  0.0878, -0.0937]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1036, -0.0742,  0.1070,  ...,  0.1169,  0.2374, -0.1906],
        [-0.0678, -0.0688, -0.2370,  ..., -0.0693, -0.2130,  0.1761],
        [ 0.1013,  0.0734, -0.1970,  ..., -0.1198, -0.1884,  0.1011],
        ...,
        [-0.1406, -0.1417, -0.0847,  ..., -0.1664, -0.0898,  0.0738],
        [ 0.2476,  0.1997, -0.0461,  ..., -0.1864, -0.0730,  0.2165],
        [-0.2271, -0.0908,  0.1250,  ...,  0.0210,  0.0723, -0.1271]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1036, -0.0742,  0.1070,  ...,  0.1169,  0.2374, -0.1906],
        [-0.0678, -0.0688, -0.2370,  ..., -0.0693, -0.2130,  0.1761],
        [ 0.1013,  0.0734, -0.1970,  ..., -0.1198, -0.1884,  0.1011],
        ...,
        [-0.1406, -0.1417, -0.0847,  ..., -0.1664, -0.0898,  0.0738],
        [ 0.2476,  0.1997, -0.0461,  ..., -0.1864, -0.0730,  0.2165],
        [-0.2271, -0.0908,  0.1250,  ...,  0.0210,  0.0723, -0.1271]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.3141],
        [-0.3600],
        [-0.2375],
        [ 0.1309],
        [-0.0319],
        [-0.3930],
        [ 0.3336],
        [-0.3340],
        [ 0.1779],
        [-0.4033],
        [-0.0564],
        [-0.3246],
        [-0.2702],
        [ 0.1716],
        [-0.2771],
        [ 0.2704],
        [-0.1656],
        [-0.1283],
        [ 0.2218],
        [ 0.0340],
        [ 0.2888],
        [ 0.0016],
        [ 0.2263],
        [ 0.1983],
        [ 0.2146],
        [-0.2073],
        [ 0.0240],
        [ 0.0419],
        [ 0.3625],
        [ 0.0371],
        [-0.0654],
        [-0.0399]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.3141],
        [-0.3600],
        [-0.2375],
        [ 0.1309],
        [-0.0319],
        [-0.3930],
        [ 0.3336],
        [-0.3340],
        [ 0.1779],
        [-0.4033],
        [-0.0564],
        [-0.3246],
        [-0.2702],
        [ 0.1716],
        [-0.2771],
        [ 0.2704],
        [-0.1656],
        [-0.1283],
        [ 0.2218],
        [ 0.0340],
        [ 0.2888],
        [ 0.0016],
        [ 0.2263],
        [ 0.1983],
        [ 0.2146],
        [-0.2073],
        [ 0.0240],
        [ 0.0419],
        [ 0.3625],
        [ 0.0371],
        [-0.0654],
        [-0.0399]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0115,  0.1342, -0.0887, -0.0959,  0.0789,  0.0025, -0.0861,  0.0166,
          0.1415,  0.0631,  0.0745,  0.0833,  0.0397, -0.0224, -0.1183,  0.0100,
          0.1468, -0.0700, -0.0191,  0.0495, -0.0883,  0.1008,  0.0379,  0.1421,
         -0.0676,  0.0013, -0.0844,  0.0544, -0.0571,  0.0210,  0.0449,  0.0422,
         -0.0223,  0.0729,  0.0180, -0.0929, -0.0832,  0.0110,  0.0790,  0.0900,
         -0.0857,  0.0522,  0.0815, -0.1152,  0.0730,  0.0385, -0.0134, -0.0264,
          0.0700, -0.1082, -0.0189,  0.0302,  0.0849, -0.1297,  0.1407,  0.0035,
         -0.0393,  0.0656, -0.0081,  0.1426, -0.0369, -0.0884,  0.0947,  0.0115,
          0.0211, -0.0470,  0.1504,  0.0077, -0.1391,  0.1331, -0.0722, -0.0486,
          0.0907, -0.1487,  0.0510,  0.0837,  0.0719,  0.0740,  0.0377, -0.0015,
         -0.1155,  0.1124,  0.1115,  0.1138, -0.0009, -0.0652,  0.0489,  0.1453,
         -0.0621,  0.0823,  0.1350, -0.1099,  0.0663,  0.0132, -0.1039,  0.1205,
         -0.0609, -0.0891, -0.0158, -0.0842, -0.0027,  0.1146, -0.0760,  0.0085,
          0.1343,  0.1009,  0.0616, -0.1527, -0.0231, -0.1327, -0.1170,  0.0585,
         -0.0854,  0.1121,  0.0433, -0.1341, -0.0185,  0.0578, -0.0372, -0.1320,
          0.1489, -0.0468, -0.1191,  0.0392,  0.0706, -0.0801, -0.1428, -0.0358,
          0.0520,  0.1342,  0.1404,  0.0087,  0.0009,  0.0527,  0.0429,  0.1060,
          0.0785,  0.0281,  0.0523, -0.0952,  0.0608,  0.0525,  0.1177, -0.1251,
         -0.0798,  0.0831, -0.1437,  0.0759,  0.1454,  0.0533,  0.0943, -0.0395,
         -0.0972, -0.0572,  0.1255,  0.0178,  0.0125,  0.1356,  0.0214, -0.0047,
         -0.0813,  0.0682,  0.0031,  0.0233,  0.1387,  0.0460,  0.0255,  0.0997,
         -0.0562,  0.0993,  0.1173,  0.1323, -0.1342,  0.0133,  0.0847,  0.0415,
         -0.0838,  0.0035,  0.0657, -0.1344,  0.0908,  0.0622, -0.0207,  0.0764,
         -0.1337,  0.0752,  0.0724, -0.1011, -0.1085,  0.0987, -0.0617, -0.0084,
         -0.1259,  0.0365,  0.0879, -0.0943,  0.1192,  0.0061, -0.0767, -0.1347,
          0.0232,  0.1357,  0.1406,  0.0938, -0.0281,  0.1365,  0.0249, -0.0916,
          0.0846, -0.0329, -0.1004, -0.0292, -0.0820,  0.1385, -0.0977,  0.0344,
         -0.1345, -0.0773, -0.0574, -0.1341, -0.1406, -0.1234, -0.0679, -0.0408,
         -0.1483,  0.0089, -0.0652,  0.0721,  0.0023, -0.0311, -0.1108, -0.0962,
         -0.1307, -0.0297, -0.0663,  0.0461, -0.0013,  0.0594, -0.0663,  0.1463,
         -0.1107,  0.0080, -0.0532, -0.0492,  0.0971, -0.0352, -0.0018,  0.0691,
         -0.1042, -0.1259, -0.1449,  0.0204,  0.0090,  0.1277, -0.1411, -0.1210]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0115,  0.1342, -0.0887, -0.0959,  0.0789,  0.0025, -0.0861,  0.0166,
          0.1415,  0.0631,  0.0745,  0.0833,  0.0397, -0.0224, -0.1183,  0.0100,
          0.1468, -0.0700, -0.0191,  0.0495, -0.0883,  0.1008,  0.0379,  0.1421,
         -0.0676,  0.0013, -0.0844,  0.0544, -0.0571,  0.0210,  0.0449,  0.0422,
         -0.0223,  0.0729,  0.0180, -0.0929, -0.0832,  0.0110,  0.0790,  0.0900,
         -0.0857,  0.0522,  0.0815, -0.1152,  0.0730,  0.0385, -0.0134, -0.0264,
          0.0700, -0.1082, -0.0189,  0.0302,  0.0849, -0.1297,  0.1407,  0.0035,
         -0.0393,  0.0656, -0.0081,  0.1426, -0.0369, -0.0884,  0.0947,  0.0115,
          0.0211, -0.0470,  0.1504,  0.0077, -0.1391,  0.1331, -0.0722, -0.0486,
          0.0907, -0.1487,  0.0510,  0.0837,  0.0719,  0.0740,  0.0377, -0.0015,
         -0.1155,  0.1124,  0.1115,  0.1138, -0.0009, -0.0652,  0.0489,  0.1453,
         -0.0621,  0.0823,  0.1350, -0.1099,  0.0663,  0.0132, -0.1039,  0.1205,
         -0.0609, -0.0891, -0.0158, -0.0842, -0.0027,  0.1146, -0.0760,  0.0085,
          0.1343,  0.1009,  0.0616, -0.1527, -0.0231, -0.1327, -0.1170,  0.0585,
         -0.0854,  0.1121,  0.0433, -0.1341, -0.0185,  0.0578, -0.0372, -0.1320,
          0.1489, -0.0468, -0.1191,  0.0392,  0.0706, -0.0801, -0.1428, -0.0358,
          0.0520,  0.1342,  0.1404,  0.0087,  0.0009,  0.0527,  0.0429,  0.1060,
          0.0785,  0.0281,  0.0523, -0.0952,  0.0608,  0.0525,  0.1177, -0.1251,
         -0.0798,  0.0831, -0.1437,  0.0759,  0.1454,  0.0533,  0.0943, -0.0395,
         -0.0972, -0.0572,  0.1255,  0.0178,  0.0125,  0.1356,  0.0214, -0.0047,
         -0.0813,  0.0682,  0.0031,  0.0233,  0.1387,  0.0460,  0.0255,  0.0997,
         -0.0562,  0.0993,  0.1173,  0.1323, -0.1342,  0.0133,  0.0847,  0.0415,
         -0.0838,  0.0035,  0.0657, -0.1344,  0.0908,  0.0622, -0.0207,  0.0764,
         -0.1337,  0.0752,  0.0724, -0.1011, -0.1085,  0.0987, -0.0617, -0.0084,
         -0.1259,  0.0365,  0.0879, -0.0943,  0.1192,  0.0061, -0.0767, -0.1347,
          0.0232,  0.1357,  0.1406,  0.0938, -0.0281,  0.1365,  0.0249, -0.0916,
          0.0846, -0.0329, -0.1004, -0.0292, -0.0820,  0.1385, -0.0977,  0.0344,
         -0.1345, -0.0773, -0.0574, -0.1341, -0.1406, -0.1234, -0.0679, -0.0408,
         -0.1483,  0.0089, -0.0652,  0.0721,  0.0023, -0.0311, -0.1108, -0.0962,
         -0.1307, -0.0297, -0.0663,  0.0461, -0.0013,  0.0594, -0.0663,  0.1463,
         -0.1107,  0.0080, -0.0532, -0.0492,  0.0971, -0.0352, -0.0018,  0.0691,
         -0.1042, -0.1259, -0.1449,  0.0204,  0.0090,  0.1277, -0.1411, -0.1210]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0442,  0.0588, -0.1247,  ..., -0.0099,  0.0920,  0.0434],
        [ 0.0618, -0.0068,  0.0251,  ..., -0.0207,  0.0956, -0.1136],
        [-0.0053, -0.0405,  0.0826,  ..., -0.0367,  0.1234, -0.0096],
        ...,
        [-0.0492,  0.0320, -0.0396,  ..., -0.0809,  0.0324,  0.0469],
        [-0.0995,  0.0895, -0.0289,  ...,  0.0280,  0.1146, -0.1144],
        [ 0.0938,  0.0736, -0.0822,  ..., -0.0083,  0.0438,  0.0843]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0442,  0.0588, -0.1247,  ..., -0.0099,  0.0920,  0.0434],
        [ 0.0618, -0.0068,  0.0251,  ..., -0.0207,  0.0956, -0.1136],
        [-0.0053, -0.0405,  0.0826,  ..., -0.0367,  0.1234, -0.0096],
        ...,
        [-0.0492,  0.0320, -0.0396,  ..., -0.0809,  0.0324,  0.0469],
        [-0.0995,  0.0895, -0.0289,  ...,  0.0280,  0.1146, -0.1144],
        [ 0.0938,  0.0736, -0.0822,  ..., -0.0083,  0.0438,  0.0843]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0126,  0.0711, -0.0791,  ...,  0.1100, -0.0358, -0.0901],
        [ 0.0172,  0.1324, -0.0421,  ...,  0.1008, -0.1731,  0.1438],
        [-0.0957, -0.0213, -0.1712,  ..., -0.0617, -0.0641, -0.0273],
        ...,
        [-0.0568, -0.0486, -0.1375,  ..., -0.1649,  0.0019, -0.0995],
        [-0.1195, -0.1346, -0.1121,  ...,  0.1482, -0.0977,  0.1326],
        [-0.1210, -0.0724, -0.0356,  ...,  0.1734,  0.0985,  0.1465]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0126,  0.0711, -0.0791,  ...,  0.1100, -0.0358, -0.0901],
        [ 0.0172,  0.1324, -0.0421,  ...,  0.1008, -0.1731,  0.1438],
        [-0.0957, -0.0213, -0.1712,  ..., -0.0617, -0.0641, -0.0273],
        ...,
        [-0.0568, -0.0486, -0.1375,  ..., -0.1649,  0.0019, -0.0995],
        [-0.1195, -0.1346, -0.1121,  ...,  0.1482, -0.0977,  0.1326],
        [-0.1210, -0.0724, -0.0356,  ...,  0.1734,  0.0985,  0.1465]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1994,  0.1511, -0.1792,  ...,  0.2180,  0.2409, -0.0188],
        [-0.0768, -0.1583, -0.2283,  ...,  0.1845,  0.0867,  0.0104],
        [ 0.1158,  0.1159,  0.1687,  ..., -0.0706,  0.0086, -0.1200],
        ...,
        [-0.0582,  0.0936, -0.0532,  ..., -0.1895,  0.2185,  0.1744],
        [-0.0071, -0.0813,  0.2458,  ..., -0.0151, -0.2155, -0.1632],
        [-0.1363,  0.2103,  0.0630,  ...,  0.0229, -0.2281, -0.2264]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1994,  0.1511, -0.1792,  ...,  0.2180,  0.2409, -0.0188],
        [-0.0768, -0.1583, -0.2283,  ...,  0.1845,  0.0867,  0.0104],
        [ 0.1158,  0.1159,  0.1687,  ..., -0.0706,  0.0086, -0.1200],
        ...,
        [-0.0582,  0.0936, -0.0532,  ..., -0.1895,  0.2185,  0.1744],
        [-0.0071, -0.0813,  0.2458,  ..., -0.0151, -0.2155, -0.1632],
        [-0.1363,  0.2103,  0.0630,  ...,  0.0229, -0.2281, -0.2264]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3016],
        [-0.2659],
        [-0.1635],
        [ 0.2931],
        [-0.3592],
        [-0.3005],
        [ 0.2474],
        [ 0.1586],
        [-0.0154],
        [-0.1080],
        [ 0.0874],
        [ 0.1608],
        [ 0.0577],
        [ 0.0250],
        [ 0.0654],
        [-0.0660],
        [-0.3428],
        [-0.0532],
        [-0.0668],
        [-0.3270],
        [ 0.3074],
        [-0.3427],
        [-0.2482],
        [ 0.2500],
        [-0.0589],
        [ 0.4041],
        [-0.2921],
        [ 0.1144],
        [-0.1028],
        [-0.0139],
        [ 0.0699],
        [-0.2259]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3016],
        [-0.2659],
        [-0.1635],
        [ 0.2931],
        [-0.3592],
        [-0.3005],
        [ 0.2474],
        [ 0.1586],
        [-0.0154],
        [-0.1080],
        [ 0.0874],
        [ 0.1608],
        [ 0.0577],
        [ 0.0250],
        [ 0.0654],
        [-0.0660],
        [-0.3428],
        [-0.0532],
        [-0.0668],
        [-0.3270],
        [ 0.3074],
        [-0.3427],
        [-0.2482],
        [ 0.2500],
        [-0.0589],
        [ 0.4041],
        [-0.2921],
        [ 0.1144],
        [-0.1028],
        [-0.0139],
        [ 0.0699],
        [-0.2259]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-5.8732, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-3.0229, device='cuda:0')



h[100].sum tensor(2.8620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9374, device='cuda:0')



h[200].sum tensor(-2.0093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2979.9155, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0012, 0.0004, 0.0006,  ..., 0.0001, 0.0000, 0.0012],
        [0.0062, 0.0022, 0.0029,  ..., 0.0008, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(13639.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(241.3068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.3039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-15.5028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36.4682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2.9174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0153],
        [0.0188],
        [0.0271],
        ...,
        [0.0043],
        [0.0043],
        [0.0034]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(387.2088, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0153],
        [0.0188],
        [0.0271],
        ...,
        [0.0043],
        [0.0043],
        [0.0034]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-1.1450, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.0392, device='cuda:0')



h[100].sum tensor(-12.6646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.5620, device='cuda:0')



h[200].sum tensor(-14.4349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-14.3179, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15089.4512, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0231, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0155],
        [0.0048, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(87986.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1866.8076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(131.3386, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-81.2674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.0460, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2140],
        [-0.1312],
        [-0.0803],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-12572.7754, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0153],
        [0.0188],
        [0.0271],
        ...,
        [0.0043],
        [0.0043],
        [0.0034]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 1 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0811,  0.0908,  0.1428,  0.1442, -0.0272,  0.1296,  0.0758, -0.0892,
         -0.0177, -0.0572,  0.0605, -0.1267,  0.0312,  0.1305, -0.0452,  0.1397,
         -0.0716,  0.1121,  0.0750,  0.0655, -0.0240, -0.0949,  0.0544, -0.0051,
          0.0887, -0.0639, -0.0131,  0.0125, -0.0102, -0.1418,  0.0477,  0.0012,
         -0.1278,  0.0469, -0.0371,  0.0979,  0.1120,  0.0795,  0.0663, -0.0798,
          0.1494,  0.1037,  0.1025, -0.0763, -0.1226,  0.0899,  0.0021, -0.1387,
          0.1305, -0.1357, -0.0840, -0.1203, -0.0995, -0.1118, -0.0731,  0.1170,
         -0.0130,  0.0913,  0.0431, -0.0802,  0.0506, -0.0093,  0.1334,  0.0982,
         -0.0423, -0.1010,  0.1243,  0.1462,  0.0622, -0.0617,  0.1387, -0.0522,
         -0.1002,  0.0785,  0.1403, -0.1117, -0.0745, -0.1089,  0.1370, -0.1131,
          0.1369, -0.1280,  0.0134,  0.0219,  0.0584,  0.0575, -0.0041,  0.1449,
          0.0810, -0.0438, -0.1358,  0.0258,  0.1326, -0.0812, -0.1367, -0.0038,
         -0.0935,  0.0327,  0.0223, -0.1345, -0.0513,  0.0163,  0.1108,  0.0952,
          0.0659, -0.1451,  0.0493, -0.1472,  0.0415,  0.1439,  0.0317,  0.0734,
         -0.0088,  0.1133,  0.0264,  0.0246, -0.0456, -0.0786, -0.0661,  0.0335,
          0.0005, -0.0336,  0.0813,  0.0815,  0.0801, -0.0693, -0.0241,  0.1422,
         -0.1462,  0.1005, -0.0381, -0.0486,  0.0855,  0.0961,  0.0788,  0.0541,
         -0.0699,  0.0941,  0.1023, -0.0277,  0.1131,  0.1279,  0.1502, -0.0903,
          0.0125, -0.0641,  0.1078, -0.1518, -0.0781,  0.1040, -0.0949, -0.0333,
         -0.1177, -0.0323, -0.0541, -0.1353, -0.0878, -0.0408,  0.0805,  0.1120,
          0.0165,  0.0752, -0.0792,  0.1496, -0.0677,  0.0885,  0.1495,  0.0307,
         -0.0216,  0.1153,  0.0273, -0.0993, -0.0947,  0.0461, -0.0747, -0.1488,
          0.0548,  0.0092,  0.1129, -0.0342,  0.0683, -0.1299,  0.0079,  0.1076,
         -0.0618,  0.0136,  0.0015,  0.0258, -0.1401,  0.0681, -0.0336,  0.1269,
          0.0702,  0.1030,  0.0856, -0.0231,  0.1382,  0.0556, -0.0361, -0.0529,
         -0.1450,  0.0133,  0.0005,  0.0797,  0.0595,  0.0547, -0.0031,  0.0842,
         -0.0365, -0.1235, -0.0193, -0.1049, -0.0542, -0.0563,  0.0604, -0.1338,
          0.1121,  0.0126,  0.0591, -0.0784,  0.0459,  0.0435,  0.1377,  0.0634,
         -0.1161, -0.1400, -0.0564, -0.1465,  0.1353,  0.1086, -0.0334, -0.0565,
         -0.1480,  0.0944, -0.0267,  0.0559, -0.0613,  0.1244,  0.0745,  0.1474,
          0.0687, -0.0697, -0.0715,  0.0661,  0.0424, -0.1417, -0.0058,  0.0248,
          0.0843, -0.1200,  0.1092,  0.0488,  0.0262,  0.0223, -0.0826, -0.1295]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1205, -0.0271,  0.0663,  ..., -0.0156, -0.0283, -0.0218],
        [-0.0746, -0.0105, -0.0697,  ...,  0.1175,  0.0719,  0.0245],
        [ 0.0429, -0.0397,  0.0609,  ..., -0.1158,  0.0588,  0.0206],
        ...,
        [ 0.0679,  0.1106, -0.1244,  ...,  0.0100, -0.1099,  0.0402],
        [ 0.0079, -0.0394,  0.0223,  ...,  0.0565,  0.0444, -0.0483],
        [-0.0070, -0.1247, -0.0923,  ...,  0.0796,  0.0676, -0.0352]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1079, -0.0052,  0.1606,  ..., -0.0947,  0.0662, -0.1163],
        [ 0.0681,  0.0476, -0.0721,  ..., -0.0010, -0.0260, -0.0138],
        [-0.1034, -0.0101, -0.0723,  ...,  0.1651,  0.0313, -0.0790],
        ...,
        [ 0.0096,  0.0025, -0.0197,  ..., -0.0881,  0.0353, -0.0442],
        [-0.0842, -0.1505, -0.0777,  ...,  0.0759, -0.1586, -0.1315],
        [-0.1511,  0.0857,  0.0433,  ..., -0.0733,  0.1560,  0.1053]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0918, -0.0107, -0.1045,  ...,  0.0602,  0.1726, -0.0667],
        [-0.2325, -0.1748,  0.2015,  ..., -0.0927,  0.0816,  0.1640],
        [-0.2086,  0.0311,  0.0215,  ..., -0.0300, -0.0605, -0.0947],
        ...,
        [-0.1422,  0.1766, -0.0505,  ..., -0.0458, -0.2372, -0.1312],
        [-0.0093, -0.0475, -0.0307,  ..., -0.0323,  0.1658,  0.1397],
        [-0.0020, -0.0422,  0.0454,  ...,  0.1229,  0.0162, -0.1805]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4118],
        [ 0.1484],
        [ 0.3595],
        [ 0.0903],
        [ 0.1981],
        [-0.1415],
        [ 0.0228],
        [ 0.3647],
        [ 0.0590],
        [ 0.1138],
        [-0.1080],
        [ 0.1486],
        [ 0.3550],
        [ 0.3053],
        [ 0.2435],
        [ 0.1891],
        [-0.3214],
        [-0.2351],
        [ 0.0453],
        [ 0.1045],
        [-0.0468],
        [ 0.4201],
        [-0.1399],
        [ 0.0156],
        [ 0.2897],
        [-0.0255],
        [ 0.2953],
        [-0.2714],
        [ 0.4102],
        [ 0.2103],
        [-0.2332],
        [ 0.3517]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0811,  0.0908,  0.1428,  0.1442, -0.0272,  0.1296,  0.0758, -0.0892,
         -0.0177, -0.0572,  0.0605, -0.1267,  0.0312,  0.1305, -0.0452,  0.1397,
         -0.0716,  0.1121,  0.0750,  0.0655, -0.0240, -0.0949,  0.0544, -0.0051,
          0.0887, -0.0639, -0.0131,  0.0125, -0.0102, -0.1418,  0.0477,  0.0012,
         -0.1278,  0.0469, -0.0371,  0.0979,  0.1120,  0.0795,  0.0663, -0.0798,
          0.1494,  0.1037,  0.1025, -0.0763, -0.1226,  0.0899,  0.0021, -0.1387,
          0.1305, -0.1357, -0.0840, -0.1203, -0.0995, -0.1118, -0.0731,  0.1170,
         -0.0130,  0.0913,  0.0431, -0.0802,  0.0506, -0.0093,  0.1334,  0.0982,
         -0.0423, -0.1010,  0.1243,  0.1462,  0.0622, -0.0617,  0.1387, -0.0522,
         -0.1002,  0.0785,  0.1403, -0.1117, -0.0745, -0.1089,  0.1370, -0.1131,
          0.1369, -0.1280,  0.0134,  0.0219,  0.0584,  0.0575, -0.0041,  0.1449,
          0.0810, -0.0438, -0.1358,  0.0258,  0.1326, -0.0812, -0.1367, -0.0038,
         -0.0935,  0.0327,  0.0223, -0.1345, -0.0513,  0.0163,  0.1108,  0.0952,
          0.0659, -0.1451,  0.0493, -0.1472,  0.0415,  0.1439,  0.0317,  0.0734,
         -0.0088,  0.1133,  0.0264,  0.0246, -0.0456, -0.0786, -0.0661,  0.0335,
          0.0005, -0.0336,  0.0813,  0.0815,  0.0801, -0.0693, -0.0241,  0.1422,
         -0.1462,  0.1005, -0.0381, -0.0486,  0.0855,  0.0961,  0.0788,  0.0541,
         -0.0699,  0.0941,  0.1023, -0.0277,  0.1131,  0.1279,  0.1502, -0.0903,
          0.0125, -0.0641,  0.1078, -0.1518, -0.0781,  0.1040, -0.0949, -0.0333,
         -0.1177, -0.0323, -0.0541, -0.1353, -0.0878, -0.0408,  0.0805,  0.1120,
          0.0165,  0.0752, -0.0792,  0.1496, -0.0677,  0.0885,  0.1495,  0.0307,
         -0.0216,  0.1153,  0.0273, -0.0993, -0.0947,  0.0461, -0.0747, -0.1488,
          0.0548,  0.0092,  0.1129, -0.0342,  0.0683, -0.1299,  0.0079,  0.1076,
         -0.0618,  0.0136,  0.0015,  0.0258, -0.1401,  0.0681, -0.0336,  0.1269,
          0.0702,  0.1030,  0.0856, -0.0231,  0.1382,  0.0556, -0.0361, -0.0529,
         -0.1450,  0.0133,  0.0005,  0.0797,  0.0595,  0.0547, -0.0031,  0.0842,
         -0.0365, -0.1235, -0.0193, -0.1049, -0.0542, -0.0563,  0.0604, -0.1338,
          0.1121,  0.0126,  0.0591, -0.0784,  0.0459,  0.0435,  0.1377,  0.0634,
         -0.1161, -0.1400, -0.0564, -0.1465,  0.1353,  0.1086, -0.0334, -0.0565,
         -0.1480,  0.0944, -0.0267,  0.0559, -0.0613,  0.1244,  0.0745,  0.1474,
          0.0687, -0.0697, -0.0715,  0.0661,  0.0424, -0.1417, -0.0058,  0.0248,
          0.0843, -0.1200,  0.1092,  0.0488,  0.0262,  0.0223, -0.0826, -0.1295]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1205, -0.0271,  0.0663,  ..., -0.0156, -0.0283, -0.0218],
        [-0.0746, -0.0105, -0.0697,  ...,  0.1175,  0.0719,  0.0245],
        [ 0.0429, -0.0397,  0.0609,  ..., -0.1158,  0.0588,  0.0206],
        ...,
        [ 0.0679,  0.1106, -0.1244,  ...,  0.0100, -0.1099,  0.0402],
        [ 0.0079, -0.0394,  0.0223,  ...,  0.0565,  0.0444, -0.0483],
        [-0.0070, -0.1247, -0.0923,  ...,  0.0796,  0.0676, -0.0352]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1079, -0.0052,  0.1606,  ..., -0.0947,  0.0662, -0.1163],
        [ 0.0681,  0.0476, -0.0721,  ..., -0.0010, -0.0260, -0.0138],
        [-0.1034, -0.0101, -0.0723,  ...,  0.1651,  0.0313, -0.0790],
        ...,
        [ 0.0096,  0.0025, -0.0197,  ..., -0.0881,  0.0353, -0.0442],
        [-0.0842, -0.1505, -0.0777,  ...,  0.0759, -0.1586, -0.1315],
        [-0.1511,  0.0857,  0.0433,  ..., -0.0733,  0.1560,  0.1053]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0918, -0.0107, -0.1045,  ...,  0.0602,  0.1726, -0.0667],
        [-0.2325, -0.1748,  0.2015,  ..., -0.0927,  0.0816,  0.1640],
        [-0.2086,  0.0311,  0.0215,  ..., -0.0300, -0.0605, -0.0947],
        ...,
        [-0.1422,  0.1766, -0.0505,  ..., -0.0458, -0.2372, -0.1312],
        [-0.0093, -0.0475, -0.0307,  ..., -0.0323,  0.1658,  0.1397],
        [-0.0020, -0.0422,  0.0454,  ...,  0.1229,  0.0162, -0.1805]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4118],
        [ 0.1484],
        [ 0.3595],
        [ 0.0903],
        [ 0.1981],
        [-0.1415],
        [ 0.0228],
        [ 0.3647],
        [ 0.0590],
        [ 0.1138],
        [-0.1080],
        [ 0.1486],
        [ 0.3550],
        [ 0.3053],
        [ 0.2435],
        [ 0.1891],
        [-0.3214],
        [-0.2351],
        [ 0.0453],
        [ 0.1045],
        [-0.0468],
        [ 0.4201],
        [-0.1399],
        [ 0.0156],
        [ 0.2897],
        [-0.0255],
        [ 0.2953],
        [-0.2714],
        [ 0.4102],
        [ 0.2103],
        [-0.2332],
        [ 0.3517]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0119,  0.0133,  0.0210,  ...,  0.0033, -0.0121, -0.0190],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(622.5011, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3328, device='cuda:0')



h[100].sum tensor(-14.9961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.9169, device='cuda:0')



h[200].sum tensor(-42.4124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3794e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0449, 0.0503, 0.0792,  ..., 0.0124, 0.0000, 0.0000],
        [0.0370, 0.0414, 0.0651,  ..., 0.0102, 0.0000, 0.0000],
        [0.0087, 0.0097, 0.0152,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33546.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2752, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2358, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1893, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169397.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3833.3889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-245.9583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.6904, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(12.5482, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.8200e+00],
        [1.9694e+00],
        [2.1731e+00],
        ...,
        [2.1865e-06],
        [2.8728e-07],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(33920.2773, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(112.4094, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365924.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0000e-04, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(525.8971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5445, device='cuda:0')



h[100].sum tensor(-13.8054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.3220, device='cuda:0')



h[200].sum tensor(-39.0937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2689e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33787.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0013, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180346.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4003.8804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-227.3465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.4610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39.0077, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.4942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2406e-03],
        [ 8.1742e-03],
        [ 2.5050e-02],
        ...,
        [-4.2354e-05],
        [-4.3871e-05],
        [-6.5410e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(21771.1445, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365924.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365919.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9976e-04, -1.9861e-04,  4.7213e-05,  ..., -1.9913e-04,
          0.0000e+00,  0.0000e+00],
        [-1.9976e-04, -1.9861e-04,  4.7213e-05,  ..., -1.9913e-04,
          0.0000e+00,  0.0000e+00],
        [-1.9976e-04, -1.9861e-04,  4.7213e-05,  ..., -1.9913e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.9976e-04, -1.9861e-04,  4.7213e-05,  ..., -1.9913e-04,
          0.0000e+00,  0.0000e+00],
        [-1.9976e-04, -1.9861e-04,  4.7213e-05,  ..., -1.9913e-04,
          0.0000e+00,  0.0000e+00],
        [-1.9976e-04, -1.9861e-04,  4.7213e-05,  ..., -1.9913e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(290.6017, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(8.7221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5998, device='cuda:0')



h[100].sum tensor(-9.7953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0204, device='cuda:0')



h[200].sum tensor(-27.7729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.0175e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0393, 0.0441, 0.0707,  ..., 0.0103, 0.0000, 0.0000],
        [0.0261, 0.0293, 0.0470,  ..., 0.0068, 0.0000, 0.0000],
        [0.0215, 0.0242, 0.0389,  ..., 0.0056, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27205.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2341, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1879, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1429, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0014, 0.0003, 0.0020,  ..., 0.0000, 0.0011, 0.0000],
        [0.0014, 0.0003, 0.0020,  ..., 0.0000, 0.0011, 0.0000],
        [0.0014, 0.0003, 0.0020,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153503.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3206.9736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.5809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47.5544, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.7016, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(49.4460, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4797],
        [ 0.4307],
        [ 0.3490],
        ...,
        [-0.0007],
        [-0.0007],
        [-0.0007]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(7780.4365, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365919.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9998],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365916., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1104e-03,  4.6444e-03,  7.8455e-03,  ...,  9.0085e-04,
         -4.4834e-03, -7.0400e-03],
        [ 1.0376e-02,  1.1666e-02,  1.8946e-02,  ...,  2.6091e-03,
         -1.0869e-02, -1.7066e-02],
        [ 7.5146e-03,  8.4597e-03,  1.3877e-02,  ...,  1.8290e-03,
         -7.9526e-03, -1.2488e-02],
        ...,
        [-2.8890e-04, -2.8607e-04,  5.1471e-05,  ..., -2.9858e-04,
          0.0000e+00,  0.0000e+00],
        [-2.8890e-04, -2.8607e-04,  5.1471e-05,  ..., -2.9858e-04,
          0.0000e+00,  0.0000e+00],
        [-2.8890e-04, -2.8607e-04,  5.1471e-05,  ..., -2.9858e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(276.7434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.1946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0503, device='cuda:0')



h[100].sum tensor(-10.7340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3140, device='cuda:0')



h[200].sum tensor(-30.4731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9135e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0369, 0.0415, 0.0677,  ..., 0.0092, 0.0000, 0.0000],
        [0.0234, 0.0264, 0.0437,  ..., 0.0055, 0.0000, 0.0000],
        [0.0198, 0.0223, 0.0367,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28968.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1559, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1390, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1197, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0016, 0.0014, 0.0029,  ..., 0.0000, 0.0016, 0.0000],
        [0.0016, 0.0014, 0.0029,  ..., 0.0000, 0.0016, 0.0000],
        [0.0016, 0.0014, 0.0029,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158323., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3020.4216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-146.5241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(190.4053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(54.7548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3421],
        [ 0.3198],
        [ 0.2747],
        ...,
        [-0.0044],
        [-0.0044],
        [-0.0044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-1736.6477, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9998],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365916., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9998],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365913.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.5733e-04, -3.6677e-04,  6.8469e-05,  ..., -3.9279e-04,
          0.0000e+00,  0.0000e+00],
        [-3.5733e-04, -3.6677e-04,  6.8469e-05,  ..., -3.9279e-04,
          0.0000e+00,  0.0000e+00],
        [-3.5733e-04, -3.6677e-04,  6.8469e-05,  ..., -3.9279e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.5733e-04, -3.6677e-04,  6.8469e-05,  ..., -3.9279e-04,
          0.0000e+00,  0.0000e+00],
        [-3.5733e-04, -3.6677e-04,  6.8469e-05,  ..., -3.9279e-04,
          0.0000e+00,  0.0000e+00],
        [-3.5733e-04, -3.6677e-04,  6.8469e-05,  ..., -3.9279e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(188.6986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.6263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9448, device='cuda:0')



h[100].sum tensor(-9.9397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3281, device='cuda:0')



h[200].sum tensor(-28.2537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.2306e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27710.0254, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0028, 0.0033,  ..., 0.0000, 0.0019, 0.0000],
        [0.0037, 0.0023, 0.0029,  ..., 0.0000, 0.0015, 0.0000],
        [0.0104, 0.0014, 0.0017,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0021, 0.0028, 0.0033,  ..., 0.0000, 0.0018, 0.0000],
        [0.0021, 0.0028, 0.0033,  ..., 0.0000, 0.0018, 0.0000],
        [0.0021, 0.0028, 0.0033,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153263.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2635.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.8659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.5591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(63.7606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0363],
        [-0.0316],
        [-0.0176],
        ...,
        [-0.0163],
        [-0.0128],
        [-0.0119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-8069.3916, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9998],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365913.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9998],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365910.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.0690e-03,  7.9325e-03,  1.3373e-02,  ...,  1.5515e-03,
         -7.6126e-03, -1.1964e-02],
        [-4.0580e-04, -4.4730e-04,  9.1072e-05,  ..., -4.7356e-04,
          0.0000e+00,  0.0000e+00],
        [-4.0580e-04, -4.4730e-04,  9.1072e-05,  ..., -4.7356e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.0580e-04, -4.4730e-04,  9.1072e-05,  ..., -4.7356e-04,
          0.0000e+00,  0.0000e+00],
        [-4.0580e-04, -4.4730e-04,  9.1072e-05,  ..., -4.7356e-04,
          0.0000e+00,  0.0000e+00],
        [-4.0580e-04, -4.4730e-04,  9.1072e-05,  ..., -4.7356e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(327.6658, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.1105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4157, device='cuda:0')



h[100].sum tensor(-14.4187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.0989, device='cuda:0')



h[200].sum tensor(-41.0376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3227e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0271, 0.0304, 0.0500,  ..., 0.0066, 0.0000, 0.0000],
        [0.0071, 0.0079, 0.0136,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40712.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3846e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.3343e-02, 3.3873e-04, 1.0406e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.9469e-02, 2.1701e-03, 1.6742e-03,  ..., 0.0000e+00, 1.0829e-03,
         0.0000e+00],
        ...,
        [2.6818e-03, 4.2843e-03, 3.2750e-03,  ..., 0.0000e+00, 2.1323e-03,
         0.0000e+00],
        [2.6818e-03, 4.2841e-03, 3.2748e-03,  ..., 0.0000e+00, 2.1322e-03,
         0.0000e+00],
        [2.6817e-03, 4.2840e-03, 3.2746e-03,  ..., 0.0000e+00, 2.1321e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234488.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4337.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-210.4605, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.4157, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(74.0081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0309],
        [-0.0080],
        [-0.0364],
        ...,
        [-0.0185],
        [-0.0184],
        [-0.0184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9362.6582, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9998],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365910.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9998],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004, -0.0005,  0.0001,  ..., -0.0006,  0.0000,  0.0000],
        [-0.0004, -0.0005,  0.0001,  ..., -0.0006,  0.0000,  0.0000],
        [-0.0004, -0.0005,  0.0001,  ..., -0.0006,  0.0000,  0.0000],
        ...,
        [-0.0004, -0.0005,  0.0001,  ..., -0.0006,  0.0000,  0.0000],
        [-0.0004, -0.0005,  0.0001,  ..., -0.0006,  0.0000,  0.0000],
        [-0.0004, -0.0005,  0.0001,  ..., -0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(214.0268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(5.4647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9002, device='cuda:0')



h[100].sum tensor(-12.8050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.8556, device='cuda:0')



h[200].sum tensor(-36.4911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1674e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37379.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0253, 0.0026, 0.0014,  ..., 0.0000, 0.0006, 0.0000],
        [0.0075, 0.0042, 0.0023,  ..., 0.0000, 0.0013, 0.0000],
        [0.0030, 0.0056, 0.0031,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0030, 0.0056, 0.0031,  ..., 0.0000, 0.0026, 0.0000],
        [0.0030, 0.0056, 0.0031,  ..., 0.0000, 0.0026, 0.0000],
        [0.0030, 0.0056, 0.0031,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(216548.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3608.5889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-182.1877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.0006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(81.5794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0246],
        [-0.0449],
        [-0.0537],
        ...,
        [-0.0234],
        [-0.0234],
        [-0.0233]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-11906.8164, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9998],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9997],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0133,  0.0148,  0.0246,  ...,  0.0031, -0.0140, -0.0220],
        [-0.0005, -0.0006,  0.0002,  ..., -0.0006,  0.0000,  0.0000],
        [-0.0005, -0.0006,  0.0002,  ..., -0.0006,  0.0000,  0.0000],
        ...,
        [-0.0005, -0.0006,  0.0002,  ..., -0.0006,  0.0000,  0.0000],
        [-0.0005, -0.0006,  0.0002,  ..., -0.0006,  0.0000,  0.0000],
        [-0.0005, -0.0006,  0.0002,  ..., -0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47.8559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.1328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0049, device='cuda:0')



h[100].sum tensor(-10.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3817, device='cuda:0')



h[200].sum tensor(-28.5346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.2677e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0237, 0.0264, 0.0447,  ..., 0.0054, 0.0000, 0.0000],
        [0.0133, 0.0148, 0.0252,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31400.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9033e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [9.6511e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.3240e-02, 6.9579e-04, 8.9445e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.1833e-03, 6.5196e-03, 3.0760e-03,  ..., 0.0000e+00, 3.4294e-03,
         0.0000e+00],
        [3.1831e-03, 6.5190e-03, 3.0755e-03,  ..., 0.0000e+00, 3.4290e-03,
         0.0000e+00],
        [3.1831e-03, 6.5188e-03, 3.0753e-03,  ..., 0.0000e+00, 3.4289e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191426.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2790.9565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-134.3477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(154.7806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(84.6223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0545],
        [-0.0005],
        [-0.0365],
        ...,
        [-0.0273],
        [-0.0272],
        [-0.0272]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17769.5820, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9997],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005, -0.0007,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        [-0.0005, -0.0007,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        [-0.0005, -0.0007,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        ...,
        [-0.0005, -0.0007,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        [-0.0005, -0.0007,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        [-0.0005, -0.0007,  0.0003,  ..., -0.0007,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-73.1333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.3824, device='cuda:0')



h[100].sum tensor(-8.1407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0429, device='cuda:0')



h[200].sum tensor(-23.2582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.6480e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0204, 0.0357,  ..., 0.0038, 0.0000, 0.0000],
        [0.0093, 0.0102, 0.0184,  ..., 0.0019, 0.0000, 0.0000],
        [0.0165, 0.0183, 0.0312,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27938.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0858, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0780, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0952, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0036, 0.0070, 0.0032,  ..., 0.0000, 0.0044, 0.0000],
        [0.0036, 0.0070, 0.0032,  ..., 0.0000, 0.0044, 0.0000],
        [0.0036, 0.0070, 0.0032,  ..., 0.0000, 0.0044, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180600.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2341.0242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.8557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.2759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(88.2414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0544],
        [-0.0487],
        [-0.0406],
        ...,
        [-0.0303],
        [-0.0302],
        [-0.0302]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-18206.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005, -0.0008,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        [ 0.0050,  0.0053,  0.0101,  ...,  0.0007, -0.0056, -0.0087],
        [-0.0005, -0.0008,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        ...,
        [-0.0005, -0.0008,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        [-0.0005, -0.0008,  0.0003,  ..., -0.0007,  0.0000,  0.0000],
        [-0.0005, -0.0008,  0.0003,  ..., -0.0007,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-142.3630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.1171, device='cuda:0')



h[100].sum tensor(-7.3654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9144, device='cuda:0')



h[200].sum tensor(-21.0701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-6.8664e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0053, 0.0111,  ..., 0.0007, 0.0000, 0.0000],
        [0.0073, 0.0079, 0.0151,  ..., 0.0013, 0.0000, 0.0000],
        [0.0245, 0.0264, 0.0484,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27275.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0361, 0.0009, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0517, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0782, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0072, 0.0033,  ..., 0.0000, 0.0056, 0.0000],
        [0.0041, 0.0072, 0.0033,  ..., 0.0000, 0.0056, 0.0000],
        [0.0041, 0.0072, 0.0033,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183399.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2283.3899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.0498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.0633, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(88.8412, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0723],
        [-0.0802],
        [-0.0856],
        ...,
        [-0.0330],
        [-0.0329],
        [-0.0329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-18852.3418, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005, -0.0009,  0.0004,  ..., -0.0008,  0.0000,  0.0000],
        [ 0.0176,  0.0194,  0.0326,  ...,  0.0040, -0.0183, -0.0289],
        [ 0.0178,  0.0196,  0.0331,  ...,  0.0041, -0.0186, -0.0292],
        ...,
        [-0.0005, -0.0009,  0.0004,  ..., -0.0008,  0.0000,  0.0000],
        [-0.0005, -0.0009,  0.0004,  ..., -0.0008,  0.0000,  0.0000],
        [-0.0005, -0.0009,  0.0004,  ..., -0.0008,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-27.6366, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.2710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0191, device='cuda:0')



h[100].sum tensor(-10.6214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2861, device='cuda:0')



h[200].sum tensor(-30.4234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.8941e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0228, 0.0249, 0.0441,  ..., 0.0047, 0.0000, 0.0000],
        [0.0316, 0.0344, 0.0606,  ..., 0.0064, 0.0000, 0.0000],
        [0.0673, 0.0740, 0.1251,  ..., 0.0153, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34444.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1262, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1969, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2983, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0046, 0.0073, 0.0036,  ..., 0.0000, 0.0070, 0.0000],
        [0.0046, 0.0073, 0.0036,  ..., 0.0000, 0.0070, 0.0000],
        [0.0046, 0.0073, 0.0036,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218310.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2929.5164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-165.9097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.2468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(90.9684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0232],
        [ 0.0040],
        [ 0.0348],
        ...,
        [-0.0345],
        [-0.0343],
        [-0.0343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-18302.5117, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(691.9186, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0062,  0.0119,  ...,  0.0008, -0.0065, -0.0102],
        [ 0.0073,  0.0077,  0.0143,  ...,  0.0012, -0.0078, -0.0123],
        [ 0.0123,  0.0133,  0.0232,  ...,  0.0025, -0.0129, -0.0203],
        ...,
        [-0.0004, -0.0010,  0.0005,  ..., -0.0008,  0.0000,  0.0000],
        [-0.0004, -0.0010,  0.0005,  ..., -0.0008,  0.0000,  0.0000],
        [-0.0004, -0.0010,  0.0005,  ..., -0.0008,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(142.7660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(8.9527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5131, device='cuda:0')



h[100].sum tensor(-14.9688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.0777, device='cuda:0')



h[200].sum tensor(-42.9313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3905e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0315, 0.0334, 0.0615,  ..., 0.0055, 0.0000, 0.0000],
        [0.0340, 0.0362, 0.0660,  ..., 0.0061, 0.0000, 0.0000],
        [0.0230, 0.0239, 0.0463,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43432.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1357, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1660, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0048, 0.0072, 0.0041,  ..., 0.0000, 0.0088, 0.0000],
        [0.0048, 0.0072, 0.0041,  ..., 0.0000, 0.0088, 0.0000],
        [0.0048, 0.0072, 0.0041,  ..., 0.0000, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(256855.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3656.1387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-238.7982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(227.1383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(94.2936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0439],
        [-0.0328],
        [-0.0189],
        ...,
        [-0.0351],
        [-0.0349],
        [-0.0349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-16289.7266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365909.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004, -0.0011,  0.0006,  ..., -0.0009,  0.0000,  0.0000],
        [-0.0004, -0.0011,  0.0006,  ..., -0.0009,  0.0000,  0.0000],
        [-0.0004, -0.0011,  0.0006,  ..., -0.0009,  0.0000,  0.0000],
        ...,
        [-0.0004, -0.0011,  0.0006,  ..., -0.0009,  0.0000,  0.0000],
        [-0.0004, -0.0011,  0.0006,  ..., -0.0009,  0.0000,  0.0000],
        [-0.0004, -0.0011,  0.0006,  ..., -0.0009,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(377.5627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3604, device='cuda:0')



h[100].sum tensor(-20.5772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.9678, device='cuda:0')



h[200].sum tensor(-59.0927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9370e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0084, 0.0088, 0.0182,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55703.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0072, 0.0064, 0.0041,  ..., 0.0000, 0.0086, 0.0000],
        [0.0158, 0.0036, 0.0027,  ..., 0.0000, 0.0053, 0.0000],
        [0.0403, 0.0008, 0.0009,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0050, 0.0071, 0.0045,  ..., 0.0000, 0.0106, 0.0000],
        [0.0050, 0.0071, 0.0045,  ..., 0.0000, 0.0106, 0.0000],
        [0.0050, 0.0071, 0.0045,  ..., 0.0000, 0.0106, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321283.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4899.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-334.7636, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(334.8905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(97.6197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0371],
        [-0.0354],
        [-0.0386],
        ...,
        [-0.0353],
        [-0.0352],
        [-0.0351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-13814.9199, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365909.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365910.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0177,  0.0191,  0.0331,  ...,  0.0039, -0.0183, -0.0289],
        [ 0.0260,  0.0283,  0.0478,  ...,  0.0061, -0.0267, -0.0421],
        [ 0.0184,  0.0199,  0.0344,  ...,  0.0041, -0.0191, -0.0301],
        ...,
        [-0.0004, -0.0012,  0.0007,  ..., -0.0009,  0.0000,  0.0000],
        [-0.0004, -0.0012,  0.0007,  ..., -0.0009,  0.0000,  0.0000],
        [-0.0004, -0.0012,  0.0007,  ..., -0.0009,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(113.4147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.1139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9379, device='cuda:0')



h[100].sum tensor(-15.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.4565, device='cuda:0')



h[200].sum tensor(-43.1405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4168e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0797, 0.0863, 0.1482,  ..., 0.0178, 0.0000, 0.0000],
        [0.0933, 0.1015, 0.1725,  ..., 0.0215, 0.0000, 0.0000],
        [0.0770, 0.0833, 0.1434,  ..., 0.0171, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45791.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3171, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3855, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3713, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0072, 0.0050,  ..., 0.0000, 0.0124, 0.0000],
        [0.0050, 0.0072, 0.0050,  ..., 0.0000, 0.0124, 0.0000],
        [0.0050, 0.0072, 0.0050,  ..., 0.0000, 0.0124, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(281772.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3825.6655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-267.8015, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(27.6769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(240.8352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(97.0037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0054],
        [ 0.0229],
        [ 0.0336],
        ...,
        [-0.0354],
        [-0.0352],
        [-0.0352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-11884.4082, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365910.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365912.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003, -0.0013,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0013,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0013,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        ...,
        [-0.0003, -0.0013,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0013,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0013,  0.0008,  ..., -0.0010,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78.2046, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.4923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7538, device='cuda:0')



h[100].sum tensor(-14.4379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.4005, device='cuda:0')



h[200].sum tensor(-41.5694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3436e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46018.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0050, 0.0074, 0.0054,  ..., 0.0000, 0.0142, 0.0000],
        [0.0050, 0.0073, 0.0054,  ..., 0.0000, 0.0142, 0.0000],
        [0.0050, 0.0074, 0.0054,  ..., 0.0000, 0.0142, 0.0000],
        ...,
        [0.0050, 0.0073, 0.0053,  ..., 0.0000, 0.0141, 0.0000],
        [0.0050, 0.0073, 0.0053,  ..., 0.0000, 0.0141, 0.0000],
        [0.0050, 0.0073, 0.0053,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(294678.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3914.0601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-272.5203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111.4813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(239.4105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(97.7246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0409],
        [-0.0381],
        [-0.0326],
        ...,
        [-0.0309],
        [-0.0345],
        [-0.0348]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-8280.0586, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365912.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365916.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0077,  0.0075,  0.0151,  ...,  0.0011, -0.0080, -0.0127],
        [-0.0003, -0.0014,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        [ 0.0132,  0.0138,  0.0251,  ...,  0.0026, -0.0137, -0.0216],
        ...,
        [-0.0003, -0.0014,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0014,  0.0008,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0014,  0.0008,  ..., -0.0010,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2.8596, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.4394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7388, device='cuda:0')



h[100].sum tensor(-12.9231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.6034, device='cuda:0')



h[200].sum tensor(-37.2562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2192e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0059, 0.0149,  ..., 0.0007, 0.0000, 0.0000],
        [0.0333, 0.0331, 0.0651,  ..., 0.0051, 0.0000, 0.0000],
        [0.0184, 0.0175, 0.0378,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43595.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0498, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0966, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0972, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0050, 0.0076, 0.0055,  ..., 0.0000, 0.0157, 0.0000],
        [0.0050, 0.0076, 0.0055,  ..., 0.0000, 0.0157, 0.0000],
        [0.0050, 0.0076, 0.0055,  ..., 0.0000, 0.0157, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(284678.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3560.4006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-253.4673, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(171.9328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(210.6172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(94.2684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0209],
        [ 0.0312],
        [ 0.0387],
        ...,
        [-0.0354],
        [-0.0353],
        [-0.0353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-3083.8552, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365916.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365920.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0068,  0.0064,  0.0135,  ...,  0.0008, -0.0071, -0.0112],
        [-0.0003, -0.0015,  0.0009,  ..., -0.0010,  0.0000,  0.0000],
        [ 0.0068,  0.0064,  0.0135,  ...,  0.0008, -0.0071, -0.0112],
        ...,
        [-0.0003, -0.0015,  0.0009,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0015,  0.0009,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0015,  0.0009,  ..., -0.0010,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-139.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.0445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2184, device='cuda:0')



h[100].sum tensor(-9.9300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5721, device='cuda:0')



h[200].sum tensor(-28.6647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3996e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0055, 0.0050, 0.0138,  ..., 0.0005, 0.0000, 0.0000],
        [0.0245, 0.0228, 0.0494,  ..., 0.0027, 0.0000, 0.0000],
        [0.0055, 0.0050, 0.0137,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36481.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0347, 0.0003, 0.0040,  ..., 0.0000, 0.0003, 0.0000],
        [0.0560, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0347, 0.0003, 0.0040,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0053, 0.0080, 0.0054,  ..., 0.0000, 0.0172, 0.0000],
        [0.0053, 0.0080, 0.0054,  ..., 0.0000, 0.0172, 0.0000],
        [0.0053, 0.0080, 0.0054,  ..., 0.0000, 0.0172, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255316.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2729.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-205.6341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(371.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.7292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(96.3935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0118],
        [-0.0055],
        [-0.0044],
        ...,
        [-0.0373],
        [-0.0372],
        [-0.0371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-6487.5508, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365920.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365920.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0150,  0.0156,  0.0283,  ...,  0.0030, -0.0154, -0.0243],
        [ 0.0139,  0.0143,  0.0262,  ...,  0.0027, -0.0143, -0.0225],
        [ 0.0215,  0.0229,  0.0399,  ...,  0.0047, -0.0220, -0.0347],
        ...,
        [-0.0003, -0.0015,  0.0009,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0015,  0.0009,  ..., -0.0010,  0.0000,  0.0000],
        [-0.0003, -0.0015,  0.0009,  ..., -0.0010,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-48.7361, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.2086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2896, device='cuda:0')



h[100].sum tensor(-11.8879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.3110, device='cuda:0')



h[200].sum tensor(-34.3165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1297e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0598, 0.0623, 0.1127,  ..., 0.0120, 0.0000, 0.0000],
        [0.0779, 0.0824, 0.1451,  ..., 0.0167, 0.0000, 0.0000],
        [0.0941, 0.1006, 0.1742,  ..., 0.0210, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39699.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2924, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3519, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4199, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0053, 0.0080, 0.0054,  ..., 0.0000, 0.0172, 0.0000],
        [0.0053, 0.0080, 0.0054,  ..., 0.0000, 0.0172, 0.0000],
        [0.0053, 0.0080, 0.0054,  ..., 0.0000, 0.0172, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(265064., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2970.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-228.1625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(321.4581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(173.4542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(96.1932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0093],
        [ 0.0146],
        [ 0.0194],
        ...,
        [-0.0373],
        [-0.0372],
        [-0.0371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-4904.9253, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365920.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365925.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0045,  0.0105,  ...,  0.0004, -0.0054, -0.0086],
        [ 0.0048,  0.0041,  0.0099,  ...,  0.0003, -0.0051, -0.0080],
        [-0.0002, -0.0015,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0015,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0015,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0015,  0.0009,  ..., -0.0011,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(263.6836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5597, device='cuda:0')



h[100].sum tensor(-18.7132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.4701, device='cuda:0')



h[200].sum tensor(-54.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7640e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0408, 0.0404, 0.0783,  ..., 0.0068, 0.0000, 0.0000],
        [0.0090, 0.0075, 0.0205,  ..., 0.0004, 0.0000, 0.0000],
        [0.0199, 0.0184, 0.0404,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58197.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1622, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0951, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.1039, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0056, 0.0085, 0.0052,  ..., 0.0000, 0.0187, 0.0000],
        [0.0056, 0.0085, 0.0052,  ..., 0.0000, 0.0187, 0.0000],
        [0.0056, 0.0085, 0.0052,  ..., 0.0000, 0.0187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367818.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5147.6289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-358.3851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(352.2483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(336.6620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(90.8009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0497],
        [ 0.0686],
        [ 0.0813],
        ...,
        [-0.0343],
        [-0.0341],
        [-0.0353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-712.3374, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365925.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38.1595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.3888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5005, device='cuda:0')



h[100].sum tensor(-14.0602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.1746, device='cuda:0')



h[200].sum tensor(-40.6931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3280e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45992.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0202, 0.0000],
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0202, 0.0000],
        [0.0058, 0.0094, 0.0046,  ..., 0.0000, 0.0203, 0.0000],
        ...,
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(307160.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3722.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-262.5509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(325.4125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(217.6013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(82.9064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0585],
        [-0.0594],
        [-0.0569],
        ...,
        [-0.0421],
        [-0.0415],
        [-0.0398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-623.5643, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0059,  0.0129,  ...,  0.0007, -0.0068, -0.0107],
        [ 0.0069,  0.0063,  0.0135,  ...,  0.0008, -0.0071, -0.0112],
        [ 0.0065,  0.0059,  0.0129,  ...,  0.0007, -0.0068, -0.0107],
        ...,
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(151.1466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.2998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3746, device='cuda:0')



h[100].sum tensor(-16.4708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.6296, device='cuda:0')



h[200].sum tensor(-47.6699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5673e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0304, 0.0283, 0.0593,  ..., 0.0039, 0.0000, 0.0000],
        [0.0294, 0.0273, 0.0576,  ..., 0.0036, 0.0000, 0.0000],
        [0.0122, 0.0108, 0.0259,  ..., 0.0011, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53110.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0900, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0905, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        [0.0704, 0.0000, 0.0054,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(345807.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4531.6616, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-315.9701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(366.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(283.9759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(86.0343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0907],
        [ 0.1063],
        [ 0.1166],
        ...,
        [-0.0416],
        [-0.0414],
        [-0.0414]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-2574.5972, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0041,  0.0100,  ...,  0.0003, -0.0051, -0.0081],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0016,  0.0009,  ..., -0.0011,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-87.9043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.0251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6401, device='cuda:0')



h[100].sum tensor(-11.3705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.7318, device='cuda:0')



h[200].sum tensor(-32.9087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0895e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0187, 0.0387,  ..., 0.0030, 0.0000, 0.0000],
        [0.0049, 0.0041, 0.0126,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39868.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1112, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0581, 0.0017, 0.0024,  ..., 0.0000, 0.0035, 0.0000],
        [0.0225, 0.0047, 0.0035,  ..., 0.0000, 0.0102, 0.0000],
        ...,
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        [0.0058, 0.0094, 0.0045,  ..., 0.0000, 0.0201, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276886.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2996.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-220.9136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(403.3522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(163.7796, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(87.0554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0583],
        [ 0.0451],
        [ 0.0246],
        ...,
        [-0.0422],
        [-0.0420],
        [-0.0419]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-6178.3657, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365935.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0002, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-171.9939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.5512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0586, device='cuda:0')



h[100].sum tensor(-9.7576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.4296, device='cuda:0')



h[200].sum tensor(-28.2775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3009e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0121, 0.0120, 0.0253,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36050.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0085, 0.0041,  ..., 0.0000, 0.0170, 0.0000],
        [0.0226, 0.0054, 0.0026,  ..., 0.0000, 0.0107, 0.0000],
        [0.0584, 0.0016, 0.0009,  ..., 0.0000, 0.0028, 0.0000],
        ...,
        [0.0062, 0.0104, 0.0036,  ..., 0.0000, 0.0214, 0.0000],
        [0.0062, 0.0104, 0.0036,  ..., 0.0000, 0.0214, 0.0000],
        [0.0062, 0.0104, 0.0036,  ..., 0.0000, 0.0214, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(260154.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2526.5042, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-191.3382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(484.2110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.3924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(84.1710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0301],
        [ 0.0209],
        [ 0.0141],
        ...,
        [-0.0455],
        [-0.0453],
        [-0.0453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-13777.2012, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365935.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365942.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0001, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0001, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0001, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000],
        [-0.0001, -0.0017,  0.0008,  ..., -0.0011,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36.7232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.0023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8875, device='cuda:0')



h[100].sum tensor(-14.2501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.5197, device='cuda:0')



h[200].sum tensor(-41.3510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3519e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48218.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0314, 0.0053, 0.0020,  ..., 0.0000, 0.0104, 0.0000],
        [0.0131, 0.0088, 0.0026,  ..., 0.0000, 0.0169, 0.0000],
        [0.0078, 0.0110, 0.0031,  ..., 0.0000, 0.0216, 0.0000],
        ...,
        [0.0065, 0.0114, 0.0028,  ..., 0.0000, 0.0227, 0.0000],
        [0.0065, 0.0114, 0.0028,  ..., 0.0000, 0.0227, 0.0000],
        [0.0065, 0.0114, 0.0028,  ..., 0.0000, 0.0227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326374.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3944.2827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-270.2270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(348.3605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(228.6200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(74.8871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0037],
        [-0.0087],
        [-0.0013],
        ...,
        [-0.0490],
        [-0.0488],
        [-0.0487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-8631.2754, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365942.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365948.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2781e-03,  3.1058e-03,  8.6342e-03,  ...,  1.6146e-05,
         -4.3897e-03, -6.9486e-03],
        [ 5.4721e-03,  4.4384e-03,  1.0782e-02,  ...,  3.3005e-04,
         -5.5876e-03, -8.8448e-03],
        [-9.7530e-05, -1.7778e-03,  7.6276e-04,  ..., -1.1342e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.0858e-03,  6.2394e-03,  1.3685e-02,  ...,  7.5428e-04,
         -7.2066e-03, -1.1407e-02],
        [-9.7530e-05, -1.7778e-03,  7.6276e-04,  ..., -1.1342e-03,
          0.0000e+00,  0.0000e+00],
        [-9.7530e-05, -1.7778e-03,  7.6276e-04,  ..., -1.1342e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-41.4513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6563, device='cuda:0')



h[100].sum tensor(-12.6800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.5299, device='cuda:0')



h[200].sum tensor(-36.8431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2141e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0411, 0.0392, 0.0777,  ..., 0.0064, 0.0000, 0.0000],
        [0.0164, 0.0150, 0.0330,  ..., 0.0021, 0.0000, 0.0000],
        [0.0055, 0.0044, 0.0131,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0517, 0.0527, 0.0966,  ..., 0.0103, 0.0000, 0.0000],
        [0.0458, 0.0461, 0.0859,  ..., 0.0087, 0.0000, 0.0000],
        [0.0329, 0.0333, 0.0625,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43211.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1442, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0971, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0518, 0.0037, 0.0018,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.3485, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2791, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1948, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(302928.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3312.8147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-231.6366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(360.4237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(179.4770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(73.7988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0967],
        [0.0793],
        [0.0492],
        ...,
        [0.0736],
        [0.0535],
        [0.0312]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17927.0664, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365948.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [1.0000],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365955.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.2186e-05, -1.8197e-03,  7.2889e-04,  ..., -1.1497e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.4148e-03,  7.6500e-03,  1.6002e-02,  ...,  1.0797e-03,
         -8.5041e-03, -1.3467e-02],
        [ 8.4148e-03,  7.6500e-03,  1.6002e-02,  ...,  1.0797e-03,
         -8.5041e-03, -1.3467e-02],
        ...,
        [-7.2186e-05, -1.8197e-03,  7.2889e-04,  ..., -1.1497e-03,
          0.0000e+00,  0.0000e+00],
        [-7.2186e-05, -1.8197e-03,  7.2889e-04,  ..., -1.1497e-03,
          0.0000e+00,  0.0000e+00],
        [-7.2186e-05, -1.8197e-03,  7.2889e-04,  ..., -1.1497e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-126.0455, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.4115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9268, device='cuda:0')



h[100].sum tensor(-10.9242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0957, device='cuda:0')



h[200].sum tensor(-31.7831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0455e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0153, 0.0136, 0.0307,  ..., 0.0018, 0.0000, 0.0000],
        [0.0221, 0.0194, 0.0430,  ..., 0.0024, 0.0000, 0.0000],
        [0.0322, 0.0307, 0.0612,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40396.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1152, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1639, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2257, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0138, 0.0009,  ..., 0.0000, 0.0252, 0.0000],
        [0.0071, 0.0138, 0.0009,  ..., 0.0000, 0.0252, 0.0000],
        [0.0071, 0.0138, 0.0009,  ..., 0.0000, 0.0252, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(294402.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3012.0554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-208.8566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(397.7530, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.6025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(70.6970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0619],
        [ 0.0766],
        [ 0.0925],
        ...,
        [-0.0579],
        [-0.0576],
        [-0.0576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-23798.0293, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9996],
        [0.9995],
        ...,
        [1.0000],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365955.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9996],
        [0.9995],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365962.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.0028e-05, -1.8523e-03,  6.9812e-04,  ..., -1.1636e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0028e-05, -1.8523e-03,  6.9812e-04,  ..., -1.1636e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2499e-03,  4.0603e-03,  1.0238e-02,  ...,  2.2755e-04,
         -5.3042e-03, -8.4038e-03],
        ...,
        [-5.0028e-05, -1.8523e-03,  6.9812e-04,  ..., -1.1636e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0028e-05, -1.8523e-03,  6.9812e-04,  ..., -1.1636e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0028e-05, -1.8523e-03,  6.9812e-04,  ..., -1.1636e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-104.2938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.8433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7751, device='cuda:0')



h[100].sum tensor(-11.3169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.8522, device='cuda:0')



h[200].sum tensor(-32.9690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0979e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0052, 0.0041, 0.0123,  ..., 0.0002, 0.0000, 0.0000],
        [0.0120, 0.0098, 0.0246,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42963.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0104, 0.0026,  ..., 0.0000, 0.0157, 0.0000],
        [0.0459, 0.0051, 0.0023,  ..., 0.0000, 0.0076, 0.0000],
        [0.0843, 0.0011, 0.0012,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0073, 0.0147, 0.0004,  ..., 0.0000, 0.0262, 0.0000],
        [0.0073, 0.0147, 0.0004,  ..., 0.0000, 0.0262, 0.0000],
        [0.0073, 0.0147, 0.0004,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(311697.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3376.5034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.7963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(271.4710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(167.4146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(61.3038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0454],
        [ 0.0737],
        [ 0.0930],
        ...,
        [-0.0621],
        [-0.0619],
        [-0.0618]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-20807.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9996],
        [0.9995],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365962.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365969.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4779e-02,  1.4636e-02,  2.7338e-02,  ...,  2.7076e-03,
         -1.4801e-02, -2.3461e-02],
        [ 1.5303e-02,  1.5221e-02,  2.8282e-02,  ...,  2.8451e-03,
         -1.5325e-02, -2.4291e-02],
        [ 1.2915e-02,  1.2558e-02,  2.3983e-02,  ...,  2.2188e-03,
         -1.2939e-02, -2.0509e-02],
        ...,
        [-2.8228e-05, -1.8807e-03,  6.7659e-04,  ..., -1.1765e-03,
          0.0000e+00,  0.0000e+00],
        [-2.8228e-05, -1.8807e-03,  6.7659e-04,  ..., -1.1765e-03,
          0.0000e+00,  0.0000e+00],
        [-2.8228e-05, -1.8807e-03,  6.7659e-04,  ..., -1.1765e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-174.1488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.0197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1432, device='cuda:0')



h[100].sum tensor(-9.7326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5050, device='cuda:0')



h[200].sum tensor(-28.3910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3531e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0557, 0.0547, 0.1032,  ..., 0.0099, 0.0000, 0.0000],
        [0.0599, 0.0594, 0.1108,  ..., 0.0110, 0.0000, 0.0000],
        [0.0739, 0.0750, 0.1360,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38082.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2946e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4962e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7188e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.5008e-03, 1.5411e-02, 9.3310e-05,  ..., 0.0000e+00, 2.7024e-02,
         0.0000e+00],
        [7.5002e-03, 1.5410e-02, 9.3151e-05,  ..., 0.0000e+00, 2.7022e-02,
         0.0000e+00],
        [7.5001e-03, 1.5409e-02, 9.3140e-05,  ..., 0.0000e+00, 2.7021e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287238.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2738.9177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-188.8031, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(306.0116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.3577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.5544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0755],
        [ 0.0781],
        [ 0.0818],
        ...,
        [-0.0658],
        [-0.0655],
        [-0.0654]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26991.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365969.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365977.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8850e-06, -1.9055e-03,  6.6092e-04,  ..., -1.1883e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6894e-03,  4.4479e-03,  1.0920e-02,  ...,  3.0506e-04,
         -5.6872e-03, -9.0188e-03],
        [-6.8850e-06, -1.9055e-03,  6.6092e-04,  ..., -1.1883e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.8850e-06, -1.9055e-03,  6.6092e-04,  ..., -1.1883e-03,
          0.0000e+00,  0.0000e+00],
        [-6.8850e-06, -1.9055e-03,  6.6092e-04,  ..., -1.1883e-03,
          0.0000e+00,  0.0000e+00],
        [-6.8850e-06, -1.9055e-03,  6.6092e-04,  ..., -1.1883e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-91.8932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.2096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4715, device='cuda:0')



h[100].sum tensor(-11.2114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5814, device='cuda:0')



h[200].sum tensor(-32.7482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0791e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.4713e-02, 1.9965e-02, 4.7206e-02,  ..., 1.7284e-03, 0.0000e+00,
         0.0000e+00],
        [4.6487e-03, 3.2853e-03, 1.1030e-02,  ..., 3.1053e-05, 0.0000e+00,
         0.0000e+00],
        [5.6874e-03, 4.4464e-03, 1.2898e-02,  ..., 3.0495e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.6461e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6458e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6458e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42111.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0894, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0564, 0.0027, 0.0019,  ..., 0.0000, 0.0036, 0.0000],
        [0.0357, 0.0062, 0.0021,  ..., 0.0000, 0.0091, 0.0000],
        ...,
        [0.0077, 0.0159, 0.0000,  ..., 0.0000, 0.0276, 0.0000],
        [0.0077, 0.0159, 0.0000,  ..., 0.0000, 0.0276, 0.0000],
        [0.0077, 0.0159, 0.0000,  ..., 0.0000, 0.0276, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(307672.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3113.3354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-216.5289, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(266.5222, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.4766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(50.1818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1129],
        [ 0.1000],
        [ 0.0791],
        ...,
        [-0.0692],
        [-0.0689],
        [-0.0688]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26450.1699, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9996],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365977.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365985.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3853e-05, -1.9261e-03,  6.5644e-04,  ..., -1.1991e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3853e-05, -1.9261e-03,  6.5644e-04,  ..., -1.1991e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3853e-05, -1.9261e-03,  6.5644e-04,  ..., -1.1991e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.3853e-05, -1.9261e-03,  6.5644e-04,  ..., -1.1991e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3853e-05, -1.9261e-03,  6.5644e-04,  ..., -1.1991e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3853e-05, -1.9261e-03,  6.5644e-04,  ..., -1.1991e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23.9572, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7172, device='cuda:0')



h[100].sum tensor(-13.2640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.4760, device='cuda:0')



h[200].sum tensor(-38.7949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2796e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.5463e-05, 0.0000e+00, 2.6283e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.5425e-05, 0.0000e+00, 2.6264e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0002e-02, 7.2434e-03, 2.0544e-02,  ..., 2.3627e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.5465e-05, 0.0000e+00, 2.6284e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.5461e-05, 0.0000e+00, 2.6282e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.2018e-03, 6.0427e-03, 1.5503e-02,  ..., 6.7246e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47085.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0116, 0.0146, 0.0003,  ..., 0.0000, 0.0243, 0.0000],
        [0.0323, 0.0077, 0.0007,  ..., 0.0000, 0.0130, 0.0000],
        [0.0970, 0.0025, 0.0005,  ..., 0.0000, 0.0036, 0.0000],
        ...,
        [0.0093, 0.0156, 0.0000,  ..., 0.0000, 0.0266, 0.0000],
        [0.0262, 0.0090, 0.0000,  ..., 0.0000, 0.0143, 0.0000],
        [0.0616, 0.0039, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331478.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3560.4155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-251.8142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(226.1454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(200.8780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.3975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0200],
        [ 0.0085],
        [ 0.0441],
        ...,
        [-0.0522],
        [-0.0230],
        [ 0.0127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-23063.6543, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365985.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365993.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.2983e-05, -1.9397e-03,  6.5318e-04,  ..., -1.2089e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2983e-05, -1.9397e-03,  6.5318e-04,  ..., -1.2089e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2983e-05, -1.9397e-03,  6.5318e-04,  ..., -1.2089e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 3.2983e-05, -1.9397e-03,  6.5318e-04,  ..., -1.2089e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2983e-05, -1.9397e-03,  6.5318e-04,  ..., -1.2089e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2983e-05, -1.9397e-03,  6.5318e-04,  ..., -1.2089e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-168.9851, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.0454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0997, device='cuda:0')



h[100].sum tensor(-9.0363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5744, device='cuda:0')



h[200].sum tensor(-26.4649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.7087e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37290.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0160, 0.0000,  ..., 0.0000, 0.0271, 0.0000],
        [0.0077, 0.0166, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0077, 0.0166, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        ...,
        [0.0077, 0.0165, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0077, 0.0165, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0077, 0.0165, 0.0000,  ..., 0.0000, 0.0282, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(285995.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2492.6353, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-186.9260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(299.9264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.6823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.7308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0497],
        [-0.0815],
        [-0.1022],
        ...,
        [-0.0745],
        [-0.0742],
        [-0.0741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30060.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9997],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365993.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(972.2399, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366002.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2448e-05, -1.9509e-03,  6.5097e-04,  ..., -1.2176e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2448e-05, -1.9509e-03,  6.5097e-04,  ..., -1.2176e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2448e-05, -1.9509e-03,  6.5097e-04,  ..., -1.2176e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.2448e-05, -1.9509e-03,  6.5097e-04,  ..., -1.2176e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2448e-05, -1.9509e-03,  6.5097e-04,  ..., -1.2176e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2448e-05, -1.9509e-03,  6.5097e-04,  ..., -1.2176e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-38.7041, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7331, device='cuda:0')



h[100].sum tensor(-11.2798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.8148, device='cuda:0')



h[200].sum tensor(-33.0792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0953e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43137.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0083, 0.0166, 0.0009,  ..., 0.0000, 0.0272, 0.0000],
        [0.0079, 0.0167, 0.0003,  ..., 0.0000, 0.0280, 0.0000],
        [0.0077, 0.0168, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        ...,
        [0.0077, 0.0167, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0077, 0.0167, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0077, 0.0167, 0.0000,  ..., 0.0000, 0.0283, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313658.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3037.8452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-229.2619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(220.3801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(166.8539, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.2210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0006],
        [-0.0277],
        [-0.0536],
        ...,
        [-0.0772],
        [-0.0768],
        [-0.0767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-24805.6211, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366002.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366011.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9576e-05, -1.9586e-03,  6.4712e-04,  ..., -1.2256e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9576e-05, -1.9586e-03,  6.4712e-04,  ..., -1.2256e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2940e-03,  4.9839e-03,  1.1868e-02,  ...,  4.0319e-04,
         -6.1846e-03, -9.8254e-03],
        ...,
        [ 6.9576e-05, -1.9586e-03,  6.4712e-04,  ..., -1.2256e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9576e-05, -1.9586e-03,  6.4712e-04,  ..., -1.2256e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9576e-05, -1.9586e-03,  6.4712e-04,  ..., -1.2256e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59.0638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7187, device='cuda:0')



h[100].sum tensor(-12.8405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.5855, device='cuda:0')



h[200].sum tensor(-37.7062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2179e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0113, 0.0084, 0.0224,  ..., 0.0004, 0.0000, 0.0000],
        [0.0278, 0.0248, 0.0522,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48193.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0348, 0.0072, 0.0025,  ..., 0.0000, 0.0110, 0.0000],
        [0.0828, 0.0020, 0.0022,  ..., 0.0000, 0.0025, 0.0000],
        [0.1547, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0076, 0.0169, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        [0.0076, 0.0169, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0076, 0.0169, 0.0000,  ..., 0.0000, 0.0283, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344143.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3588.0034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-268.4156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(229.6553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(213.4813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(26.1076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0603],
        [ 0.0928],
        [ 0.1107],
        ...,
        [-0.0777],
        [-0.0792],
        [-0.0795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-24947.3613, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366011.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366020.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.3031e-05, -1.9619e-03,  6.4275e-04,  ..., -1.2336e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3031e-05, -1.9619e-03,  6.4275e-04,  ..., -1.2336e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3031e-05, -1.9619e-03,  6.4275e-04,  ..., -1.2336e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.3031e-05, -1.9619e-03,  6.4275e-04,  ..., -1.2336e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3031e-05, -1.9619e-03,  6.4275e-04,  ..., -1.2336e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3031e-05, -1.9619e-03,  6.4275e-04,  ..., -1.2336e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-115.4691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0256, device='cuda:0')



h[100].sum tensor(-8.9876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5084, device='cuda:0')



h[200].sum tensor(-26.4274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.6629e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38295.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0075, 0.0170, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        [0.0075, 0.0170, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        [0.0075, 0.0170, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        ...,
        [0.0075, 0.0169, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0075, 0.0169, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0075, 0.0169, 0.0000,  ..., 0.0000, 0.0283, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(292440.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2370.6592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-197.5738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(215.1886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.6630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(18.3150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1185],
        [-0.1230],
        [-0.1270],
        ...,
        [-0.0829],
        [-0.0826],
        [-0.0825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26405.0020, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366020.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366029.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3083e-02,  2.3687e-02,  4.2097e-02,  ...,  4.7703e-03,
         -2.2789e-02, -3.6237e-02],
        [ 2.4628e-02,  2.5412e-02,  4.4884e-02,  ...,  5.1744e-03,
         -2.4321e-02, -3.8673e-02],
        [ 1.9690e-02,  1.9901e-02,  3.5976e-02,  ...,  3.8830e-03,
         -1.9425e-02, -3.0888e-02],
        ...,
        [ 9.4382e-05, -1.9616e-03,  6.3345e-04,  ..., -1.2407e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.4382e-05, -1.9616e-03,  6.3345e-04,  ..., -1.2407e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.4382e-05, -1.9616e-03,  6.3345e-04,  ..., -1.2407e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(263.9805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.9651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9195, device='cuda:0')



h[100].sum tensor(-16.0737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.2237, device='cuda:0')



h[200].sum tensor(-47.3268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5391e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0963, 0.0991, 0.1755,  ..., 0.0201, 0.0000, 0.0000],
        [0.0991, 0.1023, 0.1806,  ..., 0.0208, 0.0000, 0.0000],
        [0.1084, 0.1127, 0.1974,  ..., 0.0233, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58014.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4237, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4624, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4869, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0074, 0.0169, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0074, 0.0169, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0074, 0.0169, 0.0000,  ..., 0.0000, 0.0282, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(401582.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4642.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-344.0203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(177.5084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(301.1402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(14.7922, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0661],
        [ 0.0696],
        [ 0.0707],
        ...,
        [-0.0863],
        [-0.0860],
        [-0.0859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-25076.3965, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366029.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366038.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1609e-03,  4.8298e-03,  1.1557e-02,  ...,  3.3924e-04,
         -6.0113e-03, -9.5633e-03],
        [ 9.0241e-05, -1.9463e-03,  6.0565e-04,  ..., -1.2477e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0241e-05, -1.9463e-03,  6.0565e-04,  ..., -1.2477e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.0241e-05, -1.9463e-03,  6.0565e-04,  ..., -1.2477e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0241e-05, -1.9463e-03,  6.0565e-04,  ..., -1.2477e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0241e-05, -1.9463e-03,  6.0565e-04,  ..., -1.2477e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-81.1811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4504, device='cuda:0')



h[100].sum tensor(-9.1364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8871, device='cuda:0')



h[200].sum tensor(-26.9368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.9252e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.3231e-03, 3.5900e-03, 1.1376e-02,  ..., 4.8087e-05, 0.0000e+00,
         0.0000e+00],
        [6.4357e-03, 4.8329e-03, 1.3381e-02,  ..., 3.3946e-04, 0.0000e+00,
         0.0000e+00],
        [3.6080e-04, 0.0000e+00, 2.4215e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.6153e-04, 0.0000e+00, 2.4264e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.6151e-04, 0.0000e+00, 2.4262e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.6152e-04, 0.0000e+00, 2.4264e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39389.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0295, 0.0058, 0.0014,  ..., 0.0000, 0.0070, 0.0000],
        [0.0248, 0.0090, 0.0007,  ..., 0.0000, 0.0135, 0.0000],
        [0.0127, 0.0142, 0.0000,  ..., 0.0000, 0.0226, 0.0000],
        ...,
        [0.0073, 0.0171, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0073, 0.0171, 0.0000,  ..., 0.0000, 0.0283, 0.0000],
        [0.0073, 0.0171, 0.0000,  ..., 0.0000, 0.0283, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(299550.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2364.6455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-207.6857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93.4661, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.0675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.1279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0754],
        [-0.0797],
        [-0.0796],
        ...,
        [-0.0905],
        [-0.0901],
        [-0.0900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-28438.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366038.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366047.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1191e-02,  1.0468e-02,  2.0613e-02,  ...,  1.6477e-03,
         -1.0982e-02, -1.7480e-02],
        [ 1.2782e-02,  1.2244e-02,  2.3482e-02,  ...,  2.0634e-03,
         -1.2556e-02, -1.9984e-02],
        [ 3.5636e-02,  3.7764e-02,  6.4715e-02,  ...,  8.0362e-03,
         -3.5162e-02, -5.5963e-02],
        ...,
        [ 8.7664e-05, -1.9298e-03,  5.8088e-04,  ..., -1.2540e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.7664e-05, -1.9298e-03,  5.8088e-04,  ..., -1.2540e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.7664e-05, -1.9298e-03,  5.8088e-04,  ..., -1.2540e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(174.7739, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5477, device='cuda:0')



h[100].sum tensor(-13.8546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.2167, device='cuda:0')



h[200].sum tensor(-40.9023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3309e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0871, 0.0892, 0.1589,  ..., 0.0177, 0.0000, 0.0000],
        [0.0986, 0.1020, 0.1797,  ..., 0.0207, 0.0000, 0.0000],
        [0.0601, 0.0590, 0.1101,  ..., 0.0106, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0023,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53181.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3830, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4111, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3389, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0171, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0072, 0.0171, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0072, 0.0171, 0.0000,  ..., 0.0000, 0.0282, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(374689., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3892.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-310.3636, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38.5322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(252.3241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5.6856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0313],
        [ 0.0308],
        [ 0.0292],
        ...,
        [-0.0944],
        [-0.0940],
        [-0.0939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-28595.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366047.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366057.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.3923e-05, -1.9112e-03,  5.5500e-04,  ..., -1.2597e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3923e-05, -1.9112e-03,  5.5500e-04,  ..., -1.2597e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3923e-05, -1.9112e-03,  5.5500e-04,  ..., -1.2597e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.3923e-05, -1.9112e-03,  5.5500e-04,  ..., -1.2597e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3923e-05, -1.9112e-03,  5.5500e-04,  ..., -1.2597e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3923e-05, -1.9112e-03,  5.5500e-04,  ..., -1.2597e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(151.0769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9744, device='cuda:0')



h[100].sum tensor(-13.1464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.7054, device='cuda:0')



h[200].sum tensor(-38.8634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2955e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0154, 0.0130, 0.0294,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50353.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0152, 0.0130, 0.0000,  ..., 0.0000, 0.0201, 0.0000],
        [0.0224, 0.0089, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0557, 0.0025, 0.0015,  ..., 0.0000, 0.0023, 0.0000],
        ...,
        [0.0071, 0.0172, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0071, 0.0172, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0071, 0.0172, 0.0000,  ..., 0.0000, 0.0282, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(356662.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3433.7876, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-288.3788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1.5708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.7076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-0.0002, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0100],
        [ 0.0183],
        [ 0.0566],
        ...,
        [-0.0984],
        [-0.0980],
        [-0.0979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-27344.2676, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366057.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366067.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.3189e-03,  5.0946e-03,  1.1793e-02,  ...,  3.6606e-04,
         -6.1628e-03, -9.8177e-03],
        [ 4.0923e-03,  2.6059e-03,  7.7750e-03,  ..., -2.1561e-04,
         -3.9652e-03, -6.3168e-03],
        [ 1.0337e-02,  9.5849e-03,  1.9043e-02,  ...,  1.4156e-03,
         -1.0128e-02, -1.6135e-02],
        ...,
        [ 7.4653e-05, -1.8843e-03,  5.2501e-04,  ..., -1.2651e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.4653e-05, -1.8843e-03,  5.2501e-04,  ..., -1.2651e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.4653e-05, -1.8843e-03,  5.2501e-04,  ..., -1.2651e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-49.3629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3786, device='cuda:0')



h[100].sum tensor(-9.0587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8231, device='cuda:0')



h[200].sum tensor(-26.8154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8809e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0101, 0.0245,  ..., 0.0009, 0.0000, 0.0000],
        [0.0344, 0.0305, 0.0636,  ..., 0.0038, 0.0000, 0.0000],
        [0.0208, 0.0173, 0.0392,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40551.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0720, 0.0012, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.1051, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0947, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0071, 0.0172, 0.0000,  ..., 0.0000, 0.0281, 0.0000],
        [0.0071, 0.0172, 0.0000,  ..., 0.0000, 0.0281, 0.0000],
        [0.0071, 0.0172, 0.0000,  ..., 0.0000, 0.0281, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(309433.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2351.1924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-220.9266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.1319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-0.3123, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0774],
        [ 0.0864],
        [ 0.0830],
        ...,
        [-0.0992],
        [-0.0962],
        [-0.0936]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38613.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366067.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366077.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6501e-05, -1.8597e-03,  5.0270e-04,  ..., -1.2698e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.6501e-05, -1.8597e-03,  5.0270e-04,  ..., -1.2698e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.6501e-05, -1.8597e-03,  5.0270e-04,  ..., -1.2698e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.6501e-05, -1.8597e-03,  5.0270e-04,  ..., -1.2698e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.6501e-05, -1.8597e-03,  5.0270e-04,  ..., -1.2698e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.6501e-05, -1.8597e-03,  5.0270e-04,  ..., -1.2698e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(430.5421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.2532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0884, device='cuda:0')



h[100].sum tensor(-17.8470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.0498, device='cuda:0')



h[200].sum tensor(-52.9015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7349e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0036, 0.0018, 0.0080,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60146.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.0180e-03, 1.7164e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8021e-02,
         0.0000e+00],
        [7.0134e-03, 1.7150e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7997e-02,
         0.0000e+00],
        [9.0392e-03, 1.6068e-02, 0.0000e+00,  ..., 0.0000e+00, 2.5936e-02,
         0.0000e+00],
        ...,
        [9.7320e-03, 1.5448e-02, 0.0000e+00,  ..., 0.0000e+00, 2.4192e-02,
         0.0000e+00],
        [1.3692e-02, 1.2997e-02, 3.2174e-05,  ..., 0.0000e+00, 1.8726e-02,
         0.0000e+00],
        [2.0165e-02, 8.9943e-03, 1.4035e-03,  ..., 0.0000e+00, 1.0667e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398773.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4145.0161, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-361.7334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(310.0593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1.4417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1377],
        [-0.1167],
        [-0.0800],
        ...,
        [-0.0881],
        [-0.0675],
        [-0.0462]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-35273.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366077.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366087.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1178e-05, -1.8378e-03,  4.8376e-04,  ..., -1.2739e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1178e-05, -1.8378e-03,  4.8376e-04,  ..., -1.2739e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1178e-05, -1.8378e-03,  4.8376e-04,  ..., -1.2739e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.1178e-05, -1.8378e-03,  4.8376e-04,  ..., -1.2739e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1178e-05, -1.8378e-03,  4.8376e-04,  ..., -1.2739e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1178e-05, -1.8378e-03,  4.8376e-04,  ..., -1.2739e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(134.2740, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2690, device='cuda:0')



h[100].sum tensor(-11.9046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.1845, device='cuda:0')



h[200].sum tensor(-35.3348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1901e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0124, 0.0118, 0.0239,  ..., 0.0019, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47118.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.7520e-02, 7.3713e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.9190e-02, 4.9183e-03, 0.0000e+00,  ..., 0.0000e+00, 7.1560e-03,
         0.0000e+00],
        [2.4537e-02, 7.1886e-03, 0.0000e+00,  ..., 0.0000e+00, 1.0071e-02,
         0.0000e+00],
        ...,
        [7.0517e-03, 1.6937e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7705e-02,
         0.0000e+00],
        [7.0514e-03, 1.6936e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7704e-02,
         0.0000e+00],
        [7.0520e-03, 1.6938e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7706e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339569.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2860.5505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-267.0895, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(187.7524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1.5506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0477],
        [ 0.0396],
        [ 0.0366],
        ...,
        [-0.1076],
        [-0.1071],
        [-0.1070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-37873.8711, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366087.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(581.1838, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366097.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2708e-03,  2.9034e-03,  8.0830e-03,  ..., -1.7612e-04,
         -4.1488e-03, -6.6184e-03],
        [ 4.2708e-03,  2.9034e-03,  8.0830e-03,  ..., -1.7612e-04,
         -4.1488e-03, -6.6184e-03],
        [ 5.2927e-05, -1.8161e-03,  4.6936e-04,  ..., -1.2775e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.2927e-05, -1.8161e-03,  4.6936e-04,  ..., -1.2775e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2927e-05, -1.8161e-03,  4.6936e-04,  ..., -1.2775e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2927e-05, -1.8161e-03,  4.6936e-04,  ..., -1.2775e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90.7925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1968, device='cuda:0')



h[100].sum tensor(-10.7341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.3365, device='cuda:0')



h[200].sum tensor(-31.9035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0622e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0049, 0.0157,  ..., 0.0000, 0.0000, 0.0000],
        [0.0079, 0.0049, 0.0157,  ..., 0.0000, 0.0000, 0.0000],
        [0.0079, 0.0049, 0.0157,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44451.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0408, 0.0005, 0.0045,  ..., 0.0000, 0.0000, 0.0000],
        [0.0324, 0.0025, 0.0033,  ..., 0.0000, 0.0022, 0.0000],
        [0.0259, 0.0054, 0.0018,  ..., 0.0000, 0.0074, 0.0000],
        ...,
        [0.0072, 0.0167, 0.0000,  ..., 0.0000, 0.0275, 0.0000],
        [0.0072, 0.0167, 0.0000,  ..., 0.0000, 0.0275, 0.0000],
        [0.0072, 0.0167, 0.0000,  ..., 0.0000, 0.0275, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324460.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2566.4331, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-247.6464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(161.2557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-0.5020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0445],
        [ 0.0055],
        [-0.0442],
        ...,
        [-0.1100],
        [-0.1096],
        [-0.1094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36948.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366097.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366108.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.0543e-05, -1.7983e-03,  4.6514e-04,  ..., -1.2809e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9137e-02,  1.9564e-02,  3.4922e-02,  ...,  3.7023e-03,
         -1.8752e-02, -2.9928e-02],
        [ 5.0543e-05, -1.7983e-03,  4.6514e-04,  ..., -1.2809e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.0543e-05, -1.7983e-03,  4.6514e-04,  ..., -1.2809e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.0543e-05, -1.7983e-03,  4.6514e-04,  ..., -1.2809e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.0543e-05, -1.7983e-03,  4.6514e-04,  ..., -1.2809e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(336.6672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.8971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4720, device='cuda:0')



h[100].sum tensor(-14.8951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.9328, device='cuda:0')



h[200].sum tensor(-44.3307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4497e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0302, 0.0300, 0.0560,  ..., 0.0053, 0.0000, 0.0000],
        [0.0375, 0.0382, 0.0693,  ..., 0.0072, 0.0000, 0.0000],
        [0.0804, 0.0826, 0.1466,  ..., 0.0158, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55638.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1221, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1611, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1976, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0072, 0.0164, 0.0000,  ..., 0.0000, 0.0273, 0.0000],
        [0.0072, 0.0164, 0.0000,  ..., 0.0000, 0.0273, 0.0000],
        [0.0072, 0.0164, 0.0000,  ..., 0.0000, 0.0273, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383153.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3705.2544, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-329.2405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(261.6953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1.0808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0194],
        [-0.0167],
        [-0.0230],
        ...,
        [-0.1124],
        [-0.1119],
        [-0.1118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36241.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [0.9997],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366108.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366119.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.1201e-02,  3.3081e-02,  5.6703e-02,  ...,  6.8470e-03,
         -3.0563e-02, -4.8801e-02],
        [ 2.9502e-02,  3.1180e-02,  5.3636e-02,  ...,  6.4035e-03,
         -2.8896e-02, -4.6139e-02],
        [ 2.9847e-02,  3.1566e-02,  5.4259e-02,  ...,  6.4936e-03,
         -2.9234e-02, -4.6680e-02],
        ...,
        [ 5.4760e-05, -1.7840e-03,  4.6765e-04,  ..., -1.2837e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.4760e-05, -1.7840e-03,  4.6765e-04,  ..., -1.2837e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.4760e-05, -1.7840e-03,  4.6765e-04,  ..., -1.2837e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(235.5790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1265, device='cuda:0')



h[100].sum tensor(-12.5559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.9492, device='cuda:0')



h[200].sum tensor(-37.4195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2431e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1799e-01, 1.2469e-01, 2.1451e-01,  ..., 2.5603e-02, 0.0000e+00,
         0.0000e+00],
        [1.2838e-01, 1.3632e-01, 2.3327e-01,  ..., 2.8318e-02, 0.0000e+00,
         0.0000e+00],
        [1.2575e-01, 1.3339e-01, 2.2852e-01,  ..., 2.7637e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.1957e-04, 0.0000e+00, 1.8751e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.1955e-04, 0.0000e+00, 1.8750e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.1957e-04, 0.0000e+00, 1.8751e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49659.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5238, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.5412, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.5107, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0073, 0.0161, 0.0000,  ..., 0.0000, 0.0271, 0.0000],
        [0.0073, 0.0161, 0.0000,  ..., 0.0000, 0.0271, 0.0000],
        [0.0073, 0.0161, 0.0000,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(352458.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3027.4983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.2374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(204.1339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(0.7764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0483],
        [-0.0489],
        [-0.0492],
        ...,
        [-0.1144],
        [-0.1140],
        [-0.1138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33328.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366119.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366130.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5529e-05, -1.7692e-03,  4.7534e-04,  ..., -1.2854e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5529e-05, -1.7692e-03,  4.7534e-04,  ..., -1.2854e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5529e-05, -1.7692e-03,  4.7534e-04,  ..., -1.2854e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.5529e-05, -1.7692e-03,  4.7534e-04,  ..., -1.2854e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5529e-05, -1.7692e-03,  4.7534e-04,  ..., -1.2854e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5529e-05, -1.7692e-03,  4.7534e-04,  ..., -1.2854e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(355.4186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1784, device='cuda:0')



h[100].sum tensor(-14.2564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.6710, device='cuda:0')



h[200].sum tensor(-42.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4316e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53921.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0215, 0.0083, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0096, 0.0143, 0.0000,  ..., 0.0000, 0.0228, 0.0000],
        [0.0082, 0.0153, 0.0000,  ..., 0.0000, 0.0249, 0.0000],
        ...,
        [0.0075, 0.0157, 0.0000,  ..., 0.0000, 0.0267, 0.0000],
        [0.0075, 0.0157, 0.0000,  ..., 0.0000, 0.0267, 0.0000],
        [0.0075, 0.0157, 0.0000,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375082.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3540.8872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-321.0335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(243.1787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(5.7609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0135],
        [-0.0238],
        [-0.0630],
        ...,
        [-0.1159],
        [-0.1154],
        [-0.1153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38880.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366130.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366142.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6054e-05, -1.7561e-03,  4.8418e-04,  ..., -1.2868e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6054e-05, -1.7561e-03,  4.8418e-04,  ..., -1.2868e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.4094e-03,  7.5982e-03,  1.5571e-02,  ...,  8.9365e-04,
         -8.1774e-03, -1.3069e-02],
        ...,
        [ 5.6054e-05, -1.7561e-03,  4.8418e-04,  ..., -1.2868e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6054e-05, -1.7561e-03,  4.8418e-04,  ..., -1.2868e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6054e-05, -1.7561e-03,  4.8418e-04,  ..., -1.2868e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(132.5529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6773, device='cuda:0')



h[100].sum tensor(-9.7134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9813, device='cuda:0')



h[200].sum tensor(-29.0268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6831e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0148, 0.0146, 0.0283,  ..., 0.0025, 0.0000, 0.0000],
        [0.0086, 0.0076, 0.0170,  ..., 0.0009, 0.0000, 0.0000],
        [0.0247, 0.0240, 0.0462,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43429.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0968, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0906, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1257, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0077, 0.0154, 0.0000,  ..., 0.0000, 0.0264, 0.0000],
        [0.0077, 0.0154, 0.0000,  ..., 0.0000, 0.0264, 0.0000],
        [0.0077, 0.0154, 0.0000,  ..., 0.0000, 0.0264, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326418.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2561.8757, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.5099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.4017, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(6.2312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0452],
        [ 0.0430],
        [ 0.0354],
        ...,
        [-0.1176],
        [-0.1171],
        [-0.1169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38129.1328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366142.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366153.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0393e-02,  9.8290e-03,  1.9160e-02,  ...,  1.4094e-03,
         -1.0104e-02, -1.6156e-02],
        [ 5.0663e-03,  3.8638e-03,  9.5386e-03,  ...,  1.9179e-05,
         -4.8965e-03, -7.8293e-03],
        [ 5.8388e-05, -1.7448e-03,  4.9232e-04,  ..., -1.2880e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.8388e-05, -1.7448e-03,  4.9232e-04,  ..., -1.2880e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.8388e-05, -1.7448e-03,  4.9232e-04,  ..., -1.2880e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.8388e-05, -1.7448e-03,  4.9232e-04,  ..., -1.2880e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(468.6903, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6618, device='cuda:0')



h[100].sum tensor(-15.3962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.9939, device='cuda:0')



h[200].sum tensor(-46.0715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5232e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.4501e-02, 4.2590e-02, 8.1935e-02,  ..., 6.3965e-03, 0.0000e+00,
         0.0000e+00],
        [2.6753e-02, 2.6206e-02, 4.9874e-02,  ..., 4.3429e-03, 0.0000e+00,
         0.0000e+00],
        [5.2403e-03, 3.8630e-03, 1.1013e-02,  ..., 1.9175e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.3420e-04, 0.0000e+00, 1.9748e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.3417e-04, 0.0000e+00, 1.9745e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.3418e-04, 0.0000e+00, 1.9745e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56789.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1359, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0856, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0080, 0.0151, 0.0000,  ..., 0.0000, 0.0263, 0.0000],
        [0.0080, 0.0151, 0.0000,  ..., 0.0000, 0.0263, 0.0000],
        [0.0080, 0.0151, 0.0000,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385006.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3792.5999, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-336.4199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.6932, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(7.1418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0338],
        [-0.0227],
        [-0.0002],
        ...,
        [-0.1194],
        [-0.1189],
        [-0.1188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-31394.8535, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366153.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366165.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0794e-05, -1.7337e-03,  4.9367e-04,  ..., -1.2886e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0794e-05, -1.7337e-03,  4.9367e-04,  ..., -1.2886e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0794e-05, -1.7337e-03,  4.9367e-04,  ..., -1.2886e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.0794e-05, -1.7337e-03,  4.9367e-04,  ..., -1.2886e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0794e-05, -1.7337e-03,  4.9367e-04,  ..., -1.2886e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0794e-05, -1.7337e-03,  4.9367e-04,  ..., -1.2886e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(180.8862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.8873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9103, device='cuda:0')



h[100].sum tensor(-9.8509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1892, device='cuda:0')



h[200].sum tensor(-29.5179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.8270e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43063.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.1627e-02, 4.2929e-03, 0.0000e+00,  ..., 0.0000e+00, 7.6600e-03,
         0.0000e+00],
        [2.4803e-02, 8.0824e-03, 0.0000e+00,  ..., 0.0000e+00, 1.4204e-02,
         0.0000e+00],
        [1.1101e-02, 1.3148e-02, 9.5192e-05,  ..., 0.0000e+00, 2.2198e-02,
         0.0000e+00],
        ...,
        [8.3481e-03, 1.4973e-02, 0.0000e+00,  ..., 0.0000e+00, 2.6381e-02,
         0.0000e+00],
        [8.3470e-03, 1.4970e-02, 0.0000e+00,  ..., 0.0000e+00, 2.6376e-02,
         0.0000e+00],
        [8.3469e-03, 1.4970e-02, 0.0000e+00,  ..., 0.0000e+00, 2.6375e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(319100.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2447.1746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-234.1959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.2879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(10.0877, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0303],
        [ 0.0390],
        [ 0.0473],
        ...,
        [-0.1213],
        [-0.1209],
        [-0.1208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36813.1797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366165.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366177.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5049e-05, -1.7261e-03,  4.9457e-04,  ..., -1.2890e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5049e-05, -1.7261e-03,  4.9457e-04,  ..., -1.2890e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.9189e-03,  4.8306e-03,  1.1072e-02,  ...,  2.3894e-04,
         -5.7094e-03, -9.1376e-03],
        ...,
        [ 6.5049e-05, -1.7261e-03,  4.9457e-04,  ..., -1.2890e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5049e-05, -1.7261e-03,  4.9457e-04,  ..., -1.2890e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5049e-05, -1.7261e-03,  4.9457e-04,  ..., -1.2890e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(292.1390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.0156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9073, device='cuda:0')



h[100].sum tensor(-11.5744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.8619, device='cuda:0')



h[200].sum tensor(-34.7296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1678e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0110, 0.0225,  ..., 0.0017, 0.0000, 0.0000],
        [0.0061, 0.0048, 0.0126,  ..., 0.0002, 0.0000, 0.0000],
        [0.0132, 0.0110, 0.0253,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47102.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0546, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0757, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0088, 0.0149, 0.0000,  ..., 0.0000, 0.0267, 0.0000],
        [0.0088, 0.0149, 0.0000,  ..., 0.0000, 0.0267, 0.0000],
        [0.0088, 0.0149, 0.0000,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(338746.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2944.0994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-262.8973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(169.8276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.4610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0752],
        [ 0.0813],
        [ 0.0870],
        ...,
        [-0.1242],
        [-0.1237],
        [-0.1235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39029.3203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366177.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366190.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9087e-05, -1.7202e-03,  4.9329e-04,  ..., -1.2892e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9087e-05, -1.7202e-03,  4.9329e-04,  ..., -1.2892e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9087e-05, -1.7202e-03,  4.9329e-04,  ..., -1.2892e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.9087e-05, -1.7202e-03,  4.9329e-04,  ..., -1.2892e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9087e-05, -1.7202e-03,  4.9329e-04,  ..., -1.2892e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9087e-05, -1.7202e-03,  4.9329e-04,  ..., -1.2892e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(235.0948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8309, device='cuda:0')



h[100].sum tensor(-10.3278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0102, device='cuda:0')



h[200].sum tensor(-31.0314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0396e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0091, 0.0082, 0.0179,  ..., 0.0010, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44273.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3135e-02, 1.2264e-02, 2.1542e-05,  ..., 0.0000e+00, 2.1758e-02,
         0.0000e+00],
        [2.0445e-02, 8.6163e-03, 0.0000e+00,  ..., 0.0000e+00, 1.5956e-02,
         0.0000e+00],
        [4.5615e-02, 3.5365e-03, 0.0000e+00,  ..., 0.0000e+00, 6.9675e-03,
         0.0000e+00],
        ...,
        [9.2540e-03, 1.4960e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7269e-02,
         0.0000e+00],
        [9.2525e-03, 1.4957e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7263e-02,
         0.0000e+00],
        [9.2522e-03, 1.4956e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7261e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(328865.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2792.4536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-238.0405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.9539, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(17.1934, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0224],
        [-0.0430],
        [-0.0502],
        ...,
        [-0.1276],
        [-0.1271],
        [-0.1269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38418.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366190.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366202.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2467e-05, -1.7138e-03,  4.8798e-04,  ..., -1.2895e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1848e-02,  1.1477e-02,  2.1772e-02,  ...,  1.7841e-03,
         -1.1457e-02, -1.8353e-02],
        [ 7.2467e-05, -1.7138e-03,  4.8798e-04,  ..., -1.2895e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.2467e-05, -1.7138e-03,  4.8798e-04,  ..., -1.2895e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.2467e-05, -1.7138e-03,  4.8798e-04,  ..., -1.2895e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.2467e-05, -1.7138e-03,  4.8798e-04,  ..., -1.2895e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(229.6708, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4750, device='cuda:0')



h[100].sum tensor(-10.0475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6927, device='cuda:0')



h[200].sum tensor(-30.2306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0176e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0154, 0.0323,  ..., 0.0018, 0.0000, 0.0000],
        [0.0099, 0.0091, 0.0193,  ..., 0.0012, 0.0000, 0.0000],
        [0.0554, 0.0549, 0.1016,  ..., 0.0092, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45266.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.6837e-02, 0.0000e+00, 2.9026e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.0089e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.3932e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [9.7223e-03, 1.5107e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7850e-02,
         0.0000e+00],
        [9.7206e-03, 1.5104e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7844e-02,
         0.0000e+00],
        [9.7202e-03, 1.5103e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7842e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342830.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3154.3013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-241.3868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.8110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(21.0762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0749],
        [ 0.0589],
        [ 0.0373],
        ...,
        [-0.1317],
        [-0.1312],
        [-0.1310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38889.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366202.7812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(579.6059, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366215.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7534e-05, -1.7097e-03,  4.8590e-04,  ..., -1.2899e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7534e-05, -1.7097e-03,  4.8590e-04,  ..., -1.2899e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.3104e-03,  6.3918e-03,  1.3561e-02,  ...,  5.9789e-04,
         -7.0279e-03, -1.1264e-02],
        ...,
        [ 7.7534e-05, -1.7097e-03,  4.8590e-04,  ..., -1.2899e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7534e-05, -1.7097e-03,  4.8590e-04,  ..., -1.2899e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7534e-05, -1.7097e-03,  4.8590e-04,  ..., -1.2899e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1053.6100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(44.5888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.2243, device='cuda:0')



h[100].sum tensor(-24.3665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.9810, device='cuda:0')



h[200].sum tensor(-73.4138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.4227e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0115, 0.0091, 0.0221,  ..., 0.0006, 0.0000, 0.0000],
        [0.0263, 0.0240, 0.0489,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81783.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0329, 0.0055, 0.0003,  ..., 0.0000, 0.0102, 0.0000],
        [0.0715, 0.0011, 0.0002,  ..., 0.0000, 0.0020, 0.0000],
        [0.1259, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0102, 0.0154, 0.0000,  ..., 0.0000, 0.0285, 0.0000],
        [0.0102, 0.0154, 0.0000,  ..., 0.0000, 0.0285, 0.0000],
        [0.0102, 0.0154, 0.0000,  ..., 0.0000, 0.0285, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529198., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7050.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-500.8058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(475.3842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(27.5533, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0510],
        [ 0.0590],
        [ 0.0660],
        ...,
        [-0.1359],
        [-0.1353],
        [-0.1352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33428.1797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366215.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366229.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.0300e-05, -1.7003e-03,  4.7895e-04,  ..., -1.2907e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.0300e-05, -1.7003e-03,  4.7895e-04,  ..., -1.2907e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.0300e-05, -1.7003e-03,  4.7895e-04,  ..., -1.2907e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.0300e-05, -1.7003e-03,  4.7895e-04,  ..., -1.2907e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.0300e-05, -1.7003e-03,  4.7895e-04,  ..., -1.2907e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.0300e-05, -1.7003e-03,  4.7895e-04,  ..., -1.2907e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(397.7890, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5399, device='cuda:0')



h[100].sum tensor(-12.6588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.3179, device='cuda:0')



h[200].sum tensor(-38.1919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2686e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54431.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0198, 0.0085, 0.0005,  ..., 0.0000, 0.0159, 0.0000],
        [0.0173, 0.0104, 0.0003,  ..., 0.0000, 0.0194, 0.0000],
        [0.0127, 0.0140, 0.0000,  ..., 0.0000, 0.0260, 0.0000],
        ...,
        [0.0106, 0.0156, 0.0000,  ..., 0.0000, 0.0290, 0.0000],
        [0.0106, 0.0156, 0.0000,  ..., 0.0000, 0.0289, 0.0000],
        [0.0106, 0.0156, 0.0000,  ..., 0.0000, 0.0289, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403981.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4531.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.9480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.6711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.8872, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0039],
        [-0.0144],
        [-0.0362],
        ...,
        [-0.1398],
        [-0.1392],
        [-0.1390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50301.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366229.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366242., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4658e-02,  1.4638e-02,  2.6813e-02,  ...,  2.5129e-03,
         -1.4125e-02, -2.2659e-02],
        [ 7.3956e-03,  6.5024e-03,  1.3683e-02,  ...,  6.1752e-04,
         -7.0861e-03, -1.1367e-02],
        [ 7.3858e-03,  6.4915e-03,  1.3665e-02,  ...,  6.1497e-04,
         -7.0766e-03, -1.1352e-02],
        ...,
        [ 8.4559e-05, -1.6878e-03,  4.6420e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.4559e-05, -1.6878e-03,  4.6420e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.4559e-05, -1.6878e-03,  4.6420e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(347.6358, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.0335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1455, device='cuda:0')



h[100].sum tensor(-11.6604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.0744, device='cuda:0')



h[200].sum tensor(-35.2280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1825e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0388, 0.0364, 0.0715,  ..., 0.0049, 0.0000, 0.0000],
        [0.0401, 0.0378, 0.0738,  ..., 0.0052, 0.0000, 0.0000],
        [0.0136, 0.0115, 0.0258,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49935.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1266, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1145, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0722, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0110, 0.0160, 0.0000,  ..., 0.0000, 0.0294, 0.0000],
        [0.0110, 0.0160, 0.0000,  ..., 0.0000, 0.0294, 0.0000],
        [0.0110, 0.0160, 0.0000,  ..., 0.0000, 0.0294, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370720.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3900.1428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-261.0889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(174.9719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(31.0767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0944],
        [ 0.0894],
        [ 0.0774],
        ...,
        [-0.1437],
        [-0.1431],
        [-0.1430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44365.1836, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9995],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366242., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366254.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5054e-02,  1.5088e-02,  2.7511e-02,  ...,  2.6148e-03,
         -1.4485e-02, -2.3247e-02],
        [ 1.0460e-02,  9.9414e-03,  1.9205e-02,  ...,  1.4159e-03,
         -1.0038e-02, -1.6110e-02],
        [ 1.4748e-02,  1.4745e-02,  2.6957e-02,  ...,  2.5349e-03,
         -1.4188e-02, -2.2771e-02],
        ...,
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(335.8038, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6605, device='cuda:0')



h[100].sum tensor(-11.2599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6418, device='cuda:0')



h[200].sum tensor(-34.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1526e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0375, 0.0349, 0.0690,  ..., 0.0047, 0.0000, 0.0000],
        [0.0420, 0.0399, 0.0771,  ..., 0.0060, 0.0000, 0.0000],
        [0.0263, 0.0240, 0.0487,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48194.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1690, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1656, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1379, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(360508.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3715.0757, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.1993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.6494, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(33.4632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0799],
        [ 0.0870],
        [ 0.0921],
        ...,
        [-0.1472],
        [-0.1466],
        [-0.1464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-47570.3008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366254.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366254.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.0420e-05, -1.6758e-03,  4.5439e-04,  ..., -1.2906e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(252.6970, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0160, device='cuda:0')



h[100].sum tensor(-9.8272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2834, device='cuda:0')



h[200].sum tensor(-29.7306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.8922e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44666.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0298, 0.0000],
        ...,
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0114, 0.0163, 0.0000,  ..., 0.0000, 0.0297, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(345749.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3411.7336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-220.4051, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.4594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.2501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1806],
        [-0.1752],
        [-0.1602],
        ...,
        [-0.1472],
        [-0.1466],
        [-0.1464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53223.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366254.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366268.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9770e-05, -1.6658e-03,  4.5433e-04,  ..., -1.2905e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9770e-05, -1.6658e-03,  4.5433e-04,  ..., -1.2905e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9770e-05, -1.6658e-03,  4.5433e-04,  ..., -1.2905e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.9770e-05, -1.6658e-03,  4.5433e-04,  ..., -1.2905e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9770e-05, -1.6658e-03,  4.5433e-04,  ..., -1.2905e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9770e-05, -1.6658e-03,  4.5433e-04,  ..., -1.2905e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(391.9158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6846, device='cuda:0')



h[100].sum tensor(-11.8944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.5551, device='cuda:0')



h[200].sum tensor(-36.0341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2158e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48124.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0117, 0.0165, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0117, 0.0165, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0211, 0.0117, 0.0000,  ..., 0.0000, 0.0215, 0.0000],
        ...,
        [0.0118, 0.0165, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0118, 0.0165, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0118, 0.0165, 0.0000,  ..., 0.0000, 0.0297, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355408.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3623.4155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-242.6089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.3080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.9140, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0579],
        [-0.0826],
        [-0.0837],
        ...,
        [-0.1491],
        [-0.1485],
        [-0.1483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55028.5078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366268.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366281.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0017,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0017,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0017,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0017,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0017,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0017,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(670.1544, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.0644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6259, device='cuda:0')



h[100].sum tensor(-16.1997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.7455, device='cuda:0')



h[200].sum tensor(-49.1448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6445e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61083.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0306, 0.0032, 0.0018,  ..., 0.0000, 0.0059, 0.0000],
        [0.0240, 0.0071, 0.0012,  ..., 0.0000, 0.0130, 0.0000],
        [0.0199, 0.0103, 0.0004,  ..., 0.0000, 0.0189, 0.0000],
        ...,
        [0.0121, 0.0166, 0.0000,  ..., 0.0000, 0.0296, 0.0000],
        [0.0121, 0.0166, 0.0000,  ..., 0.0000, 0.0295, 0.0000],
        [0.0121, 0.0166, 0.0000,  ..., 0.0000, 0.0295, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423965.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5104.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-331.8584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.2989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.3865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0740],
        [ 0.0453],
        [ 0.0072],
        ...,
        [-0.1497],
        [-0.1491],
        [-0.1489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42339.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366281.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366294.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0203,  0.0210,  0.0370,  ...,  0.0040, -0.0195, -0.0313],
        [ 0.0058,  0.0047,  0.0107,  ...,  0.0002, -0.0054, -0.0087],
        [ 0.0178,  0.0182,  0.0324,  ...,  0.0033, -0.0171, -0.0274],
        ...,
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(954.6548, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(40.2288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.7880, device='cuda:0')



h[100].sum tensor(-20.6195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1328, device='cuda:0')



h[200].sum tensor(-62.6396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0869e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0306, 0.0288, 0.0563,  ..., 0.0040, 0.0000, 0.0000],
        [0.0746, 0.0764, 0.1359,  ..., 0.0142, 0.0000, 0.0000],
        [0.0476, 0.0462, 0.0870,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76296.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1819, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2753, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2825, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0125, 0.0169, 0.0000,  ..., 0.0000, 0.0293, 0.0000],
        [0.0188, 0.0123, 0.0000,  ..., 0.0000, 0.0222, 0.0000],
        [0.0386, 0.0052, 0.0000,  ..., 0.0000, 0.0096, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524939.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7217.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-440.5874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(413.0026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.5211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0360],
        [ 0.0159],
        [ 0.0050],
        ...,
        [-0.1151],
        [-0.0742],
        [-0.0315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-41218.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366294.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366307.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.5006, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.7333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6455, device='cuda:0')



h[100].sum tensor(-14.9765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.9794, device='cuda:0')



h[200].sum tensor(-45.5597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5222e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58240.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0143, 0.0160, 0.0009,  ..., 0.0000, 0.0259, 0.0000],
        [0.0129, 0.0171, 0.0003,  ..., 0.0000, 0.0284, 0.0000],
        [0.0127, 0.0172, 0.0000,  ..., 0.0000, 0.0290, 0.0000],
        ...,
        [0.0127, 0.0173, 0.0000,  ..., 0.0000, 0.0290, 0.0000],
        [0.0127, 0.0173, 0.0000,  ..., 0.0000, 0.0290, 0.0000],
        [0.0127, 0.0173, 0.0000,  ..., 0.0000, 0.0290, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(416565.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5030.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.3265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(246.8259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.5450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0893],
        [-0.1385],
        [-0.1763],
        ...,
        [-0.1520],
        [-0.1514],
        [-0.1512]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39323.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366307.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366320.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(405.1922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6030, device='cuda:0')



h[100].sum tensor(-10.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6987, device='cuda:0')



h[200].sum tensor(-32.2094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0872e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48643.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0129, 0.0178, 0.0000,  ..., 0.0000, 0.0287, 0.0000],
        [0.0129, 0.0177, 0.0000,  ..., 0.0000, 0.0287, 0.0000],
        [0.0129, 0.0178, 0.0000,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0129, 0.0178, 0.0000,  ..., 0.0000, 0.0287, 0.0000],
        [0.0129, 0.0178, 0.0000,  ..., 0.0000, 0.0287, 0.0000],
        [0.0129, 0.0178, 0.0000,  ..., 0.0000, 0.0287, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(371325.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4019.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-235.1099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.9820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.8944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2152],
        [-0.2077],
        [-0.1916],
        ...,
        [-0.1538],
        [-0.1532],
        [-0.1530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44909.9453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366320.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(517.4390, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366333.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(553.5101, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1627, device='cuda:0')



h[100].sum tensor(-12.7291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.8733, device='cuda:0')



h[200].sum tensor(-38.8304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3071e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0062, 0.0048, 0.0120,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52437.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0143, 0.0172, 0.0002,  ..., 0.0000, 0.0266, 0.0000],
        [0.0235, 0.0099, 0.0010,  ..., 0.0000, 0.0146, 0.0000],
        [0.0431, 0.0040, 0.0020,  ..., 0.0000, 0.0056, 0.0000],
        ...,
        [0.0131, 0.0183, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        [0.0131, 0.0183, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        [0.0131, 0.0183, 0.0000,  ..., 0.0000, 0.0284, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383715.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4277.0215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-261.1713, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(191.3340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.6114, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0500],
        [ 0.0035],
        [ 0.0547],
        ...,
        [-0.1557],
        [-0.1551],
        [-0.1549]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-48588.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9995],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366333.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366346.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0136,  0.0136,  0.0249,  ...,  0.0022, -0.0130, -0.0209],
        [ 0.0180,  0.0185,  0.0327,  ...,  0.0034, -0.0171, -0.0276],
        [ 0.0090,  0.0083,  0.0164,  ...,  0.0010, -0.0085, -0.0136],
        ...,
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0016,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(287.7105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.0120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4864, device='cuda:0')



h[100].sum tensor(-8.1147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.0274, device='cuda:0')



h[200].sum tensor(-24.7885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.3298e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0814, 0.0845, 0.1479,  ..., 0.0159, 0.0000, 0.0000],
        [0.0680, 0.0694, 0.1237,  ..., 0.0124, 0.0000, 0.0000],
        [0.0568, 0.0584, 0.1033,  ..., 0.0108, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42811.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4120, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3725, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2947, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0132, 0.0189, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        [0.0132, 0.0189, 0.0000,  ..., 0.0000, 0.0284, 0.0000],
        [0.0132, 0.0189, 0.0000,  ..., 0.0000, 0.0284, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(346742.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3490.9436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-184.9619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.0499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.6630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0016],
        [-0.0017],
        [-0.0113],
        ...,
        [-0.1586],
        [-0.1579],
        [-0.1577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45744.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366346.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366359.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0033,  0.0082,  ..., -0.0002, -0.0041, -0.0067],
        [ 0.0107,  0.0104,  0.0196,  ...,  0.0015, -0.0102, -0.0164],
        [ 0.0001, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(844.6075, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.9884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9281, device='cuda:0')



h[100].sum tensor(-17.1127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.7987, device='cuda:0')



h[200].sum tensor(-52.3482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7867e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0386, 0.0365, 0.0703,  ..., 0.0047, 0.0000, 0.0000],
        [0.0172, 0.0141, 0.0317,  ..., 0.0010, 0.0000, 0.0000],
        [0.0147, 0.0128, 0.0271,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61460.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1133, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0874, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0688, 0.0017, 0.0010,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0134, 0.0194, 0.0000,  ..., 0.0000, 0.0286, 0.0000],
        [0.0134, 0.0194, 0.0000,  ..., 0.0000, 0.0286, 0.0000],
        [0.0134, 0.0194, 0.0000,  ..., 0.0000, 0.0286, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425210.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5097.5908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-321.6790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(272.2355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.8158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0780],
        [ 0.0772],
        [ 0.0629],
        ...,
        [-0.1611],
        [-0.1583],
        [-0.1534]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59355.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366359.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366372.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(381.6571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5956, device='cuda:0')



h[100].sum tensor(-9.3365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9085, device='cuda:0')



h[200].sum tensor(-28.6003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6326e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44652.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0162, 0.0176, 0.0003,  ..., 0.0000, 0.0248, 0.0000],
        [0.0139, 0.0196, 0.0000,  ..., 0.0000, 0.0282, 0.0000],
        [0.0137, 0.0198, 0.0000,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0137, 0.0199, 0.0000,  ..., 0.0000, 0.0288, 0.0000],
        [0.0137, 0.0199, 0.0000,  ..., 0.0000, 0.0288, 0.0000],
        [0.0137, 0.0199, 0.0000,  ..., 0.0000, 0.0288, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(354348.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3647.1313, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-190.5978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.4176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.8606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0778],
        [-0.1015],
        [-0.1188],
        ...,
        [-0.1645],
        [-0.1638],
        [-0.1636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-54024.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366372.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366386.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0262,  0.0277,  0.0475,  ...,  0.0055, -0.0249, -0.0402],
        [ 0.0124,  0.0123,  0.0226,  ...,  0.0019, -0.0117, -0.0189],
        ...,
        [ 0.0095,  0.0090,  0.0173,  ...,  0.0012, -0.0090, -0.0145],
        [ 0.0068,  0.0059,  0.0123,  ...,  0.0004, -0.0063, -0.0102],
        [ 0.0095,  0.0090,  0.0173,  ...,  0.0012, -0.0090, -0.0145]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(526.5172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.7481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3475, device='cuda:0')



h[100].sum tensor(-11.4320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.2545, device='cuda:0')



h[200].sum tensor(-35.0685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1950e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0882, 0.0922, 0.1598,  ..., 0.0177, 0.0000, 0.0000],
        [0.0342, 0.0332, 0.0623,  ..., 0.0049, 0.0000, 0.0000],
        [0.0888, 0.0928, 0.1609,  ..., 0.0178, 0.0000, 0.0000],
        ...,
        [0.0150, 0.0130, 0.0274,  ..., 0.0011, 0.0000, 0.0000],
        [0.0403, 0.0385, 0.0733,  ..., 0.0052, 0.0000, 0.0000],
        [0.0324, 0.0296, 0.0589,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48351.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3251, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2469, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3329, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0706, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1083, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1175, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367446.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3936.7097, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-217.4694, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.7477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.0173, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0249],
        [ 0.0227],
        [ 0.0264],
        ...,
        [-0.0343],
        [-0.0049],
        [ 0.0023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59095.4961, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366386.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366400., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(419.1012, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7467, device='cuda:0')



h[100].sum tensor(-9.3827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0432, device='cuda:0')



h[200].sum tensor(-28.8224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7259e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45716.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0065, 0.0000,  ..., 0.0000, 0.0088, 0.0000],
        [0.0243, 0.0122, 0.0006,  ..., 0.0000, 0.0159, 0.0000],
        [0.0176, 0.0175, 0.0014,  ..., 0.0000, 0.0230, 0.0000],
        ...,
        [0.0142, 0.0205, 0.0000,  ..., 0.0000, 0.0288, 0.0000],
        [0.0142, 0.0205, 0.0000,  ..., 0.0000, 0.0288, 0.0000],
        [0.0142, 0.0205, 0.0000,  ..., 0.0000, 0.0288, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364359.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3862.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-195.2545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.7690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.0634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0159],
        [ 0.0056],
        [ 0.0075],
        ...,
        [-0.1678],
        [-0.1672],
        [-0.1669]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58896.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [0.9999],
        [0.9996],
        ...,
        [0.9999],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366400., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366414., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(533.6078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.6095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2171, device='cuda:0')



h[100].sum tensor(-10.8269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.2464, device='cuda:0')



h[200].sum tensor(-33.3050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1252e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49974.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.3052e-02, 1.0337e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4430e-02,
         0.0000e+00],
        [1.9392e-02, 1.6843e-02, 3.6780e-05,  ..., 0.0000e+00, 2.3461e-02,
         0.0000e+00],
        [2.0217e-02, 1.5952e-02, 2.0221e-04,  ..., 0.0000e+00, 2.2002e-02,
         0.0000e+00],
        ...,
        [1.4491e-02, 2.0733e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8932e-02,
         0.0000e+00],
        [1.4488e-02, 2.0729e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8925e-02,
         0.0000e+00],
        [1.4487e-02, 2.0726e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8921e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383004.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4319.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-220.5171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.2522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.4202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0810],
        [-0.0775],
        [-0.0501],
        ...,
        [-0.1687],
        [-0.1681],
        [-0.1679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-47381.3008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366414., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366428.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(343.4035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.6378, device='cuda:0')



h[100].sum tensor(-7.4746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2707, device='cuda:0')



h[200].sum tensor(-23.0252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.8057e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0045, 0.0027, 0.0083,  ..., 0.0000, 0.0000, 0.0000],
        [0.0083, 0.0054, 0.0151,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40510.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.1080e-02, 7.2436e-03, 2.0523e-03,  ..., 0.0000e+00, 8.5142e-03,
         0.0000e+00],
        [4.3004e-02, 3.2765e-03, 2.7635e-03,  ..., 0.0000e+00, 4.0741e-03,
         0.0000e+00],
        [5.3908e-02, 1.3855e-03, 3.0672e-03,  ..., 0.0000e+00, 8.7399e-04,
         0.0000e+00],
        ...,
        [1.4718e-02, 2.0925e-02, 3.3584e-05,  ..., 0.0000e+00, 2.9169e-02,
         0.0000e+00],
        [1.4715e-02, 2.0921e-02, 3.3210e-05,  ..., 0.0000e+00, 2.9163e-02,
         0.0000e+00],
        [1.4713e-02, 2.0918e-02, 3.3087e-05,  ..., 0.0000e+00, 2.9158e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(338990.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3402.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-154.3336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(66.6140, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.6669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0542],
        [ 0.0773],
        [ 0.0886],
        ...,
        [-0.1705],
        [-0.1698],
        [-0.1696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62521.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366428.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366442.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0100,  0.0189,  ...,  0.0014, -0.0097, -0.0157],
        [ 0.0044,  0.0033,  0.0081,  ..., -0.0002, -0.0040, -0.0065],
        [ 0.0062,  0.0053,  0.0113,  ...,  0.0003, -0.0057, -0.0092],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(576.4121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4123, device='cuda:0')



h[100].sum tensor(-10.8884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4205, device='cuda:0')



h[200].sum tensor(-33.5885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1372e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0178, 0.0146, 0.0323,  ..., 0.0011, 0.0000, 0.0000],
        [0.0391, 0.0369, 0.0709,  ..., 0.0049, 0.0000, 0.0000],
        [0.0178, 0.0160, 0.0323,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49105., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0910, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.1173, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0979, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0150, 0.0211, 0.0001,  ..., 0.0000, 0.0296, 0.0000],
        [0.0150, 0.0211, 0.0001,  ..., 0.0000, 0.0296, 0.0000],
        [0.0150, 0.0211, 0.0001,  ..., 0.0000, 0.0296, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380091.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4291.7207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-216.1756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.8857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.3362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1041],
        [ 0.1067],
        [ 0.1059],
        ...,
        [-0.1722],
        [-0.1716],
        [-0.1715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62994.5430, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366442.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366456.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(556.0137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4812, device='cuda:0')



h[100].sum tensor(-10.2858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5901, device='cuda:0')



h[200].sum tensor(-31.7740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0797e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49800.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0276, 0.0105, 0.0015,  ..., 0.0000, 0.0141, 0.0000],
        [0.0183, 0.0183, 0.0006,  ..., 0.0000, 0.0254, 0.0000],
        [0.0151, 0.0212, 0.0002,  ..., 0.0000, 0.0299, 0.0000],
        ...,
        [0.0152, 0.0213, 0.0002,  ..., 0.0000, 0.0300, 0.0000],
        [0.0152, 0.0213, 0.0002,  ..., 0.0000, 0.0300, 0.0000],
        [0.0152, 0.0213, 0.0002,  ..., 0.0000, 0.0300, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389342.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4501.2065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-213.0058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.4875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.9608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0173],
        [-0.0378],
        [-0.0920],
        ...,
        [-0.1745],
        [-0.1738],
        [-0.1736]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44875.1211, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366456.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(561.2795, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366470.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0154,  0.0155,  0.0278,  ...,  0.0027, -0.0144, -0.0232],
        [ 0.0205,  0.0212,  0.0370,  ...,  0.0040, -0.0192, -0.0310],
        [ 0.0263,  0.0277,  0.0476,  ...,  0.0055, -0.0247, -0.0399],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(825.9341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.8875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2670, device='cuda:0')



h[100].sum tensor(-14.3471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.6418, device='cuda:0')



h[200].sum tensor(-44.3822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4988e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0691, 0.0704, 0.1249,  ..., 0.0127, 0.0000, 0.0000],
        [0.0754, 0.0775, 0.1364,  ..., 0.0143, 0.0000, 0.0000],
        [0.0894, 0.0932, 0.1617,  ..., 0.0180, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58327.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5916e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.0914e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.5213e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5220e-02, 2.1529e-02, 1.7180e-04,  ..., 0.0000e+00, 3.0575e-02,
         0.0000e+00],
        [1.5217e-02, 2.1525e-02, 1.7141e-04,  ..., 0.0000e+00, 3.0568e-02,
         0.0000e+00],
        [1.5215e-02, 2.1522e-02, 1.7124e-04,  ..., 0.0000e+00, 3.0564e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423787.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5175.7983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-274.8030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(221.1061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(33.0583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0381],
        [ 0.0440],
        [ 0.0527],
        ...,
        [-0.1773],
        [-0.1766],
        [-0.1764]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46209.6367, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366470.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366483.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0015,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(624.2473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.4761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0146, device='cuda:0')



h[100].sum tensor(-11.1446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.9576, device='cuda:0')



h[200].sum tensor(-34.5241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1744e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0141, 0.0118, 0.0254,  ..., 0.0009, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51559.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0007, 0.0007,  ..., 0.0000, 0.0010, 0.0000],
        [0.0378, 0.0078, 0.0011,  ..., 0.0000, 0.0106, 0.0000],
        [0.0243, 0.0137, 0.0010,  ..., 0.0000, 0.0189, 0.0000],
        ...,
        [0.0151, 0.0219, 0.0000,  ..., 0.0000, 0.0313, 0.0000],
        [0.0151, 0.0219, 0.0000,  ..., 0.0000, 0.0313, 0.0000],
        [0.0151, 0.0219, 0.0000,  ..., 0.0000, 0.0313, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(399713.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4607.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.3536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.9835, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.0581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0889],
        [ 0.0461],
        [-0.0124],
        ...,
        [-0.1815],
        [-0.1808],
        [-0.1804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50182.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366483.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366497.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0087,  0.0080,  0.0156,  ...,  0.0009, -0.0080, -0.0129],
        [ 0.0064,  0.0054,  0.0114,  ...,  0.0003, -0.0058, -0.0093],
        ...,
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(623.7991, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.7458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0657, device='cuda:0')



h[100].sum tensor(-11.1433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.0032, device='cuda:0')



h[200].sum tensor(-34.5689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1776e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0195, 0.0377,  ..., 0.0027, 0.0000, 0.0000],
        [0.0190, 0.0172, 0.0340,  ..., 0.0021, 0.0000, 0.0000],
        [0.0427, 0.0410, 0.0770,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52906.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0934, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0978, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1349, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0151, 0.0224, 0.0000,  ..., 0.0000, 0.0320, 0.0000],
        [0.0151, 0.0224, 0.0000,  ..., 0.0000, 0.0320, 0.0000],
        [0.0150, 0.0224, 0.0000,  ..., 0.0000, 0.0320, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(412068.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4762.2632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-227.6138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(167.9266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.0507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0735],
        [ 0.0773],
        [ 0.0815],
        ...,
        [-0.1861],
        [-0.1853],
        [-0.1851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55446.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0000],
        [0.9996],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366497.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0000],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366510.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0042,  0.0030,  0.0076,  ..., -0.0002, -0.0038, -0.0061],
        [ 0.0042,  0.0030,  0.0076,  ..., -0.0002, -0.0038, -0.0061],
        ...,
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(664.4004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.8026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1436, device='cuda:0')



h[100].sum tensor(-11.6546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.9645, device='cuda:0')



h[200].sum tensor(-36.2060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2442e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0083, 0.0053, 0.0146,  ..., 0.0000, 0.0000, 0.0000],
        [0.0083, 0.0053, 0.0146,  ..., 0.0000, 0.0000, 0.0000],
        [0.0083, 0.0053, 0.0146,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52013.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0326, 0.0070, 0.0026,  ..., 0.0000, 0.0095, 0.0000],
        [0.0446, 0.0026, 0.0023,  ..., 0.0000, 0.0032, 0.0000],
        [0.0757, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0150, 0.0227, 0.0000,  ..., 0.0000, 0.0324, 0.0000],
        [0.0150, 0.0227, 0.0000,  ..., 0.0000, 0.0324, 0.0000],
        [0.0150, 0.0227, 0.0000,  ..., 0.0000, 0.0324, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402757.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4528.8901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-216.9743, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(158.0711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(37.2729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0713],
        [-0.0076],
        [ 0.0296],
        ...,
        [-0.1892],
        [-0.1884],
        [-0.1882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56868.4023, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0000],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366510.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366524.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(508.7981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.1957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4422, device='cuda:0')



h[100].sum tensor(-9.0314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7717, device='cuda:0')



h[200].sum tensor(-28.0966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.5378e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0010, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45112.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0149, 0.0228, 0.0000,  ..., 0.0000, 0.0325, 0.0000],
        [0.0149, 0.0228, 0.0000,  ..., 0.0000, 0.0325, 0.0000],
        [0.0149, 0.0228, 0.0000,  ..., 0.0000, 0.0325, 0.0000],
        ...,
        [0.0150, 0.0229, 0.0000,  ..., 0.0000, 0.0326, 0.0000],
        [0.0150, 0.0229, 0.0000,  ..., 0.0000, 0.0326, 0.0000],
        [0.0150, 0.0229, 0.0000,  ..., 0.0000, 0.0326, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(371229.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3770.9587, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-168.8963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.9650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.6084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2736],
        [-0.2717],
        [-0.2663],
        ...,
        [-0.1911],
        [-0.1904],
        [-0.1901]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74816.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366524.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366538.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0003, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(637.7394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.3175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4272, device='cuda:0')



h[100].sum tensor(-10.6835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4338, device='cuda:0')



h[200].sum tensor(-33.2833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1382e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0174, 0.0155, 0.0310,  ..., 0.0020, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53779.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0784, 0.0010, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0378, 0.0095, 0.0006,  ..., 0.0000, 0.0129, 0.0000],
        [0.0220, 0.0162, 0.0008,  ..., 0.0000, 0.0222, 0.0000],
        ...,
        [0.0150, 0.0229, 0.0000,  ..., 0.0000, 0.0328, 0.0000],
        [0.0150, 0.0229, 0.0000,  ..., 0.0000, 0.0328, 0.0000],
        [0.0150, 0.0229, 0.0000,  ..., 0.0000, 0.0328, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434309.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5078.0259, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-226.8824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(175.3404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.2348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0249],
        [-0.0344],
        [-0.0980],
        ...,
        [-0.1921],
        [-0.1914],
        [-0.1911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-64016.8086, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366538.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366550.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(585.9402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4221, device='cuda:0')



h[100].sum tensor(-9.5065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6456, device='cuda:0')



h[200].sum tensor(-29.6586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0143e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0088, 0.0075, 0.0157,  ..., 0.0008, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0009, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48923.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0511, 0.0068, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0238, 0.0148, 0.0000,  ..., 0.0000, 0.0223, 0.0000],
        [0.0167, 0.0212, 0.0000,  ..., 0.0000, 0.0308, 0.0000],
        ...,
        [0.0151, 0.0228, 0.0000,  ..., 0.0000, 0.0329, 0.0000],
        [0.0151, 0.0228, 0.0000,  ..., 0.0000, 0.0329, 0.0000],
        [0.0151, 0.0228, 0.0000,  ..., 0.0000, 0.0329, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398587.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4294.7612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-188.9318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.6307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.6317, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0586],
        [-0.1283],
        [-0.1849],
        ...,
        [-0.1923],
        [-0.1915],
        [-0.1912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61136.8633, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366550.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366563.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0004,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(539.3568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5528, device='cuda:0')



h[100].sum tensor(-8.3918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.9785, device='cuda:0')



h[200].sum tensor(-26.2181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.9885e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0185, 0.0173, 0.0338,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0008, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45644.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0205, 0.0176, 0.0000,  ..., 0.0000, 0.0262, 0.0000],
        [0.0406, 0.0106, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0905, 0.0011, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        ...,
        [0.0151, 0.0227, 0.0000,  ..., 0.0000, 0.0329, 0.0000],
        [0.0151, 0.0227, 0.0000,  ..., 0.0000, 0.0329, 0.0000],
        [0.0151, 0.0227, 0.0000,  ..., 0.0000, 0.0329, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(378275.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3867.1423, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-164.9810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.7759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.9799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1346],
        [-0.0680],
        [-0.0108],
        ...,
        [-0.1910],
        [-0.1903],
        [-0.1900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61928.5820, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0001],
        [0.9996],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366563.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366577.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3080e-03,  4.4588e-03,  9.7793e-03,  ...,  2.5691e-05,
         -4.8443e-03, -7.8638e-03],
        [ 1.5565e-04, -1.3403e-03,  4.3067e-04,  ..., -1.3167e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5565e-04, -1.3403e-03,  4.3067e-04,  ..., -1.3167e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5565e-04, -1.3403e-03,  4.3067e-04,  ..., -1.3167e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5565e-04, -1.3403e-03,  4.3067e-04,  ..., -1.3167e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5565e-04, -1.3403e-03,  4.3067e-04,  ..., -1.3167e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(566.4959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.5400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4637, device='cuda:0')



h[100].sum tensor(-8.3796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8990, device='cuda:0')



h[200].sum tensor(-26.2173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.9335e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7621e-02, 1.6447e-02, 3.2566e-02,  ..., 1.9229e-03, 0.0000e+00,
         0.0000e+00],
        [5.7831e-03, 4.4653e-03, 1.1087e-02,  ..., 2.5728e-05, 0.0000e+00,
         0.0000e+00],
        [2.8692e-02, 2.8913e-02, 5.2653e-02,  ..., 4.6803e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.2701e-04, 0.0000e+00, 1.7349e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.2691e-04, 0.0000e+00, 1.7346e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.2684e-04, 0.0000e+00, 1.7345e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45780.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1389e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [9.7807e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.4255e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5146e-02, 2.2581e-02, 9.5058e-05,  ..., 0.0000e+00, 3.2775e-02,
         0.0000e+00],
        [1.5143e-02, 2.2576e-02, 9.4692e-05,  ..., 0.0000e+00, 3.2768e-02,
         0.0000e+00],
        [1.5141e-02, 2.2573e-02, 9.4544e-05,  ..., 0.0000e+00, 3.2763e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(376062.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3781.7700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-163.8703, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.4855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.4022, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0426],
        [ 0.0342],
        [ 0.0209],
        ...,
        [-0.1889],
        [-0.1882],
        [-0.1880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57983.0234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366577.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0002],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366590.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(616.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.3957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2557, device='cuda:0')



h[100].sum tensor(-8.7764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.6053, device='cuda:0')



h[200].sum tensor(-27.4979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.4226e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47917.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0151, 0.0225, 0.0003,  ..., 0.0000, 0.0326, 0.0000],
        [0.0162, 0.0214, 0.0003,  ..., 0.0000, 0.0312, 0.0000],
        [0.0261, 0.0129, 0.0005,  ..., 0.0000, 0.0193, 0.0000],
        ...,
        [0.0152, 0.0227, 0.0003,  ..., 0.0000, 0.0328, 0.0000],
        [0.0151, 0.0226, 0.0003,  ..., 0.0000, 0.0328, 0.0000],
        [0.0151, 0.0226, 0.0003,  ..., 0.0000, 0.0328, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389121.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4033.5288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-181.1739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.0341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.6632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1799],
        [-0.1232],
        [-0.0500],
        ...,
        [-0.1891],
        [-0.1883],
        [-0.1881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65087.4102, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0002],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366590.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(565.8870, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0002],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366603.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1362e-04, -1.3298e-03,  4.8278e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0916e-02,  1.0853e-02,  2.0117e-02,  ...,  1.4755e-03,
         -1.0142e-02, -1.6480e-02],
        [ 5.5745e-03,  4.8289e-03,  1.0408e-02,  ...,  8.5438e-05,
         -5.1270e-03, -8.3307e-03],
        ...,
        [ 1.1362e-04, -1.3298e-03,  4.8278e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1362e-04, -1.3298e-03,  4.8278e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1362e-04, -1.3298e-03,  4.8278e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(773.9440, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5733, device='cuda:0')



h[100].sum tensor(-10.7639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5640, device='cuda:0')



h[200].sum tensor(-33.7731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1472e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0157, 0.0146, 0.0297,  ..., 0.0015, 0.0000, 0.0000],
        [0.0179, 0.0156, 0.0336,  ..., 0.0007, 0.0000, 0.0000],
        [0.0631, 0.0654, 0.1158,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50181.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.8631e-02, 1.6475e-03, 7.1838e-04,  ..., 0.0000e+00, 2.7521e-03,
         0.0000e+00],
        [1.2768e-01, 0.0000e+00, 5.4815e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.3284e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5224e-02, 2.2649e-02, 5.7102e-04,  ..., 0.0000e+00, 3.2762e-02,
         0.0000e+00],
        [1.5221e-02, 2.2645e-02, 5.7058e-04,  ..., 0.0000e+00, 3.2755e-02,
         0.0000e+00],
        [1.5219e-02, 2.2641e-02, 5.7024e-04,  ..., 0.0000e+00, 3.2749e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389693.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4000.6990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-191.1564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.4814, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.3344, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0342],
        [ 0.0310],
        [ 0.0077],
        ...,
        [-0.1887],
        [-0.1880],
        [-0.1877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52535.9023, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0002],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366603.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366616.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.8778e-05, -1.3279e-03,  4.9879e-04,  ..., -1.3431e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8778e-05, -1.3279e-03,  4.9879e-04,  ..., -1.3431e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8778e-05, -1.3279e-03,  4.9879e-04,  ..., -1.3431e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.8778e-05, -1.3279e-03,  4.9879e-04,  ..., -1.3431e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8778e-05, -1.3279e-03,  4.9879e-04,  ..., -1.3431e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8778e-05, -1.3279e-03,  4.9879e-04,  ..., -1.3431e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(914.6691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.4053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9495, device='cuda:0')



h[100].sum tensor(-12.6028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.5750, device='cuda:0')



h[200].sum tensor(-39.5993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3557e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59487.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0152, 0.0227, 0.0007,  ..., 0.0000, 0.0328, 0.0000],
        [0.0152, 0.0227, 0.0007,  ..., 0.0000, 0.0328, 0.0000],
        [0.0152, 0.0227, 0.0007,  ..., 0.0000, 0.0328, 0.0000],
        ...,
        [0.0153, 0.0228, 0.0007,  ..., 0.0000, 0.0330, 0.0000],
        [0.0153, 0.0228, 0.0007,  ..., 0.0000, 0.0330, 0.0000],
        [0.0153, 0.0228, 0.0007,  ..., 0.0000, 0.0330, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445950.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5101.7632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-255.9611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.4833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(49.3960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1493],
        [-0.1722],
        [-0.1924],
        ...,
        [-0.1898],
        [-0.1891],
        [-0.1888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-49866.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366616.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366629.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5023e-05, -1.3215e-03,  5.0384e-04,  ..., -1.3494e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.5023e-05, -1.3215e-03,  5.0384e-04,  ..., -1.3494e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.5023e-05, -1.3215e-03,  5.0384e-04,  ..., -1.3494e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.5023e-05, -1.3215e-03,  5.0384e-04,  ..., -1.3494e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.5023e-05, -1.3215e-03,  5.0384e-04,  ..., -1.3494e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.5023e-05, -1.3215e-03,  5.0384e-04,  ..., -1.3494e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1074.7412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.9566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6528, device='cuda:0')



h[100].sum tensor(-14.7744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.8777, device='cuda:0')



h[200].sum tensor(-46.4890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5844e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0031, 0.0091,  ..., 0.0000, 0.0000, 0.0000],
        [0.0081, 0.0062, 0.0162,  ..., 0.0000, 0.0000, 0.0000],
        [0.0042, 0.0031, 0.0091,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62596.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0381, 0.0015, 0.0053,  ..., 0.0000, 0.0021, 0.0000],
        [0.0412, 0.0007, 0.0062,  ..., 0.0000, 0.0011, 0.0000],
        [0.0408, 0.0008, 0.0053,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0153, 0.0232, 0.0006,  ..., 0.0000, 0.0334, 0.0000],
        [0.0153, 0.0232, 0.0006,  ..., 0.0000, 0.0334, 0.0000],
        [0.0153, 0.0232, 0.0006,  ..., 0.0000, 0.0334, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457767.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5291.9976, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-278.7134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(237.7626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(53.2578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1040],
        [ 0.1092],
        [ 0.1057],
        ...,
        [-0.1924],
        [-0.1917],
        [-0.1915]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59807.3477, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366629.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366641.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3738e-05, -1.3172e-03,  4.9844e-04,  ..., -1.3546e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.3738e-05, -1.3172e-03,  4.9844e-04,  ..., -1.3546e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.3738e-05, -1.3172e-03,  4.9844e-04,  ..., -1.3546e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.3738e-05, -1.3172e-03,  4.9844e-04,  ..., -1.3546e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.3738e-05, -1.3172e-03,  4.9844e-04,  ..., -1.3546e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.3738e-05, -1.3172e-03,  4.9844e-04,  ..., -1.3546e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(739.5278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.5323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2628, device='cuda:0')



h[100].sum tensor(-9.8218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.3953, device='cuda:0')



h[200].sum tensor(-30.9497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0662e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50888.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0624, 0.0048, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0431, 0.0056, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0358, 0.0083, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        ...,
        [0.0153, 0.0236, 0.0004,  ..., 0.0000, 0.0342, 0.0000],
        [0.0153, 0.0236, 0.0004,  ..., 0.0000, 0.0341, 0.0000],
        [0.0153, 0.0236, 0.0004,  ..., 0.0000, 0.0341, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(404449.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4197.6846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-191.6884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.0066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(54.8673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0948],
        [-0.1055],
        [-0.1144],
        ...,
        [-0.1953],
        [-0.1945],
        [-0.1934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59913.6758, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366641.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366654.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0541e-02,  3.3143e-02,  5.6014e-02,  ...,  6.5586e-03,
         -2.8506e-02, -4.6408e-02],
        [ 2.1948e-02,  2.3427e-02,  4.0359e-02,  ...,  4.3261e-03,
         -2.0468e-02, -3.3322e-02],
        [ 1.1612e-02,  1.1739e-02,  2.1526e-02,  ...,  1.6406e-03,
         -1.0799e-02, -1.7581e-02],
        ...,
        [ 6.7295e-05, -1.3142e-03,  4.9291e-04,  ..., -1.3588e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.7295e-05, -1.3142e-03,  4.9291e-04,  ..., -1.3588e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.7295e-05, -1.3142e-03,  4.9291e-04,  ..., -1.3588e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(729.1638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7788, device='cuda:0')



h[100].sum tensor(-9.6337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.9637, device='cuda:0')



h[200].sum tensor(-30.4002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0363e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0937, 0.1004, 0.1723,  ..., 0.0188, 0.0000, 0.0000],
        [0.0861, 0.0918, 0.1584,  ..., 0.0169, 0.0000, 0.0000],
        [0.0712, 0.0749, 0.1312,  ..., 0.0130, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48931.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.4010e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.9717e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.3940e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5341e-02, 2.4091e-02, 2.1967e-04,  ..., 0.0000e+00, 3.4899e-02,
         0.0000e+00],
        [1.5338e-02, 2.4086e-02, 2.1930e-04,  ..., 0.0000e+00, 3.4891e-02,
         0.0000e+00],
        [1.5335e-02, 2.4083e-02, 2.1909e-04,  ..., 0.0000e+00, 3.4885e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(392680.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3928.3291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-178.6293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.9834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.3904, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1213],
        [-0.0886],
        [-0.0486],
        ...,
        [-0.2020],
        [-0.2012],
        [-0.2010]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67358.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366654.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366666.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4291e-02,  3.7395e-02,  6.2876e-02,  ...,  7.5264e-03,
         -3.1981e-02, -5.2090e-02],
        [ 1.0493e-02,  1.0476e-02,  1.9495e-02,  ...,  1.3453e-03,
         -9.7425e-03, -1.5868e-02],
        [ 6.7525e-05, -1.3166e-03,  4.9003e-04,  ..., -1.3626e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.7525e-05, -1.3166e-03,  4.9003e-04,  ..., -1.3626e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.7525e-05, -1.3166e-03,  4.9003e-04,  ..., -1.3626e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.7525e-05, -1.3166e-03,  4.9003e-04,  ..., -1.3626e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(724.1210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8100, device='cuda:0')



h[100].sum tensor(-9.5284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.9915, device='cuda:0')



h[200].sum tensor(-30.1114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0383e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0578, 0.0598, 0.1068,  ..., 0.0095, 0.0000, 0.0000],
        [0.0685, 0.0732, 0.1264,  ..., 0.0136, 0.0000, 0.0000],
        [0.0275, 0.0282, 0.0516,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49887.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4540e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4613e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.8185e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5303e-02, 2.4566e-02, 7.8334e-05,  ..., 1.8273e-04, 3.5640e-02,
         0.0000e+00],
        [1.5300e-02, 2.4561e-02, 7.7979e-05,  ..., 1.8268e-04, 3.5632e-02,
         0.0000e+00],
        [1.5298e-02, 2.4557e-02, 7.7831e-05,  ..., 1.8264e-04, 3.5626e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402070.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4018.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-181.8458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.9746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(60.2473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0678],
        [-0.0567],
        [-0.0370],
        ...,
        [-0.2067],
        [-0.2060],
        [-0.2057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66700.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366666.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366679.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2950e-05, -1.3231e-03,  4.9755e-04,  ..., -1.3655e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.2950e-05, -1.3231e-03,  4.9755e-04,  ..., -1.3655e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.2950e-05, -1.3231e-03,  4.9755e-04,  ..., -1.3655e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.2950e-05, -1.3231e-03,  4.9755e-04,  ..., -1.3655e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.2950e-05, -1.3231e-03,  4.9755e-04,  ..., -1.3655e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.2950e-05, -1.3231e-03,  4.9755e-04,  ..., -1.3655e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1087.2089, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.4078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0538, device='cuda:0')



h[100].sum tensor(-14.5887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.2353, device='cuda:0')



h[200].sum tensor(-46.1690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6092e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62926.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0152, 0.0248, 0.0001,  ..., 0.0000, 0.0357, 0.0000],
        [0.0152, 0.0248, 0.0001,  ..., 0.0000, 0.0357, 0.0000],
        [0.0152, 0.0248, 0.0001,  ..., 0.0000, 0.0357, 0.0000],
        ...,
        [0.0153, 0.0250, 0.0001,  ..., 0.0000, 0.0360, 0.0000],
        [0.0153, 0.0249, 0.0001,  ..., 0.0000, 0.0360, 0.0000],
        [0.0153, 0.0249, 0.0001,  ..., 0.0000, 0.0359, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469667.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5273.1362, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-273.0774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(220.2021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(62.8129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2688],
        [-0.2785],
        [-0.2834],
        ...,
        [-0.2093],
        [-0.2085],
        [-0.2082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-63276.4102, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366679.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366691.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2458e-02,  1.2683e-02,  2.3095e-02,  ...,  1.8466e-03,
         -1.1543e-02, -1.8820e-02],
        [ 2.5875e-02,  2.7868e-02,  4.7576e-02,  ...,  5.3296e-03,
         -2.4054e-02, -3.9216e-02],
        [ 2.0690e-02,  2.1999e-02,  3.8115e-02,  ...,  3.9836e-03,
         -1.9219e-02, -3.1334e-02],
        ...,
        [ 7.8057e-05, -1.3288e-03,  5.0677e-04,  ..., -1.3672e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.8057e-05, -1.3288e-03,  5.0677e-04,  ..., -1.3672e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.8057e-05, -1.3288e-03,  5.0677e-04,  ..., -1.3672e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1000.4915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9989, device='cuda:0')



h[100].sum tensor(-13.1423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.5109, device='cuda:0')



h[200].sum tensor(-41.6515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4205e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0638, 0.0665, 0.1178,  ..., 0.0110, 0.0000, 0.0000],
        [0.0677, 0.0709, 0.1250,  ..., 0.0120, 0.0000, 0.0000],
        [0.0835, 0.0889, 0.1539,  ..., 0.0161, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60042.3711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.0097e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4281e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7350e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5379e-02, 2.5379e-02, 1.5864e-04,  ..., 0.0000e+00, 3.5984e-02,
         0.0000e+00],
        [1.5376e-02, 2.5374e-02, 1.5828e-04,  ..., 0.0000e+00, 3.5976e-02,
         0.0000e+00],
        [1.5374e-02, 2.5370e-02, 1.5809e-04,  ..., 0.0000e+00, 3.5970e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450414.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4886.6890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-251.0719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(192.0882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(60.4483, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0278],
        [-0.0568],
        [-0.0781],
        ...,
        [-0.2121],
        [-0.2113],
        [-0.2111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58873.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366691.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366704.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0041,  0.0093,  ..., -0.0001, -0.0045, -0.0073],
        [ 0.0161,  0.0168,  0.0298,  ...,  0.0028, -0.0149, -0.0243],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1272.8970, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.5202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5664, device='cuda:0')



h[100].sum tensor(-16.7846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.3680, device='cuda:0')



h[200].sum tensor(-53.2715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8262e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0518, 0.0528, 0.0960,  ..., 0.0079, 0.0000, 0.0000],
        [0.0215, 0.0198, 0.0406,  ..., 0.0015, 0.0000, 0.0000],
        [0.0349, 0.0337, 0.0652,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68149.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1896, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1468, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1297, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0154, 0.0256, 0.0003,  ..., 0.0000, 0.0360, 0.0000],
        [0.0154, 0.0256, 0.0003,  ..., 0.0000, 0.0360, 0.0000],
        [0.0154, 0.0256, 0.0003,  ..., 0.0000, 0.0360, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486704.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5517.8857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-309.3388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(265.2341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.8040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0804],
        [-0.0851],
        [-0.0814],
        ...,
        [-0.2134],
        [-0.2125],
        [-0.2123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57603.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0001],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366704.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0000],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366716.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1208e-04, -1.3492e-03,  5.3155e-04,  ..., -1.3680e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1208e-04, -1.3492e-03,  5.3155e-04,  ..., -1.3680e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2567e-03,  4.4741e-03,  9.9265e-03,  ..., -3.2971e-05,
         -4.7854e-03, -7.8096e-03],
        ...,
        [ 1.1208e-04, -1.3492e-03,  5.3155e-04,  ..., -1.3680e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1208e-04, -1.3492e-03,  5.3155e-04,  ..., -1.3680e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1208e-04, -1.3492e-03,  5.3155e-04,  ..., -1.3680e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1216.6400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.1217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6838, device='cuda:0')



h[100].sum tensor(-15.7835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.6890, device='cuda:0')



h[200].sum tensor(-50.1668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7099e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0056, 0.0045, 0.0115,  ..., 0.0000, 0.0000, 0.0000],
        [0.0178, 0.0170, 0.0339,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67684.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0273, 0.0143, 0.0010,  ..., 0.0000, 0.0201, 0.0000],
        [0.0486, 0.0066, 0.0013,  ..., 0.0000, 0.0096, 0.0000],
        [0.0794, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0154, 0.0260, 0.0003,  ..., 0.0000, 0.0360, 0.0000],
        [0.0154, 0.0260, 0.0003,  ..., 0.0000, 0.0360, 0.0000],
        [0.0154, 0.0260, 0.0003,  ..., 0.0000, 0.0360, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493530.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5625.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-306.4496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(259.5476, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.9636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0477],
        [ 0.0259],
        [ 0.0759],
        ...,
        [-0.2153],
        [-0.2145],
        [-0.2142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61086.2266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0000],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366716.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(582.4326, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366727.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(854.3021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5193, device='cuda:0')



h[100].sum tensor(-10.5570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5159, device='cuda:0')



h[200].sum tensor(-33.6034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1438e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56353.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0214, 0.0014,  ..., 0.0000, 0.0288, 0.0000],
        [0.0173, 0.0238, 0.0017,  ..., 0.0000, 0.0315, 0.0000],
        [0.0255, 0.0152, 0.0018,  ..., 0.0000, 0.0207, 0.0000],
        ...,
        [0.0155, 0.0265, 0.0002,  ..., 0.0000, 0.0361, 0.0000],
        [0.0155, 0.0265, 0.0002,  ..., 0.0000, 0.0361, 0.0000],
        [0.0155, 0.0265, 0.0002,  ..., 0.0000, 0.0361, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(453002.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4820.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.5071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.3479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.7716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0139],
        [ 0.0069],
        [ 0.0249],
        ...,
        [-0.2182],
        [-0.2174],
        [-0.2171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59495.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366727.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366727.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(757.1310, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4608, device='cuda:0')



h[100].sum tensor(-9.1973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6801, device='cuda:0')



h[200].sum tensor(-29.2754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0167e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49222.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0319, 0.0132, 0.0001,  ..., 0.0000, 0.0194, 0.0000],
        [0.0241, 0.0170, 0.0006,  ..., 0.0000, 0.0241, 0.0000],
        [0.0354, 0.0105, 0.0012,  ..., 0.0000, 0.0143, 0.0000],
        ...,
        [0.0155, 0.0265, 0.0002,  ..., 0.0000, 0.0361, 0.0000],
        [0.0155, 0.0265, 0.0002,  ..., 0.0000, 0.0361, 0.0000],
        [0.0155, 0.0265, 0.0002,  ..., 0.0000, 0.0361, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402654.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3853.0811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-175.5429, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.5400, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(53.4955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0330],
        [-0.0169],
        [ 0.0069],
        ...,
        [-0.2173],
        [-0.2161],
        [-0.2133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70358.0859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366727.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366739.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7409e-03,  5.0024e-03,  1.0796e-02,  ...,  9.2191e-05,
         -5.2111e-03, -8.5126e-03],
        [ 4.5911e-03,  3.7009e-03,  8.6952e-03,  ..., -2.0611e-04,
         -4.1443e-03, -6.7698e-03],
        [ 1.0207e-02,  1.0058e-02,  1.8958e-02,  ...,  1.2509e-03,
         -9.3554e-03, -1.5282e-02],
        ...,
        [ 1.2461e-04, -1.3549e-03,  5.3321e-04,  ..., -1.3648e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2461e-04, -1.3549e-03,  5.3321e-04,  ..., -1.3648e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2461e-04, -1.3549e-03,  5.3321e-04,  ..., -1.3648e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(663.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.6788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.8136, device='cuda:0')



h[100].sum tensor(-7.7447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3192, device='cuda:0')



h[200].sum tensor(-24.6873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.5319e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0173, 0.0162, 0.0327,  ..., 0.0018, 0.0000, 0.0000],
        [0.0371, 0.0360, 0.0690,  ..., 0.0040, 0.0000, 0.0000],
        [0.0172, 0.0149, 0.0327,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45990.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.7292e-02, 0.0000e+00, 5.8726e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0419e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.1570e-02, 0.0000e+00, 5.3779e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5706e-02, 2.6950e-02, 8.3772e-05,  ..., 0.0000e+00, 3.6156e-02,
         0.0000e+00],
        [1.5703e-02, 2.6945e-02, 8.3402e-05,  ..., 0.0000e+00, 3.6149e-02,
         0.0000e+00],
        [1.5700e-02, 2.6940e-02, 8.3238e-05,  ..., 0.0000e+00, 3.6142e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(392138.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3639.0769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-149.6648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(58.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(50.5972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0938],
        [ 0.0993],
        [ 0.0867],
        ...,
        [-0.2176],
        [-0.2133],
        [-0.2019]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67648.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366739.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366751.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0063,  0.0056,  0.0118,  ...,  0.0002, -0.0057, -0.0093],
        ...,
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(999.9897, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.4505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0990, device='cuda:0')



h[100].sum tensor(-12.2372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.7084, device='cuda:0')



h[200].sum tensor(-39.0645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3649e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8658e-02, 1.9144e-02, 3.5287e-02,  ..., 3.3346e-03, 0.0000e+00,
         0.0000e+00],
        [6.7059e-03, 5.6170e-03, 1.3438e-02,  ..., 2.3596e-04, 0.0000e+00,
         0.0000e+00],
        [1.1211e-02, 9.3555e-03, 2.1672e-02,  ..., 9.9574e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.4381e-04, 0.0000e+00, 2.1830e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.4373e-04, 0.0000e+00, 2.1827e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.4366e-04, 0.0000e+00, 2.1824e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57675.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.3848e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.1824e-02, 0.0000e+00, 9.3585e-05,  ..., 0.0000e+00, 5.8898e-04,
         0.0000e+00],
        [6.9192e-02, 0.0000e+00, 9.3519e-05,  ..., 0.0000e+00, 5.8856e-04,
         0.0000e+00],
        ...,
        [1.5877e-02, 2.7314e-02, 5.5812e-05,  ..., 0.0000e+00, 3.6140e-02,
         0.0000e+00],
        [1.5875e-02, 2.7309e-02, 5.5445e-05,  ..., 0.0000e+00, 3.6133e-02,
         0.0000e+00],
        [1.5872e-02, 2.7304e-02, 5.5295e-05,  ..., 0.0000e+00, 3.6126e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447811.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4728.2998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-235.0553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6.8771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(166.8906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(50.6223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0331],
        [ 0.0526],
        [ 0.0733],
        ...,
        [-0.2225],
        [-0.2216],
        [-0.2213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70171.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0000],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366751.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366763.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0066,  0.0059,  0.0123,  ...,  0.0003, -0.0060, -0.0097],
        ...,
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1079.8420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.6134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5188, device='cuda:0')



h[100].sum tensor(-13.1693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.9745, device='cuda:0')



h[200].sum tensor(-42.1008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4526e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.8292e-04, 0.0000e+00, 2.1898e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.0216e-03, 5.9155e-03, 1.3963e-02,  ..., 3.0799e-04, 0.0000e+00,
         0.0000e+00],
        [5.8385e-03, 4.5774e-03, 1.1798e-02,  ..., 1.5274e-06, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.8753e-04, 0.0000e+00, 2.2071e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8745e-04, 0.0000e+00, 2.2068e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8737e-04, 0.0000e+00, 2.2065e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62204.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0215, 0.0209, 0.0012,  ..., 0.0000, 0.0265, 0.0000],
        [0.0327, 0.0115, 0.0017,  ..., 0.0000, 0.0143, 0.0000],
        [0.0416, 0.0058, 0.0024,  ..., 0.0000, 0.0072, 0.0000],
        ...,
        [0.0160, 0.0277, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0160, 0.0277, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0160, 0.0277, 0.0000,  ..., 0.0000, 0.0362, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476047.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5257.9639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-264.2573, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21.2837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(207.2438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(48.5535, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0809],
        [ 0.0006],
        [ 0.0742],
        ...,
        [-0.2244],
        [-0.2235],
        [-0.2232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-64669.1445, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366763.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366774.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0001, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1285.3794, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.6449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9591, device='cuda:0')



h[100].sum tensor(-15.8231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.9345, device='cuda:0')



h[200].sum tensor(-50.6585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7269e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67211.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0160, 0.0279, 0.0000,  ..., 0.0000, 0.0359, 0.0000],
        [0.0160, 0.0279, 0.0000,  ..., 0.0000, 0.0359, 0.0000],
        [0.0160, 0.0279, 0.0000,  ..., 0.0000, 0.0359, 0.0000],
        ...,
        [0.0161, 0.0282, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0161, 0.0281, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0161, 0.0281, 0.0000,  ..., 0.0000, 0.0362, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(490988.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5504.7964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-296.4585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22.0044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.3856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.5025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3025],
        [-0.2971],
        [-0.2838],
        ...,
        [-0.2250],
        [-0.2248],
        [-0.2247]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61245.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366774.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366785.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4404e-03,  4.6335e-03,  1.0217e-02,  ...,  1.4690e-05,
         -4.8878e-03, -8.0000e-03],
        [ 1.4535e-04, -1.3590e-03,  5.3038e-04,  ..., -1.3584e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4535e-04, -1.3590e-03,  5.3038e-04,  ..., -1.3584e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.4535e-04, -1.3590e-03,  5.3038e-04,  ..., -1.3584e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4535e-04, -1.3590e-03,  5.3038e-04,  ..., -1.3584e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4535e-04, -1.3590e-03,  5.3038e-04,  ..., -1.3584e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(879.1984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2807, device='cuda:0')



h[100].sum tensor(-10.1567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.3031, device='cuda:0')



h[200].sum tensor(-32.5646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1291e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8288e-02, 1.7317e-02, 3.4514e-02,  ..., 1.8714e-03, 0.0000e+00,
         0.0000e+00],
        [5.8832e-03, 4.6390e-03, 1.1821e-02,  ..., 1.4707e-05, 0.0000e+00,
         0.0000e+00],
        [5.8129e-04, 0.0000e+00, 2.1212e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.8693e-04, 0.0000e+00, 2.1418e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8685e-04, 0.0000e+00, 2.1415e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8677e-04, 0.0000e+00, 2.1412e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52639.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0813, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0493, 0.0080, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0293, 0.0147, 0.0000,  ..., 0.0000, 0.0190, 0.0000],
        ...,
        [0.0161, 0.0287, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0161, 0.0287, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0161, 0.0286, 0.0000,  ..., 0.0000, 0.0364, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424305.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4186.0732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-190.4008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30.0059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.2765, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.1235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0792],
        [ 0.0243],
        [-0.0470],
        ...,
        [-0.2308],
        [-0.2299],
        [-0.2296]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70024.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366785.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366797.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0309,  0.0335,  0.0568,  ...,  0.0066, -0.0284, -0.0465],
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0184,  0.0193,  0.0339,  ...,  0.0034, -0.0168, -0.0275],
        ...,
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(996.2061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5619, device='cuda:0')



h[100].sum tensor(-11.5551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.3375, device='cuda:0')



h[200].sum tensor(-37.1022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2700e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0548, 0.0573, 0.1013,  ..., 0.0100, 0.0000, 0.0000],
        [0.0795, 0.0838, 0.1465,  ..., 0.0150, 0.0000, 0.0000],
        [0.0155, 0.0155, 0.0294,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56913.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2621, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2138, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0162, 0.0289, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0162, 0.0289, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0162, 0.0289, 0.0000,  ..., 0.0000, 0.0364, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451496.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4625.6807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-221.6322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48.6611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.8118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.2238, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0397],
        [-0.0139],
        [-0.0197],
        ...,
        [-0.2322],
        [-0.2314],
        [-0.2311]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72935.1484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366797.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366808.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0088,  0.0084,  0.0163,  ...,  0.0009, -0.0079, -0.0130],
        ...,
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0014,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1157.4705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.7484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8963, device='cuda:0')



h[100].sum tensor(-13.5200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.3112, device='cuda:0')



h[200].sum tensor(-43.4744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4760e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0092, 0.0084, 0.0179,  ..., 0.0009, 0.0000, 0.0000],
        [0.0306, 0.0312, 0.0570,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64492.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0109, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0964, 0.0051, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.1933, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0162, 0.0292, 0.0000,  ..., 0.0000, 0.0363, 0.0000],
        [0.0162, 0.0292, 0.0000,  ..., 0.0000, 0.0363, 0.0000],
        [0.0162, 0.0292, 0.0000,  ..., 0.0000, 0.0363, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488778., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5346.6689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-274.7608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48.9777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(225.2698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(37.2041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0471],
        [-0.0983],
        [-0.1844],
        ...,
        [-0.2334],
        [-0.2325],
        [-0.2322]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67898., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [0.9999],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366808.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366819.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(717.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4663, device='cuda:0')



h[100].sum tensor(-7.4202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.0095, device='cuda:0')



h[200].sum tensor(-23.8950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.3174e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0193, 0.0198, 0.0363,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46090.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0202, 0.0249, 0.0000,  ..., 0.0000, 0.0310, 0.0000],
        [0.0335, 0.0145, 0.0000,  ..., 0.0000, 0.0180, 0.0000],
        [0.0764, 0.0028, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        ...,
        [0.0162, 0.0294, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0162, 0.0293, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0162, 0.0293, 0.0000,  ..., 0.0000, 0.0362, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(399272.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3618.7156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-144.5681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(58.5206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(33.0077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1862],
        [-0.1008],
        [-0.0100],
        ...,
        [-0.2075],
        [-0.2257],
        [-0.2310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73999.8828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366819.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(480.1327, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366831.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6140e-04, -1.3489e-03,  5.2339e-04,  ..., -1.3527e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.6140e-04, -1.3489e-03,  5.2339e-04,  ..., -1.3527e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.3992e-03,  4.5763e-03,  1.0111e-02,  ...,  5.0243e-06,
         -4.8089e-03, -7.8863e-03],
        ...,
        [ 1.6140e-04, -1.3489e-03,  5.2339e-04,  ..., -1.3527e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.6140e-04, -1.3489e-03,  5.2339e-04,  ..., -1.3527e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.6140e-04, -1.3489e-03,  5.2339e-04,  ..., -1.3527e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(769.5856, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.5583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3227, device='cuda:0')



h[100].sum tensor(-7.9412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7733, device='cuda:0')



h[200].sum tensor(-25.6103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8464e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.4641e-04, 0.0000e+00, 2.0962e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8838e-03, 4.5761e-03, 1.1683e-02,  ..., 5.0241e-06, 0.0000e+00,
         0.0000e+00],
        [4.9215e-03, 3.4885e-03, 9.9205e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.5210e-04, 0.0000e+00, 2.1147e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5201e-04, 0.0000e+00, 2.1144e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5193e-04, 0.0000e+00, 2.1141e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47412.9492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0198, 0.0244, 0.0000,  ..., 0.0000, 0.0297, 0.0000],
        [0.0280, 0.0153, 0.0000,  ..., 0.0000, 0.0187, 0.0000],
        [0.0318, 0.0096, 0.0003,  ..., 0.0000, 0.0108, 0.0000],
        ...,
        [0.0163, 0.0295, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0163, 0.0295, 0.0000,  ..., 0.0000, 0.0362, 0.0000],
        [0.0166, 0.0290, 0.0000,  ..., 0.0000, 0.0355, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405423.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3713.7017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-152.5148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(67.0873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(69.6205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(29.5675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1394],
        [-0.0747],
        [ 0.0004],
        ...,
        [-0.2288],
        [-0.2174],
        [-0.2021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69644.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366831.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366843.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0051,  0.0109,  ...,  0.0001, -0.0052, -0.0085],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0058,  0.0051,  0.0109,  ...,  0.0001, -0.0052, -0.0085],
        ...,
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1294.5934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.6259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0110, device='cuda:0')



h[100].sum tensor(-14.7789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.0890, device='cuda:0')



h[200].sum tensor(-47.7317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6683e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0182, 0.0171, 0.0341,  ..., 0.0018, 0.0000, 0.0000],
        [0.0296, 0.0274, 0.0551,  ..., 0.0021, 0.0000, 0.0000],
        [0.0094, 0.0086, 0.0182,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67878.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0852, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0831, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0560, 0.0019, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        ...,
        [0.0164, 0.0297, 0.0000,  ..., 0.0000, 0.0363, 0.0000],
        [0.0164, 0.0297, 0.0000,  ..., 0.0000, 0.0363, 0.0000],
        [0.0164, 0.0297, 0.0000,  ..., 0.0000, 0.0363, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501960.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5525.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-293.1619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44.0243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.7195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(25.2177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1370],
        [ 0.1259],
        [ 0.0959],
        ...,
        [-0.2367],
        [-0.2358],
        [-0.2355]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-49364.0664, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366843.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366843.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        ...,
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000],
        [ 0.0002, -0.0013,  0.0005,  ..., -0.0014,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1091.5037, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.5069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8802, device='cuda:0')



h[100].sum tensor(-12.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.5132, device='cuda:0')



h[200].sum tensor(-39.0415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3514e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0054, 0.0041, 0.0108,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58693.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0213, 0.0222, 0.0008,  ..., 0.0000, 0.0241, 0.0000],
        [0.0247, 0.0187, 0.0002,  ..., 0.0000, 0.0210, 0.0000],
        [0.0373, 0.0109, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        ...,
        [0.0176, 0.0283, 0.0000,  ..., 0.0000, 0.0345, 0.0000],
        [0.0164, 0.0297, 0.0000,  ..., 0.0000, 0.0363, 0.0000],
        [0.0164, 0.0297, 0.0000,  ..., 0.0000, 0.0363, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458105.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4658.1943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-230.9123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(68.9143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(170.1521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(27.0986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0231],
        [ 0.0232],
        [ 0.0386],
        ...,
        [-0.2096],
        [-0.2270],
        [-0.2338]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65896.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366843.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366854.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0081,  0.0077,  0.0151,  ...,  0.0007, -0.0073, -0.0120],
        [ 0.0070,  0.0064,  0.0130,  ...,  0.0004, -0.0063, -0.0103],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(707.7518, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.6164, device='cuda:0')



h[100].sum tensor(-6.9672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2515, device='cuda:0')



h[200].sum tensor(-22.5352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.7925e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0077, 0.0166,  ..., 0.0007, 0.0000, 0.0000],
        [0.0195, 0.0188, 0.0367,  ..., 0.0022, 0.0000, 0.0000],
        [0.0474, 0.0476, 0.0877,  ..., 0.0067, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45292.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0732, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1488, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0164, 0.0302, 0.0000,  ..., 0.0000, 0.0365, 0.0000],
        [0.0164, 0.0302, 0.0000,  ..., 0.0000, 0.0365, 0.0000],
        [0.0164, 0.0302, 0.0000,  ..., 0.0000, 0.0365, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(400159.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3541.4121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-135.2233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55.8992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(46.1395, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(23.0379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0768],
        [ 0.0849],
        [ 0.0869],
        ...,
        [-0.2409],
        [-0.2397],
        [-0.2377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75623.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0004],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366854.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366866.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0104,  0.0102,  0.0192,  ...,  0.0013, -0.0093, -0.0153],
        [ 0.0175,  0.0183,  0.0322,  ...,  0.0031, -0.0159, -0.0260],
        [ 0.0182,  0.0191,  0.0335,  ...,  0.0033, -0.0165, -0.0271],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(866.8353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5001, device='cuda:0')



h[100].sum tensor(-9.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7151, device='cuda:0')



h[200].sum tensor(-29.1556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0191e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0545, 0.0558, 0.1007,  ..., 0.0088, 0.0000, 0.0000],
        [0.0724, 0.0760, 0.1334,  ..., 0.0132, 0.0000, 0.0000],
        [0.0640, 0.0665, 0.1180,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51801.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2267, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2437, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2076, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0164, 0.0306, 0.0000,  ..., 0.0000, 0.0367, 0.0000],
        [0.0164, 0.0306, 0.0000,  ..., 0.0000, 0.0367, 0.0000],
        [0.0164, 0.0306, 0.0000,  ..., 0.0000, 0.0367, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(437769.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4232.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-180.9928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34.4317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.5484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(20.1166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0805],
        [ 0.0744],
        [ 0.0725],
        ...,
        [-0.2439],
        [-0.2430],
        [-0.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74384.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366866.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366877.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0133,  0.0136,  0.0246,  ...,  0.0021, -0.0120, -0.0198],
        [ 0.0079,  0.0075,  0.0146,  ...,  0.0007, -0.0071, -0.0116],
        [ 0.0275,  0.0297,  0.0506,  ...,  0.0058, -0.0250, -0.0411],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(909.5821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3349, device='cuda:0')



h[100].sum tensor(-9.4856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4597, device='cuda:0')



h[200].sum tensor(-30.7713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0707e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0395, 0.0388, 0.0732,  ..., 0.0047, 0.0000, 0.0000],
        [0.0691, 0.0723, 0.1273,  ..., 0.0124, 0.0000, 0.0000],
        [0.0307, 0.0301, 0.0570,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52014.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1237, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1701, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1381, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0165, 0.0310, 0.0000,  ..., 0.0000, 0.0369, 0.0000],
        [0.0165, 0.0309, 0.0000,  ..., 0.0000, 0.0369, 0.0000],
        [0.0165, 0.0309, 0.0000,  ..., 0.0000, 0.0369, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431971.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4071.9421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-180.0447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19.5514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(101.3300, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(16.8079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0091],
        [ 0.0243],
        [ 0.0480],
        ...,
        [-0.2469],
        [-0.2460],
        [-0.2457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69327.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366877.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366889.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(857.7831, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.0330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8155, device='cuda:0')



h[100].sum tensor(-8.6675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1046, device='cuda:0')



h[200].sum tensor(-28.1587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7684e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0057, 0.0131,  ..., 0.0003, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49654.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0392, 0.0100, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0245, 0.0207, 0.0000,  ..., 0.0000, 0.0226, 0.0000],
        [0.0177, 0.0290, 0.0000,  ..., 0.0000, 0.0334, 0.0000],
        ...,
        [0.0163, 0.0314, 0.0000,  ..., 0.0000, 0.0369, 0.0000],
        [0.0163, 0.0314, 0.0000,  ..., 0.0000, 0.0369, 0.0000],
        [0.0163, 0.0314, 0.0000,  ..., 0.0000, 0.0369, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420471.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3775.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-162.3878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7.8124, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(77.7539, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(14.3542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0533],
        [-0.0022],
        [-0.0546],
        ...,
        [-0.2496],
        [-0.2487],
        [-0.2484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70318.5078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366889.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366902.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044,  0.0036,  0.0083,  ..., -0.0002, -0.0039, -0.0064],
        [ 0.0044,  0.0036,  0.0083,  ..., -0.0002, -0.0039, -0.0064],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1518.6230, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.2546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1884, device='cuda:0')



h[100].sum tensor(-17.0121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8145, device='cuda:0')



h[200].sum tensor(-55.3501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9263e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0279, 0.0258, 0.0520,  ..., 0.0022, 0.0000, 0.0000],
        [0.0167, 0.0144, 0.0315,  ..., 0.0007, 0.0000, 0.0000],
        [0.0328, 0.0313, 0.0610,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75546.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0948, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1117, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0161, 0.0318, 0.0000,  ..., 0.0000, 0.0369, 0.0000],
        [0.0161, 0.0318, 0.0000,  ..., 0.0000, 0.0369, 0.0000],
        [0.0161, 0.0318, 0.0000,  ..., 0.0000, 0.0369, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556660.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6274.2017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-351.3732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5.1469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(314.4711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(17.7843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1384],
        [ 0.1375],
        [ 0.1264],
        ...,
        [-0.2520],
        [-0.2511],
        [-0.2508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75052.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366902.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366914.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1102.3293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1424, device='cuda:0')



h[100].sum tensor(-11.5194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.8553, device='cuda:0')



h[200].sum tensor(-37.5347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3059e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0196, 0.0202, 0.0367,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57992.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0283, 0.0169, 0.0000,  ..., 0.0000, 0.0192, 0.0000],
        [0.0442, 0.0114, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0889, 0.0032, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        ...,
        [0.0162, 0.0320, 0.0000,  ..., 0.0000, 0.0371, 0.0000],
        [0.0162, 0.0320, 0.0000,  ..., 0.0000, 0.0371, 0.0000],
        [0.0162, 0.0320, 0.0000,  ..., 0.0000, 0.0371, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460744.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4426.5762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.1591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3.9376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.1812, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.2830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0763],
        [ 0.0800],
        [ 0.0907],
        ...,
        [-0.2348],
        [-0.2413],
        [-0.2467]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69402.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366914.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366927.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0118,  0.0118,  0.0217,  ...,  0.0017, -0.0106, -0.0174],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1300.6392, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.5318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3174, device='cuda:0')



h[100].sum tensor(-13.9276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.5786, device='cuda:0')



h[200].sum tensor(-45.4486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5637e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0195, 0.0175, 0.0366,  ..., 0.0010, 0.0000, 0.0000],
        [0.0171, 0.0160, 0.0321,  ..., 0.0017, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67331.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0268, 0.0000,  ..., 0.0000, 0.0309, 0.0000],
        [0.0161, 0.0318, 0.0000,  ..., 0.0000, 0.0370, 0.0000],
        [0.0161, 0.0318, 0.0000,  ..., 0.0000, 0.0370, 0.0000],
        ...,
        [0.0948, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0745, 0.0031, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0355, 0.0154, 0.0000,  ..., 0.0000, 0.0178, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516297.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5459.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-289.2129, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4.1542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(235.3905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(14.0953, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0769],
        [-0.1400],
        [-0.1804],
        ...,
        [ 0.0699],
        [ 0.0149],
        [-0.0732]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67775.4297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366927.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366927.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4382e-03,  3.5483e-03,  8.3411e-03,  ..., -2.2619e-04,
         -3.9075e-03, -6.4333e-03],
        [ 5.6535e-03,  4.9222e-03,  1.0567e-02,  ...,  8.8660e-05,
         -5.0113e-03, -8.2506e-03],
        [ 1.3583e-04, -1.3153e-03,  4.6088e-04,  ..., -1.3408e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.3583e-04, -1.3153e-03,  4.6088e-04,  ..., -1.3408e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3583e-04, -1.3153e-03,  4.6088e-04,  ..., -1.3408e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3583e-04, -1.3153e-03,  4.6088e-04,  ..., -1.3408e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1107.5802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2608, device='cuda:0')



h[100].sum tensor(-11.4640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.9608, device='cuda:0')



h[200].sum tensor(-37.4093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3132e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.3259e-02, 4.3020e-02, 8.0084e-02,  ..., 5.6963e-03, 0.0000e+00,
         0.0000e+00],
        [1.8072e-02, 1.7181e-02, 3.3950e-02,  ..., 2.0824e-03, 0.0000e+00,
         0.0000e+00],
        [6.0599e-03, 4.9213e-03, 1.1948e-02,  ..., 8.8644e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.4942e-04, 0.0000e+00, 1.8642e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.4934e-04, 0.0000e+00, 1.8639e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.4926e-04, 0.0000e+00, 1.8637e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57515.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1279, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0897, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0527, 0.0093, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        ...,
        [0.0163, 0.0322, 0.0000,  ..., 0.0000, 0.0374, 0.0000],
        [0.0163, 0.0322, 0.0000,  ..., 0.0000, 0.0374, 0.0000],
        [0.0163, 0.0321, 0.0000,  ..., 0.0000, 0.0374, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460076.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4373.7549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-218.0484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4.1886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.3566, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(12.2879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1082],
        [ 0.0856],
        [ 0.0303],
        ...,
        [-0.2567],
        [-0.2557],
        [-0.2554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-68613.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366927.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366927.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0044,  0.0036,  0.0084,  ..., -0.0002, -0.0039, -0.0064],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1352.4685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.8100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7050, device='cuda:0')



h[100].sum tensor(-14.5890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.8161, device='cuda:0')



h[200].sum tensor(-47.6068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6494e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0026, 0.0083,  ..., 0.0000, 0.0000, 0.0000],
        [0.0187, 0.0165, 0.0350,  ..., 0.0013, 0.0000, 0.0000],
        [0.0318, 0.0314, 0.0591,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68567., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0471, 0.0094, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0822, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1126, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0163, 0.0322, 0.0000,  ..., 0.0000, 0.0374, 0.0000],
        [0.0163, 0.0322, 0.0000,  ..., 0.0000, 0.0374, 0.0000],
        [0.0163, 0.0321, 0.0000,  ..., 0.0000, 0.0374, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516943.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5447.7842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-295.5402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5.0183, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.0000, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(11.6859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0316],
        [ 0.0934],
        [ 0.1309],
        ...,
        [-0.2567],
        [-0.2557],
        [-0.2554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61914.5078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366927.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366939.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(934.4883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.2193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8340, device='cuda:0')



h[100].sum tensor(-9.1266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0129, device='cuda:0')



h[200].sum tensor(-29.8263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0397e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52342.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0163, 0.0318, 0.0000,  ..., 0.0000, 0.0372, 0.0000],
        [0.0163, 0.0318, 0.0000,  ..., 0.0000, 0.0372, 0.0000],
        [0.0163, 0.0318, 0.0000,  ..., 0.0000, 0.0372, 0.0000],
        ...,
        [0.0165, 0.0322, 0.0000,  ..., 0.0000, 0.0376, 0.0000],
        [0.0165, 0.0322, 0.0000,  ..., 0.0000, 0.0376, 0.0000],
        [0.0165, 0.0322, 0.0000,  ..., 0.0000, 0.0376, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440507.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4042.7842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-183.9365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7.3731, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.7614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(12.7744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3176],
        [-0.2825],
        [-0.2293],
        ...,
        [-0.2589],
        [-0.2579],
        [-0.2576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78697.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366939.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366951.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3234e-04, -1.3153e-03,  4.7143e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3234e-04, -1.3153e-03,  4.7143e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.3677e-03,  4.6026e-03,  1.0062e-02,  ...,  1.7208e-05,
         -4.7423e-03, -7.8155e-03],
        ...,
        [ 1.3234e-04, -1.3153e-03,  4.7143e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3234e-04, -1.3153e-03,  4.7143e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3234e-04, -1.3153e-03,  4.7143e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1108.5698, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7527, device='cuda:0')



h[100].sum tensor(-11.1533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.5077, device='cuda:0')



h[200].sum tensor(-36.5038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2818e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.3005e-04, 0.0000e+00, 1.8882e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.7649e-03, 4.6025e-03, 1.1477e-02,  ..., 1.7207e-05, 0.0000e+00,
         0.0000e+00],
        [8.3292e-03, 7.5017e-03, 1.6174e-02,  ..., 6.8169e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.3545e-04, 0.0000e+00, 1.9074e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3538e-04, 0.0000e+00, 1.9071e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3530e-04, 0.0000e+00, 1.9069e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58389.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0231, 0.0000,  ..., 0.0000, 0.0267, 0.0000],
        [0.0390, 0.0108, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0539, 0.0028, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        ...,
        [0.0167, 0.0321, 0.0000,  ..., 0.0000, 0.0377, 0.0000],
        [0.0167, 0.0321, 0.0000,  ..., 0.0000, 0.0377, 0.0000],
        [0.0166, 0.0321, 0.0000,  ..., 0.0000, 0.0376, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467905.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4593.3159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.3051, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4.4108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.2838, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.2926, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1561],
        [-0.0576],
        [ 0.0294],
        ...,
        [-0.2603],
        [-0.2593],
        [-0.2590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-63039.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9998],
        [0.9996],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366951.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366963.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1055.7693, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1563, device='cuda:0')



h[100].sum tensor(-10.2881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.0840, device='cuda:0')



h[200].sum tensor(-33.7220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1832e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.3460e-04, 0.0000e+00, 1.9130e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3434e-04, 0.0000e+00, 1.9120e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0334e-02, 8.4486e-03, 1.9865e-02,  ..., 5.9356e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.4012e-04, 0.0000e+00, 1.9327e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.4005e-04, 0.0000e+00, 1.9325e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3997e-04, 0.0000e+00, 1.9322e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54245.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0191, 0.0281, 0.0000,  ..., 0.0000, 0.0327, 0.0000],
        [0.0278, 0.0166, 0.0000,  ..., 0.0000, 0.0193, 0.0000],
        [0.0496, 0.0058, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        ...,
        [0.0167, 0.0321, 0.0000,  ..., 0.0000, 0.0377, 0.0000],
        [0.0167, 0.0321, 0.0000,  ..., 0.0000, 0.0376, 0.0000],
        [0.0167, 0.0321, 0.0000,  ..., 0.0000, 0.0376, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445491.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4178.0249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-195.0142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3.8603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.9463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.3217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1738],
        [-0.0791],
        [ 0.0145],
        ...,
        [-0.2573],
        [-0.2552],
        [-0.2544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69579.4297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366963.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366975.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0281,  0.0303,  0.0518,  ...,  0.0059, -0.0253, -0.0417],
        [ 0.0071,  0.0066,  0.0133,  ...,  0.0005, -0.0063, -0.0105],
        [ 0.0229,  0.0244,  0.0422,  ...,  0.0046, -0.0206, -0.0339],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1566.6501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.6014, device='cuda:0')



h[100].sum tensor(-16.4887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.2910, device='cuda:0')



h[200].sum tensor(-54.1267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8901e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0508, 0.0516, 0.0940,  ..., 0.0078, 0.0000, 0.0000],
        [0.1134, 0.1223, 0.2087,  ..., 0.0239, 0.0000, 0.0000],
        [0.0720, 0.0756, 0.1329,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74123.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2484, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3384, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3088, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0169, 0.0320, 0.0000,  ..., 0.0000, 0.0377, 0.0000],
        [0.0169, 0.0320, 0.0000,  ..., 0.0000, 0.0377, 0.0000],
        [0.0169, 0.0320, 0.0000,  ..., 0.0000, 0.0377, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542613.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6029.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-336.7595, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3.6702, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.8111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.1236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0930],
        [ 0.0661],
        [ 0.0426],
        ...,
        [-0.2604],
        [-0.2572],
        [-0.2546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58126.1758, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366975.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366975.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0163,  0.0170,  0.0301,  ...,  0.0029, -0.0146, -0.0241],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(993.7601, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5028, device='cuda:0')



h[100].sum tensor(-9.3309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6093, device='cuda:0')



h[200].sum tensor(-30.6299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0811e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0170, 0.0316,  ..., 0.0029, 0.0000, 0.0000],
        [0.0137, 0.0136, 0.0261,  ..., 0.0021, 0.0000, 0.0000],
        [0.0593, 0.0612, 0.1096,  ..., 0.0099, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52827.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0637, 0.0041, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0764, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0169, 0.0320, 0.0000,  ..., 0.0000, 0.0377, 0.0000],
        [0.0169, 0.0320, 0.0000,  ..., 0.0000, 0.0377, 0.0000],
        [0.0169, 0.0320, 0.0000,  ..., 0.0000, 0.0377, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442548.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4138.0884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-185.7201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4.7648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(101.0742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.7294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1080],
        [-0.0627],
        [-0.0235],
        ...,
        [-0.2632],
        [-0.2622],
        [-0.2619]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73239.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366975.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366988.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1029.0369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8042, device='cuda:0')



h[100].sum tensor(-9.6175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.8781, device='cuda:0')



h[200].sum tensor(-31.6179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0997e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52768.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0166, 0.0318, 0.0000,  ..., 0.0000, 0.0373, 0.0000],
        [0.0168, 0.0315, 0.0000,  ..., 0.0000, 0.0366, 0.0000],
        [0.0173, 0.0307, 0.0000,  ..., 0.0000, 0.0350, 0.0000],
        ...,
        [0.0168, 0.0322, 0.0000,  ..., 0.0000, 0.0378, 0.0000],
        [0.0168, 0.0322, 0.0000,  ..., 0.0000, 0.0378, 0.0000],
        [0.0168, 0.0322, 0.0000,  ..., 0.0000, 0.0378, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440681.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4044.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-183.6717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.5253, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.1528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.2696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2021],
        [-0.2182],
        [-0.2147],
        ...,
        [-0.2573],
        [-0.2627],
        [-0.2635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69956.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366988.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367000.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0044,  0.0036,  0.0084,  ..., -0.0002, -0.0039, -0.0064],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(969.9637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.9915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4410, device='cuda:0')



h[100].sum tensor(-8.6985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6625, device='cuda:0')



h[200].sum tensor(-28.6394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0155e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0083, 0.0063, 0.0163,  ..., 0.0000, 0.0000, 0.0000],
        [0.0116, 0.0087, 0.0224,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49949.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0258, 0.0190, 0.0000,  ..., 0.0000, 0.0218, 0.0000],
        [0.0436, 0.0075, 0.0000,  ..., 0.0000, 0.0085, 0.0000],
        [0.0548, 0.0013, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        ...,
        [0.0168, 0.0323, 0.0000,  ..., 0.0000, 0.0378, 0.0000],
        [0.0168, 0.0323, 0.0000,  ..., 0.0000, 0.0378, 0.0000],
        [0.0168, 0.0323, 0.0000,  ..., 0.0000, 0.0378, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427679.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3807.7407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-169.7953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(76.9738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.0743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0564],
        [ 0.0308],
        [ 0.0912],
        ...,
        [-0.2661],
        [-0.2651],
        [-0.2648]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83860.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367000.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367013.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1602e-04, -1.2988e-03,  5.0026e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1602e-04, -1.2988e-03,  5.0026e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.3126e-03,  4.5755e-03,  1.0022e-02,  ...,  9.8513e-06,
         -4.6764e-03, -7.7260e-03],
        ...,
        [ 1.1602e-04, -1.2988e-03,  5.0026e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4035e-02,  1.4436e-02,  2.6006e-02,  ...,  2.2694e-03,
         -1.2526e-02, -2.0694e-02],
        [ 1.1602e-04, -1.2988e-03,  5.0026e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1247.6401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.3035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3805, device='cuda:0')



h[100].sum tensor(-11.9019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.9594, device='cuda:0')



h[200].sum tensor(-39.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3823e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.6470e-04, 0.0000e+00, 2.0037e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.6612e-03, 4.5757e-03, 1.1525e-02,  ..., 9.8517e-06, 0.0000e+00,
         0.0000e+00],
        [1.7767e-02, 1.6962e-02, 3.3707e-02,  ..., 1.8099e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.4555e-02, 1.4608e-02, 2.7836e-02,  ..., 2.2965e-03, 0.0000e+00,
         0.0000e+00],
        [1.1970e-02, 1.1686e-02, 2.3098e-02,  ..., 1.6268e-03, 0.0000e+00,
         0.0000e+00],
        [5.1641e-02, 5.2589e-02, 9.5791e-02,  ..., 7.8469e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61130.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0382, 0.0069, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        [0.0518, 0.0064, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0778, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0574, 0.0055, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0686, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.1059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482841.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4829.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.5917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(174.6207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.3642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0976],
        [ 0.1173],
        [ 0.1326],
        ...,
        [-0.0801],
        [-0.0191],
        [ 0.0110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56297.8164, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367013.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(875.5322, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367025.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0001, -0.0013,  0.0005,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1088.5404, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4272, device='cuda:0')



h[100].sum tensor(-9.7998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4337, device='cuda:0')



h[200].sum tensor(-32.3619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1382e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56511.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0194, 0.0285, 0.0000,  ..., 0.0000, 0.0332, 0.0000],
        [0.0166, 0.0319, 0.0000,  ..., 0.0000, 0.0371, 0.0000],
        [0.0165, 0.0320, 0.0000,  ..., 0.0000, 0.0375, 0.0000],
        ...,
        [0.0167, 0.0324, 0.0000,  ..., 0.0000, 0.0379, 0.0000],
        [0.0167, 0.0324, 0.0000,  ..., 0.0000, 0.0379, 0.0000],
        [0.0167, 0.0324, 0.0000,  ..., 0.0000, 0.0379, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467325.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4510.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-217.8704, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.2486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.3580, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1969],
        [-0.2714],
        [-0.3222],
        ...,
        [-0.2684],
        [-0.2674],
        [-0.2671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74701.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [0.9999],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367025.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367037.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(982.1818, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6685, device='cuda:0')



h[100].sum tensor(-8.3748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9735, device='cuda:0')



h[200].sum tensor(-27.6977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6776e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.6128e-03, 4.6295e-03, 1.1560e-02,  ..., 1.8295e-05, 0.0000e+00,
         0.0000e+00],
        [3.8047e-04, 0.0000e+00, 1.9712e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0809e-02, 1.0508e-02, 2.1081e-02,  ..., 1.3667e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.8487e-04, 0.0000e+00, 1.9939e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.8481e-04, 0.0000e+00, 1.9937e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.8476e-04, 0.0000e+00, 1.9934e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51879.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0303, 0.0140, 0.0000,  ..., 0.0000, 0.0171, 0.0000],
        [0.0289, 0.0156, 0.0000,  ..., 0.0000, 0.0194, 0.0000],
        [0.0460, 0.0070, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        ...,
        [0.0167, 0.0326, 0.0000,  ..., 0.0000, 0.0382, 0.0000],
        [0.0167, 0.0326, 0.0000,  ..., 0.0000, 0.0382, 0.0000],
        [0.0167, 0.0326, 0.0000,  ..., 0.0000, 0.0382, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445169.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4050.7373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-183.9894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(92.9986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(14.3977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2018],
        [-0.2076],
        [-0.1869],
        ...,
        [-0.2708],
        [-0.2698],
        [-0.2694]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74730.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367037.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367037.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5032e-05, -1.2847e-03,  4.9234e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1233.5076, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.7698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3760, device='cuda:0')



h[100].sum tensor(-11.4341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.0635, device='cuda:0')



h[200].sum tensor(-37.8155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3203e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58332.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0174, 0.0309, 0.0000,  ..., 0.0000, 0.0360, 0.0000],
        [0.0178, 0.0302, 0.0000,  ..., 0.0000, 0.0347, 0.0000],
        [0.0218, 0.0242, 0.0000,  ..., 0.0000, 0.0272, 0.0000],
        ...,
        [0.0167, 0.0326, 0.0000,  ..., 0.0000, 0.0382, 0.0000],
        [0.0167, 0.0326, 0.0000,  ..., 0.0000, 0.0382, 0.0000],
        [0.0167, 0.0326, 0.0000,  ..., 0.0000, 0.0382, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465279.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4429.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-225.5922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.1261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(11.1758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1847],
        [-0.1310],
        [-0.0522],
        ...,
        [-0.2707],
        [-0.2697],
        [-0.2694]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57227.0859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9997],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367037.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367049.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7068e-05, -1.2581e-03,  4.5810e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7068e-05, -1.2581e-03,  4.5810e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7068e-05, -1.2581e-03,  4.5810e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.7068e-05, -1.2581e-03,  4.5810e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7068e-05, -1.2581e-03,  4.5810e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7068e-05, -1.2581e-03,  4.5810e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1052.7385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3351, device='cuda:0')



h[100].sum tensor(-9.1932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4598, device='cuda:0')



h[200].sum tensor(-30.4499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0707e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0457, 0.0475, 0.0849,  ..., 0.0077, 0.0000, 0.0000],
        [0.0077, 0.0070, 0.0153,  ..., 0.0006, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52574.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1657, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1060, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0808, 0.0038, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        ...,
        [0.0163, 0.0331, 0.0000,  ..., 0.0000, 0.0388, 0.0000],
        [0.0163, 0.0331, 0.0000,  ..., 0.0000, 0.0388, 0.0000],
        [0.0163, 0.0331, 0.0000,  ..., 0.0000, 0.0388, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(444238.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3880.4756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-185.9627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.6051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(15.4267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1041],
        [ 0.1013],
        [ 0.0941],
        ...,
        [-0.2765],
        [-0.2755],
        [-0.2751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74808.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367049.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367062.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.4102e-05, -1.2392e-03,  4.3500e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.4102e-05, -1.2392e-03,  4.3500e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.4102e-05, -1.2392e-03,  4.3500e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.4102e-05, -1.2392e-03,  4.3500e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.4102e-05, -1.2392e-03,  4.3500e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.4102e-05, -1.2392e-03,  4.3500e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1066.3809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.5332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5494, device='cuda:0')



h[100].sum tensor(-9.2703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6509, device='cuda:0')



h[200].sum tensor(-30.7514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0839e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54202.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0216, 0.0251, 0.0000,  ..., 0.0000, 0.0295, 0.0000],
        [0.0172, 0.0312, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0158, 0.0331, 0.0000,  ..., 0.0000, 0.0387, 0.0000],
        ...,
        [0.0160, 0.0336, 0.0000,  ..., 0.0000, 0.0392, 0.0000],
        [0.0160, 0.0335, 0.0000,  ..., 0.0000, 0.0392, 0.0000],
        [0.0160, 0.0335, 0.0000,  ..., 0.0000, 0.0391, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456978.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4054.7358, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-196.5667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.6304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(15.7185, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1914],
        [-0.2744],
        [-0.3345],
        ...,
        [-0.2802],
        [-0.2793],
        [-0.2789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74179.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367062.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367075.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5616e-05, -1.2305e-03,  4.2246e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5616e-05, -1.2305e-03,  4.2246e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5616e-05, -1.2305e-03,  4.2246e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.5616e-05, -1.2305e-03,  4.2246e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5616e-05, -1.2305e-03,  4.2246e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5616e-05, -1.2305e-03,  4.2246e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1103.1986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.9314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1198, device='cuda:0')



h[100].sum tensor(-9.5958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.1596, device='cuda:0')



h[200].sum tensor(-31.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1192e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53665.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0340, 0.0099, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0287, 0.0150, 0.0000,  ..., 0.0000, 0.0188, 0.0000],
        [0.0231, 0.0224, 0.0000,  ..., 0.0000, 0.0268, 0.0000],
        ...,
        [0.0159, 0.0338, 0.0000,  ..., 0.0000, 0.0394, 0.0000],
        [0.0159, 0.0338, 0.0000,  ..., 0.0000, 0.0394, 0.0000],
        [0.0159, 0.0338, 0.0000,  ..., 0.0000, 0.0394, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449531.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3836.0515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-188.6201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.0476, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(15.2380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0363],
        [ 0.0256],
        [ 0.0114],
        ...,
        [-0.2826],
        [-0.2817],
        [-0.2816]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74008.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0000],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367075.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0001],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367088.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3239e-05, -1.2319e-03,  4.2807e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.3239e-05, -1.2319e-03,  4.2807e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.3239e-05, -1.2319e-03,  4.2807e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.3239e-05, -1.2319e-03,  4.2807e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.3239e-05, -1.2319e-03,  4.2807e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.3239e-05, -1.2319e-03,  4.2807e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1099.5679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.3994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6594, device='cuda:0')



h[100].sum tensor(-9.3476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.7490, device='cuda:0')



h[200].sum tensor(-31.1013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0907e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52842.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0333, 0.0000,  ..., 0.0000, 0.0388, 0.0000],
        [0.0158, 0.0333, 0.0000,  ..., 0.0000, 0.0388, 0.0000],
        [0.0158, 0.0333, 0.0000,  ..., 0.0000, 0.0388, 0.0000],
        ...,
        [0.0160, 0.0337, 0.0000,  ..., 0.0000, 0.0394, 0.0000],
        [0.0160, 0.0337, 0.0000,  ..., 0.0000, 0.0394, 0.0000],
        [0.0160, 0.0337, 0.0000,  ..., 0.0000, 0.0393, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449406.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3838.0396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-187.9897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.7374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(18.2585, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2827],
        [-0.2930],
        [-0.2974],
        ...,
        [-0.2840],
        [-0.2829],
        [-0.2826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87990.4766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0001],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367088.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0001],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367101.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4359e-05, -1.2439e-03,  4.4771e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.4359e-05, -1.2439e-03,  4.4771e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.4359e-05, -1.2439e-03,  4.4771e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.4359e-05, -1.2439e-03,  4.4771e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.4359e-05, -1.2439e-03,  4.4771e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.4359e-05, -1.2439e-03,  4.4771e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1117.9910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6258, device='cuda:0')



h[100].sum tensor(-9.3206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.7190, device='cuda:0')



h[200].sum tensor(-31.0581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0887e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53916.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0190, 0.0290, 0.0000,  ..., 0.0000, 0.0342, 0.0000],
        [0.0296, 0.0157, 0.0000,  ..., 0.0000, 0.0198, 0.0000],
        [0.0419, 0.0064, 0.0000,  ..., 0.0000, 0.0081, 0.0000],
        ...,
        [0.0162, 0.0335, 0.0000,  ..., 0.0000, 0.0390, 0.0000],
        [0.0162, 0.0335, 0.0000,  ..., 0.0000, 0.0390, 0.0000],
        [0.0162, 0.0335, 0.0000,  ..., 0.0000, 0.0390, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(454773.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3990.2212, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-193.6662, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(106.9285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(16.1189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0219],
        [ 0.0160],
        [ 0.0522],
        ...,
        [-0.2833],
        [-0.2823],
        [-0.2819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73847.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0001],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367101.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0001],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367115.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6247e-05, -1.2583e-03,  4.7163e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6247e-05, -1.2583e-03,  4.7163e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6247e-05, -1.2583e-03,  4.7163e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.6247e-05, -1.2583e-03,  4.7163e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6247e-05, -1.2583e-03,  4.7163e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6247e-05, -1.2583e-03,  4.7163e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1137.3423, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8058, device='cuda:0')



h[100].sum tensor(-9.2729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.8796, device='cuda:0')



h[200].sum tensor(-30.9460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0998e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53454.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7283e-02, 3.0833e-02, 1.9629e-04,  ..., 0.0000e+00, 3.4812e-02,
         0.0000e+00],
        [1.7188e-02, 3.0961e-02, 9.4504e-05,  ..., 0.0000e+00, 3.5072e-02,
         0.0000e+00],
        [1.8994e-02, 2.8724e-02, 0.0000e+00,  ..., 0.0000e+00, 3.3044e-02,
         0.0000e+00],
        ...,
        [2.0926e-02, 2.7066e-02, 0.0000e+00,  ..., 0.0000e+00, 3.2223e-02,
         0.0000e+00],
        [2.9985e-02, 1.4938e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9416e-02,
         0.0000e+00],
        [3.4510e-02, 8.9415e-03, 0.0000e+00,  ..., 0.0000e+00, 1.3013e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449139.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3951.9736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-194.1932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.4902, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(18.2639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1711],
        [-0.1437],
        [-0.0994],
        ...,
        [-0.1753],
        [-0.1093],
        [-0.0680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78125.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0001],
        [0.9998],
        ...,
        [1.0003],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367115.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0001],
        [0.9999],
        ...,
        [1.0004],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367128.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8630e-05, -1.2745e-03,  4.9330e-04,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.8630e-05, -1.2745e-03,  4.9330e-04,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3131e-02,  1.3510e-02,  2.4455e-02,  ...,  2.0511e-03,
         -1.1628e-02, -1.9297e-02],
        ...,
        [ 5.8630e-05, -1.2745e-03,  4.9330e-04,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.8630e-05, -1.2745e-03,  4.9330e-04,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.8630e-05, -1.2745e-03,  4.9330e-04,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1748.3234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.4494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9881, device='cuda:0')



h[100].sum tensor(-16.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.6358, device='cuda:0')



h[200].sum tensor(-54.1814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9140e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0240, 0.0243, 0.0455,  ..., 0.0035, 0.0000, 0.0000],
        [0.0240, 0.0243, 0.0455,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72556.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0944, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1160, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0166, 0.0330, 0.0000,  ..., 0.0000, 0.0384, 0.0000],
        [0.0166, 0.0330, 0.0000,  ..., 0.0000, 0.0384, 0.0000],
        [0.0166, 0.0330, 0.0000,  ..., 0.0000, 0.0383, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539738.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5698.7075, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-330.2158, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(278.2981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(20.0841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1163],
        [ 0.1131],
        [ 0.1110],
        ...,
        [-0.2797],
        [-0.2787],
        [-0.2784]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-63746.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0001],
        [0.9999],
        ...,
        [1.0004],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367128.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(531.3041, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0001],
        [0.9999],
        ...,
        [1.0004],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367141.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4233e-03,  3.6446e-03,  8.5022e-03,  ..., -2.0576e-04,
         -3.8744e-03, -6.4331e-03],
        [ 1.9206e-02,  2.0362e-02,  3.5600e-02,  ...,  3.6237e-03,
         -1.7006e-02, -2.8236e-02],
        [ 2.9770e-02,  3.2309e-02,  5.4965e-02,  ...,  6.3603e-03,
         -2.6390e-02, -4.3818e-02],
        ...,
        [ 6.1730e-05, -1.2880e-03,  5.0699e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1730e-05, -1.2880e-03,  5.0699e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1730e-05, -1.2880e-03,  5.0699e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1439.2666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8079, device='cuda:0')



h[100].sum tensor(-12.4012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.2324, device='cuda:0')



h[200].sum tensor(-41.5114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4705e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0630, 0.0658, 0.1171,  ..., 0.0109, 0.0000, 0.0000],
        [0.0705, 0.0743, 0.1308,  ..., 0.0131, 0.0000, 0.0000],
        [0.0934, 0.1002, 0.1727,  ..., 0.0188, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64860.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2788, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3402, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0166, 0.0329, 0.0000,  ..., 0.0000, 0.0384, 0.0000],
        [0.0166, 0.0329, 0.0000,  ..., 0.0000, 0.0384, 0.0000],
        [0.0166, 0.0329, 0.0000,  ..., 0.0000, 0.0384, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510253.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5135.7314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-273.6326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.6554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(19.5597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0999],
        [ 0.0979],
        [ 0.0933],
        ...,
        [-0.2807],
        [-0.2797],
        [-0.2794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-64682.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0001],
        [0.9999],
        ...,
        [1.0004],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367141.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367155.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1039e-02,  1.1115e-02,  2.0632e-02,  ...,  1.5075e-03,
         -9.7370e-03, -1.6176e-02],
        [ 6.2976e-05, -1.2972e-03,  5.1098e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.0130e-02,  3.2705e-02,  5.5630e-02,  ...,  6.4531e-03,
         -2.6673e-02, -4.4311e-02],
        ...,
        [ 6.2976e-05, -1.2972e-03,  5.1098e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2976e-05, -1.2972e-03,  5.1098e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2976e-05, -1.2972e-03,  5.1098e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1719.4236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.4610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7025, device='cuda:0')



h[100].sum tensor(-15.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.4893, device='cuda:0')



h[200].sum tensor(-52.2087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8346e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0092, 0.0089, 0.0185,  ..., 0.0010, 0.0000, 0.0000],
        [0.0663, 0.0695, 0.1231,  ..., 0.0118, 0.0000, 0.0000],
        [0.0618, 0.0657, 0.1148,  ..., 0.0119, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69304.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0905, 0.0013, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.1903, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2383, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0167, 0.0332, 0.0000,  ..., 0.0000, 0.0388, 0.0000],
        [0.0210, 0.0265, 0.0000,  ..., 0.0000, 0.0319, 0.0000],
        [0.0273, 0.0168, 0.0000,  ..., 0.0000, 0.0218, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513226.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5193.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.7189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.5462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(22.1667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0833],
        [ 0.1021],
        [ 0.1090],
        ...,
        [-0.2450],
        [-0.1893],
        [-0.1138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69866.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367155.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367168.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1406e-05, -1.3017e-03,  5.0321e-04,  ..., -1.3358e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1406e-05, -1.3017e-03,  5.0321e-04,  ..., -1.3358e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0722e-02,  1.0754e-02,  2.0047e-02,  ...,  1.4258e-03,
         -9.4448e-03, -1.5698e-02],
        ...,
        [ 6.1406e-05, -1.3017e-03,  5.0321e-04,  ..., -1.3358e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1406e-05, -1.3017e-03,  5.0321e-04,  ..., -1.3358e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1406e-05, -1.3017e-03,  5.0321e-04,  ..., -1.3358e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1000.6594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.7046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9141, device='cuda:0')



h[100].sum tensor(-7.2519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4089, device='cuda:0')



h[200].sum tensor(-24.3486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.5940e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0148, 0.0139, 0.0287,  ..., 0.0014, 0.0000, 0.0000],
        [0.0137, 0.0113, 0.0267,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48683.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8684e-02, 1.5931e-02, 0.0000e+00,  ..., 0.0000e+00, 2.0005e-02,
         0.0000e+00],
        [5.5314e-02, 3.4191e-03, 0.0000e+00,  ..., 0.0000e+00, 5.6160e-03,
         0.0000e+00],
        [6.3929e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.1228e-05,
         0.0000e+00],
        ...,
        [1.6676e-02, 3.3624e-02, 0.0000e+00,  ..., 0.0000e+00, 3.9496e-02,
         0.0000e+00],
        [1.6673e-02, 3.3619e-02, 0.0000e+00,  ..., 0.0000e+00, 3.9489e-02,
         0.0000e+00],
        [1.6671e-02, 3.3614e-02, 0.0000e+00,  ..., 0.0000e+00, 3.9482e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433092.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3653.8174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-153.8639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(54.4186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(20.6558, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0479],
        [ 0.0401],
        [ 0.1007],
        ...,
        [-0.2895],
        [-0.2884],
        [-0.2881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81067.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367168.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367181.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9299e-02,  2.0447e-02,  3.5762e-02,  ...,  3.6469e-03,
         -1.7019e-02, -2.8302e-02],
        [ 1.9438e-02,  2.0604e-02,  3.6017e-02,  ...,  3.6828e-03,
         -1.7142e-02, -2.8506e-02],
        [ 6.3350e-05, -1.3067e-03,  4.9620e-04,  ..., -1.3359e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.3350e-05, -1.3067e-03,  4.9620e-04,  ..., -1.3359e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.3350e-05, -1.3067e-03,  4.9620e-04,  ..., -1.3359e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.3350e-05, -1.3067e-03,  4.9620e-04,  ..., -1.3359e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1396.0681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8428, device='cuda:0')



h[100].sum tensor(-11.8288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.3716, device='cuda:0')



h[200].sum tensor(-39.7762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4109e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0793, 0.0841, 0.1469,  ..., 0.0151, 0.0000, 0.0000],
        [0.0571, 0.0604, 0.1062,  ..., 0.0107, 0.0000, 0.0000],
        [0.0525, 0.0538, 0.0977,  ..., 0.0082, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62224.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2418, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2411, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2262, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0166, 0.0341, 0.0000,  ..., 0.0000, 0.0402, 0.0000],
        [0.0166, 0.0341, 0.0000,  ..., 0.0000, 0.0402, 0.0000],
        [0.0166, 0.0341, 0.0000,  ..., 0.0000, 0.0402, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(499128.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4851.1870, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-245.3573, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(173.6778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(20.4400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1110],
        [ 0.1136],
        [ 0.1160],
        ...,
        [-0.2939],
        [-0.2928],
        [-0.2925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75129.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367181.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367193.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2149e-03,  3.3872e-03,  8.0961e-03,  ..., -2.6075e-04,
         -3.6678e-03, -6.1024e-03],
        [ 5.7371e-03,  5.1085e-03,  1.0887e-02,  ...,  1.3354e-04,
         -5.0127e-03, -8.3401e-03],
        [ 6.3828e-05, -1.3071e-03,  4.8569e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.3828e-05, -1.3071e-03,  4.8569e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.3828e-05, -1.3071e-03,  4.8569e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.3828e-05, -1.3071e-03,  4.8569e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1080.7490, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6659, device='cuda:0')



h[100].sum tensor(-8.2029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9712, device='cuda:0')



h[200].sum tensor(-27.6256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6760e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0375, 0.0369, 0.0702,  ..., 0.0043, 0.0000, 0.0000],
        [0.0124, 0.0112, 0.0243,  ..., 0.0007, 0.0000, 0.0000],
        [0.0059, 0.0051, 0.0123,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51389.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1307, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0764, 0.0021, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0429, 0.0117, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        ...,
        [0.0166, 0.0345, 0.0000,  ..., 0.0000, 0.0408, 0.0000],
        [0.0166, 0.0345, 0.0000,  ..., 0.0000, 0.0408, 0.0000],
        [0.0166, 0.0345, 0.0000,  ..., 0.0000, 0.0408, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448950.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3833.8159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-163.6071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(71.9491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(19.7656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1278],
        [ 0.0845],
        [ 0.0118],
        ...,
        [-0.2980],
        [-0.2969],
        [-0.2965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82273.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0004],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367193.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367206.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2451e-05, -1.2983e-03,  4.6950e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2451e-05, -1.2983e-03,  4.6950e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2451e-05, -1.2983e-03,  4.6950e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.2451e-05, -1.2983e-03,  4.6950e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2451e-05, -1.2983e-03,  4.6950e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2451e-05, -1.2983e-03,  4.6950e-04,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1141.0936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2680, device='cuda:0')



h[100].sum tensor(-8.8393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4000, device='cuda:0')



h[200].sum tensor(-29.8143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0666e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0067, 0.0149,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53920.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0306, 0.0148, 0.0000,  ..., 0.0000, 0.0199, 0.0000],
        [0.0408, 0.0083, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0597, 0.0035, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        ...,
        [0.0165, 0.0349, 0.0000,  ..., 0.0000, 0.0413, 0.0000],
        [0.0165, 0.0349, 0.0000,  ..., 0.0000, 0.0413, 0.0000],
        [0.0165, 0.0349, 0.0000,  ..., 0.0000, 0.0413, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467305.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4158.6465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-179.5091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.5930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(21.1467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0718],
        [ 0.0918],
        [ 0.1182],
        ...,
        [-0.3013],
        [-0.3002],
        [-0.2998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84819.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367206.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367218.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7680e-05, -1.2861e-03,  4.5123e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7680e-05, -1.2861e-03,  4.5123e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7680e-05, -1.2861e-03,  4.5123e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.7680e-05, -1.2861e-03,  4.5123e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7680e-05, -1.2861e-03,  4.5123e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7680e-05, -1.2861e-03,  4.5123e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1138.7876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7704, device='cuda:0')



h[100].sum tensor(-8.7322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.9561, device='cuda:0')



h[200].sum tensor(-29.4979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0358e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52863.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0192, 0.0300, 0.0003,  ..., 0.0000, 0.0355, 0.0000],
        [0.0171, 0.0332, 0.0002,  ..., 0.0000, 0.0390, 0.0000],
        [0.0163, 0.0345, 0.0000,  ..., 0.0000, 0.0407, 0.0000],
        ...,
        [0.0165, 0.0352, 0.0000,  ..., 0.0000, 0.0417, 0.0000],
        [0.0165, 0.0352, 0.0000,  ..., 0.0000, 0.0416, 0.0000],
        [0.0165, 0.0352, 0.0000,  ..., 0.0000, 0.0416, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457255.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3929.7166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-168.6405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(84.1853, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(22.2152, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0730],
        [-0.1617],
        [-0.2435],
        ...,
        [-0.3037],
        [-0.3028],
        [-0.3026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87989.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367218.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367231.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5386e-03,  9.4670e-03,  1.7853e-02,  ...,  1.1248e-03,
         -8.3598e-03, -1.3930e-02],
        [ 1.0775e-02,  1.0866e-02,  2.0119e-02,  ...,  1.4451e-03,
         -9.4481e-03, -1.5743e-02],
        [ 1.1019e-02,  1.1142e-02,  2.0567e-02,  ...,  1.5084e-03,
         -9.6631e-03, -1.6102e-02],
        ...,
        [ 3.9955e-05, -1.2772e-03,  4.3925e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.9955e-05, -1.2772e-03,  4.3925e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.9955e-05, -1.2772e-03,  4.3925e-04,  ..., -1.3357e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1173.7524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5095, device='cuda:0')



h[100].sum tensor(-9.0138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6153, device='cuda:0')



h[200].sum tensor(-30.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0815e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0349, 0.0342, 0.0655,  ..., 0.0040, 0.0000, 0.0000],
        [0.0406, 0.0406, 0.0759,  ..., 0.0055, 0.0000, 0.0000],
        [0.0512, 0.0527, 0.0954,  ..., 0.0079, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54145.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2064, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1867, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1659, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0165, 0.0355, 0.0000,  ..., 0.0000, 0.0419, 0.0000],
        [0.0165, 0.0355, 0.0000,  ..., 0.0000, 0.0418, 0.0000],
        [0.0165, 0.0355, 0.0000,  ..., 0.0000, 0.0418, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466191.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4071.5474, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-174.7604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.2882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(23.7614, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1206],
        [ 0.1296],
        [ 0.1248],
        ...,
        [-0.3068],
        [-0.3057],
        [-0.3053]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84790.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367231.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367243.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0103e-02,  1.0126e-02,  1.8890e-02,  ...,  1.2730e-03,
         -8.8513e-03, -1.4756e-02],
        [ 4.2792e-03,  3.5375e-03,  8.2140e-03,  ..., -2.3565e-04,
         -3.7323e-03, -6.2223e-03],
        [ 5.8568e-03,  5.3221e-03,  1.1106e-02,  ...,  1.7302e-04,
         -5.1190e-03, -8.5341e-03],
        ...,
        [ 3.2889e-05, -1.2661e-03,  4.2978e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2889e-05, -1.2661e-03,  4.2978e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2889e-05, -1.2661e-03,  4.2978e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1109.4762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7620, device='cuda:0')



h[100].sum tensor(-8.1395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0568, device='cuda:0')



h[200].sum tensor(-27.5799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7353e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0168, 0.0151, 0.0323,  ..., 0.0010, 0.0000, 0.0000],
        [0.0374, 0.0371, 0.0701,  ..., 0.0043, 0.0000, 0.0000],
        [0.0168, 0.0163, 0.0323,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50779.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0799, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1023, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0754, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        ...,
        [0.0165, 0.0357, 0.0000,  ..., 0.0000, 0.0419, 0.0000],
        [0.0165, 0.0357, 0.0000,  ..., 0.0000, 0.0418, 0.0000],
        [0.0165, 0.0357, 0.0000,  ..., 0.0000, 0.0418, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451685.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3752.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-153.0243, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(66.9222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.0636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0959],
        [ 0.0968],
        [ 0.0549],
        ...,
        [-0.3086],
        [-0.3074],
        [-0.3070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100247.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367243.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367243.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6728e-03,  3.9827e-03,  8.9356e-03,  ..., -1.3369e-04,
         -4.0783e-03, -6.7991e-03],
        [ 1.0521e-02,  1.0598e-02,  1.9657e-02,  ...,  1.3812e-03,
         -9.2187e-03, -1.5369e-02],
        [ 3.2889e-05, -1.2661e-03,  4.2978e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 3.2889e-05, -1.2661e-03,  4.2978e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2889e-05, -1.2661e-03,  4.2978e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.2889e-05, -1.2661e-03,  4.2978e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1585.8838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.6639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3323, device='cuda:0')



h[100].sum tensor(-13.4824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.4836, device='cuda:0')



h[200].sum tensor(-45.6839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6264e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0366, 0.0362, 0.0685,  ..., 0.0041, 0.0000, 0.0000],
        [0.0167, 0.0150, 0.0321,  ..., 0.0008, 0.0000, 0.0000],
        [0.0144, 0.0136, 0.0279,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71829.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0787, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0611, 0.0039, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0165, 0.0357, 0.0000,  ..., 0.0000, 0.0419, 0.0000],
        [0.0165, 0.0357, 0.0000,  ..., 0.0000, 0.0418, 0.0000],
        [0.0165, 0.0357, 0.0000,  ..., 0.0000, 0.0418, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565103.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5939.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-302.1239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(258.2475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(27.3391, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1114],
        [ 0.0832],
        [ 0.0178],
        ...,
        [-0.3086],
        [-0.3074],
        [-0.3070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79990.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367243.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(486.2354, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367256.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1425e-02,  2.2957e-02,  3.9667e-02,  ...,  4.2095e-03,
         -1.8789e-02, -3.1340e-02],
        [ 4.2800e-02,  4.7140e-02,  7.8853e-02,  ...,  9.7468e-03,
         -3.7553e-02, -6.2639e-02],
        [ 2.1552e-02,  2.3101e-02,  3.9900e-02,  ...,  4.2424e-03,
         -1.8901e-02, -3.1527e-02],
        ...,
        [ 2.0857e-05, -1.2591e-03,  4.2847e-04,  ..., -1.3353e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.0857e-05, -1.2591e-03,  4.2847e-04,  ..., -1.3353e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.0857e-05, -1.2591e-03,  4.2847e-04,  ..., -1.3353e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1103.8711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.8362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3666, device='cuda:0')



h[100].sum tensor(-7.8761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7043, device='cuda:0')



h[200].sum tensor(-26.7283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.4912e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0470e-01, 1.1331e-01, 1.9349e-01,  ..., 2.1750e-02, 0.0000e+00,
         0.0000e+00],
        [9.7372e-02, 1.0503e-01, 1.8007e-01,  ..., 1.9855e-02, 0.0000e+00,
         0.0000e+00],
        [1.0581e-01, 1.1458e-01, 1.9553e-01,  ..., 2.2046e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.4672e-05, 0.0000e+00, 1.7395e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.4659e-05, 0.0000e+00, 1.7392e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.4651e-05, 0.0000e+00, 1.7390e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50355.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3098, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3279, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3261, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0167, 0.0358, 0.0000,  ..., 0.0000, 0.0416, 0.0000],
        [0.0167, 0.0358, 0.0000,  ..., 0.0000, 0.0416, 0.0000],
        [0.0167, 0.0358, 0.0000,  ..., 0.0000, 0.0416, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449529.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3753.6790, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-148.1191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(62.4163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.5465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0944],
        [ 0.0916],
        [ 0.0966],
        ...,
        [-0.3094],
        [-0.3082],
        [-0.3078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94067.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367256.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367268.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6069e-05, -1.2601e-03,  4.4017e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2796e-03,  4.6952e-03,  1.0090e-02,  ...,  2.8413e-05,
         -4.6145e-03, -7.7009e-03],
        [ 1.6069e-05, -1.2601e-03,  4.4017e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.6069e-05, -1.2601e-03,  4.4017e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.6069e-05, -1.2601e-03,  4.4017e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.6069e-05, -1.2601e-03,  4.4017e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1181.4185, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.0428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6533, device='cuda:0')



h[100].sum tensor(-8.5257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.8517, device='cuda:0')



h[200].sum tensor(-28.9775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0286e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.7484e-02, 2.5975e-02, 5.2032e-02,  ..., 1.9764e-03, 0.0000e+00,
         0.0000e+00],
        [4.3708e-03, 3.6098e-03, 9.6579e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3289e-03, 4.6962e-03, 1.1413e-02,  ..., 2.8418e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.5244e-05, 0.0000e+00, 1.7872e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5233e-05, 0.0000e+00, 1.7869e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5227e-05, 0.0000e+00, 1.7868e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52394.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0720, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0430, 0.0072, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0406, 0.0063, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        ...,
        [0.0169, 0.0357, 0.0001,  ..., 0.0000, 0.0410, 0.0000],
        [0.0169, 0.0357, 0.0001,  ..., 0.0000, 0.0410, 0.0000],
        [0.0169, 0.0357, 0.0001,  ..., 0.0000, 0.0410, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456407.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3963.0415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-164.8820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(81.3434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(29.9928, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0853],
        [-0.1307],
        [-0.1542],
        ...,
        [-0.3090],
        [-0.3078],
        [-0.3075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92696.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367268.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367281.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3558e-02,  1.4055e-02,  2.5289e-02,  ...,  2.1734e-03,
         -1.1858e-02, -1.9799e-02],
        [ 2.0272e-02,  2.1652e-02,  3.7600e-02,  ...,  3.9128e-03,
         -1.7736e-02, -2.9614e-02],
        ...,
        [ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1345.2289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7788, device='cuda:0')



h[100].sum tensor(-10.1026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.6391, device='cuda:0')



h[200].sum tensor(-34.3899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2216e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.5732e-02, 2.5237e-02, 4.8909e-02,  ..., 2.6662e-03, 0.0000e+00,
         0.0000e+00],
        [3.6711e-02, 3.7661e-02, 6.9039e-02,  ..., 5.5121e-03, 0.0000e+00,
         0.0000e+00],
        [5.6027e-02, 5.8250e-02, 1.0445e-01,  ..., 9.1578e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.8760e-05, 0.0000e+00, 1.8624e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8751e-05, 0.0000e+00, 1.8621e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8745e-05, 0.0000e+00, 1.8620e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57438., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1800, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2127, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2518, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0354, 0.0005,  ..., 0.0000, 0.0404, 0.0000],
        [0.0171, 0.0354, 0.0005,  ..., 0.0000, 0.0404, 0.0000],
        [0.0171, 0.0354, 0.0005,  ..., 0.0000, 0.0404, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477520.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4412.5415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-197.8746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.2762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.2719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0403],
        [-0.0285],
        [-0.0178],
        ...,
        [-0.3076],
        [-0.3064],
        [-0.3060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77510.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367281.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367281.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.6528e-03,  5.1109e-03,  1.0796e-02,  ...,  1.2559e-04,
         -4.9367e-03, -8.2427e-03],
        ...,
        [ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4470e-05, -1.2685e-03,  4.5863e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1209.2665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.1818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9050, device='cuda:0')



h[100].sum tensor(-8.6006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0763, device='cuda:0')



h[200].sum tensor(-29.2769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0441e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.7971e-05, 0.0000e+00, 1.8374e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.6985e-03, 5.1129e-03, 1.2178e-02,  ..., 1.2564e-04, 0.0000e+00,
         0.0000e+00],
        [3.0840e-02, 3.2290e-02, 5.8272e-02,  ..., 5.3036e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.8760e-05, 0.0000e+00, 1.8624e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8751e-05, 0.0000e+00, 1.8621e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8745e-05, 0.0000e+00, 1.8620e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53548.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0395, 0.0136, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0706, 0.0050, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.1333, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0354, 0.0005,  ..., 0.0000, 0.0404, 0.0000],
        [0.0171, 0.0354, 0.0005,  ..., 0.0000, 0.0404, 0.0000],
        [0.0171, 0.0354, 0.0005,  ..., 0.0000, 0.0404, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463647.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4149.3154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-171.2296, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(89.3246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.9378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0256],
        [ 0.0809],
        [ 0.1138],
        ...,
        [-0.3078],
        [-0.3066],
        [-0.3063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81241.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367281.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367294.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2970e-05, -1.2751e-03,  4.6991e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2970e-05, -1.2751e-03,  4.6991e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2970e-05, -1.2751e-03,  4.6991e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.2970e-05, -1.2751e-03,  4.6991e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2970e-05, -1.2751e-03,  4.6991e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2970e-05, -1.2751e-03,  4.6991e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(998.3196, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.0493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.9333, device='cuda:0')



h[100].sum tensor(-6.1162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6424, device='cuda:0')



h[200].sum tensor(-20.8521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.3706e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.5698e-02, 2.6464e-02, 4.8908e-02,  ..., 3.9700e-03, 0.0000e+00,
         0.0000e+00],
        [5.1941e-05, 0.0000e+00, 1.8819e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.1892e-05, 0.0000e+00, 1.8801e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.2676e-05, 0.0000e+00, 1.9085e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.2667e-05, 0.0000e+00, 1.9082e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.2662e-05, 0.0000e+00, 1.9080e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47337.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.8658e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.0774e-02, 1.5273e-02, 7.4207e-05,  ..., 0.0000e+00, 1.7668e-02,
         0.0000e+00],
        [2.5703e-02, 2.4264e-02, 4.6462e-04,  ..., 0.0000e+00, 2.8131e-02,
         0.0000e+00],
        ...,
        [1.7222e-02, 3.5348e-02, 9.4082e-04,  ..., 0.0000e+00, 4.0202e-02,
         0.0000e+00],
        [1.7218e-02, 3.5341e-02, 9.4020e-04,  ..., 0.0000e+00, 4.0194e-02,
         0.0000e+00],
        [1.7216e-02, 3.5337e-02, 9.3957e-04,  ..., 0.0000e+00, 4.0188e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(437681.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3684.6147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-124.9103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(30.5045, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(29.0707, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0157],
        [-0.0912],
        [-0.2035],
        ...,
        [-0.3081],
        [-0.3070],
        [-0.3066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79278.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367294.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367306.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1076e-02,  1.1236e-02,  2.0755e-02,  ...,  1.5301e-03,
         -9.6579e-03, -1.6142e-02],
        [ 2.2327e-02,  2.3967e-02,  4.1387e-02,  ...,  4.4450e-03,
         -1.9483e-02, -3.2564e-02],
        [ 3.2361e-02,  3.5319e-02,  5.9784e-02,  ...,  7.0442e-03,
         -2.8245e-02, -4.7208e-02],
        ...,
        [ 1.5510e-05, -1.2776e-03,  4.7496e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5510e-05, -1.2776e-03,  4.7496e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5510e-05, -1.2776e-03,  4.7496e-04,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1067.6429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.5054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3577, device='cuda:0')



h[100].sum tensor(-6.7653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9127, device='cuda:0')



h[200].sum tensor(-23.1004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.2503e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.2515e-02, 7.6857e-02, 1.3476e-01,  ..., 1.3421e-02, 0.0000e+00,
         0.0000e+00],
        [1.0289e-01, 1.1122e-01, 1.9045e-01,  ..., 2.1291e-02, 0.0000e+00,
         0.0000e+00],
        [1.5175e-01, 1.6652e-01, 2.8005e-01,  ..., 3.3955e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.3005e-05, 0.0000e+00, 1.9293e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.2995e-05, 0.0000e+00, 1.9290e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.2989e-05, 0.0000e+00, 1.9288e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48097.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3344, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4773, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.6265, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0173, 0.0354, 0.0013,  ..., 0.0000, 0.0402, 0.0000],
        [0.0173, 0.0354, 0.0013,  ..., 0.0000, 0.0402, 0.0000],
        [0.0173, 0.0354, 0.0013,  ..., 0.0000, 0.0402, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438105.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3708.2827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-132.6508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(37.5913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(32.8430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0389],
        [-0.0250],
        [-0.0863],
        ...,
        [-0.3097],
        [-0.3085],
        [-0.3082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87708.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [0.9999],
        ...,
        [1.0006],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367306.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367319.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8405e-05, -1.2826e-03,  4.7787e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8405e-05, -1.2826e-03,  4.7787e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8405e-05, -1.2826e-03,  4.7787e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8405e-05, -1.2826e-03,  4.7787e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8405e-05, -1.2826e-03,  4.7787e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8405e-05, -1.2826e-03,  4.7787e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1249.2228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.6093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3203, device='cuda:0')



h[100].sum tensor(-8.6840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4466, device='cuda:0')



h[200].sum tensor(-29.6979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0698e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.3701e-03, 5.8391e-03, 1.3460e-02,  ..., 2.9368e-04, 0.0000e+00,
         0.0000e+00],
        [7.3710e-05, 0.0000e+00, 1.9138e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.8308e-03, 7.4938e-03, 1.6136e-02,  ..., 6.7418e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.4774e-05, 0.0000e+00, 1.9415e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.4760e-05, 0.0000e+00, 1.9411e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.4754e-05, 0.0000e+00, 1.9409e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55115.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0338, 0.0136, 0.0004,  ..., 0.0000, 0.0176, 0.0000],
        [0.0291, 0.0166, 0.0002,  ..., 0.0000, 0.0216, 0.0000],
        [0.0450, 0.0093, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        ...,
        [0.0173, 0.0356, 0.0016,  ..., 0.0000, 0.0405, 0.0000],
        [0.0173, 0.0356, 0.0015,  ..., 0.0000, 0.0405, 0.0000],
        [0.0173, 0.0356, 0.0015,  ..., 0.0000, 0.0405, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(475975.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4416.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-179.9530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.4670, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(32.7565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1014],
        [-0.0711],
        [-0.0102],
        ...,
        [-0.3126],
        [-0.3115],
        [-0.3111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80822.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367319.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367331.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1546e-03,  3.3957e-03,  8.0626e-03,  ..., -2.6309e-04,
         -3.6043e-03, -6.0303e-03],
        [ 1.5996e-05, -1.2867e-03,  4.7324e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5996e-05, -1.2867e-03,  4.7324e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5996e-05, -1.2867e-03,  4.7324e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5996e-05, -1.2867e-03,  4.7324e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5996e-05, -1.2867e-03,  4.7324e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1529.8159, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9416, device='cuda:0')



h[100].sum tensor(-11.7530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.4598, device='cuda:0')



h[200].sum tensor(-40.2553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4170e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.4518e-02, 1.3776e-02, 2.8403e-02,  ..., 1.3333e-03, 0.0000e+00,
         0.0000e+00],
        [2.6409e-02, 2.5941e-02, 5.0208e-02,  ..., 3.0766e-03, 0.0000e+00,
         0.0000e+00],
        [4.3874e-02, 4.5706e-02, 8.2234e-02,  ..., 7.3425e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.4997e-05, 0.0000e+00, 1.9229e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.4986e-05, 0.0000e+00, 1.9226e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.4980e-05, 0.0000e+00, 1.9224e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63929.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1336, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1759, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2244, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0174, 0.0360, 0.0016,  ..., 0.0000, 0.0412, 0.0000],
        [0.0174, 0.0360, 0.0016,  ..., 0.0000, 0.0412, 0.0000],
        [0.0174, 0.0360, 0.0016,  ..., 0.0000, 0.0412, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515429.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5152.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-240.9523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(175.5775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.5092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0938],
        [ 0.0891],
        [ 0.0845],
        ...,
        [-0.3171],
        [-0.3159],
        [-0.3155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78532.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367331.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367343.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.8591e-06, -1.2877e-03,  4.6547e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.8591e-06, -1.2877e-03,  4.6547e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.8591e-06, -1.2877e-03,  4.6547e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.8591e-06, -1.2877e-03,  4.6547e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.8591e-06, -1.2877e-03,  4.6547e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.8591e-06, -1.2877e-03,  4.6547e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1095.7141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.9939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0337, device='cuda:0')



h[100].sum tensor(-7.0959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5155, device='cuda:0')



h[200].sum tensor(-24.3418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.6679e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.6183e-03, 4.8783e-03, 1.3952e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.3250e-03, 2.4407e-03, 7.9117e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7445e-05, 0.0000e+00, 1.8624e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.7875e-05, 0.0000e+00, 1.8916e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7870e-05, 0.0000e+00, 1.8913e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7867e-05, 0.0000e+00, 1.8911e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50802.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0318, 0.0099, 0.0040,  ..., 0.0000, 0.0126, 0.0000],
        [0.0275, 0.0175, 0.0031,  ..., 0.0000, 0.0211, 0.0000],
        [0.0225, 0.0264, 0.0023,  ..., 0.0000, 0.0310, 0.0000],
        ...,
        [0.0174, 0.0365, 0.0016,  ..., 0.0000, 0.0420, 0.0000],
        [0.0174, 0.0365, 0.0016,  ..., 0.0000, 0.0420, 0.0000],
        [0.0174, 0.0365, 0.0016,  ..., 0.0000, 0.0420, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458859.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4080.8521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-143.3396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(51.3776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(37.6854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1838],
        [-0.2125],
        [-0.2537],
        ...,
        [-0.3231],
        [-0.3219],
        [-0.3215]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80252.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367343.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367356.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2035e-03,  4.6018e-03,  9.9990e-03,  ...,  1.3691e-05,
         -4.5210e-03, -7.5719e-03],
        [ 5.0674e-03,  4.4477e-03,  9.7493e-03,  ..., -2.1586e-05,
         -4.4028e-03, -7.3738e-03],
        [ 1.0272e-02,  1.0337e-02,  1.9294e-02,  ...,  1.3268e-03,
         -8.9238e-03, -1.4946e-02],
        ...,
        [-9.0783e-07, -1.2876e-03,  4.5404e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00],
        [-9.0783e-07, -1.2876e-03,  4.5404e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00],
        [-9.0783e-07, -1.2876e-03,  4.5404e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1302.1824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.4829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5535, device='cuda:0')



h[100].sum tensor(-9.3598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5464, device='cuda:0')



h[200].sum tensor(-32.1578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1460e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0166, 0.0330,  ..., 0.0017, 0.0000, 0.0000],
        [0.0352, 0.0346, 0.0663,  ..., 0.0038, 0.0000, 0.0000],
        [0.0170, 0.0153, 0.0329,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56053.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0867, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.1181, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1061, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0174, 0.0370, 0.0014,  ..., 0.0000, 0.0428, 0.0000],
        [0.0174, 0.0370, 0.0014,  ..., 0.0000, 0.0428, 0.0000],
        [0.0174, 0.0370, 0.0014,  ..., 0.0000, 0.0428, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478807.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4421.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-179.0851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.2670, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.2340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1440],
        [ 0.1594],
        [ 0.1666],
        ...,
        [-0.3281],
        [-0.3269],
        [-0.3265]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81824.0234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0002],
        [1.0000],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367356.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(465.8081, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367368.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.9705e-06, -1.2861e-03,  4.3809e-04,  ..., -1.3345e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9705e-06, -1.2861e-03,  4.3809e-04,  ..., -1.3345e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9705e-06, -1.2861e-03,  4.3809e-04,  ..., -1.3345e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.9705e-06, -1.2861e-03,  4.3809e-04,  ..., -1.3345e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9705e-06, -1.2861e-03,  4.3809e-04,  ..., -1.3345e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9705e-06, -1.2861e-03,  4.3809e-04,  ..., -1.3345e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1236.6371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.9225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5737, device='cuda:0')



h[100].sum tensor(-8.7071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6726, device='cuda:0')



h[200].sum tensor(-29.9618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0854e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54862.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0170, 0.0370, 0.0012,  ..., 0.0000, 0.0428, 0.0000],
        [0.0173, 0.0365, 0.0014,  ..., 0.0000, 0.0422, 0.0000],
        [0.0179, 0.0354, 0.0020,  ..., 0.0000, 0.0406, 0.0000],
        ...,
        [0.0173, 0.0376, 0.0012,  ..., 0.0000, 0.0436, 0.0000],
        [0.0173, 0.0376, 0.0012,  ..., 0.0000, 0.0436, 0.0000],
        [0.0173, 0.0376, 0.0012,  ..., 0.0000, 0.0436, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(484220.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4416.1587, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-173.0294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(84.5968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.2239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2528],
        [-0.2589],
        [-0.2550],
        ...,
        [-0.3333],
        [-0.3321],
        [-0.3318]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102719.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367368.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367381.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.5476e-06, -1.2958e-03,  4.3850e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5476e-06, -1.2958e-03,  4.3850e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5476e-06, -1.2958e-03,  4.3850e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-9.5476e-06, -1.2958e-03,  4.3850e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5476e-06, -1.2958e-03,  4.3850e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5476e-06, -1.2958e-03,  4.3850e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1074.2373, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.2075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.8691, device='cuda:0')



h[100].sum tensor(-6.9389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3687, device='cuda:0')



h[200].sum tensor(-23.9146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.5662e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0033, 0.0024, 0.0078,  ..., 0.0000, 0.0000, 0.0000],
        [0.0066, 0.0049, 0.0138,  ..., 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0024, 0.0078,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50966.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0328, 0.0121, 0.0027,  ..., 0.0000, 0.0155, 0.0000],
        [0.0371, 0.0064, 0.0030,  ..., 0.0000, 0.0082, 0.0000],
        [0.0315, 0.0141, 0.0028,  ..., 0.0000, 0.0174, 0.0000],
        ...,
        [0.0173, 0.0378, 0.0015,  ..., 0.0000, 0.0440, 0.0000],
        [0.0173, 0.0378, 0.0015,  ..., 0.0000, 0.0440, 0.0000],
        [0.0173, 0.0378, 0.0015,  ..., 0.0000, 0.0440, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470440.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4155.2627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-141.0603, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(44.2800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.5905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0201],
        [ 0.0268],
        [ 0.0425],
        ...,
        [-0.3370],
        [-0.3357],
        [-0.3353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92029.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0003],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367381.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367395.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0690e-05, -1.3073e-03,  4.4898e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0690e-05, -1.3073e-03,  4.4898e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0859e-02,  1.0994e-02,  2.0389e-02,  ...,  1.4819e-03,
         -9.4055e-03, -1.5777e-02],
        ...,
        [-1.0690e-05, -1.3073e-03,  4.4898e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0690e-05, -1.3073e-03,  4.4898e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0690e-05, -1.3073e-03,  4.4898e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1477.9956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.8843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0563, device='cuda:0')



h[100].sum tensor(-11.0755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.6703, device='cuda:0')



h[200].sum tensor(-38.2303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3623e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0054, 0.0126,  ..., 0.0002, 0.0000, 0.0000],
        [0.0321, 0.0324, 0.0607,  ..., 0.0043, 0.0000, 0.0000],
        [0.0483, 0.0508, 0.0905,  ..., 0.0085, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61456.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.2806e-02, 4.8159e-03, 1.0838e-05,  ..., 0.0000e+00, 6.0862e-03,
         0.0000e+00],
        [1.6808e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4352e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7542e-02, 3.7996e-02, 2.0028e-03,  ..., 0.0000e+00, 4.4020e-02,
         0.0000e+00],
        [1.7538e-02, 3.7987e-02, 2.0018e-03,  ..., 0.0000e+00, 4.4010e-02,
         0.0000e+00],
        [1.7536e-02, 3.7982e-02, 2.0009e-03,  ..., 0.0000e+00, 4.4003e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508184.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4918.7515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-217.6256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.5208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.3620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0960],
        [ 0.1250],
        [ 0.1335],
        ...,
        [-0.3386],
        [-0.3373],
        [-0.3369]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88957.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367395.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367408.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2209e-03,  6.8814e-03,  1.3721e-02,  ...,  5.3961e-04,
         -6.2512e-03, -1.0491e-02],
        [-1.3192e-05, -1.3051e-03,  4.4992e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3192e-05, -1.3051e-03,  4.4992e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.3192e-05, -1.3051e-03,  4.4992e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3192e-05, -1.3051e-03,  4.4992e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3192e-05, -1.3051e-03,  4.4992e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1107.9385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.2727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0952, device='cuda:0')



h[100].sum tensor(-6.9972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5704, device='cuda:0')



h[200].sum tensor(-24.1908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.7059e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0231, 0.0235, 0.0442,  ..., 0.0033, 0.0000, 0.0000],
        [0.0072, 0.0069, 0.0151,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50071.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1256, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0667, 0.0082, 0.0002,  ..., 0.0000, 0.0110, 0.0000],
        [0.0324, 0.0169, 0.0014,  ..., 0.0000, 0.0213, 0.0000],
        ...,
        [0.0177, 0.0381, 0.0023,  ..., 0.0000, 0.0438, 0.0000],
        [0.0177, 0.0381, 0.0023,  ..., 0.0000, 0.0438, 0.0000],
        [0.0177, 0.0381, 0.0023,  ..., 0.0000, 0.0438, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460324.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4018.7632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-136.7113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(34.0423, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.2343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1254],
        [ 0.0802],
        [ 0.0136],
        ...,
        [-0.3402],
        [-0.3389],
        [-0.3385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93841.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0003],
        [1.0000],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367408.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0003],
        [1.0000],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367420.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6033e-05, -1.3003e-03,  4.4908e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9330e-02,  2.0594e-02,  3.5940e-02,  ...,  3.6778e-03,
         -1.6695e-02, -2.8033e-02],
        [ 2.2649e-02,  2.4350e-02,  4.2029e-02,  ...,  4.5379e-03,
         -1.9560e-02, -3.2843e-02],
        ...,
        [-1.6033e-05, -1.3003e-03,  4.4908e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6033e-05, -1.3003e-03,  4.4908e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6033e-05, -1.3003e-03,  4.4908e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1154.0715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.8206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5949, device='cuda:0')



h[100].sum tensor(-7.2868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0160, device='cuda:0')



h[200].sum tensor(-25.2313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.0145e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0206, 0.0374,  ..., 0.0037, 0.0000, 0.0000],
        [0.0426, 0.0457, 0.0800,  ..., 0.0084, 0.0000, 0.0000],
        [0.1009, 0.1091, 0.1871,  ..., 0.0208, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51936.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1085, 0.0036, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.1864, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2812, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0178, 0.0382, 0.0025,  ..., 0.0000, 0.0434, 0.0000],
        [0.0178, 0.0382, 0.0025,  ..., 0.0000, 0.0434, 0.0000],
        [0.0178, 0.0382, 0.0025,  ..., 0.0000, 0.0434, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474155.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4275.1372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-152.6451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(52.4344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.7995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1139],
        [ 0.1386],
        [ 0.1515],
        ...,
        [-0.3412],
        [-0.3399],
        [-0.3395]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101811.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0003],
        [1.0000],
        ...,
        [1.0005],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367420.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0003],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367433.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.2805e-05, -1.2953e-03,  4.4960e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00],
        [-2.2805e-05, -1.2953e-03,  4.4960e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00],
        [-2.2805e-05, -1.2953e-03,  4.4960e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-2.2805e-05, -1.2953e-03,  4.4960e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00],
        [-2.2805e-05, -1.2953e-03,  4.4960e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00],
        [-2.2805e-05, -1.2953e-03,  4.4960e-04,  ..., -1.3347e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1528.0453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1902, device='cuda:0')



h[100].sum tensor(-10.9756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.7897, device='cuda:0')



h[200].sum tensor(-38.0632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3706e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60222.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0355, 0.0169, 0.0020,  ..., 0.0000, 0.0186, 0.0000],
        [0.0228, 0.0296, 0.0032,  ..., 0.0000, 0.0333, 0.0000],
        [0.0182, 0.0367, 0.0032,  ..., 0.0000, 0.0408, 0.0000],
        ...,
        [0.0180, 0.0383, 0.0028,  ..., 0.0000, 0.0429, 0.0000],
        [0.0180, 0.0383, 0.0028,  ..., 0.0000, 0.0429, 0.0000],
        [0.0180, 0.0383, 0.0028,  ..., 0.0000, 0.0429, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498491.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4827.0518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-208.0658, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.6127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.7482, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0365],
        [-0.0568],
        [-0.1472],
        ...,
        [-0.3396],
        [-0.3384],
        [-0.3382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81355.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0003],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367433.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0004],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367446.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.0093e-05, -1.2870e-03,  4.3897e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-3.0093e-05, -1.2870e-03,  4.3897e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-3.0093e-05, -1.2870e-03,  4.3897e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.0093e-05, -1.2870e-03,  4.3897e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-3.0093e-05, -1.2870e-03,  4.3897e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-3.0093e-05, -1.2870e-03,  4.3897e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1240.9917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.6323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9207, device='cuda:0')



h[100].sum tensor(-7.8628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1984, device='cuda:0')



h[200].sum tensor(-27.3109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.8334e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53051.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0177, 0.0380, 0.0027,  ..., 0.0000, 0.0419, 0.0000],
        [0.0209, 0.0336, 0.0022,  ..., 0.0000, 0.0377, 0.0000],
        [0.0252, 0.0279, 0.0013,  ..., 0.0000, 0.0322, 0.0000],
        ...,
        [0.0180, 0.0386, 0.0028,  ..., 0.0000, 0.0427, 0.0000],
        [0.0180, 0.0386, 0.0028,  ..., 0.0000, 0.0427, 0.0000],
        [0.0180, 0.0386, 0.0028,  ..., 0.0000, 0.0427, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473114.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4326.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.7792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(61.6989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.3422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3360],
        [-0.3028],
        [-0.2337],
        ...,
        [-0.3439],
        [-0.3427],
        [-0.3423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84003.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0004],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367446.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0004],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367459.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9463e-03,  5.4937e-03,  1.1398e-02,  ...,  2.1562e-04,
         -5.1425e-03, -8.6481e-03],
        [ 1.4088e-02,  1.4710e-02,  2.6334e-02,  ...,  2.3253e-03,
         -1.2141e-02, -2.0417e-02],
        [ 1.7810e-02,  1.8923e-02,  3.3162e-02,  ...,  3.2898e-03,
         -1.5341e-02, -2.5798e-02],
        ...,
        [-3.6055e-05, -1.2783e-03,  4.2379e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-3.6055e-05, -1.2783e-03,  4.2379e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-3.6055e-05, -1.2783e-03,  4.2379e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2085.9556, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.7932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.4726, device='cuda:0')



h[100].sum tensor(-16.4947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.9597, device='cuda:0')



h[200].sum tensor(-57.3828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0057e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0267, 0.0278, 0.0508,  ..., 0.0043, 0.0000, 0.0000],
        [0.0596, 0.0625, 0.1114,  ..., 0.0101, 0.0000, 0.0000],
        [0.0719, 0.0765, 0.1339,  ..., 0.0133, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80269.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1466, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2328, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2950, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0179, 0.0391, 0.0025,  ..., 0.0000, 0.0428, 0.0000],
        [0.0179, 0.0390, 0.0025,  ..., 0.0000, 0.0428, 0.0000],
        [0.0179, 0.0390, 0.0025,  ..., 0.0000, 0.0428, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608615.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6799.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-355.0569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.4763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.9671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1326],
        [ 0.1077],
        [ 0.0750],
        ...,
        [-0.3430],
        [-0.3443],
        [-0.3447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94003.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0004],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367459.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0004],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367471.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6814e-02,  1.7808e-02,  3.1330e-02,  ...,  3.0327e-03,
         -1.4468e-02, -2.4344e-02],
        [ 2.9528e-02,  3.2201e-02,  5.4654e-02,  ...,  6.3273e-03,
         -2.5383e-02, -4.2709e-02],
        [ 1.5955e-02,  1.6836e-02,  2.9755e-02,  ...,  2.8102e-03,
         -1.3732e-02, -2.3104e-02],
        ...,
        [-4.0328e-05, -1.2715e-03,  4.1222e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-4.0328e-05, -1.2715e-03,  4.1222e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00],
        [-4.0328e-05, -1.2715e-03,  4.1222e-04,  ..., -1.3346e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1230.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.7577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4488, device='cuda:0')



h[100].sum tensor(-7.5794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7776, device='cuda:0')



h[200].sum tensor(-26.4092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.5419e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0826, 0.0886, 0.1535,  ..., 0.0161, 0.0000, 0.0000],
        [0.0853, 0.0916, 0.1584,  ..., 0.0168, 0.0000, 0.0000],
        [0.0900, 0.0970, 0.1671,  ..., 0.0180, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52090.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2461, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2830, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2878, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0178, 0.0394, 0.0023,  ..., 0.0000, 0.0428, 0.0000],
        [0.0178, 0.0394, 0.0023,  ..., 0.0000, 0.0428, 0.0000],
        [0.0178, 0.0394, 0.0023,  ..., 0.0000, 0.0428, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470792.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4184.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-149.0491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(54.1695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.9312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0889],
        [ 0.1161],
        [ 0.1357],
        ...,
        [-0.3503],
        [-0.3491],
        [-0.3487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93295.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0004],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367471.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367484.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1387e-02,  2.2994e-02,  3.9711e-02,  ...,  4.2180e-03,
         -1.8371e-02, -3.0927e-02],
        [ 4.1640e-02,  4.5922e-02,  7.6865e-02,  ...,  9.4661e-03,
         -3.5735e-02, -6.0157e-02],
        [ 4.0744e-02,  4.4908e-02,  7.5221e-02,  ...,  9.2339e-03,
         -3.4967e-02, -5.8864e-02],
        ...,
        [-4.2826e-05, -1.2657e-03,  4.0008e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [-4.2826e-05, -1.2657e-03,  4.0008e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00],
        [-4.2826e-05, -1.2657e-03,  4.0008e-04,  ..., -1.3348e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1260.6891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.2051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8637, device='cuda:0')



h[100].sum tensor(-7.8115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1476, device='cuda:0')



h[200].sum tensor(-27.2606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7982e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1055, 0.1145, 0.1954,  ..., 0.0220, 0.0000, 0.0000],
        [0.1395, 0.1530, 0.2578,  ..., 0.0308, 0.0000, 0.0000],
        [0.1458, 0.1601, 0.2693,  ..., 0.0325, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53973., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3591, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4614, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4753, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0178, 0.0399, 0.0021,  ..., 0.0000, 0.0430, 0.0000],
        [0.0178, 0.0399, 0.0021,  ..., 0.0000, 0.0430, 0.0000],
        [0.0178, 0.0398, 0.0021,  ..., 0.0000, 0.0430, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485328.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4402.4341, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-160.0137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(69.5225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.4734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0640],
        [ 0.0803],
        [ 0.0962],
        ...,
        [-0.3535],
        [-0.3522],
        [-0.3516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91365.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367484.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(547.7937, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367497.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7961e-03,  8.7436e-03,  1.6607e-02,  ...,  9.5554e-04,
         -7.5682e-03, -1.2747e-02],
        [ 2.1233e-02,  2.2823e-02,  3.9423e-02,  ...,  4.1782e-03,
         -1.8216e-02, -3.0682e-02],
        [ 1.8191e-02,  1.9379e-02,  3.3842e-02,  ...,  3.3898e-03,
         -1.5611e-02, -2.6295e-02],
        ...,
        [-4.3805e-05, -1.2635e-03,  3.9101e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3805e-05, -1.2635e-03,  3.9101e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3805e-05, -1.2635e-03,  3.9101e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1604.1470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8921, device='cuda:0')



h[100].sum tensor(-11.2203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.4156, device='cuda:0')



h[200].sum tensor(-39.2183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4139e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0593, 0.0623, 0.1107,  ..., 0.0101, 0.0000, 0.0000],
        [0.0588, 0.0617, 0.1098,  ..., 0.0099, 0.0000, 0.0000],
        [0.0612, 0.0644, 0.1141,  ..., 0.0106, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63038.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1746, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1929, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1908, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0177, 0.0403, 0.0020,  ..., 0.0000, 0.0433, 0.0000],
        [0.0177, 0.0403, 0.0020,  ..., 0.0000, 0.0433, 0.0000],
        [0.0177, 0.0403, 0.0020,  ..., 0.0000, 0.0433, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528048.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5162.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-225.3143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.4697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.5764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1562],
        [ 0.1645],
        [ 0.1646],
        ...,
        [-0.3570],
        [-0.3560],
        [-0.3558]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89556.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367497.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367497.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1853e-02,  1.2204e-02,  2.2215e-02,  ...,  1.7476e-03,
         -1.0185e-02, -1.7155e-02],
        [ 1.0675e-02,  1.0871e-02,  2.0054e-02,  ...,  1.4424e-03,
         -9.1768e-03, -1.5457e-02],
        [ 1.1634e-02,  1.1956e-02,  2.1813e-02,  ...,  1.6909e-03,
         -9.9978e-03, -1.6839e-02],
        ...,
        [-4.3805e-05, -1.2635e-03,  3.9101e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3323e-02,  1.3868e-02,  2.4912e-02,  ...,  2.1285e-03,
         -1.1444e-02, -1.9275e-02],
        [-4.3805e-05, -1.2635e-03,  3.9101e-04,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1494.2859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7756, device='cuda:0')



h[100].sum tensor(-10.1044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.5281, device='cuda:0')



h[200].sum tensor(-35.3180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2832e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0410, 0.0415, 0.0770,  ..., 0.0053, 0.0000, 0.0000],
        [0.0420, 0.0427, 0.0789,  ..., 0.0056, 0.0000, 0.0000],
        [0.0406, 0.0411, 0.0764,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0136, 0.0141, 0.0265,  ..., 0.0022, 0.0000, 0.0000],
        [0.0111, 0.0113, 0.0220,  ..., 0.0015, 0.0000, 0.0000],
        [0.0492, 0.0508, 0.0922,  ..., 0.0074, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59387.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1432, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1435, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1413, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0574, 0.0081, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0683, 0.0006, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.1049, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504802.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4757.3325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-195.9285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.5911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.7198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1774],
        [ 0.1784],
        [ 0.1769],
        ...,
        [-0.0650],
        [-0.0329],
        [-0.0227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79423.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367497.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367511.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.3528e-05, -1.2629e-03,  3.8630e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3528e-05, -1.2629e-03,  3.8630e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3528e-05, -1.2629e-03,  3.8630e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.3528e-05, -1.2629e-03,  3.8630e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3528e-05, -1.2629e-03,  3.8630e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3528e-05, -1.2629e-03,  3.8630e-04,  ..., -1.3352e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1659.1404, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6586, device='cuda:0')



h[100].sum tensor(-11.6451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.0992, device='cuda:0')



h[200].sum tensor(-40.7675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4613e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64091.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0283, 0.0220, 0.0013,  ..., 0.0000, 0.0249, 0.0000],
        [0.0216, 0.0323, 0.0019,  ..., 0.0000, 0.0350, 0.0000],
        [0.0194, 0.0361, 0.0022,  ..., 0.0000, 0.0386, 0.0000],
        ...,
        [0.0176, 0.0406, 0.0019,  ..., 0.0000, 0.0435, 0.0000],
        [0.0176, 0.0406, 0.0019,  ..., 0.0000, 0.0435, 0.0000],
        [0.0176, 0.0406, 0.0019,  ..., 0.0000, 0.0435, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525017.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5090.1387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-230.5304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(159.1022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.4052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0095],
        [-0.0763],
        [-0.1604],
        ...,
        [-0.3606],
        [-0.3593],
        [-0.3589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79710.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0005],
        [1.0001],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367511.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0006],
        [1.0002],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367524.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5756e-05, -1.2445e-03,  3.6170e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [-4.5756e-05, -1.2445e-03,  3.6170e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0527e-02,  1.0725e-02,  1.9755e-02,  ...,  1.4039e-03,
         -9.0270e-03, -1.5220e-02],
        ...,
        [-4.5756e-05, -1.2445e-03,  3.6170e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [-4.5756e-05, -1.2445e-03,  3.6170e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00],
        [-4.5756e-05, -1.2445e-03,  3.6170e-04,  ..., -1.3354e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1596.7502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3519, device='cuda:0')



h[100].sum tensor(-10.9272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.9339, device='cuda:0')



h[200].sum tensor(-38.3147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3806e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0191, 0.0193, 0.0367,  ..., 0.0023, 0.0000, 0.0000],
        [0.0264, 0.0263, 0.0501,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0140, 0.0135, 0.0274,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60687.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0422, 0.0163, 0.0002,  ..., 0.0000, 0.0178, 0.0000],
        [0.0887, 0.0022, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.1265, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0239, 0.0307, 0.0010,  ..., 0.0000, 0.0337, 0.0000],
        [0.0374, 0.0173, 0.0004,  ..., 0.0000, 0.0187, 0.0000],
        [0.0705, 0.0028, 0.0000,  ..., 0.0000, 0.0034, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509560.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4709.3564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-206.8403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.8033, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.5708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0081],
        [ 0.0724],
        [ 0.1300],
        ...,
        [-0.2099],
        [-0.0849],
        [ 0.0275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87413.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0006],
        [1.0002],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367524.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0006],
        [1.0002],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367537.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5961e-03,  9.6822e-03,  1.8036e-02,  ...,  1.1629e-03,
         -8.2226e-03, -1.3871e-02],
        [ 2.5611e-02,  2.7813e-02,  4.7412e-02,  ...,  5.3122e-03,
         -2.1878e-02, -3.6906e-02],
        [ 4.2891e-03,  3.6740e-03,  8.3014e-03,  ..., -2.1207e-04,
         -3.6975e-03, -6.2374e-03],
        ...,
        [-4.7345e-05, -1.2353e-03,  3.4712e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [-4.7345e-05, -1.2353e-03,  3.4712e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00],
        [-4.7345e-05, -1.2353e-03,  3.4712e-04,  ..., -1.3356e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1275.2415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.7465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6420, device='cuda:0')



h[100].sum tensor(-7.5975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9499, device='cuda:0')



h[200].sum tensor(-26.6817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6613e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0910, 0.0983, 0.1687,  ..., 0.0183, 0.0000, 0.0000],
        [0.0458, 0.0471, 0.0858,  ..., 0.0068, 0.0000, 0.0000],
        [0.0509, 0.0529, 0.0952,  ..., 0.0079, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51270.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2641, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1708, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0414, 0.0012,  ..., 0.0000, 0.0441, 0.0000],
        [0.0170, 0.0414, 0.0012,  ..., 0.0000, 0.0441, 0.0000],
        [0.0170, 0.0414, 0.0012,  ..., 0.0000, 0.0441, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471347.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3842.8101, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-142.3606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(44.8840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.0633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1575],
        [ 0.1574],
        [ 0.1479],
        ...,
        [-0.3684],
        [-0.3670],
        [-0.3666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108729.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0006],
        [1.0002],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367537.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0006],
        [1.0002],
        ...,
        [1.0006],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367550.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.4193e-03,  8.3472e-03,  1.5875e-02,  ...,  8.5755e-04,
         -7.2090e-03, -1.2167e-02],
        [ 1.6941e-02,  1.7995e-02,  3.1508e-02,  ...,  3.0654e-03,
         -1.4465e-02, -2.4415e-02],
        [ 8.4749e-03,  8.4102e-03,  1.5977e-02,  ...,  8.7197e-04,
         -7.2564e-03, -1.2247e-02],
        ...,
        [-4.6989e-05, -1.2372e-03,  3.4513e-04,  ..., -1.3359e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6989e-05, -1.2372e-03,  3.4513e-04,  ..., -1.3359e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6989e-05, -1.2372e-03,  3.4513e-04,  ..., -1.3359e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1702.7782, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9517, device='cuda:0')



h[100].sum tensor(-11.7209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.3606, device='cuda:0')



h[200].sum tensor(-41.2276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4794e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0560, 0.0598, 0.1043,  ..., 0.0105, 0.0000, 0.0000],
        [0.0558, 0.0585, 0.1041,  ..., 0.0092, 0.0000, 0.0000],
        [0.0552, 0.0590, 0.1029,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66658.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3028, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3289, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3157, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0169, 0.0417, 0.0012,  ..., 0.0000, 0.0443, 0.0000],
        [0.0169, 0.0417, 0.0012,  ..., 0.0000, 0.0443, 0.0000],
        [0.0169, 0.0417, 0.0012,  ..., 0.0000, 0.0442, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546748.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5242.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-252.5259, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(183.6173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.6536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0579],
        [ 0.0496],
        [ 0.0528],
        ...,
        [-0.3716],
        [-0.3702],
        [-0.3698]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95967.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0006],
        [1.0002],
        ...,
        [1.0006],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367550.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0007],
        [1.0002],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367564.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1349.6230, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.6817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5680, device='cuda:0')



h[100].sum tensor(-8.0287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7757, device='cuda:0')



h[200].sum tensor(-28.2853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0233e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54859.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0384, 0.0019,  ..., 0.0000, 0.0400, 0.0000],
        [0.0168, 0.0407, 0.0015,  ..., 0.0000, 0.0427, 0.0000],
        [0.0166, 0.0411, 0.0013,  ..., 0.0000, 0.0434, 0.0000],
        ...,
        [0.0169, 0.0419, 0.0014,  ..., 0.0000, 0.0442, 0.0000],
        [0.0169, 0.0419, 0.0014,  ..., 0.0000, 0.0442, 0.0000],
        [0.0169, 0.0419, 0.0014,  ..., 0.0000, 0.0442, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493167.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4226.1514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-167.4599, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(73.4598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.6799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1928],
        [-0.2918],
        [-0.3714],
        ...,
        [-0.3740],
        [-0.3727],
        [-0.3722]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94046.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0007],
        [1.0002],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367564.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0007],
        [1.0002],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367564.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00],
        [-4.9512e-05, -1.2393e-03,  3.4573e-04,  ..., -1.3363e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1278.9849, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.1673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9705, device='cuda:0')



h[100].sum tensor(-7.3285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3510, device='cuda:0')



h[200].sum tensor(-25.8185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.2465e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51949.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0166, 0.0411, 0.0013,  ..., 0.0000, 0.0434, 0.0000],
        [0.0166, 0.0411, 0.0013,  ..., 0.0000, 0.0434, 0.0000],
        [0.0166, 0.0411, 0.0013,  ..., 0.0000, 0.0434, 0.0000],
        ...,
        [0.0169, 0.0419, 0.0014,  ..., 0.0000, 0.0442, 0.0000],
        [0.0169, 0.0419, 0.0014,  ..., 0.0000, 0.0442, 0.0000],
        [0.0169, 0.0419, 0.0014,  ..., 0.0000, 0.0442, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478305.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3932.7363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-147.5519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(47.3577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.0624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4425],
        [-0.4668],
        [-0.4854],
        ...,
        [-0.3740],
        [-0.3727],
        [-0.3722]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99458.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0007],
        [1.0002],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367564.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0008],
        [1.0003],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367577.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1959e-02,  1.2348e-02,  2.2394e-02,  ...,  1.7753e-03,
         -1.0201e-02, -1.7235e-02],
        [ 2.0134e-02,  2.1602e-02,  3.7391e-02,  ...,  3.8932e-03,
         -1.7143e-02, -2.8964e-02],
        [ 1.3710e-02,  1.4331e-02,  2.5607e-02,  ...,  2.2290e-03,
         -1.1688e-02, -1.9748e-02],
        ...,
        [-5.3002e-05, -1.2500e-03,  3.5673e-04,  ..., -1.3367e-03,
          0.0000e+00,  0.0000e+00],
        [-5.3002e-05, -1.2500e-03,  3.5673e-04,  ..., -1.3367e-03,
          0.0000e+00,  0.0000e+00],
        [-5.3002e-05, -1.2500e-03,  3.5673e-04,  ..., -1.3367e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2381.3071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.1807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.0604, device='cuda:0')



h[100].sum tensor(-17.9826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.0513, device='cuda:0')



h[200].sum tensor(-63.4535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.2890e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0331, 0.0339, 0.0625,  ..., 0.0051, 0.0000, 0.0000],
        [0.0484, 0.0501, 0.0907,  ..., 0.0078, 0.0000, 0.0000],
        [0.0928, 0.1003, 0.1721,  ..., 0.0188, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92375.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1740, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2432, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3444, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0170, 0.0418, 0.0018,  ..., 0.0000, 0.0440, 0.0000],
        [0.0170, 0.0418, 0.0018,  ..., 0.0000, 0.0440, 0.0000],
        [0.0170, 0.0418, 0.0018,  ..., 0.0000, 0.0440, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712339.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8348.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-439.9293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(415.3181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.8867, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1833],
        [ 0.1687],
        [ 0.1476],
        ...,
        [-0.3737],
        [-0.3724],
        [-0.3720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77936.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0008],
        [1.0003],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367577.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0008],
        [1.0003],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367591.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0476e-02,  1.0680e-02,  1.9686e-02,  ...,  1.3929e-03,
         -8.9357e-03, -1.5105e-02],
        [ 4.7172e-03,  4.1610e-03,  9.1215e-03,  ..., -9.9039e-05,
         -4.0518e-03, -6.8493e-03],
        [ 1.8268e-02,  1.9501e-02,  3.3980e-02,  ...,  3.4115e-03,
         -1.5544e-02, -2.6276e-02],
        ...,
        [-6.0353e-05, -1.2476e-03,  3.5693e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        [-6.0353e-05, -1.2476e-03,  3.5693e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        [-6.0353e-05, -1.2476e-03,  3.5693e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1679.9563, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.4385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1621, device='cuda:0')



h[100].sum tensor(-10.8108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.7646, device='cuda:0')



h[200].sum tensor(-38.2078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3688e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0234, 0.0217, 0.0448,  ..., 0.0013, 0.0000, 0.0000],
        [0.0420, 0.0428, 0.0789,  ..., 0.0057, 0.0000, 0.0000],
        [0.0235, 0.0230, 0.0448,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0015,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66458.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1359, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1353, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1142, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0418, 0.0018,  ..., 0.0000, 0.0435, 0.0000],
        [0.0171, 0.0417, 0.0018,  ..., 0.0000, 0.0435, 0.0000],
        [0.0171, 0.0417, 0.0018,  ..., 0.0000, 0.0435, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557771.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5512.3101, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-254.3201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(177.2394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.5955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1906],
        [ 0.1846],
        [ 0.1722],
        ...,
        [-0.3735],
        [-0.3722],
        [-0.3718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75440.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0008],
        [1.0003],
        ...,
        [1.0006],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367591.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(468.8389, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0009],
        [1.0003],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367604.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7359e-03,  7.6145e-03,  1.4669e-02,  ...,  6.8737e-04,
         -6.6172e-03, -1.1192e-02],
        [ 7.9014e-03,  7.8019e-03,  1.4973e-02,  ...,  7.3025e-04,
         -6.7574e-03, -1.1429e-02],
        [-7.6291e-05, -1.2321e-03,  3.3876e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.6291e-05, -1.2321e-03,  3.3876e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        [-7.6291e-05, -1.2321e-03,  3.3876e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        [-7.6291e-05, -1.2321e-03,  3.3876e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1668.8740, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.2817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8303, device='cuda:0')



h[100].sum tensor(-10.5044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.4687, device='cuda:0')



h[200].sum tensor(-37.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3483e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0429, 0.0440, 0.0807,  ..., 0.0058, 0.0000, 0.0000],
        [0.0142, 0.0138, 0.0277,  ..., 0.0010, 0.0000, 0.0000],
        [0.0079, 0.0078, 0.0160,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64733.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1234, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0746, 0.0023, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0466, 0.0136, 0.0000,  ..., 0.0000, 0.0143, 0.0000],
        ...,
        [0.0171, 0.0421, 0.0015,  ..., 0.0000, 0.0431, 0.0000],
        [0.0171, 0.0421, 0.0015,  ..., 0.0000, 0.0431, 0.0000],
        [0.0171, 0.0421, 0.0015,  ..., 0.0000, 0.0431, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550016.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5320.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-245.2612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(163.3280, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.1886, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0088],
        [-0.0949],
        [-0.2166],
        ...,
        [-0.3760],
        [-0.3748],
        [-0.3744]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84555.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0009],
        [1.0003],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367604.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0009],
        [1.0004],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367618.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9371e-05, -1.2245e-03,  3.2526e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.3827e-02,  2.5864e-02,  4.4193e-02,  ...,  4.8603e-03,
         -2.0233e-02, -3.4238e-02],
        [-8.9371e-05, -1.2245e-03,  3.2526e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-8.9371e-05, -1.2245e-03,  3.2526e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        [-8.9371e-05, -1.2245e-03,  3.2526e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00],
        [-8.9371e-05, -1.2245e-03,  3.2526e-04,  ..., -1.3368e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1657.6384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2233, device='cuda:0')



h[100].sum tensor(-10.2468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.9274, device='cuda:0')



h[200].sum tensor(-36.3298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3109e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0541, 0.0567, 0.1011,  ..., 0.0087, 0.0000, 0.0000],
        [0.0194, 0.0197, 0.0372,  ..., 0.0024, 0.0000, 0.0000],
        [0.0562, 0.0592, 0.1051,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63295.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1276, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1306, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0425, 0.0013,  ..., 0.0000, 0.0430, 0.0000],
        [0.0171, 0.0425, 0.0013,  ..., 0.0000, 0.0430, 0.0000],
        [0.0171, 0.0425, 0.0013,  ..., 0.0000, 0.0430, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533613.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4962.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-237.0210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.2934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.6282, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0520],
        [-0.0647],
        [-0.0825],
        ...,
        [-0.3755],
        [-0.3743],
        [-0.3744]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93570.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0009],
        [1.0004],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367618.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0010],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367632.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0058,  0.0116,  ...,  0.0003, -0.0052, -0.0088],
        [ 0.0183,  0.0196,  0.0341,  ...,  0.0034, -0.0155, -0.0263],
        [ 0.0226,  0.0245,  0.0419,  ...,  0.0045, -0.0192, -0.0324],
        ...,
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1509.5122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.4785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0421, device='cuda:0')



h[100].sum tensor(-8.6797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.0903, device='cuda:0')



h[200].sum tensor(-30.8229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1144e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0279, 0.0504,  ..., 0.0043, 0.0000, 0.0000],
        [0.0626, 0.0665, 0.1169,  ..., 0.0110, 0.0000, 0.0000],
        [0.1038, 0.1132, 0.1924,  ..., 0.0217, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57843.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1525, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2508, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3609, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0172, 0.0429, 0.0011,  ..., 0.0000, 0.0428, 0.0000],
        [0.0172, 0.0429, 0.0011,  ..., 0.0000, 0.0428, 0.0000],
        [0.0172, 0.0429, 0.0011,  ..., 0.0000, 0.0428, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506714.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4502.0269, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-195.0464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.8461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.1098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1198],
        [ 0.1055],
        [ 0.0845],
        ...,
        [-0.3828],
        [-0.3815],
        [-0.3811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83217.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0010],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367632.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0011],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367645.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0093,  0.0094,  0.0175,  ...,  0.0011, -0.0079, -0.0134],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1483.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.3855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5645, device='cuda:0')



h[100].sum tensor(-8.3252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6644, device='cuda:0')



h[200].sum tensor(-29.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0849e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0191, 0.0361,  ..., 0.0022, 0.0000, 0.0000],
        [0.0093, 0.0094, 0.0185,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55910.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0941, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0586, 0.0086, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0314, 0.0195, 0.0011,  ..., 0.0000, 0.0188, 0.0000],
        ...,
        [0.0174, 0.0432, 0.0010,  ..., 0.0000, 0.0428, 0.0000],
        [0.0174, 0.0432, 0.0010,  ..., 0.0000, 0.0428, 0.0000],
        [0.0174, 0.0432, 0.0010,  ..., 0.0000, 0.0428, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496616.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4322.6084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-185.2916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(75.8104, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.2220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0396],
        [ 0.0010],
        [-0.0462],
        ...,
        [-0.3864],
        [-0.3852],
        [-0.3848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98722.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0011],
        [1.0004],
        ...,
        [1.0006],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367645.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0011],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367659.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101,  0.0103,  0.0190,  ...,  0.0013, -0.0086, -0.0146],
        [ 0.0043,  0.0038,  0.0085,  ..., -0.0002, -0.0037, -0.0064],
        [ 0.0056,  0.0053,  0.0109,  ...,  0.0002, -0.0049, -0.0082],
        ...,
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0012,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1420.4048, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.7220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0502, device='cuda:0')



h[100].sum tensor(-7.6423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3139, device='cuda:0')



h[200].sum tensor(-27.2259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9134e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0166, 0.0155, 0.0323,  ..., 0.0009, 0.0000, 0.0000],
        [0.0368, 0.0373, 0.0696,  ..., 0.0043, 0.0000, 0.0000],
        [0.0167, 0.0167, 0.0322,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53891.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.3673e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0654e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.9585e-02, 1.7114e-05, 0.0000e+00,  ..., 0.0000e+00, 2.2959e-05,
         0.0000e+00],
        ...,
        [1.7559e-02, 4.3535e-02, 8.8881e-04,  ..., 0.0000e+00, 4.2904e-02,
         0.0000e+00],
        [1.7554e-02, 4.3524e-02, 8.8798e-04,  ..., 0.0000e+00, 4.2893e-02,
         0.0000e+00],
        [1.7552e-02, 4.3518e-02, 8.8716e-04,  ..., 0.0000e+00, 4.2886e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491567.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4205.6362, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-170.6410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(53.6408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.5404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0842],
        [ 0.1114],
        [ 0.1049],
        ...,
        [-0.3906],
        [-0.3893],
        [-0.3889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105254.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0011],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367659.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0011],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367674., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9870e-03,  4.5375e-03,  9.6833e-03,  ..., -1.4676e-05,
         -4.2958e-03, -7.2847e-03],
        [-1.1720e-04, -1.2463e-03,  3.2056e-04,  ..., -1.3375e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1720e-04, -1.2463e-03,  3.2056e-04,  ..., -1.3375e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.1720e-04, -1.2463e-03,  3.2056e-04,  ..., -1.3375e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1720e-04, -1.2463e-03,  3.2056e-04,  ..., -1.3375e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1720e-04, -1.2463e-03,  3.2056e-04,  ..., -1.3375e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1489.8887, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.1270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3386, device='cuda:0')



h[100].sum tensor(-8.2227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4630, device='cuda:0')



h[200].sum tensor(-29.3406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0709e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0171, 0.0330,  ..., 0.0019, 0.0000, 0.0000],
        [0.0050, 0.0045, 0.0107,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56534.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0823, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0513, 0.0117, 0.0002,  ..., 0.0000, 0.0113, 0.0000],
        [0.0303, 0.0229, 0.0013,  ..., 0.0000, 0.0219, 0.0000],
        ...,
        [0.0177, 0.0436, 0.0010,  ..., 0.0000, 0.0431, 0.0000],
        [0.0177, 0.0436, 0.0010,  ..., 0.0000, 0.0431, 0.0000],
        [0.0177, 0.0436, 0.0010,  ..., 0.0000, 0.0431, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501738.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4467.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-187.5667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(73.0349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.7001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1042],
        [ 0.0529],
        [-0.0220],
        ...,
        [-0.3837],
        [-0.3884],
        [-0.3898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97290.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0011],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367674., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0012],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367688., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7508e-03,  4.2459e-03,  9.2511e-03,  ..., -7.7681e-05,
         -4.0871e-03, -6.9344e-03],
        [-1.1212e-04, -1.2641e-03,  3.3015e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1212e-04, -1.2641e-03,  3.3015e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.1212e-04, -1.2641e-03,  3.3015e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1212e-04, -1.2641e-03,  3.3015e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1212e-04, -1.2641e-03,  3.3015e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1517.4425, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.7706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5187, device='cuda:0')



h[100].sum tensor(-8.4167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6236, device='cuda:0')



h[200].sum tensor(-30.0808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0820e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0100, 0.0236,  ..., 0.0000, 0.0000, 0.0000],
        [0.0086, 0.0075, 0.0176,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59062.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0623, 0.0018, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0497, 0.0084, 0.0003,  ..., 0.0000, 0.0081, 0.0000],
        [0.0296, 0.0224, 0.0014,  ..., 0.0000, 0.0216, 0.0000],
        ...,
        [0.0179, 0.0437, 0.0011,  ..., 0.0000, 0.0434, 0.0000],
        [0.0179, 0.0437, 0.0011,  ..., 0.0000, 0.0434, 0.0000],
        [0.0179, 0.0437, 0.0011,  ..., 0.0000, 0.0434, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522469.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4847.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-204.9932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(92.7054, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.4439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1114],
        [ 0.0616],
        [-0.0280],
        ...,
        [-0.3970],
        [-0.3957],
        [-0.3953]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96067.1953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0012],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367688., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0012],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367701.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0214,  0.0231,  0.0398,  ...,  0.0042, -0.0181, -0.0306],
        [ 0.0345,  0.0379,  0.0638,  ...,  0.0076, -0.0290, -0.0493],
        [ 0.0245,  0.0265,  0.0454,  ...,  0.0050, -0.0206, -0.0350],
        ...,
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1296.0613, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.5094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3043, device='cuda:0')



h[100].sum tensor(-6.3359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8650, device='cuda:0')



h[200].sum tensor(-22.6806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.2173e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0927, 0.1003, 0.1721,  ..., 0.0188, 0.0000, 0.0000],
        [0.1188, 0.1299, 0.2200,  ..., 0.0255, 0.0000, 0.0000],
        [0.1255, 0.1376, 0.2324,  ..., 0.0273, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50523.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3220, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4109, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0179, 0.0441, 0.0009,  ..., 0.0000, 0.0441, 0.0000],
        [0.0179, 0.0441, 0.0009,  ..., 0.0000, 0.0441, 0.0000],
        [0.0179, 0.0441, 0.0009,  ..., 0.0000, 0.0440, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479657.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3985.0139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-141.5460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(11.1477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.9054, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1147],
        [ 0.1244],
        [ 0.1324],
        ...,
        [-0.4029],
        [-0.4016],
        [-0.4011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110895.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0012],
        [1.0005],
        ...,
        [1.0005],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367701.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0013],
        [1.0005],
        ...,
        [1.0005],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367715.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0158e-02,  1.0315e-02,  1.9148e-02,  ...,  1.3172e-03,
         -8.5908e-03, -1.4591e-02],
        [ 4.8585e-03,  4.3118e-03,  9.4241e-03,  ..., -5.5976e-05,
         -4.1493e-03, -7.0474e-03],
        [ 5.2071e-03,  4.7067e-03,  1.0064e-02,  ...,  3.4364e-05,
         -4.4415e-03, -7.5437e-03],
        ...,
        [-9.2516e-05, -1.2964e-03,  3.4034e-04,  ..., -1.3388e-03,
          0.0000e+00,  0.0000e+00],
        [-9.2516e-05, -1.2964e-03,  3.4034e-04,  ..., -1.3388e-03,
          0.0000e+00,  0.0000e+00],
        [-9.2516e-05, -1.2964e-03,  3.4034e-04,  ..., -1.3388e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1605.6079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.2259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3258, device='cuda:0')



h[100].sum tensor(-9.1830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.2352, device='cuda:0')



h[200].sum tensor(-32.9251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1937e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0153, 0.0324,  ..., 0.0007, 0.0000, 0.0000],
        [0.0351, 0.0350, 0.0665,  ..., 0.0038, 0.0000, 0.0000],
        [0.0167, 0.0166, 0.0324,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60576.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0821, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1024, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0769, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        ...,
        [0.0180, 0.0441, 0.0011,  ..., 0.0000, 0.0444, 0.0000],
        [0.0180, 0.0441, 0.0011,  ..., 0.0000, 0.0444, 0.0000],
        [0.0180, 0.0441, 0.0011,  ..., 0.0000, 0.0444, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525532.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4924.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-208.8510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.3895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.8366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1147],
        [ 0.1067],
        [ 0.0390],
        ...,
        [-0.4053],
        [-0.4038],
        [-0.4034]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90951.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0013],
        [1.0005],
        ...,
        [1.0005],
        [1.0000],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367715.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0013],
        [1.0006],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367728.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6495e-02,  1.7476e-02,  3.0764e-02,  ...,  2.9565e-03,
         -1.3876e-02, -2.3580e-02],
        [-8.5373e-05, -1.3041e-03,  3.4216e-04,  ..., -1.3392e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5373e-05, -1.3041e-03,  3.4216e-04,  ..., -1.3392e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-8.5373e-05, -1.3041e-03,  3.4216e-04,  ..., -1.3392e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5373e-05, -1.3041e-03,  3.4216e-04,  ..., -1.3392e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5373e-05, -1.3041e-03,  3.4216e-04,  ..., -1.3392e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1676.2792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6125, device='cuda:0')



h[100].sum tensor(-9.7542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.3827, device='cuda:0')



h[200].sum tensor(-35.0297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2731e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0609, 0.0653, 0.1135,  ..., 0.0118, 0.0000, 0.0000],
        [0.0413, 0.0431, 0.0775,  ..., 0.0067, 0.0000, 0.0000],
        [0.0245, 0.0241, 0.0468,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61979.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2669, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2236, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1801, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0180, 0.0442, 0.0011,  ..., 0.0000, 0.0446, 0.0000],
        [0.0180, 0.0442, 0.0011,  ..., 0.0000, 0.0446, 0.0000],
        [0.0180, 0.0442, 0.0011,  ..., 0.0000, 0.0446, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530376.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5023.2793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-217.8046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.4997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(47.7802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1735],
        [ 0.1817],
        [ 0.1890],
        ...,
        [-0.4075],
        [-0.4061],
        [-0.4056]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88310.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0013],
        [1.0006],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367728.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(573.1549, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0014],
        [1.0006],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367741.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.0205e-05, -1.3013e-03,  3.3396e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-8.0205e-05, -1.3013e-03,  3.3396e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-8.0205e-05, -1.3013e-03,  3.3396e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-8.0205e-05, -1.3013e-03,  3.3396e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-8.0205e-05, -1.3013e-03,  3.3396e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-8.0205e-05, -1.3013e-03,  3.3396e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1601.5610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.2873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3039, device='cuda:0')



h[100].sum tensor(-8.9793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.2156, device='cuda:0')



h[200].sum tensor(-32.2987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1923e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.3392e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.3395e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.2979e-03, 4.7890e-03, 1.1206e-02,  ..., 5.2512e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.3632e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.3629e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.3628e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59883.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0196, 0.0400, 0.0015,  ..., 0.0000, 0.0398, 0.0000],
        [0.0258, 0.0300, 0.0013,  ..., 0.0000, 0.0301, 0.0000],
        [0.0442, 0.0147, 0.0005,  ..., 0.0000, 0.0148, 0.0000],
        ...,
        [0.0179, 0.0444, 0.0008,  ..., 0.0000, 0.0449, 0.0000],
        [0.0179, 0.0444, 0.0008,  ..., 0.0000, 0.0449, 0.0000],
        [0.0179, 0.0444, 0.0008,  ..., 0.0000, 0.0449, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528015.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4878.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-203.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.5676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.3926, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1844],
        [-0.0753],
        [ 0.0342],
        ...,
        [-0.4102],
        [-0.4088],
        [-0.4083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105759.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0014],
        [1.0006],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367741.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0014],
        [1.0006],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367754.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2180e-03,  5.8325e-03,  1.1864e-02,  ...,  2.9026e-04,
         -5.2521e-03, -8.9345e-03],
        [-7.4927e-05, -1.2951e-03,  3.1701e-04,  ..., -1.3400e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.2180e-03,  5.8325e-03,  1.1864e-02,  ...,  2.9026e-04,
         -5.2521e-03, -8.9345e-03],
        ...,
        [-7.4927e-05, -1.2951e-03,  3.1701e-04,  ..., -1.3400e-03,
          0.0000e+00,  0.0000e+00],
        [-7.4927e-05, -1.2951e-03,  3.1701e-04,  ..., -1.3400e-03,
          0.0000e+00,  0.0000e+00],
        [-7.4927e-05, -1.2951e-03,  3.1701e-04,  ..., -1.3400e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2489.5151, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.8712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.9511, device='cuda:0')



h[100].sum tensor(-17.1961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.0619, device='cuda:0')



h[200].sum tensor(-61.9548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.2205e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0129, 0.0122, 0.0252,  ..., 0.0010, 0.0000, 0.0000],
        [0.0266, 0.0253, 0.0506,  ..., 0.0016, 0.0000, 0.0000],
        [0.0051, 0.0045, 0.0107,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92333.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0734, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0812, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0555, 0.0063, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        ...,
        [0.0177, 0.0448, 0.0005,  ..., 0.0000, 0.0453, 0.0000],
        [0.0177, 0.0448, 0.0005,  ..., 0.0000, 0.0453, 0.0000],
        [0.0177, 0.0448, 0.0005,  ..., 0.0000, 0.0453, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716995.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8330.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-438.6387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(397.7310, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(64.6570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1449],
        [ 0.1415],
        [ 0.1031],
        ...,
        [-0.4141],
        [-0.4126],
        [-0.4122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114010.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0014],
        [1.0006],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367754.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0015],
        [1.0007],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367766.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8230e-03,  4.2465e-03,  9.2820e-03,  ..., -7.3237e-05,
         -4.0772e-03, -6.9394e-03],
        [ 4.8230e-03,  4.2465e-03,  9.2820e-03,  ..., -7.3237e-05,
         -4.0772e-03, -6.9394e-03],
        [-6.9241e-05, -1.2937e-03,  3.0538e-04,  ..., -1.3405e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.9241e-05, -1.2937e-03,  3.0538e-04,  ..., -1.3405e-03,
          0.0000e+00,  0.0000e+00],
        [-6.9241e-05, -1.2937e-03,  3.0538e-04,  ..., -1.3405e-03,
          0.0000e+00,  0.0000e+00],
        [-6.9241e-05, -1.2937e-03,  3.0538e-04,  ..., -1.3405e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1639.2004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8133, device='cuda:0')



h[100].sum tensor(-9.2552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.6699, device='cuda:0')



h[200].sum tensor(-33.3990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2238e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0154, 0.0138, 0.0299,  ..., 0.0006, 0.0000, 0.0000],
        [0.0121, 0.0100, 0.0238,  ..., 0.0000, 0.0000, 0.0000],
        [0.0088, 0.0075, 0.0176,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59930.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.4090e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.4154e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.8068e-05,
         0.0000e+00],
        [5.1605e-02, 2.7667e-03, 3.0071e-05,  ..., 0.0000e+00, 3.1937e-03,
         0.0000e+00],
        ...,
        [1.7679e-02, 4.5000e-02, 3.3653e-04,  ..., 0.0000e+00, 4.5653e-02,
         0.0000e+00],
        [1.7674e-02, 4.4988e-02, 3.3594e-04,  ..., 0.0000e+00, 4.5641e-02,
         0.0000e+00],
        [1.7672e-02, 4.4982e-02, 3.3563e-04,  ..., 0.0000e+00, 4.5634e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528361.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4781.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-197.2772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.9651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.1663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1507],
        [ 0.1366],
        [ 0.1038],
        ...,
        [-0.4166],
        [-0.4151],
        [-0.4146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102556.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0015],
        [1.0007],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367766.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0015],
        [1.0007],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367779.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4317e-05, -1.2960e-03,  3.0190e-04,  ..., -1.3409e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2650e-02,  1.3100e-02,  2.3632e-02,  ...,  1.9523e-03,
         -1.0581e-02, -1.8019e-02],
        [-6.4317e-05, -1.2960e-03,  3.0190e-04,  ..., -1.3409e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.4317e-05, -1.2960e-03,  3.0190e-04,  ..., -1.3409e-03,
          0.0000e+00,  0.0000e+00],
        [-6.4317e-05, -1.2960e-03,  3.0190e-04,  ..., -1.3409e-03,
          0.0000e+00,  0.0000e+00],
        [-6.4317e-05, -1.2960e-03,  3.0190e-04,  ..., -1.3409e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1477.4346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.0043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3441, device='cuda:0')



h[100].sum tensor(-7.6568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5760, device='cuda:0')



h[200].sum tensor(-27.6757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0095e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0265, 0.0251, 0.0503,  ..., 0.0020, 0.0000, 0.0000],
        [0.0103, 0.0092, 0.0203,  ..., 0.0002, 0.0000, 0.0000],
        [0.0320, 0.0313, 0.0604,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55487.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0886, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0808, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1030, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0177, 0.0452, 0.0004,  ..., 0.0000, 0.0458, 0.0000],
        [0.0177, 0.0451, 0.0004,  ..., 0.0000, 0.0458, 0.0000],
        [0.0177, 0.0451, 0.0004,  ..., 0.0000, 0.0458, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510266.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4403.7158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-164.3291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(57.4880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.9091, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2249],
        [ 0.2251],
        [ 0.2278],
        ...,
        [-0.4187],
        [-0.4171],
        [-0.4165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106527.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0015],
        [1.0007],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367779.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0016],
        [1.0007],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367792.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1552e-02,  1.1849e-02,  2.1616e-02,  ...,  1.6673e-03,
         -9.6524e-03, -1.6446e-02],
        [ 1.0189e-02,  1.0306e-02,  1.9115e-02,  ...,  1.3143e-03,
         -8.5197e-03, -1.4516e-02],
        [-6.3260e-05, -1.3008e-03,  3.0337e-04,  ..., -1.3410e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.3260e-05, -1.3008e-03,  3.0337e-04,  ..., -1.3410e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3260e-05, -1.3008e-03,  3.0337e-04,  ..., -1.3410e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3260e-05, -1.3008e-03,  3.0337e-04,  ..., -1.3410e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1657.9379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7823, device='cuda:0')



h[100].sum tensor(-9.1404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.6423, device='cuda:0')



h[200].sum tensor(-33.0915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2219e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0466, 0.0478, 0.0872,  ..., 0.0068, 0.0000, 0.0000],
        [0.0199, 0.0201, 0.0380,  ..., 0.0025, 0.0000, 0.0000],
        [0.0102, 0.0103, 0.0201,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58706.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.2457e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.8059e-02, 1.4398e-05, 0.0000e+00,  ..., 0.0000e+00, 5.2936e-04,
         0.0000e+00],
        [5.7937e-02, 1.1369e-02, 0.0000e+00,  ..., 0.0000e+00, 1.1749e-02,
         0.0000e+00],
        ...,
        [1.7768e-02, 4.5251e-02, 4.8287e-04,  ..., 0.0000e+00, 4.5775e-02,
         0.0000e+00],
        [1.7764e-02, 4.5239e-02, 4.8222e-04,  ..., 0.0000e+00, 4.5762e-02,
         0.0000e+00],
        [1.7761e-02, 4.5233e-02, 4.8181e-04,  ..., 0.0000e+00, 4.5755e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518780.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4546.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-190.2383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(88.2407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(59.9924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0293],
        [-0.0454],
        [-0.0715],
        ...,
        [-0.4206],
        [-0.4191],
        [-0.4186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116519.8828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0016],
        [1.0007],
        ...,
        [1.0005],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367792.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0017],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367806.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5747e-03,  3.9402e-03,  8.8244e-03,  ..., -1.3978e-04,
         -3.8494e-03, -6.5623e-03],
        [ 1.1254e-02,  1.1501e-02,  2.1081e-02,  ...,  1.5900e-03,
         -9.3922e-03, -1.6011e-02],
        [ 6.0359e-03,  5.5943e-03,  1.1506e-02,  ...,  2.3865e-04,
         -5.0620e-03, -8.6294e-03],
        ...,
        [-6.3991e-05, -1.3110e-03,  3.1253e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3991e-05, -1.3110e-03,  3.1253e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3991e-05, -1.3110e-03,  3.1253e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.6665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.8419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2044, device='cuda:0')



h[100].sum tensor(-7.1000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5596, device='cuda:0')



h[200].sum tensor(-25.7463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3910e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0325, 0.0319, 0.0614,  ..., 0.0031, 0.0000, 0.0000],
        [0.0198, 0.0174, 0.0380,  ..., 0.0003, 0.0000, 0.0000],
        [0.0330, 0.0324, 0.0622,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54193.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0907, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0915, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1119, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0193, 0.0431, 0.0004,  ..., 0.0000, 0.0435, 0.0000],
        [0.0180, 0.0452, 0.0008,  ..., 0.0000, 0.0455, 0.0000],
        [0.0180, 0.0452, 0.0008,  ..., 0.0000, 0.0455, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504444.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4347.5562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-154.2448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(44.2323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.4661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0763],
        [-0.0125],
        [ 0.0286],
        ...,
        [-0.2896],
        [-0.3529],
        [-0.3965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103132.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0017],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367806.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0017],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367819.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5862e-05, -1.3205e-03,  3.2127e-04,  ..., -1.3414e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5862e-05, -1.3205e-03,  3.2127e-04,  ..., -1.3414e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5862e-05, -1.3205e-03,  3.2127e-04,  ..., -1.3414e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.5862e-05, -1.3205e-03,  3.2127e-04,  ..., -1.3414e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5862e-05, -1.3205e-03,  3.2127e-04,  ..., -1.3414e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5862e-05, -1.3205e-03,  3.2127e-04,  ..., -1.3414e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1374.9705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.7055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.2739, device='cuda:0')



h[100].sum tensor(-6.1664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8379, device='cuda:0')



h[200].sum tensor(-22.3973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.1986e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51916.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0177, 0.0443, 0.0011,  ..., 0.0000, 0.0443, 0.0000],
        [0.0187, 0.0427, 0.0011,  ..., 0.0000, 0.0428, 0.0000],
        [0.0200, 0.0403, 0.0011,  ..., 0.0000, 0.0405, 0.0000],
        ...,
        [0.0181, 0.0452, 0.0012,  ..., 0.0000, 0.0453, 0.0000],
        [0.0181, 0.0452, 0.0012,  ..., 0.0000, 0.0453, 0.0000],
        [0.0181, 0.0452, 0.0012,  ..., 0.0000, 0.0453, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494104.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4188.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-139.5790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(21.9878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.0232, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3986],
        [-0.3135],
        [-0.2005],
        ...,
        [-0.4177],
        [-0.4135],
        [-0.4101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102933.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0017],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367819.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0018],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367832.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6105e-05, -1.3281e-03,  3.2280e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6105e-05, -1.3281e-03,  3.2280e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6105e-05, -1.3281e-03,  3.2280e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.6105e-05, -1.3281e-03,  3.2280e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6105e-05, -1.3281e-03,  3.2280e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6105e-05, -1.3281e-03,  3.2280e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1468.6998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.4285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8608, device='cuda:0')



h[100].sum tensor(-6.9155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2532, device='cuda:0')



h[200].sum tensor(-25.1590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.1788e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53911.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0178, 0.0446, 0.0014,  ..., 0.0000, 0.0445, 0.0000],
        [0.0178, 0.0446, 0.0014,  ..., 0.0000, 0.0445, 0.0000],
        [0.0178, 0.0445, 0.0014,  ..., 0.0000, 0.0445, 0.0000],
        ...,
        [0.0182, 0.0455, 0.0015,  ..., 0.0000, 0.0455, 0.0000],
        [0.0182, 0.0455, 0.0015,  ..., 0.0000, 0.0455, 0.0000],
        [0.0182, 0.0455, 0.0015,  ..., 0.0000, 0.0455, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(502953.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4352.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.6143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(38.3886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.3766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3956],
        [-0.4500],
        [-0.4919],
        ...,
        [-0.4219],
        [-0.4206],
        [-0.4203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104328.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0018],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367832.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0018],
        [1.0008],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367845.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0726e-02,  1.0882e-02,  2.0121e-02,  ...,  1.4527e-03,
         -8.9182e-03, -1.5227e-02],
        [ 1.6028e-02,  1.6882e-02,  2.9850e-02,  ...,  2.8256e-03,
         -1.3300e-02, -2.2709e-02],
        ...,
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1383.3484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.6797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.2298, device='cuda:0')



h[100].sum tensor(-6.1244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7986, device='cuda:0')



h[200].sum tensor(-22.3175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.1713e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0301, 0.0290, 0.0570,  ..., 0.0026, 0.0000, 0.0000],
        [0.0248, 0.0256, 0.0471,  ..., 0.0038, 0.0000, 0.0000],
        [0.0238, 0.0245, 0.0453,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51465.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0914, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1180, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0182, 0.0461, 0.0015,  ..., 0.0000, 0.0461, 0.0000],
        [0.0182, 0.0461, 0.0015,  ..., 0.0000, 0.0460, 0.0000],
        [0.0182, 0.0461, 0.0015,  ..., 0.0000, 0.0460, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494488.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4149.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-138.6133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(13.8131, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.6647, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1695],
        [ 0.2005],
        [ 0.2130],
        ...,
        [-0.4273],
        [-0.4259],
        [-0.4254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110901.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0018],
        [1.0008],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367845.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0018],
        [1.0008],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367845.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-6.5784e-05, -1.3320e-03,  3.1669e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1532.7548, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.7229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0087, device='cuda:0')



h[100].sum tensor(-7.4638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2769, device='cuda:0')



h[200].sum tensor(-27.1983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.8877e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58074.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0410, 0.0012,  ..., 0.0000, 0.0411, 0.0000],
        [0.0252, 0.0322, 0.0010,  ..., 0.0000, 0.0327, 0.0000],
        [0.0277, 0.0276, 0.0010,  ..., 0.0000, 0.0284, 0.0000],
        ...,
        [0.0182, 0.0461, 0.0015,  ..., 0.0000, 0.0461, 0.0000],
        [0.0182, 0.0461, 0.0015,  ..., 0.0000, 0.0460, 0.0000],
        [0.0182, 0.0461, 0.0015,  ..., 0.0000, 0.0460, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532524.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4890.5708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-186.1579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(74.2815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.4528, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0581],
        [-0.0492],
        [-0.0188],
        ...,
        [-0.4269],
        [-0.4255],
        [-0.4250]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105772.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0018],
        [1.0008],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367845.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(479.8553, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0019],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367858.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4937e-02,  1.5660e-02,  2.7853e-02,  ...,  2.5449e-03,
         -1.2388e-02, -2.1163e-02],
        [-7.1757e-05, -1.3302e-03,  3.0894e-04,  ..., -1.3420e-03,
          0.0000e+00,  0.0000e+00],
        [-7.1757e-05, -1.3302e-03,  3.0894e-04,  ..., -1.3420e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.1757e-05, -1.3302e-03,  3.0894e-04,  ..., -1.3420e-03,
          0.0000e+00,  0.0000e+00],
        [-7.1757e-05, -1.3302e-03,  3.0894e-04,  ..., -1.3420e-03,
          0.0000e+00,  0.0000e+00],
        [-7.1757e-05, -1.3302e-03,  3.0894e-04,  ..., -1.3420e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1550.0848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.8493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2849, device='cuda:0')



h[100].sum tensor(-7.5915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5232, device='cuda:0')



h[200].sum tensor(-27.7086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0058e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0467, 0.0504, 0.0872,  ..., 0.0094, 0.0000, 0.0000],
        [0.0272, 0.0283, 0.0515,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57487.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1745, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1251, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0712, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0182, 0.0469, 0.0015,  ..., 0.0000, 0.0465, 0.0000],
        [0.0182, 0.0469, 0.0015,  ..., 0.0000, 0.0465, 0.0000],
        [0.0181, 0.0469, 0.0015,  ..., 0.0000, 0.0465, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527596., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4771.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-181.4867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(65.3528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.7858, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1150],
        [ 0.1186],
        [ 0.1282],
        ...,
        [-0.4334],
        [-0.4319],
        [-0.4315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103149.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0019],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367858.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0019],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367870.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6981e-02,  1.8002e-02,  3.1607e-02,  ...,  3.0781e-03,
         -1.4068e-02, -2.4045e-02],
        [ 7.5824e-03,  7.3592e-03,  1.4359e-02,  ...,  6.4375e-04,
         -6.3196e-03, -1.0802e-02],
        [ 9.7581e-03,  9.8229e-03,  1.8351e-02,  ...,  1.2073e-03,
         -8.1132e-03, -1.3868e-02],
        ...,
        [-8.3728e-05, -1.3216e-03,  2.9021e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3728e-05, -1.3216e-03,  2.9021e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3728e-05, -1.3216e-03,  2.9021e-04,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1786.7120, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0876, device='cuda:0')



h[100].sum tensor(-9.7143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.8064, device='cuda:0')



h[200].sum tensor(-35.5147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3025e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0489, 0.0505, 0.0915,  ..., 0.0074, 0.0000, 0.0000],
        [0.0491, 0.0506, 0.0918,  ..., 0.0074, 0.0000, 0.0000],
        [0.0156, 0.0152, 0.0300,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64048.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2501, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2019, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1335, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0195, 0.0448, 0.0014,  ..., 0.0000, 0.0440, 0.0000],
        [0.0229, 0.0388, 0.0017,  ..., 0.0000, 0.0380, 0.0000],
        [0.0245, 0.0358, 0.0018,  ..., 0.0000, 0.0350, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554601.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5200.8340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-227.8385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.1929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(61.5398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2215],
        [ 0.2219],
        [ 0.2210],
        ...,
        [-0.3715],
        [-0.3193],
        [-0.2840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101330.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0019],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367870.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0020],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367882.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1569.5786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.3080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9394, device='cuda:0')



h[100].sum tensor(-7.7656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.1069, device='cuda:0')



h[200].sum tensor(-28.4372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0463e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56353.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0483, 0.0190, 0.0004,  ..., 0.0000, 0.0186, 0.0000],
        [0.0256, 0.0337, 0.0009,  ..., 0.0000, 0.0331, 0.0000],
        [0.0190, 0.0444, 0.0009,  ..., 0.0000, 0.0432, 0.0000],
        ...,
        [0.0177, 0.0485, 0.0009,  ..., 0.0000, 0.0473, 0.0000],
        [0.0177, 0.0485, 0.0009,  ..., 0.0000, 0.0473, 0.0000],
        [0.0177, 0.0485, 0.0009,  ..., 0.0000, 0.0473, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(519707.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4393.8315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-174.9568, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(52.3411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(66.8878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0117],
        [-0.0839],
        [-0.1601],
        ...,
        [-0.4473],
        [-0.4458],
        [-0.4453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122767.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0020],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367882.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0020],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367882.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0089,  0.0168,  ...,  0.0010, -0.0074, -0.0127],
        [ 0.0065,  0.0061,  0.0123,  ...,  0.0004, -0.0054, -0.0093],
        [ 0.0045,  0.0039,  0.0087,  ..., -0.0002, -0.0038, -0.0065],
        ...,
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.2128, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.0362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8380, device='cuda:0')



h[100].sum tensor(-6.7724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2328, device='cuda:0')



h[200].sum tensor(-24.7999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.1646e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0309, 0.0302, 0.0585,  ..., 0.0027, 0.0000, 0.0000],
        [0.0223, 0.0205, 0.0428,  ..., 0.0010, 0.0000, 0.0000],
        [0.0178, 0.0154, 0.0345,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53084.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0997, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0971, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0884, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0177, 0.0485, 0.0009,  ..., 0.0000, 0.0473, 0.0000],
        [0.0177, 0.0485, 0.0009,  ..., 0.0000, 0.0473, 0.0000],
        [0.0177, 0.0485, 0.0009,  ..., 0.0000, 0.0473, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506211.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4158.2319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-149.2374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(21.4390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(65.5432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1325],
        [ 0.1569],
        [ 0.1646],
        ...,
        [-0.4473],
        [-0.4458],
        [-0.4453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115653.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0020],
        [1.0009],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367882.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0020],
        [1.0010],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367895.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0128,  0.0134,  0.0240,  ...,  0.0020, -0.0106, -0.0182],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1900.7906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9803, device='cuda:0')



h[100].sum tensor(-10.6335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.4943, device='cuda:0')



h[200].sum tensor(-39.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4194e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0129, 0.0134, 0.0248,  ..., 0.0020, 0.0000, 0.0000],
        [0.0427, 0.0460, 0.0797,  ..., 0.0084, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67679.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0410, 0.0236, 0.0008,  ..., 0.0000, 0.0227, 0.0000],
        [0.1078, 0.0097, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.2206, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0175, 0.0491, 0.0006,  ..., 0.0000, 0.0474, 0.0000],
        [0.0175, 0.0491, 0.0006,  ..., 0.0000, 0.0474, 0.0000],
        [0.0175, 0.0491, 0.0006,  ..., 0.0000, 0.0474, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582874.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5545.3604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-254.5929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.8780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(71.7683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0187],
        [ 0.0558],
        [ 0.0940],
        ...,
        [-0.4529],
        [-0.4513],
        [-0.4509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115189.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0020],
        [1.0010],
        ...,
        [1.0006],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367895.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0021],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367907.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1579.7112, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.3840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1290, device='cuda:0')



h[100].sum tensor(-7.6983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2760, device='cuda:0')



h[200].sum tensor(-28.2830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0580e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56847.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0170, 0.0484, 0.0005,  ..., 0.0000, 0.0463, 0.0000],
        [0.0173, 0.0480, 0.0008,  ..., 0.0000, 0.0457, 0.0000],
        [0.0180, 0.0468, 0.0014,  ..., 0.0000, 0.0441, 0.0000],
        ...,
        [0.0174, 0.0495, 0.0006,  ..., 0.0000, 0.0474, 0.0000],
        [0.0174, 0.0494, 0.0006,  ..., 0.0000, 0.0474, 0.0000],
        [0.0174, 0.0494, 0.0006,  ..., 0.0000, 0.0474, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526428.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4491.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-174.7826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(59.5621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.4316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5002],
        [-0.4406],
        [-0.3509],
        ...,
        [-0.4553],
        [-0.4535],
        [-0.4527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104379.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0021],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367907.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0021],
        [1.0010],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367920.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0152,  0.0161,  0.0284,  ...,  0.0026, -0.0126, -0.0216],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1343.5784, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(8.0705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.9928, device='cuda:0')



h[100].sum tensor(-5.4761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6954, device='cuda:0')



h[200].sum tensor(-20.1520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.4073e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0428, 0.0452, 0.0803,  ..., 0.0074, 0.0000, 0.0000],
        [0.0277, 0.0292, 0.0523,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49860.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1712, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1172, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0484, 0.0216, 0.0000,  ..., 0.0000, 0.0206, 0.0000],
        ...,
        [0.0175, 0.0495, 0.0006,  ..., 0.0000, 0.0472, 0.0000],
        [0.0175, 0.0495, 0.0006,  ..., 0.0000, 0.0472, 0.0000],
        [0.0175, 0.0495, 0.0006,  ..., 0.0000, 0.0472, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495675.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3816.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-127.3834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-0.7484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(76.5314, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0811],
        [ 0.0102],
        [-0.1091],
        ...,
        [-0.4572],
        [-0.4557],
        [-0.4552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124075.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0021],
        [1.0010],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367920.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0022],
        [1.0011],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367933.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3672e-04, -1.2743e-03,  2.4884e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.0479e-03,  4.6062e-03,  9.7628e-03,  ...,  3.8513e-06,
         -4.2481e-03, -7.2806e-03],
        [-1.3672e-04, -1.2743e-03,  2.4884e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.3672e-04, -1.2743e-03,  2.4884e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3672e-04, -1.2743e-03,  2.4884e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3672e-04, -1.2743e-03,  2.4884e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2598.3489, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.6095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.7156, device='cuda:0')



h[100].sum tensor(-16.1430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.8519, device='cuda:0')



h[200].sum tensor(-59.5039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.2060e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0333, 0.0333, 0.0632,  ..., 0.0039, 0.0000, 0.0000],
        [0.0041, 0.0035, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        [0.0322, 0.0332, 0.0608,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85300.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1593, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1440, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2005, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0178, 0.0493, 0.0009,  ..., 0.0000, 0.0467, 0.0000],
        [0.0178, 0.0493, 0.0009,  ..., 0.0000, 0.0467, 0.0000],
        [0.0178, 0.0493, 0.0009,  ..., 0.0000, 0.0467, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657411.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7062.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-384.6299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(326.2628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(79.4088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0215],
        [ 0.0090],
        [-0.0034],
        ...,
        [-0.4548],
        [-0.4534],
        [-0.4531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102578.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0022],
        [1.0011],
        ...,
        [1.0007],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367933.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0022],
        [1.0011],
        ...,
        [1.0007],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367945.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0197,  0.0212,  0.0366,  ...,  0.0038, -0.0162, -0.0278],
        [ 0.0213,  0.0230,  0.0396,  ...,  0.0042, -0.0175, -0.0301],
        [ 0.0198,  0.0214,  0.0369,  ...,  0.0038, -0.0163, -0.0280],
        ...,
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1843.1572, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.0988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7258, device='cuda:0')



h[100].sum tensor(-9.3654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.4837, device='cuda:0')



h[200].sum tensor(-34.5783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2801e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0746, 0.0801, 0.1390,  ..., 0.0141, 0.0000, 0.0000],
        [0.0861, 0.0931, 0.1600,  ..., 0.0171, 0.0000, 0.0000],
        [0.0894, 0.0968, 0.1661,  ..., 0.0179, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64450.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3541, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3618, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3426, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0182, 0.0490, 0.0014,  ..., 0.0000, 0.0462, 0.0000],
        [0.0182, 0.0490, 0.0014,  ..., 0.0000, 0.0462, 0.0000],
        [0.0182, 0.0490, 0.0014,  ..., 0.0000, 0.0462, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560544.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5343.4966, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-232.1683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.7524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.3035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0335],
        [ 0.0382],
        [ 0.0474],
        ...,
        [-0.4538],
        [-0.4524],
        [-0.4519]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92366.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0022],
        [1.0011],
        ...,
        [1.0007],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367945.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0023],
        [1.0011],
        ...,
        [1.0008],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367958.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2501e-02,  2.4365e-02,  4.1821e-02,  ...,  4.5265e-03,
         -1.8491e-02, -3.1725e-02],
        [ 1.5678e-02,  1.6627e-02,  2.9299e-02,  ...,  2.7580e-03,
         -1.2917e-02, -2.2161e-02],
        [ 5.1580e-03,  4.6946e-03,  9.9899e-03,  ...,  3.0769e-05,
         -4.3199e-03, -7.4116e-03],
        ...,
        [-1.2854e-04, -1.3016e-03,  2.8688e-04,  ..., -1.3397e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2854e-04, -1.3016e-03,  2.8688e-04,  ..., -1.3397e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2854e-04, -1.3016e-03,  2.8688e-04,  ..., -1.3397e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1748.2501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.0842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6501, device='cuda:0')



h[100].sum tensor(-8.3888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6325, device='cuda:0')



h[200].sum tensor(-31.0236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1519e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0817, 0.0881, 0.1521,  ..., 0.0160, 0.0000, 0.0000],
        [0.0455, 0.0469, 0.0856,  ..., 0.0065, 0.0000, 0.0000],
        [0.0388, 0.0394, 0.0733,  ..., 0.0048, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60143.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2880, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2170, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1725, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0185, 0.0488, 0.0018,  ..., 0.0000, 0.0460, 0.0000],
        [0.0185, 0.0488, 0.0018,  ..., 0.0000, 0.0459, 0.0000],
        [0.0185, 0.0488, 0.0018,  ..., 0.0000, 0.0459, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537785.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4962.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-200.3348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.4203, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(74.5445, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1190],
        [ 0.1331],
        [ 0.1409],
        ...,
        [-0.4533],
        [-0.4519],
        [-0.4514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92583.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0023],
        [1.0011],
        ...,
        [1.0008],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367958.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(502.2291, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0023],
        [1.0011],
        ...,
        [1.0008],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367970.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0060,  0.0056,  0.0115,  ...,  0.0002, -0.0050, -0.0086],
        ...,
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1649.4521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.1990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5878, device='cuda:0')



h[100].sum tensor(-7.4533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7933, device='cuda:0')



h[200].sum tensor(-27.6096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0245e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0060, 0.0056, 0.0124,  ..., 0.0002, 0.0000, 0.0000],
        [0.0122, 0.0114, 0.0240,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59104.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0255, 0.0017,  ..., 0.0000, 0.0235, 0.0000],
        [0.0450, 0.0135, 0.0008,  ..., 0.0000, 0.0125, 0.0000],
        [0.0645, 0.0040, 0.0002,  ..., 0.0000, 0.0037, 0.0000],
        ...,
        [0.0187, 0.0488, 0.0019,  ..., 0.0000, 0.0461, 0.0000],
        [0.0187, 0.0488, 0.0019,  ..., 0.0000, 0.0461, 0.0000],
        [0.0187, 0.0488, 0.0019,  ..., 0.0000, 0.0461, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540119.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5016.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-192.0377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.1621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(74.1961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1303e-04],
        [ 6.0306e-02],
        [ 1.0260e-01],
        ...,
        [-4.5570e-01],
        [-4.5425e-01],
        [-4.5379e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98300.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0023],
        [1.0011],
        ...,
        [1.0008],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367970.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0024],
        [1.0012],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367983.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0170,  0.0180,  0.0316,  ...,  0.0031, -0.0139, -0.0239],
        [ 0.0157,  0.0166,  0.0294,  ...,  0.0028, -0.0129, -0.0222],
        [ 0.0039,  0.0032,  0.0077,  ..., -0.0003, -0.0033, -0.0056],
        ...,
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1849.5376, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2324, device='cuda:0')



h[100].sum tensor(-9.1208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.0437, device='cuda:0')



h[200].sum tensor(-33.8427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2496e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0768, 0.0823, 0.1431,  ..., 0.0147, 0.0000, 0.0000],
        [0.0563, 0.0591, 0.1054,  ..., 0.0096, 0.0000, 0.0000],
        [0.0714, 0.0762, 0.1331,  ..., 0.0133, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62014.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3070, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2850, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2848, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0187, 0.0490, 0.0018,  ..., 0.0000, 0.0465, 0.0000],
        [0.0187, 0.0490, 0.0018,  ..., 0.0000, 0.0465, 0.0000],
        [0.0187, 0.0490, 0.0018,  ..., 0.0000, 0.0465, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546246.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5131.7021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-212.5586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.4414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(73.5933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0850],
        [ 0.0833],
        [ 0.0840],
        ...,
        [-0.4588],
        [-0.4573],
        [-0.4568]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101653.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0024],
        [1.0012],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367983.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0024],
        [1.0012],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367995.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0063,  0.0060,  0.0121,  ...,  0.0003, -0.0052, -0.0090],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0003,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1746.5447, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.6439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4773, device='cuda:0')



h[100].sum tensor(-8.2441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4785, device='cuda:0')



h[200].sum tensor(-30.6404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1412e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.3590e-03, 5.9949e-03, 1.3090e-02,  ..., 3.3175e-04, 0.0000e+00,
         0.0000e+00],
        [5.1718e-03, 4.6487e-03, 1.0911e-02,  ..., 2.4176e-05, 0.0000e+00,
         0.0000e+00],
        [2.6558e-02, 2.5260e-02, 5.0755e-02,  ..., 1.6272e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.2450e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.2447e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.2445e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61628.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0413, 0.0179, 0.0014,  ..., 0.0000, 0.0167, 0.0000],
        [0.0530, 0.0081, 0.0004,  ..., 0.0000, 0.0077, 0.0000],
        [0.0776, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0187, 0.0494, 0.0016,  ..., 0.0000, 0.0471, 0.0000],
        [0.0187, 0.0494, 0.0016,  ..., 0.0000, 0.0471, 0.0000],
        [0.0187, 0.0494, 0.0016,  ..., 0.0000, 0.0471, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555413.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5263.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-209.1823, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.7396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(74.0016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0331],
        [ 0.1050],
        [ 0.1383],
        ...,
        [-0.4668],
        [-0.4654],
        [-0.4649]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108569.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0024],
        [1.0012],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367995.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0024],
        [1.0012],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368007.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.3989e-05, -1.3363e-03,  2.9662e-04,  ..., -1.3387e-03,
          0.0000e+00,  0.0000e+00],
        [-9.3989e-05, -1.3363e-03,  2.9662e-04,  ..., -1.3387e-03,
          0.0000e+00,  0.0000e+00],
        [-9.3989e-05, -1.3363e-03,  2.9662e-04,  ..., -1.3387e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-9.3989e-05, -1.3363e-03,  2.9662e-04,  ..., -1.3387e-03,
          0.0000e+00,  0.0000e+00],
        [-9.3989e-05, -1.3363e-03,  2.9662e-04,  ..., -1.3387e-03,
          0.0000e+00,  0.0000e+00],
        [-9.3989e-05, -1.3363e-03,  2.9662e-04,  ..., -1.3387e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1830.9653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.8083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0443, device='cuda:0')



h[100].sum tensor(-8.9692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.8759, device='cuda:0')



h[200].sum tensor(-33.3906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2380e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60054.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0191, 0.0469, 0.0010,  ..., 0.0000, 0.0452, 0.0000],
        [0.0185, 0.0480, 0.0013,  ..., 0.0000, 0.0460, 0.0000],
        [0.0194, 0.0465, 0.0018,  ..., 0.0000, 0.0441, 0.0000],
        ...,
        [0.0186, 0.0497, 0.0012,  ..., 0.0000, 0.0479, 0.0000],
        [0.0186, 0.0497, 0.0012,  ..., 0.0000, 0.0479, 0.0000],
        [0.0186, 0.0497, 0.0012,  ..., 0.0000, 0.0478, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(536326.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4838.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-194.6745, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(82.7492, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(73.5683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3184],
        [-0.3699],
        [-0.3795],
        ...,
        [-0.4722],
        [-0.4707],
        [-0.4704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115581.1328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0024],
        [1.0012],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368007.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0025],
        [1.0013],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368019.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.1198e-05, -1.3387e-03,  2.8575e-04,  ..., -1.3383e-03,
          0.0000e+00,  0.0000e+00],
        [-8.1198e-05, -1.3387e-03,  2.8575e-04,  ..., -1.3383e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.9782e-03,  4.3975e-03,  9.5739e-03,  ..., -2.7035e-05,
         -4.1048e-03, -7.0615e-03],
        ...,
        [-8.1198e-05, -1.3387e-03,  2.8575e-04,  ..., -1.3383e-03,
          0.0000e+00,  0.0000e+00],
        [-8.1198e-05, -1.3387e-03,  2.8575e-04,  ..., -1.3383e-03,
          0.0000e+00,  0.0000e+00],
        [-8.1198e-05, -1.3387e-03,  2.8575e-04,  ..., -1.3383e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1621.0884, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.1899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1221, device='cuda:0')



h[100].sum tensor(-7.2202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3780, device='cuda:0')



h[200].sum tensor(-26.9239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9578e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0050, 0.0044, 0.0105,  ..., 0.0000, 0.0000, 0.0000],
        [0.0168, 0.0165, 0.0322,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57319.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0773, 0.0043, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0863, 0.0027, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.1041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0185, 0.0501, 0.0008,  ..., 0.0000, 0.0486, 0.0000],
        [0.0251, 0.0393, 0.0004,  ..., 0.0000, 0.0387, 0.0000],
        [0.0405, 0.0189, 0.0000,  ..., 0.0000, 0.0189, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534042.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4745.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-171.1953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(54.4479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.0940, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0973],
        [ 0.1144],
        [ 0.1302],
        ...,
        [-0.3836],
        [-0.2639],
        [-0.1122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110253.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0025],
        [1.0013],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368019.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0025],
        [1.0013],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368031.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2863e-03,  4.7299e-03,  1.0109e-02,  ...,  4.9674e-05,
         -4.3382e-03, -7.4670e-03],
        [ 4.9767e-03,  4.3789e-03,  9.5405e-03,  ..., -3.0564e-05,
         -4.0874e-03, -7.0353e-03],
        [-6.8620e-05, -1.3407e-03,  2.7822e-04,  ..., -1.3381e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.8620e-05, -1.3407e-03,  2.7822e-04,  ..., -1.3381e-03,
          0.0000e+00,  0.0000e+00],
        [-6.8620e-05, -1.3407e-03,  2.7822e-04,  ..., -1.3381e-03,
          0.0000e+00,  0.0000e+00],
        [-6.8620e-05, -1.3407e-03,  2.7822e-04,  ..., -1.3381e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1686.7142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.8761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2263, device='cuda:0')



h[100].sum tensor(-7.7370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.3628, device='cuda:0')



h[200].sum tensor(-28.8994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0640e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0279, 0.0265, 0.0528,  ..., 0.0022, 0.0000, 0.0000],
        [0.0138, 0.0131, 0.0267,  ..., 0.0009, 0.0000, 0.0000],
        [0.0050, 0.0044, 0.0104,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58522.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0847, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0655, 0.0041, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0423, 0.0174, 0.0007,  ..., 0.0000, 0.0168, 0.0000],
        ...,
        [0.0183, 0.0505, 0.0006,  ..., 0.0000, 0.0492, 0.0000],
        [0.0183, 0.0504, 0.0006,  ..., 0.0000, 0.0492, 0.0000],
        [0.0183, 0.0504, 0.0006,  ..., 0.0000, 0.0492, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542263.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4806.0063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-181.5568, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(65.6705, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.1404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1421],
        [ 0.1042],
        [ 0.0192],
        ...,
        [-0.4826],
        [-0.4809],
        [-0.4805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126938.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0025],
        [1.0013],
        ...,
        [1.0009],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368031.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0026],
        [1.0013],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368044.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.7029e-05, -1.3421e-03,  2.7482e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7029e-05, -1.3421e-03,  2.7482e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7029e-05, -1.3421e-03,  2.7482e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.7029e-05, -1.3421e-03,  2.7482e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7029e-05, -1.3421e-03,  2.7482e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7029e-05, -1.3421e-03,  2.7482e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1631.6436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.0754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1700, device='cuda:0')



h[100].sum tensor(-7.2114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.4207, device='cuda:0')



h[200].sum tensor(-26.9809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9873e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56805.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0250, 0.0363, 0.0017,  ..., 0.0000, 0.0349, 0.0000],
        [0.0240, 0.0381, 0.0016,  ..., 0.0000, 0.0367, 0.0000],
        [0.0253, 0.0364, 0.0017,  ..., 0.0000, 0.0352, 0.0000],
        ...,
        [0.0182, 0.0507, 0.0004,  ..., 0.0000, 0.0497, 0.0000],
        [0.0182, 0.0507, 0.0004,  ..., 0.0000, 0.0497, 0.0000],
        [0.0182, 0.0507, 0.0004,  ..., 0.0000, 0.0497, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533380.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4574.9429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-166.4157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(48.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(70.1175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0264],
        [-0.0392],
        [-0.0316],
        ...,
        [-0.4877],
        [-0.4859],
        [-0.4854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124757.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0026],
        [1.0013],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368044.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0026],
        [1.0014],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368056.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5451e-05, -1.3410e-03,  2.8099e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-4.5451e-05, -1.3410e-03,  2.8099e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-4.5451e-05, -1.3410e-03,  2.8099e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.5451e-05, -1.3410e-03,  2.8099e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-4.5451e-05, -1.3410e-03,  2.8099e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00],
        [-4.5451e-05, -1.3410e-03,  2.8099e-04,  ..., -1.3380e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2196.6575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.2174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9662, device='cuda:0')



h[100].sum tensor(-11.7380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.0490, device='cuda:0')



h[200].sum tensor(-43.9902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6656e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73306.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0177, 0.0495, 0.0004,  ..., 0.0000, 0.0487, 0.0000],
        [0.0177, 0.0496, 0.0004,  ..., 0.0000, 0.0487, 0.0000],
        [0.0177, 0.0495, 0.0004,  ..., 0.0000, 0.0487, 0.0000],
        ...,
        [0.0182, 0.0507, 0.0005,  ..., 0.0000, 0.0500, 0.0000],
        [0.0182, 0.0507, 0.0004,  ..., 0.0000, 0.0499, 0.0000],
        [0.0182, 0.0507, 0.0004,  ..., 0.0000, 0.0499, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618192.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6154.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.1246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(200.1090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.0896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5192],
        [-0.5749],
        [-0.6116],
        ...,
        [-0.4890],
        [-0.4874],
        [-0.4870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124609.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0026],
        [1.0014],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368056.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0027],
        [1.0014],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368069.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9708e-03,  5.4721e-03,  1.1332e-02,  ...,  2.1901e-04,
         -4.8477e-03, -8.3575e-03],
        [-3.9352e-05, -1.3395e-03,  2.9707e-04,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        [-3.9352e-05, -1.3395e-03,  2.9707e-04,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.9352e-05, -1.3395e-03,  2.9707e-04,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        [-3.9352e-05, -1.3395e-03,  2.9707e-04,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        [-3.9352e-05, -1.3395e-03,  2.9707e-04,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1631.6332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.8150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3917, device='cuda:0')



h[100].sum tensor(-6.8124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7267, device='cuda:0')



h[200].sum tensor(-25.5732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.5067e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0125, 0.0116, 0.0243,  ..., 0.0008, 0.0000, 0.0000],
        [0.0060, 0.0055, 0.0123,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55231.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0684, 0.0042, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0444, 0.0180, 0.0000,  ..., 0.0000, 0.0183, 0.0000],
        [0.0254, 0.0367, 0.0003,  ..., 0.0000, 0.0373, 0.0000],
        ...,
        [0.0184, 0.0504, 0.0007,  ..., 0.0000, 0.0498, 0.0000],
        [0.0183, 0.0504, 0.0007,  ..., 0.0000, 0.0497, 0.0000],
        [0.0183, 0.0503, 0.0007,  ..., 0.0000, 0.0497, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524061., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4410.2056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-151.2493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(33.3011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(68.5228, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0256],
        [-0.1372],
        [-0.3098],
        ...,
        [-0.4885],
        [-0.4868],
        [-0.4862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122506.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0027],
        [1.0014],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368069.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0027],
        [1.0014],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368081.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.5112e-05, -1.3324e-03,  3.0348e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0944e-02,  1.1110e-02,  2.0462e-02,  ...,  1.5060e-03,
         -8.8432e-03, -1.5254e-02],
        [ 9.4844e-03,  9.4559e-03,  1.7781e-02,  ...,  1.1277e-03,
         -7.6672e-03, -1.3226e-02],
        ...,
        [-3.5112e-05, -1.3324e-03,  3.0348e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00],
        [-3.5112e-05, -1.3324e-03,  3.0348e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00],
        [-3.5112e-05, -1.3324e-03,  3.0348e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1765.0626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.0667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4130, device='cuda:0')



h[100].sum tensor(-7.6822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5292, device='cuda:0')



h[200].sum tensor(-28.8868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0755e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0110, 0.0112, 0.0215,  ..., 0.0015, 0.0000, 0.0000],
        [0.0185, 0.0184, 0.0353,  ..., 0.0021, 0.0000, 0.0000],
        [0.0580, 0.0605, 0.1079,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57212.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0617, 0.0126, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0992, 0.0007, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.1730, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0390, 0.0171, 0.0000,  ..., 0.0000, 0.0200, 0.0000],
        [0.0339, 0.0253, 0.0000,  ..., 0.0000, 0.0273, 0.0000],
        [0.0236, 0.0419, 0.0004,  ..., 0.0000, 0.0422, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527929.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4525.0610, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-165.2571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(53.5431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(69.4821, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0813],
        [ 0.1390],
        [ 0.1787],
        ...,
        [-0.1128],
        [-0.1911],
        [-0.3058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118761.7578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0027],
        [1.0014],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368081.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(531.7615, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0028],
        [1.0015],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368093.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1308e-05, -1.3245e-03,  3.0954e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1308e-05, -1.3245e-03,  3.0954e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1308e-05, -1.3245e-03,  3.0954e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.1308e-05, -1.3245e-03,  3.0954e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1308e-05, -1.3245e-03,  3.0954e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1308e-05, -1.3245e-03,  3.0954e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1482.1338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.1974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.7063, device='cuda:0')



h[100].sum tensor(-5.1457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4399, device='cuda:0')



h[200].sum tensor(-19.3816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.2304e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51275.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0207, 0.0439, 0.0007,  ..., 0.0000, 0.0436, 0.0000],
        [0.0216, 0.0431, 0.0007,  ..., 0.0000, 0.0430, 0.0000],
        [0.0263, 0.0360, 0.0004,  ..., 0.0000, 0.0368, 0.0000],
        ...,
        [0.0185, 0.0501, 0.0008,  ..., 0.0000, 0.0495, 0.0000],
        [0.0184, 0.0501, 0.0008,  ..., 0.0000, 0.0495, 0.0000],
        [0.0184, 0.0501, 0.0008,  ..., 0.0000, 0.0495, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508221.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4128.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.3888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-0.9932, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(68.6708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0958],
        [-0.1127],
        [-0.0816],
        ...,
        [-0.4879],
        [-0.4862],
        [-0.4856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113786.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0028],
        [1.0015],
        ...,
        [1.0009],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368093.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0028],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368106.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.5236e-05, -1.3190e-03,  3.1110e-04,  ..., -1.3393e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.1148e-03,  4.5052e-03,  9.7488e-03,  ..., -7.8268e-06,
         -4.1279e-03, -7.1282e-03],
        [ 5.2407e-03,  4.6478e-03,  9.9799e-03,  ...,  2.4776e-05,
         -4.2290e-03, -7.3027e-03],
        ...,
        [-2.5236e-05, -1.3190e-03,  3.1110e-04,  ..., -1.3393e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5236e-05, -1.3190e-03,  3.1110e-04,  ..., -1.3393e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5236e-05, -1.3190e-03,  3.1110e-04,  ..., -1.3393e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1854.9196, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3883, device='cuda:0')



h[100].sum tensor(-8.0527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.3991, device='cuda:0')



h[200].sum tensor(-30.3815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1358e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0045, 0.0107,  ..., 0.0000, 0.0000, 0.0000],
        [0.0171, 0.0168, 0.0327,  ..., 0.0018, 0.0000, 0.0000],
        [0.0408, 0.0410, 0.0763,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61152.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0508, 0.0153, 0.0005,  ..., 0.0000, 0.0154, 0.0000],
        [0.0858, 0.0011, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.1235, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0184, 0.0503, 0.0008,  ..., 0.0000, 0.0498, 0.0000],
        [0.0184, 0.0503, 0.0008,  ..., 0.0000, 0.0497, 0.0000],
        [0.0184, 0.0503, 0.0008,  ..., 0.0000, 0.0497, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548740., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4914.9229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-192.1685, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.3650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(68.9472, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1254],
        [ 0.1732],
        [ 0.2019],
        ...,
        [-0.4904],
        [-0.4886],
        [-0.4880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110321.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0028],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368106.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0029],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368118.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0363e-05, -1.3132e-03,  3.0504e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-2.0363e-05, -1.3132e-03,  3.0504e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-2.0363e-05, -1.3132e-03,  3.0504e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-2.0363e-05, -1.3132e-03,  3.0504e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-2.0363e-05, -1.3132e-03,  3.0504e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-2.0363e-05, -1.3132e-03,  3.0504e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1902.9272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2788, device='cuda:0')



h[100].sum tensor(-8.3554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.1932, device='cuda:0')



h[200].sum tensor(-31.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1907e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63017.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.0477, 0.0005,  ..., 0.0000, 0.0471, 0.0000],
        [0.0178, 0.0496, 0.0005,  ..., 0.0000, 0.0488, 0.0000],
        [0.0178, 0.0496, 0.0006,  ..., 0.0000, 0.0488, 0.0000],
        ...,
        [0.0183, 0.0508, 0.0006,  ..., 0.0000, 0.0501, 0.0000],
        [0.0183, 0.0508, 0.0006,  ..., 0.0000, 0.0500, 0.0000],
        [0.0183, 0.0508, 0.0006,  ..., 0.0000, 0.0500, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568465.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5226.1919, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-205.7412, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.2399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(68.6666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3655],
        [-0.4764],
        [-0.5520],
        ...,
        [-0.4946],
        [-0.4928],
        [-0.4922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115771.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0029],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368118.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0029],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368130.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9704e-05, -1.3035e-03,  2.8573e-04,  ..., -1.3399e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0154e-02,  1.0225e-02,  1.8964e-02,  ...,  1.2952e-03,
         -8.1473e-03, -1.4084e-02],
        [-1.9704e-05, -1.3035e-03,  2.8573e-04,  ..., -1.3399e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.9704e-05, -1.3035e-03,  2.8573e-04,  ..., -1.3399e-03,
          0.0000e+00,  0.0000e+00],
        [-1.9704e-05, -1.3035e-03,  2.8573e-04,  ..., -1.3399e-03,
          0.0000e+00,  0.0000e+00],
        [-1.9704e-05, -1.3035e-03,  2.8573e-04,  ..., -1.3399e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1797.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.2841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1424, device='cuda:0')



h[100].sum tensor(-7.4793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2880, device='cuda:0')



h[200].sum tensor(-28.3132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0588e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0102, 0.0103, 0.0199,  ..., 0.0013, 0.0000, 0.0000],
        [0.0163, 0.0172, 0.0312,  ..., 0.0029, 0.0000, 0.0000],
        [0.0775, 0.0827, 0.1436,  ..., 0.0147, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58284.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.2056e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.5331e-04,
         0.0000e+00],
        [1.7755e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7799e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8060e-02, 5.1539e-02, 2.8058e-04,  ..., 0.0000e+00, 5.0504e-02,
         0.0000e+00],
        [1.8054e-02, 5.1524e-02, 2.7965e-04,  ..., 0.0000e+00, 5.0488e-02,
         0.0000e+00],
        [1.8049e-02, 5.1511e-02, 2.7895e-04,  ..., 0.0000e+00, 5.0474e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537499.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4543.1914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-168.8033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(63.0960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(66.9820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2575],
        [ 0.2602],
        [ 0.2646],
        ...,
        [-0.5009],
        [-0.4991],
        [-0.4985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116311.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0029],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368130.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0030],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368142.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8556e-02,  5.3766e-02,  8.9448e-02,  ...,  1.1243e-02,
         -3.8852e-02, -6.7200e-02],
        [ 1.9248e-02,  2.0551e-02,  3.5642e-02,  ...,  3.6517e-03,
         -1.5412e-02, -2.6658e-02],
        [ 1.0088e-02,  1.0170e-02,  1.8825e-02,  ...,  1.2790e-03,
         -8.0865e-03, -1.3987e-02],
        ...,
        [-2.3202e-05, -1.2891e-03,  2.6223e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00],
        [-2.3202e-05, -1.2891e-03,  2.6223e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00],
        [-2.3202e-05, -1.2891e-03,  2.6223e-04,  ..., -1.3401e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1882.8584, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4981, device='cuda:0')



h[100].sum tensor(-8.1211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4970, device='cuda:0')



h[200].sum tensor(-30.7946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1425e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0820, 0.0879, 0.1518,  ..., 0.0159, 0.0000, 0.0000],
        [0.1046, 0.1135, 0.1933,  ..., 0.0217, 0.0000, 0.0000],
        [0.0606, 0.0636, 0.1125,  ..., 0.0103, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62063.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3689, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3745, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2791, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0178, 0.0523, 0.0000,  ..., 0.0000, 0.0508, 0.0000],
        [0.0178, 0.0523, 0.0000,  ..., 0.0000, 0.0508, 0.0000],
        [0.0178, 0.0523, 0.0000,  ..., 0.0000, 0.0508, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559639.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4890.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-195.6454, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.5090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(67.5527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2427],
        [ 0.2489],
        [ 0.2556],
        ...,
        [-0.5071],
        [-0.5053],
        [-0.5047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114277.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0030],
        [1.0015],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368142.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0030],
        [1.0016],
        ...,
        [1.0010],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368155.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7304e-02,  2.9698e-02,  5.0421e-02,  ...,  5.7386e-03,
         -2.1829e-02, -3.7777e-02],
        [ 1.9606e-02,  2.0972e-02,  3.6288e-02,  ...,  3.7444e-03,
         -1.5680e-02, -2.7136e-02],
        [-2.6536e-05, -1.2801e-03,  2.4692e-04,  ..., -1.3410e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-2.6536e-05, -1.2801e-03,  2.4692e-04,  ..., -1.3410e-03,
          0.0000e+00,  0.0000e+00],
        [-2.6536e-05, -1.2801e-03,  2.4692e-04,  ..., -1.3410e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0156e-02,  1.0261e-02,  1.8940e-02,  ...,  1.2965e-03,
         -8.1325e-03, -1.4074e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2367.0464, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.8644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5581, device='cuda:0')



h[100].sum tensor(-11.9266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.5768, device='cuda:0')



h[200].sum tensor(-45.3011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7021e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1037, 0.1125, 0.1915,  ..., 0.0215, 0.0000, 0.0000],
        [0.0502, 0.0531, 0.0933,  ..., 0.0090, 0.0000, 0.0000],
        [0.0272, 0.0283, 0.0510,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0104, 0.0105, 0.0202,  ..., 0.0013, 0.0000, 0.0000],
        [0.0188, 0.0188, 0.0356,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76740.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3773, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2350, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1294, 0.0017, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        ...,
        [0.0333, 0.0276, 0.0000,  ..., 0.0000, 0.0287, 0.0000],
        [0.0612, 0.0142, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0975, 0.0015, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640272.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6352.1807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-301.1731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(232.4727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(69.1165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1662],
        [ 0.1151],
        [-0.0110],
        ...,
        [-0.1087],
        [-0.0007],
        [ 0.0999]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111045.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0030],
        [1.0016],
        ...,
        [1.0010],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368155.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0031],
        [1.0016],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368167.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.4849e-05, -1.2757e-03,  2.3734e-04,  ..., -1.3419e-03,
          0.0000e+00,  0.0000e+00],
        [-3.4849e-05, -1.2757e-03,  2.3734e-04,  ..., -1.3419e-03,
          0.0000e+00,  0.0000e+00],
        [-3.4849e-05, -1.2757e-03,  2.3734e-04,  ..., -1.3419e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.4849e-05, -1.2757e-03,  2.3734e-04,  ..., -1.3419e-03,
          0.0000e+00,  0.0000e+00],
        [-3.4849e-05, -1.2757e-03,  2.3734e-04,  ..., -1.3419e-03,
          0.0000e+00,  0.0000e+00],
        [-3.4849e-05, -1.2757e-03,  2.3734e-04,  ..., -1.3419e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2162.7292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5795, device='cuda:0')



h[100].sum tensor(-10.2020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.0287, device='cuda:0')



h[200].sum tensor(-38.8160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4564e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66194.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0310, 0.0275, 0.0000,  ..., 0.0000, 0.0278, 0.0000],
        [0.0312, 0.0274, 0.0000,  ..., 0.0000, 0.0277, 0.0000],
        [0.0311, 0.0278, 0.0000,  ..., 0.0000, 0.0280, 0.0000],
        ...,
        [0.0175, 0.0537, 0.0000,  ..., 0.0000, 0.0511, 0.0000],
        [0.0175, 0.0536, 0.0000,  ..., 0.0000, 0.0511, 0.0000],
        [0.0175, 0.0536, 0.0000,  ..., 0.0000, 0.0511, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571870., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4989.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-226.2142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.2868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(70.9260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0014],
        [ 0.0358],
        [ 0.0679],
        ...,
        [-0.5185],
        [-0.5166],
        [-0.5160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127648.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0031],
        [1.0016],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368167.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0032],
        [1.0016],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368179.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.1592e-05, -1.2813e-03,  2.4038e-04,  ..., -1.3428e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3467e-02,  1.4036e-02,  2.5039e-02,  ...,  2.1568e-03,
         -1.0762e-02, -1.8644e-02],
        [-4.1592e-05, -1.2813e-03,  2.4038e-04,  ..., -1.3428e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.1592e-05, -1.2813e-03,  2.4038e-04,  ..., -1.3428e-03,
          0.0000e+00,  0.0000e+00],
        [-4.1592e-05, -1.2813e-03,  2.4038e-04,  ..., -1.3428e-03,
          0.0000e+00,  0.0000e+00],
        [-4.1592e-05, -1.2813e-03,  2.4038e-04,  ..., -1.3428e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2313.5269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3408, device='cuda:0')



h[100].sum tensor(-11.2689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.5995, device='cuda:0')



h[200].sum tensor(-42.9481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5652e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0491, 0.0507, 0.0914,  ..., 0.0074, 0.0000, 0.0000],
        [0.0111, 0.0113, 0.0213,  ..., 0.0015, 0.0000, 0.0000],
        [0.0135, 0.0141, 0.0258,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74340.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0678, 0.0026, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0569, 0.0129, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        ...,
        [0.0175, 0.0541, 0.0000,  ..., 0.0000, 0.0511, 0.0000],
        [0.0175, 0.0541, 0.0000,  ..., 0.0000, 0.0511, 0.0000],
        [0.0175, 0.0540, 0.0000,  ..., 0.0000, 0.0511, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625934.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6055.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-282.8767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(203.0599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(69.1765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1003],
        [-0.1603],
        [-0.2479],
        ...,
        [-0.5224],
        [-0.5204],
        [-0.5197]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109626.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0032],
        [1.0016],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368179.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0032],
        [1.0017],
        ...,
        [1.0011],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368192.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7677e-03,  5.2988e-03,  1.0935e-02,  ...,  1.6253e-04,
         -4.6262e-03, -8.0191e-03],
        [ 1.1436e-02,  1.1727e-02,  2.1342e-02,  ...,  1.6311e-03,
         -9.1362e-03, -1.5837e-02],
        [-4.6639e-05, -1.2954e-03,  2.5956e-04,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.6639e-05, -1.2954e-03,  2.5956e-04,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6639e-05, -1.2954e-03,  2.5956e-04,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6639e-05, -1.2954e-03,  2.5956e-04,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1671.3574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.0141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9250, device='cuda:0')



h[100].sum tensor(-6.0108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4186, device='cuda:0')



h[200].sum tensor(-22.9471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.6007e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0293, 0.0295, 0.0551,  ..., 0.0038, 0.0000, 0.0000],
        [0.0152, 0.0147, 0.0291,  ..., 0.0013, 0.0000, 0.0000],
        [0.0417, 0.0423, 0.0779,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54274.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1515, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1054, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0175, 0.0542, 0.0003,  ..., 0.0000, 0.0509, 0.0000],
        [0.0175, 0.0542, 0.0003,  ..., 0.0000, 0.0509, 0.0000],
        [0.0175, 0.0542, 0.0003,  ..., 0.0000, 0.0509, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526427.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4111.9150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-143.8141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.5919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(65.8397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1848],
        [ 0.1745],
        [ 0.1473],
        ...,
        [-0.5239],
        [-0.5221],
        [-0.5215]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131333.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0032],
        [1.0017],
        ...,
        [1.0011],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368192.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0033],
        [1.0017],
        ...,
        [1.0011],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368206.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.0201e-05, -1.3128e-03,  2.8188e-04,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0201e-05, -1.3128e-03,  2.8188e-04,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0201e-05, -1.3128e-03,  2.8188e-04,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.0201e-05, -1.3128e-03,  2.8188e-04,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0201e-05, -1.3128e-03,  2.8188e-04,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0201e-05, -1.3128e-03,  2.8188e-04,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2055.8606, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2392, device='cuda:0')



h[100].sum tensor(-8.8532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.0498, device='cuda:0')



h[200].sum tensor(-33.8559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2501e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66322.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0171, 0.0529, 0.0008,  ..., 0.0000, 0.0494, 0.0000],
        [0.0171, 0.0529, 0.0008,  ..., 0.0000, 0.0494, 0.0000],
        [0.0171, 0.0529, 0.0008,  ..., 0.0000, 0.0494, 0.0000],
        ...,
        [0.0184, 0.0526, 0.0009,  ..., 0.0000, 0.0493, 0.0000],
        [0.0175, 0.0542, 0.0009,  ..., 0.0000, 0.0507, 0.0000],
        [0.0175, 0.0542, 0.0009,  ..., 0.0000, 0.0507, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579648.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5299.3687, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-227.9790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.3820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(59.0087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6505],
        [-0.6452],
        [-0.6293],
        ...,
        [-0.4887],
        [-0.5102],
        [-0.5184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91886.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0033],
        [1.0017],
        ...,
        [1.0011],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368206.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(545.6761, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0033],
        [1.0017],
        ...,
        [1.0011],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368218.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0950e-02,  1.1166e-02,  2.0488e-02,  ...,  1.5054e-03,
         -8.7325e-03, -1.5154e-02],
        [ 1.1173e-02,  1.1420e-02,  2.0899e-02,  ...,  1.5634e-03,
         -8.9101e-03, -1.5462e-02],
        [ 1.4848e-02,  1.5590e-02,  2.7647e-02,  ...,  2.5157e-03,
         -1.1827e-02, -2.0523e-02],
        ...,
        [-5.4135e-05, -1.3171e-03,  2.8234e-04,  ..., -1.3456e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4135e-05, -1.3171e-03,  2.8234e-04,  ..., -1.3456e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4135e-05, -1.3171e-03,  2.8234e-04,  ..., -1.3456e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1948.2933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.4832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5045, device='cuda:0')



h[100].sum tensor(-7.9185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5027, device='cuda:0')



h[200].sum tensor(-30.3330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1429e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0497, 0.0514, 0.0929,  ..., 0.0075, 0.0000, 0.0000],
        [0.0472, 0.0486, 0.0883,  ..., 0.0069, 0.0000, 0.0000],
        [0.0505, 0.0535, 0.0942,  ..., 0.0091, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60636.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1636, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1908, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2052, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0174, 0.0547, 0.0010,  ..., 0.0000, 0.0509, 0.0000],
        [0.0174, 0.0547, 0.0010,  ..., 0.0000, 0.0508, 0.0000],
        [0.0174, 0.0547, 0.0010,  ..., 0.0000, 0.0508, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551004.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4619.9546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-195.8481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(66.5799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(62.5980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1646],
        [ 0.1739],
        [ 0.1755],
        ...,
        [-0.5266],
        [-0.5249],
        [-0.5243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127590.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0033],
        [1.0017],
        ...,
        [1.0011],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368218.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0033],
        [1.0017],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368230.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8033e-02,  1.9204e-02,  3.3499e-02,  ...,  3.3412e-03,
         -1.4337e-02, -2.4893e-02],
        [-5.6443e-05, -1.3209e-03,  2.8034e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6443e-05, -1.3209e-03,  2.8034e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.6240e-03,  7.3938e-03,  1.4384e-02,  ...,  6.4404e-04,
         -6.0873e-03, -1.0569e-02],
        [ 1.6926e-02,  1.7948e-02,  3.1465e-02,  ...,  3.0543e-03,
         -1.3460e-02, -2.3370e-02],
        [ 1.1989e-02,  1.2346e-02,  2.2399e-02,  ...,  1.7750e-03,
         -9.5466e-03, -1.6576e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2066.8657, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.5541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3621, device='cuda:0')



h[100].sum tensor(-8.7786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.1593, device='cuda:0')



h[200].sum tensor(-33.6848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2577e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0669, 0.0722, 0.1244,  ..., 0.0133, 0.0000, 0.0000],
        [0.0433, 0.0453, 0.0809,  ..., 0.0072, 0.0000, 0.0000],
        [0.0194, 0.0195, 0.0370,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0318, 0.0335, 0.0598,  ..., 0.0055, 0.0000, 0.0000],
        [0.0559, 0.0583, 0.1042,  ..., 0.0090, 0.0000, 0.0000],
        [0.0658, 0.0695, 0.1223,  ..., 0.0116, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63372.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3376, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2581, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2097, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.1357, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1841, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1899, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565130.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4781.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-216.1364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(87.8387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(61.2735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0498],
        [0.0616],
        [0.0711],
        ...,
        [0.0746],
        [0.1510],
        [0.1454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131599.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0033],
        [1.0017],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368230.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0034],
        [1.0018],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368243.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4097e-02,  2.6087e-02,  4.4637e-02,  ...,  4.9125e-03,
         -1.9118e-02, -3.3212e-02],
        [ 2.6033e-02,  2.8284e-02,  4.8192e-02,  ...,  5.4141e-03,
         -2.0650e-02, -3.5874e-02],
        [ 2.3912e-02,  2.5878e-02,  4.4297e-02,  ...,  4.8646e-03,
         -1.8972e-02, -3.2958e-02],
        ...,
        [-5.6937e-05, -1.3229e-03,  2.8146e-04,  ..., -1.3464e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6937e-05, -1.3229e-03,  2.8146e-04,  ..., -1.3464e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6937e-05, -1.3229e-03,  2.8146e-04,  ..., -1.3464e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1885.8289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.8746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0009, device='cuda:0')



h[100].sum tensor(-7.2639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.1618, device='cuda:0')



h[200].sum tensor(-27.9199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0501e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0855, 0.0919, 0.1585,  ..., 0.0168, 0.0000, 0.0000],
        [0.0878, 0.0945, 0.1627,  ..., 0.0174, 0.0000, 0.0000],
        [0.0808, 0.0866, 0.1498,  ..., 0.0156, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58900.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3014, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2906, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2563, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0171, 0.0556, 0.0012,  ..., 0.0000, 0.0511, 0.0000],
        [0.0171, 0.0556, 0.0012,  ..., 0.0000, 0.0511, 0.0000],
        [0.0171, 0.0555, 0.0012,  ..., 0.0000, 0.0511, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544213.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4419.7021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-182.5643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(44.4053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.3270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1324],
        [ 0.1171],
        [ 0.0905],
        ...,
        [-0.5334],
        [-0.5317],
        [-0.5310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116941.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0034],
        [1.0018],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368243.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0034],
        [1.0018],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368255., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1855e-03,  5.7646e-03,  1.1731e-02,  ...,  2.7014e-04,
         -4.9324e-03, -8.5734e-03],
        [ 9.6946e-03,  9.7473e-03,  1.8175e-02,  ...,  1.1794e-03,
         -7.7061e-03, -1.3395e-02],
        [ 2.0622e-02,  2.2150e-02,  3.8242e-02,  ...,  4.0111e-03,
         -1.6344e-02, -2.8409e-02],
        ...,
        [-5.4607e-05, -1.3176e-03,  2.7210e-04,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4607e-05, -1.3176e-03,  2.7210e-04,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4607e-05, -1.3176e-03,  2.7210e-04,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1995.7390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.9216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8746, device='cuda:0')



h[100].sum tensor(-8.0470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.8328, device='cuda:0')



h[200].sum tensor(-30.9829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1658e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0224, 0.0216, 0.0425,  ..., 0.0022, 0.0000, 0.0000],
        [0.0514, 0.0533, 0.0959,  ..., 0.0080, 0.0000, 0.0000],
        [0.0650, 0.0688, 0.1209,  ..., 0.0115, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62183.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1217, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1963, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2486, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0169, 0.0561, 0.0010,  ..., 0.0000, 0.0513, 0.0000],
        [0.0169, 0.0561, 0.0010,  ..., 0.0000, 0.0513, 0.0000],
        [0.0169, 0.0561, 0.0010,  ..., 0.0000, 0.0513, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561589., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4673.4380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-206.5387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(73.7099, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.5674, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1679],
        [ 0.1712],
        [ 0.1657],
        ...,
        [-0.5383],
        [-0.5365],
        [-0.5358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121673.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0034],
        [1.0018],
        ...,
        [1.0011],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368255., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0034],
        [1.0018],
        ...,
        [1.0011],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368266.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.3489e-03,  9.3578e-03,  1.7523e-02,  ...,  1.0884e-03,
         -7.4196e-03, -1.2904e-02],
        [ 1.7465e-02,  1.8570e-02,  3.2427e-02,  ...,  3.1916e-03,
         -1.3826e-02, -2.4046e-02],
        [ 4.3135e-03,  3.6424e-03,  8.2764e-03,  ..., -2.1634e-04,
         -3.4449e-03, -5.9912e-03],
        ...,
        [-5.0724e-05, -1.3113e-03,  2.6232e-04,  ..., -1.3472e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0724e-05, -1.3113e-03,  2.6232e-04,  ..., -1.3472e-03,
          0.0000e+00,  0.0000e+00],
        [-5.0724e-05, -1.3113e-03,  2.6232e-04,  ..., -1.3472e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1903.7223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.1886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9887, device='cuda:0')



h[100].sum tensor(-7.2704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.1509, device='cuda:0')



h[200].sum tensor(-28.0406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0493e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0715, 0.0762, 0.1328,  ..., 0.0132, 0.0000, 0.0000],
        [0.0389, 0.0391, 0.0728,  ..., 0.0049, 0.0000, 0.0000],
        [0.0364, 0.0363, 0.0683,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59517.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2099, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1733, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1446, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0167, 0.0566, 0.0009,  ..., 0.0000, 0.0516, 0.0000],
        [0.0167, 0.0566, 0.0009,  ..., 0.0000, 0.0515, 0.0000],
        [0.0167, 0.0565, 0.0008,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553720., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4435.1104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-188.1272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(49.8924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.2728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1872],
        [ 0.1896],
        [ 0.1913],
        ...,
        [-0.5432],
        [-0.5414],
        [-0.5408]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129541.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0034],
        [1.0018],
        ...,
        [1.0011],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368266.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0035],
        [1.0018],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368279., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3086e-02,  2.4956e-02,  4.2731e-02,  ...,  4.6459e-03,
         -1.8233e-02, -3.1727e-02],
        [ 2.3923e-02,  2.5906e-02,  4.4268e-02,  ...,  4.8627e-03,
         -1.8893e-02, -3.2875e-02],
        [ 1.0733e-02,  1.0933e-02,  2.0047e-02,  ...,  1.4449e-03,
         -8.4955e-03, -1.4783e-02],
        ...,
        [-4.4926e-05, -1.3012e-03,  2.5675e-04,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [-4.4926e-05, -1.3012e-03,  2.5675e-04,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [-4.4926e-05, -1.3012e-03,  2.5675e-04,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1995.1315, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.8263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5602, device='cuda:0')



h[100].sum tensor(-7.8359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5523, device='cuda:0')



h[200].sum tensor(-30.2733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1464e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0710, 0.0756, 0.1318,  ..., 0.0130, 0.0000, 0.0000],
        [0.0713, 0.0759, 0.1322,  ..., 0.0131, 0.0000, 0.0000],
        [0.0679, 0.0720, 0.1260,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61856.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2218, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2231, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1949, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0166, 0.0568, 0.0007,  ..., 0.0000, 0.0516, 0.0000],
        [0.0165, 0.0568, 0.0007,  ..., 0.0000, 0.0515, 0.0000],
        [0.0165, 0.0567, 0.0007,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561584.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4623.3237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-202.5024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(72.6628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(54.5093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1389],
        [ 0.1543],
        [ 0.1660],
        ...,
        [-0.5456],
        [-0.5416],
        [-0.5314]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112532.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0035],
        [1.0018],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368279., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0035],
        [1.0018],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368291.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2597e-03,  4.7300e-03,  9.9724e-03,  ...,  2.4152e-05,
         -4.1691e-03, -7.2586e-03],
        [ 4.9152e-03,  4.3389e-03,  9.3398e-03,  ..., -6.5106e-05,
         -3.8979e-03, -6.7865e-03],
        [ 1.0212e-02,  1.0352e-02,  1.9065e-02,  ...,  1.3072e-03,
         -8.0670e-03, -1.4045e-02],
        ...,
        [-3.6832e-05, -1.2828e-03,  2.4717e-04,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00],
        [-3.6832e-05, -1.2828e-03,  2.4717e-04,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00],
        [-3.6832e-05, -1.2828e-03,  2.4717e-04,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1723.6677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.6601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3513, device='cuda:0')



h[100].sum tensor(-5.6189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9069, device='cuda:0')



h[200].sum tensor(-21.7452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.2464e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0167, 0.0322,  ..., 0.0018, 0.0000, 0.0000],
        [0.0355, 0.0353, 0.0664,  ..., 0.0038, 0.0000, 0.0000],
        [0.0169, 0.0154, 0.0322,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54292.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0802, 0.0023, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.1068, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0851, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0163, 0.0569, 0.0005,  ..., 0.0000, 0.0515, 0.0000],
        [0.0163, 0.0569, 0.0005,  ..., 0.0000, 0.0515, 0.0000],
        [0.0163, 0.0569, 0.0005,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532359.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3874.2356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-151.6281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(9.1129, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.2263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1477],
        [ 0.1657],
        [ 0.1251],
        ...,
        [-0.5489],
        [-0.5471],
        [-0.5465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141197.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0035],
        [1.0018],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368291.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0036],
        [1.0019],
        ...,
        [1.0010],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368303.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.5822e-05, -1.2722e-03,  2.4262e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5822e-05, -1.2722e-03,  2.4262e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5822e-05, -1.2722e-03,  2.4262e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-2.5822e-05, -1.2722e-03,  2.4262e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5822e-05, -1.2722e-03,  2.4262e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5822e-05, -1.2722e-03,  2.4262e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2164.7754, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.0369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9006, device='cuda:0')



h[100].sum tensor(-8.8291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.6396, device='cuda:0')



h[200].sum tensor(-34.2274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2909e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65613.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0191, 0.0494, 0.0002,  ..., 0.0000, 0.0446, 0.0000],
        [0.0201, 0.0477, 0.0001,  ..., 0.0000, 0.0431, 0.0000],
        [0.0208, 0.0461, 0.0001,  ..., 0.0000, 0.0417, 0.0000],
        ...,
        [0.0246, 0.0432, 0.0002,  ..., 0.0000, 0.0395, 0.0000],
        [0.0181, 0.0539, 0.0004,  ..., 0.0000, 0.0487, 0.0000],
        [0.0161, 0.0571, 0.0004,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580363.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4817.3428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-230.2542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.7683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.7251, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3390],
        [-0.2848],
        [-0.2333],
        ...,
        [-0.4098],
        [-0.4899],
        [-0.5291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119666.7109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0036],
        [1.0019],
        ...,
        [1.0010],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368303.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0036],
        [1.0019],
        ...,
        [1.0010],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368316.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0649e-02,  1.0839e-02,  1.9821e-02,  ...,  1.4124e-03,
         -8.3683e-03, -1.4586e-02],
        [-1.2657e-05, -1.2649e-03,  2.4411e-04,  ..., -1.3497e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2657e-05, -1.2649e-03,  2.4411e-04,  ..., -1.3497e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.2657e-05, -1.2649e-03,  2.4411e-04,  ..., -1.3497e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2657e-05, -1.2649e-03,  2.4411e-04,  ..., -1.3497e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2657e-05, -1.2649e-03,  2.4411e-04,  ..., -1.3497e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1853.5931, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.3426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2326, device='cuda:0')



h[100].sum tensor(-6.3387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5848, device='cuda:0')



h[200].sum tensor(-24.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.4084e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0166, 0.0163, 0.0315,  ..., 0.0016, 0.0000, 0.0000],
        [0.0107, 0.0109, 0.0207,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58199.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1244, 0.0028, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0940, 0.0100, 0.0000,  ..., 0.0000, 0.0089, 0.0000],
        [0.0679, 0.0131, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        ...,
        [0.0160, 0.0572, 0.0004,  ..., 0.0000, 0.0515, 0.0000],
        [0.0160, 0.0572, 0.0004,  ..., 0.0000, 0.0515, 0.0000],
        [0.0160, 0.0572, 0.0004,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553297.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4177.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-178.6862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(52.3339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(59.1397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1116],
        [ 0.1073],
        [ 0.1076],
        ...,
        [-0.5536],
        [-0.5518],
        [-0.5512]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140635.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0036],
        [1.0019],
        ...,
        [1.0010],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368316.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0037],
        [1.0019],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368329.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1760e-03,  5.7426e-03,  1.1586e-02,  ...,  2.4837e-04,
         -4.8378e-03, -8.4370e-03],
        [ 3.1518e-06, -1.2649e-03,  2.5196e-04,  ..., -1.3506e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.1518e-06, -1.2649e-03,  2.5196e-04,  ..., -1.3506e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 3.1518e-06, -1.2649e-03,  2.5196e-04,  ..., -1.3506e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.1518e-06, -1.2649e-03,  2.5196e-04,  ..., -1.3506e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.1518e-06, -1.2649e-03,  2.5196e-04,  ..., -1.3506e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2065.0750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.5789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6482, device='cuda:0')



h[100].sum tensor(-7.8056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6308, device='cuda:0')



h[200].sum tensor(-30.3637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1518e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.9222e-03, 8.7096e-03, 1.9208e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.2189e-03, 5.7737e-03, 1.2409e-02,  ..., 2.4972e-04, 0.0000e+00,
         0.0000e+00],
        [1.2650e-05, 0.0000e+00, 1.0113e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.2959e-05, 0.0000e+00, 1.0359e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.2955e-05, 0.0000e+00, 1.0356e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.2951e-05, 0.0000e+00, 1.0353e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65480.1055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.8481e-02, 1.1744e-02, 0.0000e+00,  ..., 0.0000e+00, 1.0994e-02,
         0.0000e+00],
        [3.7369e-02, 2.3298e-02, 0.0000e+00,  ..., 0.0000e+00, 2.1550e-02,
         0.0000e+00],
        [2.6015e-02, 3.7008e-02, 5.6300e-05,  ..., 0.0000e+00, 3.3402e-02,
         0.0000e+00],
        ...,
        [1.5809e-02, 5.7268e-02, 4.1236e-04,  ..., 0.0000e+00, 5.1574e-02,
         0.0000e+00],
        [1.5804e-02, 5.7250e-02, 4.1120e-04,  ..., 0.0000e+00, 5.1556e-02,
         0.0000e+00],
        [1.5798e-02, 5.7231e-02, 4.0998e-04,  ..., 0.0000e+00, 5.1538e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597373.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5025.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-226.9757, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.0556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.9370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2356],
        [-0.2476],
        [-0.2201],
        ...,
        [-0.5552],
        [-0.5534],
        [-0.5527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118816.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0037],
        [1.0019],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368329.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(516.9009, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0037],
        [1.0020],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368342.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0160e-02,  1.0240e-02,  1.8884e-02,  ...,  1.2746e-03,
         -7.9349e-03, -1.3846e-02],
        [ 1.7628e-02,  1.8716e-02,  3.2596e-02,  ...,  3.2086e-03,
         -1.3779e-02, -2.4043e-02],
        [ 5.7079e-03,  5.1855e-03,  1.0708e-02,  ...,  1.2136e-04,
         -4.4504e-03, -7.7655e-03],
        ...,
        [ 2.1213e-05, -1.2697e-03,  2.6563e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1213e-05, -1.2697e-03,  2.6563e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1213e-05, -1.2697e-03,  2.6563e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2077.0396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4846, device='cuda:0')



h[100].sum tensor(-7.7877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4850, device='cuda:0')



h[200].sum tensor(-30.3463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1417e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.0479e-02, 8.6156e-02, 1.4869e-01,  ..., 1.5393e-02, 0.0000e+00,
         0.0000e+00],
        [5.5820e-02, 5.8159e-02, 1.0341e-01,  ..., 8.9996e-03, 0.0000e+00,
         0.0000e+00],
        [3.0179e-02, 3.1611e-02, 5.6327e-02,  ..., 5.0816e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.7231e-05, 0.0000e+00, 1.0923e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.7206e-05, 0.0000e+00, 1.0920e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.7182e-05, 0.0000e+00, 1.0917e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64351.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2561, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2137, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1459, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0156, 0.0573, 0.0006,  ..., 0.0000, 0.0517, 0.0000],
        [0.0156, 0.0573, 0.0006,  ..., 0.0000, 0.0517, 0.0000],
        [0.0156, 0.0573, 0.0006,  ..., 0.0000, 0.0516, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580888.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4712.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-215.0213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.3315, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(53.2592, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1924],
        [ 0.1879],
        [ 0.1715],
        ...,
        [-0.5573],
        [-0.5555],
        [-0.5548]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104664.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0037],
        [1.0020],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368342.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0038],
        [1.0020],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368354.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0714e-05, -1.2597e-03,  2.5005e-04,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.6432e-03,  9.6517e-03,  1.7901e-02,  ...,  1.1380e-03,
         -7.5118e-03, -1.3115e-02],
        [ 1.8755e-02,  1.9995e-02,  3.4632e-02,  ...,  3.4979e-03,
         -1.4632e-02, -2.5547e-02],
        ...,
        [ 3.0714e-05, -1.2597e-03,  2.5005e-04,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.0714e-05, -1.2597e-03,  2.5005e-04,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.0714e-05, -1.2597e-03,  2.5005e-04,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2225.8660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2550, device='cuda:0')



h[100].sum tensor(-8.8821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.9557, device='cuda:0')



h[200].sum tensor(-34.6706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3128e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0177, 0.0174, 0.0332,  ..., 0.0018, 0.0000, 0.0000],
        [0.0343, 0.0350, 0.0638,  ..., 0.0048, 0.0000, 0.0000],
        [0.0513, 0.0530, 0.0950,  ..., 0.0078, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67765.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.7080e-02, 4.2194e-03, 0.0000e+00,  ..., 0.0000e+00, 4.5468e-03,
         0.0000e+00],
        [1.5040e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.0909e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.1996e-02, 4.6400e-02, 1.0142e-04,  ..., 0.0000e+00, 4.2230e-02,
         0.0000e+00],
        [1.5403e-02, 5.7974e-02, 2.0220e-04,  ..., 0.0000e+00, 5.2240e-02,
         0.0000e+00],
        [1.5397e-02, 5.7955e-02, 2.0113e-04,  ..., 0.0000e+00, 5.2222e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597035.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4937.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-239.6028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.2304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.3926, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1944],
        [ 0.1958],
        [ 0.1966],
        ...,
        [-0.3656],
        [-0.4797],
        [-0.5352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113125.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0038],
        [1.0020],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368354.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0038],
        [1.0020],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368367.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5261e-03,  4.9746e-03,  1.0299e-02,  ...,  6.8084e-05,
         -4.2785e-03, -7.4739e-03],
        [ 1.4152e-02,  1.4766e-02,  2.6138e-02,  ...,  2.3019e-03,
         -1.1009e-02, -1.9232e-02],
        [ 1.7284e-02,  1.8321e-02,  3.1889e-02,  ...,  3.1131e-03,
         -1.3454e-02, -2.3501e-02],
        ...,
        [ 4.3222e-05, -1.2492e-03,  2.3196e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.3222e-05, -1.2492e-03,  2.3196e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.3222e-05, -1.2492e-03,  2.3196e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2305.1616, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.3739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7416, device='cuda:0')



h[100].sum tensor(-9.5106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.2815, device='cuda:0')



h[200].sum tensor(-37.1879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4046e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0290, 0.0289, 0.0538,  ..., 0.0036, 0.0000, 0.0000],
        [0.0533, 0.0552, 0.0984,  ..., 0.0085, 0.0000, 0.0000],
        [0.0775, 0.0828, 0.1430,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69231.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1627, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2315, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0151, 0.0589, 0.0000,  ..., 0.0000, 0.0532, 0.0000],
        [0.0151, 0.0589, 0.0000,  ..., 0.0000, 0.0532, 0.0000],
        [0.0151, 0.0589, 0.0000,  ..., 0.0000, 0.0532, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603932.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4906.5303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-249.7867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(159.7988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.9514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1228],
        [ 0.1317],
        [ 0.1418],
        ...,
        [-0.5728],
        [-0.5708],
        [-0.5701]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132966.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0038],
        [1.0020],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368367.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0039],
        [1.0021],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368379.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1845.2091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.0784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8746, device='cuda:0')



h[100].sum tensor(-6.1313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2655, device='cuda:0')



h[200].sum tensor(-24.0160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.1873e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56749.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0145, 0.0581, 0.0000,  ..., 0.0000, 0.0524, 0.0000],
        [0.0146, 0.0582, 0.0000,  ..., 0.0000, 0.0524, 0.0000],
        [0.0145, 0.0581, 0.0000,  ..., 0.0000, 0.0524, 0.0000],
        ...,
        [0.0150, 0.0597, 0.0000,  ..., 0.0000, 0.0539, 0.0000],
        [0.0150, 0.0597, 0.0000,  ..., 0.0000, 0.0539, 0.0000],
        [0.0149, 0.0596, 0.0000,  ..., 0.0000, 0.0539, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551235.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3823.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.3384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(44.6516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.9706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7178],
        [-0.7224],
        [-0.7243],
        ...,
        [-0.5774],
        [-0.5758],
        [-0.5751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135760.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0039],
        [1.0021],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368379.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0039],
        [1.0021],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368379.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.7927e-05, -1.2428e-03,  2.1796e-04,  ..., -1.3517e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1821.8835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.6462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2631, device='cuda:0')



h[100].sum tensor(-5.9598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7201, device='cuda:0')



h[200].sum tensor(-23.3441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8096e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55816.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0145, 0.0581, 0.0000,  ..., 0.0000, 0.0524, 0.0000],
        [0.0146, 0.0582, 0.0000,  ..., 0.0000, 0.0524, 0.0000],
        [0.0145, 0.0581, 0.0000,  ..., 0.0000, 0.0524, 0.0000],
        ...,
        [0.0150, 0.0597, 0.0000,  ..., 0.0000, 0.0539, 0.0000],
        [0.0150, 0.0597, 0.0000,  ..., 0.0000, 0.0539, 0.0000],
        [0.0149, 0.0596, 0.0000,  ..., 0.0000, 0.0539, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546769.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3688.7607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-151.0649, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(37.3186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(59.9794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6406],
        [-0.6809],
        [-0.7098],
        ...,
        [-0.5787],
        [-0.5766],
        [-0.5758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146614.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0039],
        [1.0021],
        ...,
        [1.0010],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368379.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0039],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368392.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1577e-05, -1.2476e-03,  2.2587e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1577e-05, -1.2476e-03,  2.2587e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.8805e-03,  5.3574e-03,  1.0911e-02,  ...,  1.5494e-04,
         -4.5277e-03, -7.9180e-03],
        ...,
        [ 6.1577e-05, -1.2476e-03,  2.2587e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1577e-05, -1.2476e-03,  2.2587e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.1577e-05, -1.2476e-03,  2.2587e-04,  ..., -1.3519e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1981.3403, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9367, device='cuda:0')



h[100].sum tensor(-7.0473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.1045, device='cuda:0')



h[200].sum tensor(-27.6514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0461e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0073, 0.0168,  ..., 0.0000, 0.0000, 0.0000],
        [0.0061, 0.0054, 0.0116,  ..., 0.0002, 0.0000, 0.0000],
        [0.0050, 0.0042, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59800.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0496, 0.0076, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0395, 0.0128, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0353, 0.0194, 0.0000,  ..., 0.0000, 0.0191, 0.0000],
        ...,
        [0.0147, 0.0598, 0.0000,  ..., 0.0000, 0.0543, 0.0000],
        [0.0147, 0.0598, 0.0000,  ..., 0.0000, 0.0543, 0.0000],
        [0.0147, 0.0598, 0.0000,  ..., 0.0000, 0.0542, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564982.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3990.7549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-177.8678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(73.3364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(60.0803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1251],
        [ 0.0244],
        [-0.1026],
        ...,
        [-0.5821],
        [-0.5800],
        [-0.5792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143862.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0039],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368392.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0040],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368405., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5237e-03,  3.7917e-03,  8.4188e-03,  ..., -2.0014e-04,
         -3.4571e-03, -6.0491e-03],
        [ 4.5237e-03,  3.7917e-03,  8.4188e-03,  ..., -2.0014e-04,
         -3.4571e-03, -6.0491e-03],
        [ 7.4299e-05, -1.2584e-03,  2.4809e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.4299e-05, -1.2584e-03,  2.4809e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.4299e-05, -1.2584e-03,  2.4809e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.4299e-05, -1.2584e-03,  2.4809e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2302.4163, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4872, device='cuda:0')



h[100].sum tensor(-9.1961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.0545, device='cuda:0')



h[200].sum tensor(-36.1455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3889e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0165, 0.0146, 0.0308,  ..., 0.0006, 0.0000, 0.0000],
        [0.0125, 0.0100, 0.0234,  ..., 0.0000, 0.0000, 0.0000],
        [0.0084, 0.0067, 0.0159,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70768.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0681, 0.0006, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0606, 0.0041, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0476, 0.0143, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        ...,
        [0.0147, 0.0595, 0.0001,  ..., 0.0000, 0.0542, 0.0000],
        [0.0147, 0.0595, 0.0001,  ..., 0.0000, 0.0542, 0.0000],
        [0.0147, 0.0594, 0.0001,  ..., 0.0000, 0.0542, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629633.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5275.5674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-257.7432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(173.9965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(61.2449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1721],
        [ 0.1456],
        [ 0.0395],
        ...,
        [-0.5827],
        [-0.5806],
        [-0.5798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136135.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0040],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368405., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0040],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368417.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.2067e-05, -1.2715e-03,  2.7701e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.2067e-05, -1.2715e-03,  2.7701e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.2067e-05, -1.2715e-03,  2.7701e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.2067e-05, -1.2715e-03,  2.7701e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.2067e-05, -1.2715e-03,  2.7701e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.2067e-05, -1.2715e-03,  2.7701e-04,  ..., -1.3523e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1867.0027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.4182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.8556, device='cuda:0')



h[100].sum tensor(-5.7621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3567, device='cuda:0')



h[200].sum tensor(-22.6872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.5579e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57495.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0152, 0.0558, 0.0014,  ..., 0.0000, 0.0504, 0.0000],
        [0.0148, 0.0568, 0.0009,  ..., 0.0000, 0.0517, 0.0000],
        [0.0145, 0.0572, 0.0007,  ..., 0.0000, 0.0522, 0.0000],
        ...,
        [0.0149, 0.0588, 0.0007,  ..., 0.0000, 0.0538, 0.0000],
        [0.0149, 0.0588, 0.0007,  ..., 0.0000, 0.0537, 0.0000],
        [0.0149, 0.0587, 0.0007,  ..., 0.0000, 0.0537, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553853.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3919.0071, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-158.2915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(49.7942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.0360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4019],
        [-0.5189],
        [-0.6033],
        ...,
        [-0.5761],
        [-0.5741],
        [-0.5733]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124409.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0040],
        [1.0021],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368417.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368430.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.1149e-05, -1.2826e-03,  3.0001e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.1149e-05, -1.2826e-03,  3.0001e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.1149e-05, -1.2826e-03,  3.0001e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 8.1149e-05, -1.2826e-03,  3.0001e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.1149e-05, -1.2826e-03,  3.0001e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.1149e-05, -1.2826e-03,  3.0001e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2027.1206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1895, device='cuda:0')



h[100].sum tensor(-6.6699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.4381, device='cuda:0')



h[200].sum tensor(-26.3073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9994e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60468.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0153, 0.0555, 0.0016,  ..., 0.0000, 0.0504, 0.0000],
        [0.0169, 0.0527, 0.0020,  ..., 0.0000, 0.0475, 0.0000],
        [0.0216, 0.0436, 0.0022,  ..., 0.0000, 0.0395, 0.0000],
        ...,
        [0.0153, 0.0581, 0.0012,  ..., 0.0000, 0.0532, 0.0000],
        [0.0153, 0.0581, 0.0012,  ..., 0.0000, 0.0532, 0.0000],
        [0.0153, 0.0581, 0.0012,  ..., 0.0000, 0.0532, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562640., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4236.3545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-178.8325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(76.2931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.9604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4093],
        [-0.2719],
        [-0.1065],
        ...,
        [-0.5735],
        [-0.5717],
        [-0.5710]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107676.6953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368430.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368443.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5232e-05, -1.2897e-03,  3.0485e-04,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5232e-05, -1.2897e-03,  3.0485e-04,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5232e-05, -1.2897e-03,  3.0485e-04,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.5232e-05, -1.2897e-03,  3.0485e-04,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5232e-05, -1.2897e-03,  3.0485e-04,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5232e-05, -1.2897e-03,  3.0485e-04,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2978.9482, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.2752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8838, device='cuda:0')



h[100].sum tensor(-13.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.4346, device='cuda:0')



h[200].sum tensor(-52.7389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9693e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89460.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0215, 0.0457, 0.0009,  ..., 0.0000, 0.0422, 0.0000],
        [0.0163, 0.0545, 0.0012,  ..., 0.0000, 0.0498, 0.0000],
        [0.0191, 0.0493, 0.0010,  ..., 0.0000, 0.0453, 0.0000],
        ...,
        [0.0157, 0.0579, 0.0013,  ..., 0.0000, 0.0529, 0.0000],
        [0.0157, 0.0579, 0.0013,  ..., 0.0000, 0.0529, 0.0000],
        [0.0157, 0.0579, 0.0013,  ..., 0.0000, 0.0529, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718900.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7300.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-390.2311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(343.7581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(63.6454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1413],
        [-0.2923],
        [-0.3558],
        ...,
        [-0.5717],
        [-0.5699],
        [-0.5692]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97452.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368443.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(500.5302, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368455.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.4372e-03,  7.0967e-03,  1.3858e-02,  ...,  5.6408e-04,
         -5.7102e-03, -1.0014e-02],
        [ 4.6248e-05, -1.2904e-03,  2.8341e-04,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.6248e-05, -1.2904e-03,  2.8341e-04,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.6248e-05, -1.2904e-03,  2.8341e-04,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.6248e-05, -1.2904e-03,  2.8341e-04,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.6248e-05, -1.2904e-03,  2.8341e-04,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2369.8765, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6964, device='cuda:0')



h[100].sum tensor(-8.9848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.3493, device='cuda:0')



h[200].sum tensor(-35.5612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3401e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0189, 0.0187, 0.0356,  ..., 0.0021, 0.0000, 0.0000],
        [0.0076, 0.0071, 0.0148,  ..., 0.0006, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70965.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1055, 0.0022, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0547, 0.0195, 0.0000,  ..., 0.0000, 0.0183, 0.0000],
        [0.0261, 0.0388, 0.0004,  ..., 0.0000, 0.0363, 0.0000],
        ...,
        [0.0157, 0.0586, 0.0009,  ..., 0.0000, 0.0535, 0.0000],
        [0.0157, 0.0586, 0.0009,  ..., 0.0000, 0.0535, 0.0000],
        [0.0157, 0.0585, 0.0009,  ..., 0.0000, 0.0535, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624126.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5450.3179, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-262.7639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(177.0288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(67.1512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1446],
        [ 0.0175],
        [-0.1194],
        ...,
        [-0.5802],
        [-0.5783],
        [-0.5776]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126901.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368455.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0042],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368467.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1541e-05, -1.2890e-03,  2.5594e-04,  ..., -1.3488e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1541e-05, -1.2890e-03,  2.5594e-04,  ..., -1.3488e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.6143e-03,  6.1926e-03,  1.2364e-02,  ...,  3.5874e-04,
         -5.0865e-03, -8.9251e-03],
        ...,
        [ 2.1541e-05, -1.2890e-03,  2.5594e-04,  ..., -1.3488e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1541e-05, -1.2890e-03,  2.5594e-04,  ..., -1.3488e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1541e-05, -1.2890e-03,  2.5594e-04,  ..., -1.3488e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2620.5107, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.4913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5657, device='cuda:0')



h[100].sum tensor(-10.8253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6918, device='cuda:0')



h[200].sum tensor(-42.9207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6408e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.6579e-05, 0.0000e+00, 1.0287e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.7115e-03, 6.2226e-03, 1.3197e-02,  ..., 3.6048e-04, 0.0000e+00,
         0.0000e+00],
        [5.4921e-03, 4.8400e-03, 1.0956e-02,  ..., 4.5593e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.8708e-05, 0.0000e+00, 1.0540e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.8681e-05, 0.0000e+00, 1.0537e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.8658e-05, 0.0000e+00, 1.0534e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76091.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0207, 0.0481, 0.0001,  ..., 0.0000, 0.0443, 0.0000],
        [0.0327, 0.0291, 0.0000,  ..., 0.0000, 0.0274, 0.0000],
        [0.0407, 0.0164, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        ...,
        [0.0158, 0.0594, 0.0003,  ..., 0.0000, 0.0542, 0.0000],
        [0.0158, 0.0594, 0.0003,  ..., 0.0000, 0.0542, 0.0000],
        [0.0158, 0.0594, 0.0003,  ..., 0.0000, 0.0542, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644130.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5899.7646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-299.8390, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(224.2373, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(68.9925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4148],
        [-0.2249],
        [-0.0231],
        ...,
        [-0.5879],
        [-0.5859],
        [-0.5852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110797.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0042],
        [1.0022],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368467.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0042],
        [1.0023],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368478.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.3019e-03,  9.2811e-03,  1.7328e-02,  ...,  1.0647e-03,
         -7.1748e-03, -1.2596e-02],
        [ 1.6472e-02,  1.7418e-02,  3.0495e-02,  ...,  2.9218e-03,
         -1.2699e-02, -2.2295e-02],
        [ 2.9746e-02,  3.2484e-02,  5.4874e-02,  ...,  6.3604e-03,
         -2.2927e-02, -4.0251e-02],
        ...,
        [-1.0134e-05, -1.2872e-03,  2.2587e-04,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0134e-05, -1.2872e-03,  2.2587e-04,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0134e-05, -1.2872e-03,  2.2587e-04,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2126.7705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.6185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1302, device='cuda:0')



h[100].sum tensor(-7.3769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.1689, device='cuda:0')



h[200].sum tensor(-29.2994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1198e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0305, 0.0320, 0.0569,  ..., 0.0052, 0.0000, 0.0000],
        [0.0716, 0.0761, 0.1324,  ..., 0.0131, 0.0000, 0.0000],
        [0.0878, 0.0945, 0.1623,  ..., 0.0173, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63039.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1495, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2479, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3218, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0271, 0.0414, 0.0000,  ..., 0.0000, 0.0386, 0.0000],
        [0.0160, 0.0603, 0.0000,  ..., 0.0000, 0.0547, 0.0000],
        [0.0160, 0.0603, 0.0000,  ..., 0.0000, 0.0547, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581457.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4630.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-208.1886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(105.3221, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.2064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2330],
        [ 0.2573],
        [ 0.2670],
        ...,
        [-0.2378],
        [-0.4204],
        [-0.5334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133518.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0042],
        [1.0023],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368478.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0043],
        [1.0023],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368490.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.2007e-03,  8.0597e-03,  1.5326e-02,  ...,  7.8763e-04,
         -6.3374e-03, -1.1132e-02],
        [-3.5905e-05, -1.2884e-03,  2.0060e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.2007e-03,  8.0597e-03,  1.5326e-02,  ...,  7.8763e-04,
         -6.3374e-03, -1.1132e-02],
        ...,
        [-3.5905e-05, -1.2884e-03,  2.0060e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [-3.5905e-05, -1.2884e-03,  2.0060e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [-3.5905e-05, -1.2884e-03,  2.0060e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2268.7979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.5345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9233, device='cuda:0')



h[100].sum tensor(-8.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.6598, device='cuda:0')



h[200].sum tensor(-33.5886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2923e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0064, 0.0132,  ..., 0.0004, 0.0000, 0.0000],
        [0.0300, 0.0290, 0.0561,  ..., 0.0024, 0.0000, 0.0000],
        [0.0067, 0.0064, 0.0132,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68258.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0444, 0.0158, 0.0000,  ..., 0.0000, 0.0156, 0.0000],
        [0.0698, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0601, 0.0016, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0161, 0.0611, 0.0000,  ..., 0.0000, 0.0554, 0.0000],
        [0.0161, 0.0611, 0.0000,  ..., 0.0000, 0.0553, 0.0000],
        [0.0161, 0.0611, 0.0000,  ..., 0.0000, 0.0553, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615280.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5319.3604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-245.3875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.5157, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.5498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0580],
        [ 0.1036],
        [ 0.1867],
        ...,
        [-0.6084],
        [-0.6063],
        [-0.6054]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122419., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0043],
        [1.0023],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368490.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0043],
        [1.0023],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368502.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4110e-05, -1.2960e-03,  1.9100e-04,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.0154e-03,  4.4571e-03,  9.5009e-03,  ..., -3.1415e-05,
         -3.8950e-03, -6.8459e-03],
        [ 4.7605e-03,  4.1678e-03,  9.0327e-03,  ..., -9.7469e-05,
         -3.6991e-03, -6.5016e-03],
        ...,
        [-5.4110e-05, -1.2960e-03,  1.9100e-04,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4110e-05, -1.2960e-03,  1.9100e-04,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4110e-05, -1.2960e-03,  1.9100e-04,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1984.4004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.7488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7886, device='cuda:0')



h[100].sum tensor(-6.4207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0806, device='cuda:0')



h[200].sum tensor(-25.5909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7518e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0045, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0165, 0.0163, 0.0313,  ..., 0.0017, 0.0000, 0.0000],
        [0.0343, 0.0340, 0.0642,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59763.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0470, 0.0201, 0.0000,  ..., 0.0000, 0.0182, 0.0000],
        [0.0766, 0.0032, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.1017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0163, 0.0615, 0.0000,  ..., 0.0000, 0.0556, 0.0000],
        [0.0163, 0.0615, 0.0000,  ..., 0.0000, 0.0556, 0.0000],
        [0.0163, 0.0614, 0.0000,  ..., 0.0000, 0.0556, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(569640.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4460.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-186.6034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(73.6094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.3922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.4526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0524],
        [ 0.1567],
        [ 0.2019],
        ...,
        [-0.6133],
        [-0.6112],
        [-0.6103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135690.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0043],
        [1.0023],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368502.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0044],
        [1.0023],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368515.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9935e-05, -1.3106e-03,  1.9537e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-6.9935e-05, -1.3106e-03,  1.9537e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-6.9935e-05, -1.3106e-03,  1.9537e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.9935e-05, -1.3106e-03,  1.9537e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-6.9935e-05, -1.3106e-03,  1.9537e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-6.9935e-05, -1.3106e-03,  1.9537e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2319.4246, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5212, device='cuda:0')



h[100].sum tensor(-8.6810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.1931, device='cuda:0')



h[200].sum tensor(-34.6608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3293e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66357.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0162, 0.0598, 0.0000,  ..., 0.0000, 0.0538, 0.0000],
        [0.0163, 0.0599, 0.0000,  ..., 0.0000, 0.0539, 0.0000],
        [0.0162, 0.0597, 0.0000,  ..., 0.0000, 0.0538, 0.0000],
        ...,
        [0.0168, 0.0615, 0.0000,  ..., 0.0000, 0.0555, 0.0000],
        [0.0168, 0.0614, 0.0000,  ..., 0.0000, 0.0554, 0.0000],
        [0.0168, 0.0614, 0.0000,  ..., 0.0000, 0.0554, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592577.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5032.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-235.8410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.7307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.9058, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.4669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5750],
        [-0.6611],
        [-0.7166],
        ...,
        [-0.6157],
        [-0.6136],
        [-0.6127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129072.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0044],
        [1.0023],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368515.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0044],
        [1.0023],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368528.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.5095e-05, -1.3277e-03,  2.0957e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5095e-05, -1.3277e-03,  2.0957e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5095e-05, -1.3277e-03,  2.0957e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-8.5095e-05, -1.3277e-03,  2.0957e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5095e-05, -1.3277e-03,  2.0957e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5095e-05, -1.3277e-03,  2.0957e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2607.0154, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.4919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9379, device='cuda:0')



h[100].sum tensor(-10.5466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.1320, device='cuda:0')



h[200].sum tensor(-42.1833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6020e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77038.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0309, 0.0343, 0.0000,  ..., 0.0000, 0.0320, 0.0000],
        [0.0285, 0.0391, 0.0000,  ..., 0.0000, 0.0361, 0.0000],
        [0.0607, 0.0244, 0.0000,  ..., 0.0000, 0.0221, 0.0000],
        ...,
        [0.0173, 0.0613, 0.0000,  ..., 0.0000, 0.0551, 0.0000],
        [0.0173, 0.0613, 0.0000,  ..., 0.0000, 0.0550, 0.0000],
        [0.0173, 0.0613, 0.0000,  ..., 0.0000, 0.0550, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660573.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6505.1685, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-312.1222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(223.9483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8.2849, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(74.3250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1361],
        [ 0.1425],
        [ 0.1744],
        ...,
        [-0.6164],
        [-0.6144],
        [-0.6136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112872.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0044],
        [1.0023],
        ...,
        [1.0008],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368528.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0045],
        [1.0024],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368540.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0121,  0.0125,  0.0227,  ...,  0.0018, -0.0094, -0.0165],
        [ 0.0178,  0.0190,  0.0331,  ...,  0.0033, -0.0137, -0.0241],
        [ 0.0121,  0.0125,  0.0227,  ...,  0.0018, -0.0094, -0.0165],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2555.8584, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9665, device='cuda:0')



h[100].sum tensor(-10.0555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.2657, device='cuda:0')



h[200].sum tensor(-40.2901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5421e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0655, 0.0694, 0.1218,  ..., 0.0117, 0.0000, 0.0000],
        [0.0701, 0.0747, 0.1304,  ..., 0.0129, 0.0000, 0.0000],
        [0.1199, 0.1311, 0.2218,  ..., 0.0258, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73636.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2697, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3162, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4095, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0176, 0.0613, 0.0000,  ..., 0.0000, 0.0548, 0.0000],
        [0.0176, 0.0612, 0.0000,  ..., 0.0000, 0.0548, 0.0000],
        [0.0208, 0.0553, 0.0000,  ..., 0.0000, 0.0498, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629807.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5937.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-290.0509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(191.5268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(30.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(73.9901, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2628],
        [ 0.2543],
        [ 0.2445],
        ...,
        [-0.5941],
        [-0.5474],
        [-0.4651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123785.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0045],
        [1.0024],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368540.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0045],
        [1.0024],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368553.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2328.3936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6127, device='cuda:0')



h[100].sum tensor(-8.3629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.3828, device='cuda:0')



h[200].sum tensor(-33.5673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2731e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68029.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0227, 0.0500, 0.0000,  ..., 0.0000, 0.0446, 0.0000],
        [0.0181, 0.0581, 0.0000,  ..., 0.0000, 0.0516, 0.0000],
        [0.0173, 0.0596, 0.0000,  ..., 0.0000, 0.0529, 0.0000],
        ...,
        [0.0178, 0.0613, 0.0000,  ..., 0.0000, 0.0545, 0.0000],
        [0.0178, 0.0613, 0.0000,  ..., 0.0000, 0.0545, 0.0000],
        [0.0178, 0.0613, 0.0000,  ..., 0.0000, 0.0545, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605810.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5526.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-249.8565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.2132, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(94.8886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(73.7236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2568],
        [-0.4201],
        [-0.5417],
        ...,
        [-0.6178],
        [-0.6157],
        [-0.6149]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117606.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0045],
        [1.0024],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368553.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0046],
        [1.0024],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368565.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0266,  0.0290,  0.0493,  ...,  0.0056, -0.0204, -0.0360],
        [ 0.0293,  0.0320,  0.0542,  ...,  0.0063, -0.0224, -0.0395],
        [ 0.0186,  0.0199,  0.0346,  ...,  0.0035, -0.0143, -0.0252],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1950.9329, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.1229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1467, device='cuda:0')



h[100].sum tensor(-5.6453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6163, device='cuda:0')



h[200].sum tensor(-22.6991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.7377e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1241, 0.1360, 0.2297,  ..., 0.0269, 0.0000, 0.0000],
        [0.1037, 0.1129, 0.1924,  ..., 0.0216, 0.0000, 0.0000],
        [0.0785, 0.0843, 0.1460,  ..., 0.0151, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57191.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4196, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3810, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3148, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0181, 0.0613, 0.0000,  ..., 0.0000, 0.0543, 0.0000],
        [0.0181, 0.0613, 0.0000,  ..., 0.0000, 0.0542, 0.0000],
        [0.0181, 0.0613, 0.0000,  ..., 0.0000, 0.0542, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555860.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4575.5439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-173.9782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(36.6383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(255.4288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(74.4407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2200],
        [ 0.2174],
        [ 0.2168],
        ...,
        [-0.6229],
        [-0.6209],
        [-0.6202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131160.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0046],
        [1.0024],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368565.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(534.8923, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0047],
        [1.0025],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368578.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0032,  0.0076,  ..., -0.0003, -0.0031, -0.0054],
        [ 0.0039,  0.0032,  0.0076,  ..., -0.0003, -0.0031, -0.0054],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2098.9268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.2129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6562, device='cuda:0')



h[100].sum tensor(-6.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.8543, device='cuda:0')



h[200].sum tensor(-26.5998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0288e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0056, 0.0143,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0056, 0.0143,  ..., 0.0000, 0.0000, 0.0000],
        [0.0070, 0.0056, 0.0143,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60084.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0400, 0.0152, 0.0014,  ..., 0.0000, 0.0122, 0.0000],
        [0.0420, 0.0121, 0.0015,  ..., 0.0000, 0.0097, 0.0000],
        [0.0400, 0.0159, 0.0014,  ..., 0.0000, 0.0131, 0.0000],
        ...,
        [0.0182, 0.0616, 0.0000,  ..., 0.0000, 0.0543, 0.0000],
        [0.0182, 0.0616, 0.0000,  ..., 0.0000, 0.0543, 0.0000],
        [0.0182, 0.0616, 0.0000,  ..., 0.0000, 0.0542, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565454.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4859.7910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-192.4934, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(58.5174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(374.9783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(73.7527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0402],
        [ 0.0919],
        [ 0.0766],
        ...,
        [-0.6274],
        [-0.6254],
        [-0.6246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110514.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0047],
        [1.0025],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368578.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0047],
        [1.0025],
        ...,
        [1.0007],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368591.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0093,  0.0094,  0.0176,  ...,  0.0011, -0.0072, -0.0127],
        [ 0.0179,  0.0192,  0.0335,  ...,  0.0034, -0.0138, -0.0243],
        ...,
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1910.9412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(8.5663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5543, device='cuda:0')



h[100].sum tensor(-5.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.0880, device='cuda:0')



h[200].sum tensor(-21.5803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.3718e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0095, 0.0183,  ..., 0.0011, 0.0000, 0.0000],
        [0.0329, 0.0350, 0.0618,  ..., 0.0059, 0.0000, 0.0000],
        [0.0811, 0.0873, 0.1509,  ..., 0.0158, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56173.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0782, 0.0185, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.1617, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2807, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0181, 0.0624, 0.0000,  ..., 0.0000, 0.0548, 0.0000],
        [0.0181, 0.0624, 0.0000,  ..., 0.0000, 0.0547, 0.0000],
        [0.0181, 0.0623, 0.0000,  ..., 0.0000, 0.0547, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555559.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4531.9683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-169.0532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(21.9162, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(627.5402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(76.6981, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0895],
        [ 0.0664],
        [ 0.1544],
        ...,
        [-0.6369],
        [-0.6349],
        [-0.6341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137305.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0047],
        [1.0025],
        ...,
        [1.0007],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368591.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0048],
        [1.0025],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368603.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0059,  0.0120,  ...,  0.0003, -0.0049, -0.0086],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0103,  0.0106,  0.0195,  ...,  0.0014, -0.0080, -0.0141],
        ...,
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2227.5964, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.1331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0961, device='cuda:0')



h[100].sum tensor(-7.5481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.0303, device='cuda:0')



h[200].sum tensor(-30.5117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1795e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0328, 0.0337, 0.0620,  ..., 0.0046, 0.0000, 0.0000],
        [0.0303, 0.0297, 0.0577,  ..., 0.0026, 0.0000, 0.0000],
        [0.0212, 0.0217, 0.0403,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0138, 0.0144, 0.0264,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63206.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1720, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1348, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1251, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0211, 0.0582, 0.0000,  ..., 0.0000, 0.0511, 0.0000],
        [0.0344, 0.0376, 0.0000,  ..., 0.0000, 0.0334, 0.0000],
        [0.0750, 0.0164, 0.0000,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587510.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5077.7310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-220.0293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.2097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(843.2424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(77.7113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2303],
        [ 0.2274],
        [ 0.2257],
        ...,
        [-0.5428],
        [-0.4006],
        [-0.2110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136817.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0048],
        [1.0025],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368603.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0048],
        [1.0025],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368616.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0112,  0.0115,  0.0211,  ...,  0.0016, -0.0086, -0.0152],
        [ 0.0097,  0.0098,  0.0183,  ...,  0.0012, -0.0075, -0.0132],
        [ 0.0095,  0.0096,  0.0180,  ...,  0.0012, -0.0073, -0.0130],
        ...,
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2213.9280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.6923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5029, device='cuda:0')



h[100].sum tensor(-7.4105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5012, device='cuda:0')



h[200].sum tensor(-30.0085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1428e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0405, 0.0412, 0.0764,  ..., 0.0053, 0.0000, 0.0000],
        [0.0536, 0.0561, 0.1006,  ..., 0.0087, 0.0000, 0.0000],
        [0.0510, 0.0532, 0.0957,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63944.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8172e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.2635e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.3147e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7858e-02, 6.3651e-02, 1.1533e-04,  ..., 0.0000e+00, 5.5745e-02,
         0.0000e+00],
        [1.7849e-02, 6.3622e-02, 1.1372e-04,  ..., 0.0000e+00, 5.5718e-02,
         0.0000e+00],
        [1.7841e-02, 6.3601e-02, 1.1241e-04,  ..., 0.0000e+00, 5.5698e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593932., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5186.2622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-225.3523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(87.2498, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1027.6174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(76.3034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1992],
        [ 0.2221],
        [ 0.2289],
        ...,
        [-0.6098],
        [-0.5765],
        [-0.5540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133238.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0048],
        [1.0025],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368616.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0048],
        [1.0026],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368629.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7444e-03,  4.2257e-03,  9.2296e-03,  ..., -6.7901e-05,
         -3.7283e-03, -6.5900e-03],
        [ 4.8435e-03,  4.3381e-03,  9.4115e-03,  ..., -4.2232e-05,
         -3.8033e-03, -6.7226e-03],
        [-1.7573e-04, -1.3586e-03,  1.9134e-04,  ..., -1.3433e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.7573e-04, -1.3586e-03,  1.9134e-04,  ..., -1.3433e-03,
          0.0000e+00,  0.0000e+00],
        [-1.7573e-04, -1.3586e-03,  1.9134e-04,  ..., -1.3433e-03,
          0.0000e+00,  0.0000e+00],
        [-1.7573e-04, -1.3586e-03,  1.9134e-04,  ..., -1.3433e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2037.9004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.3376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6073, device='cuda:0')



h[100].sum tensor(-6.1579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9189, device='cuda:0')



h[200].sum tensor(-24.9804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6398e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0338, 0.0337, 0.0641,  ..., 0.0036, 0.0000, 0.0000],
        [0.0164, 0.0162, 0.0315,  ..., 0.0017, 0.0000, 0.0000],
        [0.0049, 0.0044, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58346.9414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1053, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0805, 0.0016, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0524, 0.0144, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        ...,
        [0.0176, 0.0641, 0.0001,  ..., 0.0000, 0.0561, 0.0000],
        [0.0176, 0.0640, 0.0001,  ..., 0.0000, 0.0561, 0.0000],
        [0.0176, 0.0640, 0.0001,  ..., 0.0000, 0.0561, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566781.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4533.4697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-188.8624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(37.3741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1374.8503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(77.3094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2101],
        [ 0.2060],
        [ 0.1940],
        ...,
        [-0.6561],
        [-0.6537],
        [-0.6527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152563.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0048],
        [1.0026],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368629.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0049],
        [1.0026],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368642.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0014,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2376.9038, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.1934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0707, device='cuda:0')



h[100].sum tensor(-8.3469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.7913, device='cuda:0')



h[200].sum tensor(-33.9207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3014e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0049, 0.0132,  ..., 0.0000, 0.0000, 0.0000],
        [0.0032, 0.0025, 0.0070,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67537.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0340, 0.0281, 0.0012,  ..., 0.0000, 0.0238, 0.0000],
        [0.0290, 0.0382, 0.0008,  ..., 0.0000, 0.0329, 0.0000],
        [0.0231, 0.0500, 0.0003,  ..., 0.0000, 0.0436, 0.0000],
        ...,
        [0.0175, 0.0644, 0.0002,  ..., 0.0000, 0.0564, 0.0000],
        [0.0174, 0.0643, 0.0002,  ..., 0.0000, 0.0564, 0.0000],
        [0.0174, 0.0643, 0.0002,  ..., 0.0000, 0.0563, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614502., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5435.0020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-254.4028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.1397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1495.3408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(77.2081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3819],
        [-0.4702],
        [-0.5684],
        ...,
        [-0.6606],
        [-0.6583],
        [-0.6573]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139214.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0049],
        [1.0026],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368642.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0049],
        [1.0026],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368655.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2076.0337, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.7285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7864, device='cuda:0')



h[100].sum tensor(-6.2652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0787, device='cuda:0')



h[200].sum tensor(-25.5063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7505e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61289.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0166, 0.0629, 0.0001,  ..., 0.0000, 0.0550, 0.0000],
        [0.0166, 0.0630, 0.0001,  ..., 0.0000, 0.0551, 0.0000],
        [0.0166, 0.0629, 0.0001,  ..., 0.0000, 0.0550, 0.0000],
        ...,
        [0.0172, 0.0648, 0.0002,  ..., 0.0000, 0.0568, 0.0000],
        [0.0172, 0.0647, 0.0002,  ..., 0.0000, 0.0567, 0.0000],
        [0.0172, 0.0647, 0.0002,  ..., 0.0000, 0.0567, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589033.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4840.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-207.7068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(64.3459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1744.4338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.0495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7707],
        [-0.7974],
        [-0.8169],
        ...,
        [-0.6630],
        [-0.6613],
        [-0.6606]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141410.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0049],
        [1.0026],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368655.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0050],
        [1.0027],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368667.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2022.8713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.5630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7370, device='cuda:0')



h[100].sum tensor(-5.7809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1428, device='cuda:0')



h[200].sum tensor(-23.5769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.1023e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0045, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58271.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0538, 0.0239, 0.0002,  ..., 0.0000, 0.0207, 0.0000],
        [0.0297, 0.0398, 0.0009,  ..., 0.0000, 0.0348, 0.0000],
        [0.0255, 0.0468, 0.0006,  ..., 0.0000, 0.0407, 0.0000],
        ...,
        [0.0169, 0.0649, 0.0002,  ..., 0.0000, 0.0569, 0.0000],
        [0.0169, 0.0648, 0.0002,  ..., 0.0000, 0.0569, 0.0000],
        [0.0169, 0.0648, 0.0002,  ..., 0.0000, 0.0569, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574693.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4420.6704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-189.2907, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(41.2743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2118.5232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(77.4951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0403],
        [-0.0872],
        [-0.1628],
        ...,
        [-0.6696],
        [-0.6672],
        [-0.6663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159098.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0050],
        [1.0027],
        ...,
        [1.0006],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368667.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0050],
        [1.0027],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368679.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2281.0173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.9664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8917, device='cuda:0')



h[100].sum tensor(-7.3363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.8480, device='cuda:0')



h[200].sum tensor(-29.9739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1668e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62709.1758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0160, 0.0630, 0.0002,  ..., 0.0000, 0.0552, 0.0000],
        [0.0161, 0.0631, 0.0002,  ..., 0.0000, 0.0553, 0.0000],
        [0.0160, 0.0629, 0.0002,  ..., 0.0000, 0.0552, 0.0000],
        ...,
        [0.0166, 0.0649, 0.0003,  ..., 0.0000, 0.0570, 0.0000],
        [0.0166, 0.0648, 0.0003,  ..., 0.0000, 0.0569, 0.0000],
        [0.0166, 0.0648, 0.0003,  ..., 0.0000, 0.0569, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585684.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4642.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-218.2175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.2243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2173.8088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.9821, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8006],
        [-0.8136],
        [-0.8220],
        ...,
        [-0.6707],
        [-0.6681],
        [-0.6671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137875.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0050],
        [1.0027],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368679.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0051],
        [1.0027],
        ...,
        [1.0007],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368692.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0157,  0.0167,  0.0294,  ...,  0.0028, -0.0120, -0.0212],
        ...,
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2211.1316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.5377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0836, device='cuda:0')



h[100].sum tensor(-6.6819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2355, device='cuda:0')



h[200].sum tensor(-27.3490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0552e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0287, 0.0303, 0.0541,  ..., 0.0048, 0.0000, 0.0000],
        [0.0575, 0.0619, 0.1073,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61878.8945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0641, 0.0201, 0.0000,  ..., 0.0000, 0.0181, 0.0000],
        [0.1387, 0.0012, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.2166, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0163, 0.0646, 0.0005,  ..., 0.0000, 0.0568, 0.0000],
        [0.0163, 0.0646, 0.0005,  ..., 0.0000, 0.0568, 0.0000],
        [0.0163, 0.0646, 0.0005,  ..., 0.0000, 0.0568, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588161.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4578.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-213.2127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(79.2543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2454.8286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.3575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1510],
        [ 0.2035],
        [ 0.2279],
        ...,
        [-0.6719],
        [-0.6695],
        [-0.6686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145172.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0051],
        [1.0027],
        ...,
        [1.0007],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368692.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(518.3007, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0051],
        [1.0028],
        ...,
        [1.0008],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368705.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.3455e-03,  3.7989e-03,  8.4618e-03,  ..., -1.7715e-04,
         -3.3773e-03, -5.9900e-03],
        [ 5.3365e-03,  4.9237e-03,  1.0283e-02,  ...,  7.9711e-05,
         -4.1217e-03, -7.3103e-03],
        [-1.5038e-04, -1.3038e-03,  2.0122e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.5038e-04, -1.3038e-03,  2.0122e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        [-1.5038e-04, -1.3038e-03,  2.0122e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00],
        [-1.5038e-04, -1.3038e-03,  2.0122e-04,  ..., -1.3425e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2551.6475, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4750, device='cuda:0')



h[100].sum tensor(-8.7248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.0436, device='cuda:0')



h[200].sum tensor(-35.7746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3882e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0356, 0.0359, 0.0673,  ..., 0.0040, 0.0000, 0.0000],
        [0.0164, 0.0164, 0.0316,  ..., 0.0018, 0.0000, 0.0000],
        [0.0106, 0.0098, 0.0208,  ..., 0.0001, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73226.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1084, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0836, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0657, 0.0019, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        ...,
        [0.0162, 0.0644, 0.0007,  ..., 0.0000, 0.0566, 0.0000],
        [0.0162, 0.0643, 0.0007,  ..., 0.0000, 0.0566, 0.0000],
        [0.0162, 0.0643, 0.0007,  ..., 0.0000, 0.0566, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662990.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5981.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-295.6900, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(186.7349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2595.3784, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(76.5183, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2397],
        [ 0.2393],
        [ 0.2301],
        ...,
        [-0.6718],
        [-0.6694],
        [-0.6685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143027.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0051],
        [1.0028],
        ...,
        [1.0008],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368705.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0051],
        [1.0028],
        ...,
        [1.0008],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368705.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0002, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2638.7417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5575, device='cuda:0')



h[100].sum tensor(-9.2950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.0091, device='cuda:0')



h[200].sum tensor(-38.1127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4550e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73908.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0157, 0.0625, 0.0006,  ..., 0.0000, 0.0549, 0.0000],
        [0.0163, 0.0612, 0.0007,  ..., 0.0000, 0.0537, 0.0000],
        [0.0181, 0.0574, 0.0008,  ..., 0.0000, 0.0504, 0.0000],
        ...,
        [0.0162, 0.0644, 0.0007,  ..., 0.0000, 0.0566, 0.0000],
        [0.0190, 0.0597, 0.0007,  ..., 0.0000, 0.0527, 0.0000],
        [0.0280, 0.0443, 0.0004,  ..., 0.0000, 0.0399, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653231.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5741.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-300.5502, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(192.9671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2626.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(76.8770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5667],
        [-0.5167],
        [-0.4382],
        ...,
        [-0.6416],
        [-0.5796],
        [-0.4521]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148164.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0051],
        [1.0028],
        ...,
        [1.0008],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368705.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0052],
        [1.0028],
        ...,
        [1.0008],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368718.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0056,  0.0053,  0.0108,  ...,  0.0002, -0.0043, -0.0077],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2220.1489, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.3649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4000, device='cuda:0')



h[100].sum tensor(-6.3699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6258, device='cuda:0')



h[200].sum tensor(-26.1655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0129e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0057, 0.0053, 0.0115,  ..., 0.0002, 0.0000, 0.0000],
        [0.0046, 0.0041, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63291.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0210, 0.0515, 0.0005,  ..., 0.0000, 0.0459, 0.0000],
        [0.0302, 0.0342, 0.0001,  ..., 0.0000, 0.0312, 0.0000],
        [0.0340, 0.0259, 0.0000,  ..., 0.0000, 0.0246, 0.0000],
        ...,
        [0.0161, 0.0640, 0.0009,  ..., 0.0000, 0.0564, 0.0000],
        [0.0161, 0.0640, 0.0009,  ..., 0.0000, 0.0564, 0.0000],
        [0.0161, 0.0640, 0.0009,  ..., 0.0000, 0.0564, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599908.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4799.7100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-220.2853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.5266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2717.1313, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(71.9207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3412],
        [-0.4124],
        [-0.4516],
        ...,
        [-0.6711],
        [-0.6689],
        [-0.6680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129186.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0052],
        [1.0028],
        ...,
        [1.0008],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368718.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0052],
        [1.0028],
        ...,
        [1.0009],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368731.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2721.4563, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.5081, device='cuda:0')



h[100].sum tensor(-9.4970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.8568, device='cuda:0')



h[200].sum tensor(-39.0808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5137e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74170.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0154, 0.0622, 0.0009,  ..., 0.0000, 0.0548, 0.0000],
        [0.0154, 0.0623, 0.0009,  ..., 0.0000, 0.0549, 0.0000],
        [0.0154, 0.0621, 0.0009,  ..., 0.0000, 0.0548, 0.0000],
        ...,
        [0.0391, 0.0233, 0.0000,  ..., 0.0000, 0.0229, 0.0000],
        [0.0379, 0.0253, 0.0000,  ..., 0.0000, 0.0245, 0.0000],
        [0.0324, 0.0350, 0.0000,  ..., 0.0000, 0.0325, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653448.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5782.4189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-298.3658, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(198.5907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2907.5771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.1204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7405],
        [-0.7663],
        [-0.7774],
        ...,
        [-0.0051],
        [-0.0087],
        [-0.0335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121745.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0052],
        [1.0028],
        ...,
        [1.0009],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368731.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0053],
        [1.0028],
        ...,
        [1.0009],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368744.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [ 0.0071,  0.0070,  0.0135,  ...,  0.0005, -0.0054, -0.0096]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2356.7341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.0406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2786, device='cuda:0')



h[100].sum tensor(-7.0678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.3012, device='cuda:0')



h[200].sum tensor(-29.1369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1290e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0074, 0.0072, 0.0146,  ..., 0.0006, 0.0000, 0.0000],
        [0.0060, 0.0056, 0.0121,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65076.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0479, 0.0003,  ..., 0.0000, 0.0430, 0.0000],
        [0.0186, 0.0556, 0.0006,  ..., 0.0000, 0.0496, 0.0000],
        [0.0221, 0.0484, 0.0003,  ..., 0.0000, 0.0436, 0.0000],
        ...,
        [0.0220, 0.0525, 0.0004,  ..., 0.0000, 0.0473, 0.0000],
        [0.0360, 0.0313, 0.0000,  ..., 0.0000, 0.0291, 0.0000],
        [0.0415, 0.0201, 0.0000,  ..., 0.0000, 0.0194, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606208.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4720.9307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-232.9842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.0963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3290.2632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(69.6557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0067],
        [-0.0703],
        [-0.0784],
        ...,
        [-0.5448],
        [-0.4284],
        [-0.3236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140011.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0053],
        [1.0028],
        ...,
        [1.0009],
        [1.0002],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368744.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0053],
        [1.0028],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368756.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        ...,
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000],
        [-0.0001, -0.0013,  0.0002,  ..., -0.0013,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2253.4348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.4156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3836, device='cuda:0')



h[100].sum tensor(-6.3426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6112, device='cuda:0')



h[200].sum tensor(-26.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0119e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62100.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0253, 0.0420, 0.0003,  ..., 0.0000, 0.0383, 0.0000],
        [0.0190, 0.0541, 0.0005,  ..., 0.0000, 0.0484, 0.0000],
        [0.0193, 0.0534, 0.0006,  ..., 0.0000, 0.0478, 0.0000],
        ...,
        [0.0153, 0.0649, 0.0008,  ..., 0.0000, 0.0576, 0.0000],
        [0.0153, 0.0648, 0.0008,  ..., 0.0000, 0.0576, 0.0000],
        [0.0153, 0.0648, 0.0008,  ..., 0.0000, 0.0575, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592640.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4381.2314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-208.4762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(89.4928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3228.1685, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(65.6864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0540],
        [-0.0400],
        [-0.0928],
        ...,
        [-0.6854],
        [-0.6829],
        [-0.6819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130995.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0053],
        [1.0028],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368756.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0053],
        [1.0029],
        ...,
        [1.0010],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368769.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.6571e-05, -1.2315e-03,  1.9435e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00],
        [-9.6571e-05, -1.2315e-03,  1.9435e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00],
        [-9.6571e-05, -1.2315e-03,  1.9435e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-9.6571e-05, -1.2315e-03,  1.9435e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00],
        [-9.6571e-05, -1.2315e-03,  1.9435e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00],
        [-9.6571e-05, -1.2315e-03,  1.9435e-04,  ..., -1.3411e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2085.0454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.0309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5821, device='cuda:0')



h[100].sum tensor(-5.2531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1128, device='cuda:0')



h[200].sum tensor(-21.7339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.3890e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57997.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0636, 0.0007,  ..., 0.0000, 0.0567, 0.0000],
        [0.0140, 0.0637, 0.0007,  ..., 0.0000, 0.0567, 0.0000],
        [0.0140, 0.0636, 0.0007,  ..., 0.0000, 0.0566, 0.0000],
        ...,
        [0.0145, 0.0656, 0.0008,  ..., 0.0000, 0.0585, 0.0000],
        [0.0145, 0.0655, 0.0008,  ..., 0.0000, 0.0585, 0.0000],
        [0.0145, 0.0655, 0.0008,  ..., 0.0000, 0.0585, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580611.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3904.3384, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-179.0216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(53.9590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3364.3015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(64.4756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7416],
        [-0.6852],
        [-0.5845],
        ...,
        [-0.6933],
        [-0.6908],
        [-0.6898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145135.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0053],
        [1.0029],
        ...,
        [1.0010],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368769.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0054],
        [1.0029],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368782.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9215e-02,  3.2045e-02,  5.4035e-02,  ...,  6.2512e-03,
         -2.1819e-02, -3.8831e-02],
        [ 2.2976e-02,  2.4962e-02,  4.2569e-02,  ...,  4.6343e-03,
         -1.7173e-02, -3.0562e-02],
        [ 1.9535e-02,  2.1057e-02,  3.6247e-02,  ...,  3.7426e-03,
         -1.4611e-02, -2.6003e-02],
        ...,
        [-8.5665e-05, -1.2150e-03,  1.8981e-04,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5665e-05, -1.2150e-03,  1.8981e-04,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00],
        [-8.5665e-05, -1.2150e-03,  1.8981e-04,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2525.7400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0444, device='cuda:0')



h[100].sum tensor(-8.0198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.7679, device='cuda:0')



h[200].sum tensor(-33.2406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2998e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0865, 0.0937, 0.1603,  ..., 0.0171, 0.0000, 0.0000],
        [0.1085, 0.1187, 0.2008,  ..., 0.0228, 0.0000, 0.0000],
        [0.1141, 0.1250, 0.2111,  ..., 0.0243, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66099.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3697, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4421, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4701, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0139, 0.0661, 0.0007,  ..., 0.0000, 0.0592, 0.0000],
        [0.0139, 0.0661, 0.0007,  ..., 0.0000, 0.0592, 0.0000],
        [0.0139, 0.0660, 0.0007,  ..., 0.0000, 0.0591, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611198.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4266.3350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-240.8454, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3404.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(66.9966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2822],
        [ 0.2620],
        [ 0.2438],
        ...,
        [-0.6997],
        [-0.6972],
        [-0.6962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164154.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0054],
        [1.0029],
        ...,
        [1.0011],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368782.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0054],
        [1.0029],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368795.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3610e-02,  2.5702e-02,  4.3714e-02,  ...,  4.7964e-03,
         -1.7618e-02, -3.1373e-02],
        [ 1.5850e-02,  1.6893e-02,  2.9455e-02,  ...,  2.7856e-03,
         -1.1848e-02, -2.1098e-02],
        [ 3.2173e-02,  3.5423e-02,  5.9451e-02,  ...,  7.0157e-03,
         -2.3986e-02, -4.2712e-02],
        ...,
        [-8.3758e-05, -1.1947e-03,  1.7535e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3758e-05, -1.1947e-03,  1.7535e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3758e-05, -1.1947e-03,  1.7535e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2727.2307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2969, device='cuda:0')



h[100].sum tensor(-9.2383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.6685, device='cuda:0')



h[200].sum tensor(-38.3602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5007e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0682, 0.0729, 0.1266,  ..., 0.0123, 0.0000, 0.0000],
        [0.1013, 0.1105, 0.1874,  ..., 0.0209, 0.0000, 0.0000],
        [0.0551, 0.0581, 0.1025,  ..., 0.0090, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72817.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2546, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3066, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2414, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0135, 0.0667, 0.0005,  ..., 0.0000, 0.0597, 0.0000],
        [0.0135, 0.0666, 0.0005,  ..., 0.0000, 0.0597, 0.0000],
        [0.0135, 0.0666, 0.0005,  ..., 0.0000, 0.0596, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646400.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4903.2256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-283.3941, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(195.0875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3023.7810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(65.7972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3052],
        [ 0.3088],
        [ 0.2895],
        ...,
        [-0.7036],
        [-0.7009],
        [-0.6997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134337.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0054],
        [1.0029],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368795.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0054],
        [1.0029],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368795.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6119e-02,  1.7199e-02,  2.9950e-02,  ...,  2.8554e-03,
         -1.2048e-02, -2.1455e-02],
        [ 2.5331e-02,  2.7656e-02,  4.6877e-02,  ...,  5.2425e-03,
         -1.8898e-02, -3.3652e-02],
        [ 2.8802e-02,  3.1596e-02,  5.3255e-02,  ...,  6.1419e-03,
         -2.1479e-02, -3.8247e-02],
        ...,
        [-8.3758e-05, -1.1947e-03,  1.7535e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3758e-05, -1.1947e-03,  1.7535e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3758e-05, -1.1947e-03,  1.7535e-04,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2324.3271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.4501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5871, device='cuda:0')



h[100].sum tensor(-6.6809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6845, device='cuda:0')



h[200].sum tensor(-27.7410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0863e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0774, 0.0834, 0.1436,  ..., 0.0147, 0.0000, 0.0000],
        [0.0801, 0.0865, 0.1486,  ..., 0.0154, 0.0000, 0.0000],
        [0.0904, 0.0981, 0.1674,  ..., 0.0181, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61729.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3270, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3182, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3022, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0135, 0.0667, 0.0005,  ..., 0.0000, 0.0597, 0.0000],
        [0.0135, 0.0666, 0.0005,  ..., 0.0000, 0.0597, 0.0000],
        [0.0135, 0.0666, 0.0005,  ..., 0.0000, 0.0596, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595210.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3884.3745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-205.8691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.5147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3252.4331, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(66.6850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2772],
        [ 0.2721],
        [ 0.2573],
        ...,
        [-0.7050],
        [-0.7025],
        [-0.7016]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156038.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0054],
        [1.0029],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368795.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(427.9417, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0055],
        [1.0029],
        ...,
        [1.0011],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368808.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.2660e-05, -1.1772e-03,  1.6955e-04,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [-8.2660e-05, -1.1772e-03,  1.6955e-04,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [-8.2660e-05, -1.1772e-03,  1.6955e-04,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-8.2660e-05, -1.1772e-03,  1.6955e-04,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [-8.2660e-05, -1.1772e-03,  1.6955e-04,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [-8.2660e-05, -1.1772e-03,  1.6955e-04,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2273.5679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.2794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2628, device='cuda:0')



h[100].sum tensor(-6.2239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5035, device='cuda:0')



h[200].sum tensor(-25.8902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0045e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61371.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0134, 0.0632, 0.0008,  ..., 0.0000, 0.0562, 0.0000],
        [0.0245, 0.0431, 0.0005,  ..., 0.0000, 0.0397, 0.0000],
        [0.0407, 0.0214, 0.0000,  ..., 0.0000, 0.0198, 0.0000],
        ...,
        [0.0131, 0.0669, 0.0005,  ..., 0.0000, 0.0598, 0.0000],
        [0.0131, 0.0668, 0.0005,  ..., 0.0000, 0.0598, 0.0000],
        [0.0131, 0.0668, 0.0005,  ..., 0.0000, 0.0598, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596584.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3770.2866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-203.5730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.6748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3155.6755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(67.9454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3541],
        [-0.1434],
        [ 0.0654],
        ...,
        [-0.7084],
        [-0.7058],
        [-0.7039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163309.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0055],
        [1.0029],
        ...,
        [1.0011],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368808.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0055],
        [1.0030],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368821.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.8909e-05, -1.1647e-03,  1.7118e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8909e-05, -1.1647e-03,  1.7118e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8909e-05, -1.1647e-03,  1.7118e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.8909e-05, -1.1647e-03,  1.7118e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8909e-05, -1.1647e-03,  1.7118e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8909e-05, -1.1647e-03,  1.7118e-04,  ..., -1.3461e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2158.0747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.0051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9216, device='cuda:0')



h[100].sum tensor(-5.3355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4155, device='cuda:0')



h[200].sum tensor(-22.2350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.5986e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.1174e-03, 4.7289e-03, 1.0238e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.9006e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.4212e-03, 5.0756e-03, 1.0795e-02,  ..., 7.3699e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 7.0789e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.0754e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.0731e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58439.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5609e-02, 3.7093e-02, 3.3348e-05,  ..., 0.0000e+00, 3.5060e-02,
         0.0000e+00],
        [2.0239e-02, 4.8405e-02, 3.3478e-05,  ..., 0.0000e+00, 4.4339e-02,
         0.0000e+00],
        [2.6631e-02, 3.5380e-02, 0.0000e+00,  ..., 0.0000e+00, 3.3726e-02,
         0.0000e+00],
        ...,
        [1.2646e-02, 6.6869e-02, 6.4432e-04,  ..., 0.0000e+00, 5.9778e-02,
         0.0000e+00],
        [1.2638e-02, 6.6833e-02, 6.4200e-04,  ..., 0.0000e+00, 5.9744e-02,
         0.0000e+00],
        [1.2632e-02, 6.6807e-02, 6.3978e-04,  ..., 0.0000e+00, 5.9718e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587659.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3509.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-181.6169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(72.3594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2995.5637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(67.1426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5035],
        [-0.5844],
        [-0.5949],
        ...,
        [-0.7121],
        [-0.7095],
        [-0.7085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163107.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0055],
        [1.0030],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368821.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0056],
        [1.0030],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368834.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6587e-05, -1.1599e-03,  1.8204e-04,  ..., -1.3463e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6587e-05, -1.1599e-03,  1.8204e-04,  ..., -1.3463e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6587e-05, -1.1599e-03,  1.8204e-04,  ..., -1.3463e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.6587e-05, -1.1599e-03,  1.8204e-04,  ..., -1.3463e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6587e-05, -1.1599e-03,  1.8204e-04,  ..., -1.3463e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6587e-05, -1.1599e-03,  1.8204e-04,  ..., -1.3463e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2585.0913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.4289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9134, device='cuda:0')



h[100].sum tensor(-7.8599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.6510, device='cuda:0')



h[200].sum tensor(-32.8146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2917e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67534.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0122, 0.0635, 0.0013,  ..., 0.0000, 0.0566, 0.0000],
        [0.0127, 0.0624, 0.0018,  ..., 0.0000, 0.0554, 0.0000],
        [0.0156, 0.0558, 0.0021,  ..., 0.0000, 0.0497, 0.0000],
        ...,
        [0.0121, 0.0667, 0.0009,  ..., 0.0000, 0.0599, 0.0000],
        [0.0121, 0.0667, 0.0009,  ..., 0.0000, 0.0599, 0.0000],
        [0.0121, 0.0667, 0.0009,  ..., 0.0000, 0.0599, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619474.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4064.6255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-244.1022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.2204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2599.8391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(62.9219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5511],
        [-0.4380],
        [-0.2976],
        ...,
        [-0.7117],
        [-0.7093],
        [-0.7083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141539.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0056],
        [1.0030],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368834.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0057],
        [1.0031],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368847.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7019e-03,  5.3789e-03,  1.0767e-02,  ...,  1.4557e-04,
         -4.2551e-03, -7.5946e-03],
        [-5.3574e-05, -1.1557e-03,  1.9055e-04,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.7019e-03,  5.3789e-03,  1.0767e-02,  ...,  1.4557e-04,
         -4.2551e-03, -7.5946e-03],
        ...,
        [-5.3574e-05, -1.1557e-03,  1.9055e-04,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [-5.3574e-05, -1.1557e-03,  1.9055e-04,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [-5.3574e-05, -1.1557e-03,  1.9055e-04,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2998.1519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3838, device='cuda:0')



h[100].sum tensor(-10.2884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.4215, device='cuda:0')



h[200].sum tensor(-43.0313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6914e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0042, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        [0.0209, 0.0193, 0.0395,  ..., 0.0003, 0.0000, 0.0000],
        [0.0047, 0.0042, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78990.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0383, 0.0106, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0469, 0.0001, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0369, 0.0125, 0.0005,  ..., 0.0000, 0.0131, 0.0000],
        ...,
        [0.0118, 0.0666, 0.0012,  ..., 0.0000, 0.0600, 0.0000],
        [0.0118, 0.0666, 0.0012,  ..., 0.0000, 0.0599, 0.0000],
        [0.0118, 0.0666, 0.0012,  ..., 0.0000, 0.0599, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677837.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5097.6006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-326.4980, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.5175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2345.3806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(60.1924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2135],
        [ 0.2127],
        [ 0.2279],
        ...,
        [-0.7131],
        [-0.7107],
        [-0.7097]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129630.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0057],
        [1.0031],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368847.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0057],
        [1.0031],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368859.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1356e-05, -1.1469e-03,  1.8100e-04,  ..., -1.3449e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0055e-03,  5.7305e-03,  1.1312e-02,  ...,  2.2472e-04,
         -4.4717e-03, -7.9858e-03],
        [-5.1356e-05, -1.1469e-03,  1.8100e-04,  ..., -1.3449e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.1356e-05, -1.1469e-03,  1.8100e-04,  ..., -1.3449e-03,
          0.0000e+00,  0.0000e+00],
        [-5.1356e-05, -1.1469e-03,  1.8100e-04,  ..., -1.3449e-03,
          0.0000e+00,  0.0000e+00],
        [-5.1356e-05, -1.1469e-03,  1.8100e-04,  ..., -1.3449e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2345.4409, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.3419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3044, device='cuda:0')



h[100].sum tensor(-6.1597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5406, device='cuda:0')



h[200].sum tensor(-25.8100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0070e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0180, 0.0171, 0.0340,  ..., 0.0009, 0.0000, 0.0000],
        [0.0158, 0.0158, 0.0300,  ..., 0.0014, 0.0000, 0.0000],
        [0.0252, 0.0242, 0.0474,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62101.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0640, 0.0025, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0674, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0676, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0116, 0.0671, 0.0010,  ..., 0.0000, 0.0602, 0.0000],
        [0.0116, 0.0671, 0.0010,  ..., 0.0000, 0.0601, 0.0000],
        [0.0116, 0.0670, 0.0010,  ..., 0.0000, 0.0601, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598348.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3527.2915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-204.8400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.3772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2469.9712, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(59.9656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1995],
        [ 0.2453],
        [ 0.2228],
        ...,
        [-0.7185],
        [-0.7161],
        [-0.7151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142692.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0057],
        [1.0031],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368859.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0058],
        [1.0032],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368872.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8623e-02,  2.0071e-02,  3.4478e-02,  ...,  3.4969e-03,
         -1.3769e-02, -2.4604e-02],
        [ 1.2263e-02,  1.2849e-02,  2.2791e-02,  ...,  1.8486e-03,
         -9.0799e-03, -1.6225e-02],
        [ 1.4352e-02,  1.5221e-02,  2.6629e-02,  ...,  2.3899e-03,
         -1.0620e-02, -1.8976e-02],
        ...,
        [-5.2822e-05, -1.1370e-03,  1.5909e-04,  ..., -1.3432e-03,
          0.0000e+00,  0.0000e+00],
        [-5.2822e-05, -1.1370e-03,  1.5909e-04,  ..., -1.3432e-03,
          0.0000e+00,  0.0000e+00],
        [-5.2822e-05, -1.1370e-03,  1.5909e-04,  ..., -1.3432e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2383.9546, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.0278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0829, device='cuda:0')



h[100].sum tensor(-6.4086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2349, device='cuda:0')



h[200].sum tensor(-26.9020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0551e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0521, 0.0548, 0.0967,  ..., 0.0081, 0.0000, 0.0000],
        [0.0599, 0.0637, 0.1111,  ..., 0.0102, 0.0000, 0.0000],
        [0.0541, 0.0571, 0.1004,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65074.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2048, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2049, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1868, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0116, 0.0679, 0.0006,  ..., 0.0000, 0.0606, 0.0000],
        [0.0115, 0.0679, 0.0006,  ..., 0.0000, 0.0606, 0.0000],
        [0.0115, 0.0679, 0.0006,  ..., 0.0000, 0.0605, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623795.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3977.4658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-227.1379, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.8342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2492.1509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(62.8417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3278],
        [ 0.3304],
        [ 0.3341],
        ...,
        [-0.7286],
        [-0.7261],
        [-0.7251]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149371.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0058],
        [1.0032],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368872.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0059],
        [1.0032],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368885., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4019e-05, -1.1289e-03,  1.4052e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4019e-05, -1.1289e-03,  1.4052e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4019e-05, -1.1289e-03,  1.4052e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.4019e-05, -1.1289e-03,  1.4052e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4019e-05, -1.1289e-03,  1.4052e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [-5.4019e-05, -1.1289e-03,  1.4052e-04,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2267.4233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.9847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0516, device='cuda:0')



h[100].sum tensor(-5.6742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.4233, device='cuda:0')



h[200].sum tensor(-23.8623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.2966e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0036, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60126.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0433, 0.0137, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0280, 0.0337, 0.0000,  ..., 0.0000, 0.0318, 0.0000],
        [0.0171, 0.0549, 0.0001,  ..., 0.0000, 0.0493, 0.0000],
        ...,
        [0.0115, 0.0686, 0.0002,  ..., 0.0000, 0.0610, 0.0000],
        [0.0115, 0.0686, 0.0002,  ..., 0.0000, 0.0609, 0.0000],
        [0.0115, 0.0686, 0.0002,  ..., 0.0000, 0.0609, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596906.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3383.7661, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-191.3077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.4304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2622.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(64.0747, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.6353e-01],
        [-5.0197e-04],
        [-2.0054e-01],
        ...,
        [-7.3694e-01],
        [-7.3431e-01],
        [-7.3323e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161503.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0059],
        [1.0032],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368885., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0059],
        [1.0032],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368897.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.6380e-05, -1.1193e-03,  1.2587e-04,  ..., -1.3404e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6380e-05, -1.1193e-03,  1.2587e-04,  ..., -1.3404e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6380e-05, -1.1193e-03,  1.2587e-04,  ..., -1.3404e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.6380e-05, -1.1193e-03,  1.2587e-04,  ..., -1.3404e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6380e-05, -1.1193e-03,  1.2587e-04,  ..., -1.3404e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6380e-05, -1.1193e-03,  1.2587e-04,  ..., -1.3404e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3218.9331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.0499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0865, device='cuda:0')



h[100].sum tensor(-11.4358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.7236, device='cuda:0')



h[200].sum tensor(-48.1802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9201e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0262, 0.0277, 0.0489,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82159.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1470, 0.0047, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0523, 0.0318, 0.0000,  ..., 0.0000, 0.0281, 0.0000],
        [0.0216, 0.0486, 0.0000,  ..., 0.0000, 0.0439, 0.0000],
        ...,
        [0.0115, 0.0690, 0.0000,  ..., 0.0000, 0.0611, 0.0000],
        [0.0115, 0.0690, 0.0000,  ..., 0.0000, 0.0610, 0.0000],
        [0.0115, 0.0690, 0.0000,  ..., 0.0000, 0.0610, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(694318.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5252.5513, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-351.5824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(302.8120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2528.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(67.2064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1939],
        [ 0.1329],
        [ 0.0683],
        ...,
        [-0.7420],
        [-0.7393],
        [-0.7382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0059],
        [1.0032],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368897.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0060],
        [1.0033],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368910.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8380e-03,  5.5789e-03,  1.0951e-02,  ...,  1.8813e-04,
         -4.3270e-03, -7.7453e-03],
        [ 1.3543e-02,  1.4331e-02,  2.5108e-02,  ...,  2.1854e-03,
         -9.9836e-03, -1.7871e-02],
        [-5.5667e-05, -1.1160e-03,  1.2217e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.5667e-05, -1.1160e-03,  1.2217e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-5.5667e-05, -1.1160e-03,  1.2217e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00],
        [-5.5667e-05, -1.1160e-03,  1.2217e-04,  ..., -1.3396e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2496.9355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.4053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6617, device='cuda:0')



h[100].sum tensor(-6.8861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6429, device='cuda:0')



h[200].sum tensor(-29.0651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1526e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0643, 0.0688, 0.1190,  ..., 0.0113, 0.0000, 0.0000],
        [0.0332, 0.0346, 0.0619,  ..., 0.0046, 0.0000, 0.0000],
        [0.0345, 0.0361, 0.0643,  ..., 0.0050, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63850.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2215, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1599, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1261, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0114, 0.0692, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0114, 0.0692, 0.0000,  ..., 0.0000, 0.0608, 0.0000],
        [0.0114, 0.0691, 0.0000,  ..., 0.0000, 0.0608, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607822.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3596.9995, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-219.4428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.2556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2625.2920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(63.7012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3119],
        [ 0.2928],
        [ 0.2277],
        ...,
        [-0.7449],
        [-0.7423],
        [-0.7413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152405.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0060],
        [1.0033],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368910.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0060],
        [1.0033],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368923.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.6494e-05, -1.1219e-03,  1.3170e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6494e-05, -1.1219e-03,  1.3170e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6494e-05, -1.1219e-03,  1.3170e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.6494e-05, -1.1219e-03,  1.3170e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6494e-05, -1.1219e-03,  1.3170e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00],
        [-4.6494e-05, -1.1219e-03,  1.3170e-04,  ..., -1.3391e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2261.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.3228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2585, device='cuda:0')



h[100].sum tensor(-5.3045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7161, device='cuda:0')



h[200].sum tensor(-22.4305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8068e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0153, 0.0153, 0.0289,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59963.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4964e-02, 5.9103e-02, 5.1467e-05,  ..., 0.0000e+00, 5.1873e-02,
         0.0000e+00],
        [4.1607e-02, 3.2555e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8447e-02,
         0.0000e+00],
        [9.5680e-02, 9.5849e-03, 0.0000e+00,  ..., 0.0000e+00, 8.4170e-03,
         0.0000e+00],
        ...,
        [1.1202e-02, 6.9137e-02, 1.9136e-04,  ..., 0.0000e+00, 6.0476e-02,
         0.0000e+00],
        [1.1195e-02, 6.9100e-02, 1.8924e-04,  ..., 0.0000e+00, 6.0441e-02,
         0.0000e+00],
        [1.1190e-02, 6.9073e-02, 1.8738e-04,  ..., 0.0000e+00, 6.0415e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597773.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3346.1479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-193.6209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.6392, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2677.2729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(59.9479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0119],
        [ 0.1119],
        [ 0.2089],
        ...,
        [-0.7449],
        [-0.7424],
        [-0.7413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154670.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0060],
        [1.0033],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368923.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(526.1365, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0061],
        [1.0034],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368935.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2438.3386, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.1984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6613, device='cuda:0')



h[100].sum tensor(-6.2183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.8589, device='cuda:0')



h[200].sum tensor(-26.3428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0291e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0152, 0.0151, 0.0286,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63779.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0217, 0.0471, 0.0002,  ..., 0.0000, 0.0415, 0.0000],
        [0.0505, 0.0246, 0.0000,  ..., 0.0000, 0.0217, 0.0000],
        [0.0951, 0.0055, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        ...,
        [0.0110, 0.0690, 0.0004,  ..., 0.0000, 0.0601, 0.0000],
        [0.0110, 0.0689, 0.0004,  ..., 0.0000, 0.0600, 0.0000],
        [0.0110, 0.0689, 0.0004,  ..., 0.0000, 0.0600, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613952.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3631.5264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-225.8295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.8490, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2795.1921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.7995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0237],
        [ 0.1584],
        [ 0.2517],
        ...,
        [-0.7249],
        [-0.7300],
        [-0.7336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159508.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0061],
        [1.0034],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368935.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0061],
        [1.0034],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368935.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00],
        [-3.8729e-05, -1.1281e-03,  1.4113e-04,  ..., -1.3389e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2514.7090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.4994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8953, device='cuda:0')



h[100].sum tensor(-6.6802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.9594, device='cuda:0')



h[200].sum tensor(-28.2995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1053e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65079.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0396, 0.0183, 0.0000,  ..., 0.0000, 0.0168, 0.0000],
        [0.0437, 0.0140, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0434, 0.0138, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        ...,
        [0.0110, 0.0690, 0.0004,  ..., 0.0000, 0.0601, 0.0000],
        [0.0110, 0.0689, 0.0004,  ..., 0.0000, 0.0600, 0.0000],
        [0.0110, 0.0689, 0.0004,  ..., 0.0000, 0.0600, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617233.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3685.5325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-234.8471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.5479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2780.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.9554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1724],
        [ 0.2314],
        [ 0.2640],
        ...,
        [-0.7441],
        [-0.7416],
        [-0.7406]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159014.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0061],
        [1.0034],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368935.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0062],
        [1.0034],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368948.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7425e-03,  5.4240e-03,  1.0758e-02,  ...,  1.5780e-04,
         -4.2205e-03, -7.5679e-03],
        [-3.0121e-05, -1.1352e-03,  1.5043e-04,  ..., -1.3388e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.7425e-03,  5.4240e-03,  1.0758e-02,  ...,  1.5780e-04,
         -4.2205e-03, -7.5679e-03],
        ...,
        [-3.0121e-05, -1.1352e-03,  1.5043e-04,  ..., -1.3388e-03,
          0.0000e+00,  0.0000e+00],
        [-3.0121e-05, -1.1352e-03,  1.5043e-04,  ..., -1.3388e-03,
          0.0000e+00,  0.0000e+00],
        [-3.0121e-05, -1.1352e-03,  1.5043e-04,  ..., -1.3388e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2503.2336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.2402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4171, device='cuda:0')



h[100].sum tensor(-6.4679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5329, device='cuda:0')



h[200].sum tensor(-27.4505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0758e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0090, 0.0170,  ..., 0.0010, 0.0000, 0.0000],
        [0.0294, 0.0290, 0.0549,  ..., 0.0023, 0.0000, 0.0000],
        [0.0089, 0.0090, 0.0170,  ..., 0.0010, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64580.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0511, 0.0099, 0.0000,  ..., 0.0000, 0.0089, 0.0000],
        [0.0734, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0524, 0.0073, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        ...,
        [0.0108, 0.0689, 0.0007,  ..., 0.0000, 0.0597, 0.0000],
        [0.0108, 0.0689, 0.0007,  ..., 0.0000, 0.0597, 0.0000],
        [0.0108, 0.0689, 0.0007,  ..., 0.0000, 0.0597, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612288.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3608.9653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-229.4886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(151.0427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2649.6846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.8650, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1103],
        [ 0.2114],
        [ 0.1999],
        ...,
        [-0.7422],
        [-0.7396],
        [-0.7372]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136706., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0062],
        [1.0034],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368948.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0062],
        [1.0035],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368961.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2832e-02,  1.3470e-02,  2.3771e-02,  ...,  1.9951e-03,
         -9.3875e-03, -1.6843e-02],
        [ 5.8833e-03,  5.5733e-03,  1.1002e-02,  ...,  1.9335e-04,
         -4.3138e-03, -7.7397e-03],
        [-2.5199e-05, -1.1408e-03,  1.4517e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-2.5199e-05, -1.1408e-03,  1.4517e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5199e-05, -1.1408e-03,  1.4517e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5199e-05, -1.1408e-03,  1.4517e-04,  ..., -1.3386e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2334.9629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.4906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6176, device='cuda:0')



h[100].sum tensor(-5.4211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0363, device='cuda:0')



h[200].sum tensor(-23.0501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.0285e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0165, 0.0165, 0.0309,  ..., 0.0016, 0.0000, 0.0000],
        [0.0178, 0.0180, 0.0333,  ..., 0.0020, 0.0000, 0.0000],
        [0.0291, 0.0286, 0.0543,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59818.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0966, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0880, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0951, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0107, 0.0695, 0.0006,  ..., 0.0000, 0.0599, 0.0000],
        [0.0107, 0.0695, 0.0006,  ..., 0.0000, 0.0599, 0.0000],
        [0.0107, 0.0695, 0.0006,  ..., 0.0000, 0.0598, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592605.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3165.2544, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-197.2225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.6347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2860.1299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.3606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3155],
        [ 0.3174],
        [ 0.3180],
        ...,
        [-0.7507],
        [-0.7482],
        [-0.7472]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149636.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0062],
        [1.0035],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368961.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0063],
        [1.0035],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368973.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.3104e-03,  8.3211e-03,  1.5444e-02,  ...,  8.2136e-04,
         -6.0732e-03, -1.0903e-02],
        [ 1.1233e-02,  1.1642e-02,  2.0814e-02,  ...,  1.5791e-03,
         -8.2040e-03, -1.4728e-02],
        [ 1.1038e-02,  1.1421e-02,  2.0456e-02,  ...,  1.5287e-03,
         -8.0621e-03, -1.4473e-02],
        ...,
        [-1.9551e-05, -1.1451e-03,  1.3738e-04,  ..., -1.3384e-03,
          0.0000e+00,  0.0000e+00],
        [-1.9551e-05, -1.1451e-03,  1.3738e-04,  ..., -1.3384e-03,
          0.0000e+00,  0.0000e+00],
        [-1.9551e-05, -1.1451e-03,  1.3738e-04,  ..., -1.3384e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3080.4497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.3093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7462, device='cuda:0')



h[100].sum tensor(-9.8561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.8528, device='cuda:0')



h[200].sum tensor(-41.9845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6520e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0184, 0.0340,  ..., 0.0020, 0.0000, 0.0000],
        [0.0287, 0.0293, 0.0535,  ..., 0.0034, 0.0000, 0.0000],
        [0.0627, 0.0667, 0.1159,  ..., 0.0109, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79081.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1055, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1491, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2188, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0106, 0.0703, 0.0005,  ..., 0.0000, 0.0602, 0.0000],
        [0.0106, 0.0702, 0.0005,  ..., 0.0000, 0.0602, 0.0000],
        [0.0106, 0.0702, 0.0005,  ..., 0.0000, 0.0602, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688697.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4914.1372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-340.5892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(287.5296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2989.1233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(55.7773, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3045],
        [ 0.3110],
        [ 0.3067],
        ...,
        [-0.7587],
        [-0.7562],
        [-0.7552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159537.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0063],
        [1.0035],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368973.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0064],
        [1.0036],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368986.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6867e-05, -1.1470e-03,  1.2699e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6867e-05, -1.1470e-03,  1.2699e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8103e-03,  1.0021e-02,  1.8184e-02,  ...,  1.2102e-03,
         -7.1548e-03, -1.2852e-02],
        ...,
        [-1.6867e-05, -1.1470e-03,  1.2699e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6867e-05, -1.1470e-03,  1.2699e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6867e-05, -1.1470e-03,  1.2699e-04,  ..., -1.3379e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2360.9656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.1791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1643, device='cuda:0')



h[100].sum tensor(-5.5359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5238, device='cuda:0')



h[200].sum tensor(-23.6250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3662e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0099, 0.0101, 0.0187,  ..., 0.0012, 0.0000, 0.0000],
        [0.0081, 0.0080, 0.0153,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60325.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0213, 0.0490, 0.0001,  ..., 0.0000, 0.0420, 0.0000],
        [0.0453, 0.0275, 0.0000,  ..., 0.0000, 0.0239, 0.0000],
        [0.0748, 0.0126, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        ...,
        [0.0106, 0.0709, 0.0003,  ..., 0.0000, 0.0605, 0.0000],
        [0.0106, 0.0709, 0.0003,  ..., 0.0000, 0.0605, 0.0000],
        [0.0106, 0.0709, 0.0003,  ..., 0.0000, 0.0604, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601062.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3221.9917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-205.2820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.3406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3247.2397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(53.1396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2956],
        [-0.2249],
        [-0.0747],
        ...,
        [-0.7653],
        [-0.7627],
        [-0.7589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168799.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0064],
        [1.0036],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368986.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0065],
        [1.0036],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368998.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4801e-02,  1.5691e-02,  2.7340e-02,  ...,  2.5050e-03,
         -1.0772e-02, -1.9361e-02],
        [ 3.4255e-02,  3.7802e-02,  6.3085e-02,  ...,  7.5496e-03,
         -2.4916e-02, -4.4781e-02],
        [ 1.3983e-02,  1.4761e-02,  2.5837e-02,  ...,  2.2929e-03,
         -1.0177e-02, -1.8292e-02],
        ...,
        [-1.4920e-05, -1.1488e-03,  1.1647e-04,  ..., -1.3371e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4920e-05, -1.1488e-03,  1.1647e-04,  ..., -1.3371e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4920e-05, -1.1488e-03,  1.1647e-04,  ..., -1.3371e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2594.0220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.1336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6680, device='cuda:0')



h[100].sum tensor(-6.8848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6485, device='cuda:0')



h[200].sum tensor(-29.4359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1530e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0726, 0.0780, 0.1340,  ..., 0.0135, 0.0000, 0.0000],
        [0.0572, 0.0605, 0.1057,  ..., 0.0095, 0.0000, 0.0000],
        [0.0581, 0.0626, 0.1073,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65735.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2743e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.2263e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.0047e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0585e-02, 7.1483e-02, 1.7965e-04,  ..., 0.0000e+00, 6.0710e-02,
         0.0000e+00],
        [1.0578e-02, 7.1443e-02, 1.7747e-04,  ..., 0.0000e+00, 6.0674e-02,
         0.0000e+00],
        [1.0574e-02, 7.1418e-02, 1.7567e-04,  ..., 0.0000e+00, 6.0651e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621382.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3635.7588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.6575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(165.0296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3185.1252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(51.2410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2778],
        [ 0.2588],
        [ 0.1961],
        ...,
        [-0.7738],
        [-0.7713],
        [-0.7703]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157090.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0065],
        [1.0036],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368998.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0066],
        [1.0036],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369011.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0782e-02,  1.1129e-02,  1.9943e-02,  ...,  1.4648e-03,
         -7.8416e-03, -1.4102e-02],
        [ 6.2143e-03,  5.9377e-03,  1.1551e-02,  ...,  2.8030e-04,
         -4.5254e-03, -8.1383e-03],
        [-1.8706e-05, -1.1472e-03,  9.9325e-05,  ..., -1.3362e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.8706e-05, -1.1472e-03,  9.9325e-05,  ..., -1.3362e-03,
          0.0000e+00,  0.0000e+00],
        [-1.8706e-05, -1.1472e-03,  9.9325e-05,  ..., -1.3362e-03,
          0.0000e+00,  0.0000e+00],
        [-1.8706e-05, -1.1472e-03,  9.9325e-05,  ..., -1.3362e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2432.0806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.2510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1878, device='cuda:0')



h[100].sum tensor(-5.9043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.4366, device='cuda:0')



h[200].sum tensor(-25.2907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9984e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0362, 0.0366, 0.0670,  ..., 0.0040, 0.0000, 0.0000],
        [0.0198, 0.0202, 0.0368,  ..., 0.0024, 0.0000, 0.0000],
        [0.0063, 0.0060, 0.0119,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62048.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1069, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0790, 0.0054, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0445, 0.0237, 0.0000,  ..., 0.0000, 0.0200, 0.0000],
        ...,
        [0.0107, 0.0721, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0107, 0.0721, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0107, 0.0721, 0.0000,  ..., 0.0000, 0.0609, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609916.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3413.3643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-218.5739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.5197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3391.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.5450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2773],
        [ 0.2253],
        [ 0.1243],
        ...,
        [-0.7825],
        [-0.7799],
        [-0.7789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167736.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0066],
        [1.0036],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369011.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0066],
        [1.0036],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369023.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6659e-05, -1.1489e-03,  9.0788e-05,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6659e-05, -1.1489e-03,  9.0788e-05,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6659e-05, -1.1489e-03,  9.0788e-05,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.6659e-05, -1.1489e-03,  9.0788e-05,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6659e-05, -1.1489e-03,  9.0788e-05,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6659e-05, -1.1489e-03,  9.0788e-05,  ..., -1.3351e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2362.7993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.0072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7960, device='cuda:0')



h[100].sum tensor(-5.4311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1954, device='cuda:0')



h[200].sum tensor(-23.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.1387e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59674.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0104, 0.0702, 0.0000,  ..., 0.0000, 0.0589, 0.0000],
        [0.0104, 0.0703, 0.0000,  ..., 0.0000, 0.0590, 0.0000],
        [0.0104, 0.0702, 0.0000,  ..., 0.0000, 0.0589, 0.0000],
        ...,
        [0.0108, 0.0725, 0.0000,  ..., 0.0000, 0.0610, 0.0000],
        [0.0108, 0.0724, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0108, 0.0724, 0.0000,  ..., 0.0000, 0.0609, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598236.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3184.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-201.2236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.7783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3473.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(51.1421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4124],
        [-0.5972],
        [-0.7279],
        ...,
        [-0.7880],
        [-0.7854],
        [-0.7844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168609.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0066],
        [1.0036],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369023.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0066],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369036.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.4509e-06, -1.1536e-03,  9.5876e-05,  ..., -1.3341e-03,
          0.0000e+00,  0.0000e+00],
        [-7.4509e-06, -1.1536e-03,  9.5876e-05,  ..., -1.3341e-03,
          0.0000e+00,  0.0000e+00],
        [-7.4509e-06, -1.1536e-03,  9.5876e-05,  ..., -1.3341e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.4509e-06, -1.1536e-03,  9.5876e-05,  ..., -1.3341e-03,
          0.0000e+00,  0.0000e+00],
        [-7.4509e-06, -1.1536e-03,  9.5876e-05,  ..., -1.3341e-03,
          0.0000e+00,  0.0000e+00],
        [-7.4509e-06, -1.1536e-03,  9.5876e-05,  ..., -1.3341e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2575.2751, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.5187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1374, device='cuda:0')



h[100].sum tensor(-6.5296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.1753, device='cuda:0')



h[200].sum tensor(-28.0731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1203e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65743.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0390e-02, 7.0083e-02, 0.0000e+00,  ..., 0.0000e+00, 5.8728e-02,
         0.0000e+00],
        [1.0718e-02, 6.9473e-02, 3.4877e-05,  ..., 0.0000e+00, 5.8060e-02,
         0.0000e+00],
        [1.1473e-02, 6.7467e-02, 3.4630e-04,  ..., 0.0000e+00, 5.6008e-02,
         0.0000e+00],
        ...,
        [1.2853e-02, 6.8051e-02, 0.0000e+00,  ..., 0.0000e+00, 5.7078e-02,
         0.0000e+00],
        [1.6974e-02, 5.9378e-02, 0.0000e+00,  ..., 0.0000e+00, 4.9674e-02,
         0.0000e+00],
        [1.9031e-02, 5.5042e-02, 0.0000e+00,  ..., 0.0000e+00, 4.5972e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627033., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3793.9753, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-242.2901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(168.1949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3254.1597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.8975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8023],
        [-0.7142],
        [-0.5800],
        ...,
        [-0.6685],
        [-0.5747],
        [-0.5136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147863.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0066],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369036.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(469.2698, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0067],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369049.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9182e-06, -1.1611e-03,  1.0873e-04,  ..., -1.3335e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9182e-06, -1.1611e-03,  1.0873e-04,  ..., -1.3335e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.7322e-03,  7.6201e-03,  1.4303e-02,  ...,  6.7023e-04,
         -5.5847e-03, -1.0061e-02],
        ...,
        [ 6.9182e-06, -1.1611e-03,  1.0873e-04,  ..., -1.3335e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9182e-06, -1.1611e-03,  1.0873e-04,  ..., -1.3335e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.9182e-06, -1.1611e-03,  1.0873e-04,  ..., -1.3335e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2250.9429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.0849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.2349, device='cuda:0')



h[100].sum tensor(-4.4579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9113, device='cuda:0')



h[200].sum tensor(-19.2017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.5568e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.7861e-05, 0.0000e+00, 4.3787e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.8088e-03, 7.6750e-03, 1.4735e-02,  ..., 6.7506e-04, 0.0000e+00,
         0.0000e+00],
        [2.5914e-02, 2.7089e-02, 4.8000e-02,  ..., 4.0315e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.8682e-05, 0.0000e+00, 4.5077e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.8667e-05, 0.0000e+00, 4.5053e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.8659e-05, 0.0000e+00, 4.5040e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57784.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.1763e-02, 3.0347e-02, 0.0000e+00,  ..., 0.0000e+00, 2.4687e-02,
         0.0000e+00],
        [5.9644e-02, 2.1339e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7517e-02,
         0.0000e+00],
        [1.2886e-01, 2.0082e-03, 0.0000e+00,  ..., 0.0000e+00, 1.5270e-03,
         0.0000e+00],
        ...,
        [1.0744e-02, 7.2034e-02, 1.2484e-04,  ..., 0.0000e+00, 6.0486e-02,
         0.0000e+00],
        [1.0737e-02, 7.1993e-02, 1.2261e-04,  ..., 0.0000e+00, 6.0449e-02,
         0.0000e+00],
        [1.0733e-02, 7.1969e-02, 1.2078e-04,  ..., 0.0000e+00, 6.0427e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595508.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3121.2307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-185.4787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.6037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3440.3501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.8513, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1774],
        [ 0.2060],
        [ 0.2501],
        ...,
        [-0.7865],
        [-0.7839],
        [-0.7828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166651.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0067],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369049.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0067],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369062.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3414e-02,  2.5438e-02,  4.3113e-02,  ...,  4.7371e-03,
         -1.6895e-02, -3.0454e-02],
        [ 2.1414e-02,  2.3164e-02,  3.9438e-02,  ...,  4.2183e-03,
         -1.5451e-02, -2.7851e-02],
        [ 2.0080e-02,  2.1648e-02,  3.6987e-02,  ...,  3.8724e-03,
         -1.4488e-02, -2.6116e-02],
        ...,
        [ 9.8861e-06, -1.1642e-03,  1.0989e-04,  ..., -1.3332e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8861e-06, -1.1642e-03,  1.0989e-04,  ..., -1.3332e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8861e-06, -1.1642e-03,  1.0989e-04,  ..., -1.3332e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2716.3259, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.6635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4341, device='cuda:0')



h[100].sum tensor(-7.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.3317, device='cuda:0')



h[200].sum tensor(-30.3523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2003e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.9191e-02, 5.2351e-02, 9.0754e-02,  ..., 8.7213e-03, 0.0000e+00,
         0.0000e+00],
        [6.9679e-02, 7.4456e-02, 1.2840e-01,  ..., 1.2682e-02, 0.0000e+00,
         0.0000e+00],
        [7.5217e-02, 8.0764e-02, 1.3857e-01,  ..., 1.4133e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.0992e-05, 0.0000e+00, 4.5564e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.0970e-05, 0.0000e+00, 4.5539e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.0959e-05, 0.0000e+00, 4.5527e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67669.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4640e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7478e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.8180e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0804e-02, 7.2080e-02, 1.2843e-04,  ..., 0.0000e+00, 6.0463e-02,
         0.0000e+00],
        [1.0797e-02, 7.2039e-02, 1.2617e-04,  ..., 0.0000e+00, 6.0427e-02,
         0.0000e+00],
        [1.0792e-02, 7.2015e-02, 1.2432e-04,  ..., 0.0000e+00, 6.0405e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636985.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3977.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-255.6300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(189.2881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3263.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.9221, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3020],
        [ 0.3045],
        [ 0.3093],
        ...,
        [-0.7884],
        [-0.7858],
        [-0.7847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151648.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0067],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369062.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0068],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369074.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1751e-02,  1.2180e-02,  2.1683e-02,  ...,  1.7131e-03,
         -8.4648e-03, -1.5267e-02],
        [ 5.1266e-03,  4.6512e-03,  9.5120e-03,  ..., -4.9134e-06,
         -3.6902e-03, -6.6557e-03],
        [ 4.9734e-03,  4.4772e-03,  9.2306e-03,  ..., -4.4635e-05,
         -3.5798e-03, -6.4566e-03],
        ...,
        [ 7.1236e-06, -1.1675e-03,  1.0528e-04,  ..., -1.3327e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.1236e-06, -1.1675e-03,  1.0528e-04,  ..., -1.3327e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.1236e-06, -1.1675e-03,  1.0528e-04,  ..., -1.3327e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2485.7788, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.5077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5369, device='cuda:0')



h[100].sum tensor(-5.6133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.8561, device='cuda:0')



h[200].sum tensor(-24.2685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.5963e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.3202e-02, 2.1637e-02, 4.3004e-02,  ..., 7.0419e-04, 0.0000e+00,
         0.0000e+00],
        [3.1996e-02, 3.1622e-02, 5.9163e-02,  ..., 2.9580e-03, 0.0000e+00,
         0.0000e+00],
        [2.6232e-02, 2.5084e-02, 4.8571e-02,  ..., 1.4423e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.9542e-05, 0.0000e+00, 4.3660e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.9526e-05, 0.0000e+00, 4.3637e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.9518e-05, 0.0000e+00, 4.3625e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62275.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1264e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.1952e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0584e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0928e-02, 7.2347e-02, 5.0032e-06,  ..., 0.0000e+00, 6.0635e-02,
         0.0000e+00],
        [1.0921e-02, 7.2305e-02, 2.7957e-06,  ..., 0.0000e+00, 6.0598e-02,
         0.0000e+00],
        [1.0916e-02, 7.2282e-02, 1.0421e-06,  ..., 0.0000e+00, 6.0577e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612466., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3514.1003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-215.5647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.0158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3357.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.8530, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3391],
        [ 0.3380],
        [ 0.3370],
        ...,
        [-0.7928],
        [-0.7902],
        [-0.7891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158030.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0068],
        [1.0037],
        ...,
        [1.0014],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369074.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0068],
        [1.0038],
        ...,
        [1.0014],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369086.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6605e-02,  1.7708e-02,  3.0611e-02,  ...,  2.9751e-03,
         -1.1954e-02, -2.1573e-02],
        [ 7.4892e-03,  7.3468e-03,  1.3861e-02,  ...,  6.1072e-04,
         -5.3925e-03, -9.7317e-03],
        [ 8.2622e-03,  8.2254e-03,  1.5282e-02,  ...,  8.1122e-04,
         -5.9488e-03, -1.0736e-02],
        ...,
        [-2.7373e-06, -1.1684e-03,  9.5433e-05,  ..., -1.3325e-03,
          0.0000e+00,  0.0000e+00],
        [-2.7373e-06, -1.1684e-03,  9.5433e-05,  ..., -1.3325e-03,
          0.0000e+00,  0.0000e+00],
        [-2.7373e-06, -1.1684e-03,  9.5433e-05,  ..., -1.3325e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2549.6990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.1126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4278, device='cuda:0')



h[100].sum tensor(-5.9222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.6507, device='cuda:0')



h[200].sum tensor(-25.6515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0147e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0410, 0.0420, 0.0758,  ..., 0.0053, 0.0000, 0.0000],
        [0.0449, 0.0463, 0.0829,  ..., 0.0063, 0.0000, 0.0000],
        [0.0264, 0.0265, 0.0489,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63318.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1271, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1286, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1066, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0111, 0.0728, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0111, 0.0728, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0111, 0.0728, 0.0000,  ..., 0.0000, 0.0609, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615994., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3598.0522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-221.2487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.7857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3330.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.7064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3231],
        [ 0.2897],
        [ 0.2284],
        ...,
        [-0.7992],
        [-0.7965],
        [-0.7954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149168.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0068],
        [1.0038],
        ...,
        [1.0014],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369086.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0069],
        [1.0038],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369099.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7330e-03,  8.7621e-03,  1.6151e-02,  ...,  9.3464e-04,
         -6.2826e-03, -1.1345e-02],
        [ 5.6555e-03,  5.2644e-03,  1.0496e-02,  ...,  1.3645e-04,
         -4.0708e-03, -7.3508e-03],
        [ 4.3860e-03,  3.8215e-03,  8.1636e-03,  ..., -1.9284e-04,
         -3.1583e-03, -5.7031e-03],
        ...,
        [-8.3494e-06, -1.1729e-03,  8.9188e-05,  ..., -1.3326e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3494e-06, -1.1729e-03,  8.9188e-05,  ..., -1.3326e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3494e-06, -1.1729e-03,  8.9188e-05,  ..., -1.3326e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2917.7437, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4982, device='cuda:0')



h[100].sum tensor(-7.9918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.0644, device='cuda:0')



h[200].sum tensor(-34.6807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3896e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0300, 0.0306, 0.0556,  ..., 0.0038, 0.0000, 0.0000],
        [0.0256, 0.0256, 0.0475,  ..., 0.0028, 0.0000, 0.0000],
        [0.0450, 0.0465, 0.0831,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71687.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1910, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1529, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1653, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0110, 0.0734, 0.0000,  ..., 0.0000, 0.0615, 0.0000],
        [0.0110, 0.0734, 0.0000,  ..., 0.0000, 0.0615, 0.0000],
        [0.0110, 0.0734, 0.0000,  ..., 0.0000, 0.0614, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655837.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4282.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-284.8302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(228.3194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3499.1724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.5855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2859],
        [ 0.3066],
        [ 0.3203],
        ...,
        [-0.8068],
        [-0.8041],
        [-0.8030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167465.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0069],
        [1.0038],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369099.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0070],
        [1.0038],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369111.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2715e-05, -1.1780e-03,  8.5713e-05,  ..., -1.3329e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2715e-05, -1.1780e-03,  8.5713e-05,  ..., -1.3329e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2715e-05, -1.1780e-03,  8.5713e-05,  ..., -1.3329e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.2715e-05, -1.1780e-03,  8.5713e-05,  ..., -1.3329e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2715e-05, -1.1780e-03,  8.5713e-05,  ..., -1.3329e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2715e-05, -1.1780e-03,  8.5713e-05,  ..., -1.3329e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2364.6516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3377, device='cuda:0')



h[100].sum tensor(-4.7967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8948, device='cuda:0')



h[200].sum tensor(-20.8541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.2380e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0329, 0.0574,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59135.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1200, 0.0027, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0487, 0.0282, 0.0000,  ..., 0.0000, 0.0235, 0.0000],
        [0.0252, 0.0451, 0.0000,  ..., 0.0000, 0.0375, 0.0000],
        ...,
        [0.0109, 0.0741, 0.0000,  ..., 0.0000, 0.0620, 0.0000],
        [0.0108, 0.0741, 0.0000,  ..., 0.0000, 0.0620, 0.0000],
        [0.0108, 0.0740, 0.0000,  ..., 0.0000, 0.0619, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602344.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3203.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-191.3173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.1489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3611.8604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(37.8062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2522],
        [ 0.1059],
        [-0.0951],
        ...,
        [-0.8130],
        [-0.8098],
        [-0.8079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174072.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0070],
        [1.0038],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369111.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0070],
        [1.0039],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369124.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2903.5830, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7602, device='cuda:0')



h[100].sum tensor(-7.7733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.4062, device='cuda:0')



h[200].sum tensor(-33.8589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3440e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0156, 0.0154, 0.0291,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72571.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0214, 0.0520, 0.0000,  ..., 0.0000, 0.0436, 0.0000],
        [0.0322, 0.0373, 0.0000,  ..., 0.0000, 0.0313, 0.0000],
        [0.0734, 0.0129, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        ...,
        [0.0107, 0.0744, 0.0000,  ..., 0.0000, 0.0624, 0.0000],
        [0.0107, 0.0744, 0.0000,  ..., 0.0000, 0.0623, 0.0000],
        [0.0107, 0.0744, 0.0000,  ..., 0.0000, 0.0623, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662606.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4468.6475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-285.6505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(235.5446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3220.5186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(35.8434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2792],
        [-0.1725],
        [ 0.0260],
        ...,
        [-0.8191],
        [-0.8164],
        [-0.8153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133947.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0070],
        [1.0039],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369124.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0070],
        [1.0039],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369124.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9806e-03,  4.4978e-03,  9.2688e-03,  ..., -3.6741e-05,
         -3.5816e-03, -6.4751e-03],
        [ 5.2146e-03,  4.7639e-03,  9.6989e-03,  ...,  2.3964e-05,
         -3.7493e-03, -6.7784e-03],
        [ 1.0212e-02,  1.0444e-02,  1.8882e-02,  ...,  1.3202e-03,
         -7.3309e-03, -1.3254e-02],
        ...,
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6950e-05, -1.1824e-03,  8.5697e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3047.9302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0122, device='cuda:0')



h[100].sum tensor(-8.5911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.4146, device='cuda:0')



h[200].sum tensor(-37.4211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4831e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0171, 0.0318,  ..., 0.0017, 0.0000, 0.0000],
        [0.0347, 0.0347, 0.0642,  ..., 0.0037, 0.0000, 0.0000],
        [0.0279, 0.0270, 0.0518,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76515.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1104, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1145, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1191, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0107, 0.0744, 0.0000,  ..., 0.0000, 0.0624, 0.0000],
        [0.0107, 0.0744, 0.0000,  ..., 0.0000, 0.0623, 0.0000],
        [0.0107, 0.0744, 0.0000,  ..., 0.0000, 0.0623, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690956.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4925.3804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-316.9431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(272.9072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3385.3491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.4704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3498],
        [ 0.3537],
        [ 0.3536],
        ...,
        [-0.8180],
        [-0.8149],
        [-0.8134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155841.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0070],
        [1.0039],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369124.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0071],
        [1.0039],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369136.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.5381e-05, -1.1812e-03,  8.1992e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.7941e-03,  9.9810e-03,  1.8126e-02,  ...,  1.2140e-03,
         -7.0275e-03, -1.2713e-02],
        [ 2.1189e-02,  2.2933e-02,  3.9063e-02,  ...,  4.1697e-03,
         -1.5182e-02, -2.7464e-02],
        ...,
        [-2.5381e-05, -1.1812e-03,  8.1992e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5381e-05, -1.1812e-03,  8.1992e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5381e-05, -1.1812e-03,  8.1992e-05,  ..., -1.3330e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2692.6755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.0935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1853, device='cuda:0')



h[100].sum tensor(-6.4570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.2180, device='cuda:0')



h[200].sum tensor(-28.1782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1232e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0249, 0.0260, 0.0462,  ..., 0.0038, 0.0000, 0.0000],
        [0.0434, 0.0458, 0.0802,  ..., 0.0072, 0.0000, 0.0000],
        [0.0342, 0.0354, 0.0634,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66025.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1334, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1693, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1568, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0108, 0.0746, 0.0000,  ..., 0.0000, 0.0624, 0.0000],
        [0.0107, 0.0746, 0.0000,  ..., 0.0000, 0.0624, 0.0000],
        [0.0107, 0.0745, 0.0000,  ..., 0.0000, 0.0624, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631130.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3806.3882, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-238.8454, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(178.3379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3431.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.7654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3377],
        [ 0.3367],
        [ 0.3302],
        ...,
        [-0.8221],
        [-0.8195],
        [-0.8183]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150587., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0071],
        [1.0039],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369136.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0072],
        [1.0040],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369148.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1287e-05, -1.1816e-03,  7.9074e-05,  ..., -1.3331e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5980e-03,  9.7653e-03,  1.7773e-02,  ...,  1.1648e-03,
         -6.8818e-03, -1.2456e-02],
        [-3.1287e-05, -1.1816e-03,  7.9074e-05,  ..., -1.3331e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.1287e-05, -1.1816e-03,  7.9074e-05,  ..., -1.3331e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1287e-05, -1.1816e-03,  7.9074e-05,  ..., -1.3331e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1287e-05, -1.1816e-03,  7.9074e-05,  ..., -1.3331e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3037.8074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4380, device='cuda:0')



h[100].sum tensor(-8.2911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.9025, device='cuda:0')



h[200].sum tensor(-36.2500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4476e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0351, 0.0353, 0.0651,  ..., 0.0038, 0.0000, 0.0000],
        [0.0079, 0.0078, 0.0149,  ..., 0.0007, 0.0000, 0.0000],
        [0.0097, 0.0098, 0.0181,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73206.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0871, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0569, 0.0024, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0720, 0.0003, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        ...,
        [0.0108, 0.0748, 0.0000,  ..., 0.0000, 0.0626, 0.0000],
        [0.0107, 0.0748, 0.0000,  ..., 0.0000, 0.0625, 0.0000],
        [0.0107, 0.0747, 0.0000,  ..., 0.0000, 0.0625, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664240.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4394.7568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-291.9580, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(247.7418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3473.6167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.4117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3091],
        [ 0.3165],
        [ 0.3215],
        ...,
        [-0.8260],
        [-0.8232],
        [-0.8221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156806.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0072],
        [1.0040],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369148.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(467.9979, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0073],
        [1.0040],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369161.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.3049e-05, -1.1831e-03,  8.2252e-05,  ..., -1.3337e-03,
          0.0000e+00,  0.0000e+00],
        [-3.3049e-05, -1.1831e-03,  8.2252e-05,  ..., -1.3337e-03,
          0.0000e+00,  0.0000e+00],
        [-3.3049e-05, -1.1831e-03,  8.2252e-05,  ..., -1.3337e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.3049e-05, -1.1831e-03,  8.2252e-05,  ..., -1.3337e-03,
          0.0000e+00,  0.0000e+00],
        [-3.3049e-05, -1.1831e-03,  8.2252e-05,  ..., -1.3337e-03,
          0.0000e+00,  0.0000e+00],
        [-3.3049e-05, -1.1831e-03,  8.2252e-05,  ..., -1.3337e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2802.5713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.0680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3416, device='cuda:0')



h[100].sum tensor(-6.8422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.2492, device='cuda:0')



h[200].sum tensor(-29.9713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1946e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68697.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0102, 0.0724, 0.0000,  ..., 0.0000, 0.0605, 0.0000],
        [0.0103, 0.0726, 0.0000,  ..., 0.0000, 0.0606, 0.0000],
        [0.0102, 0.0724, 0.0000,  ..., 0.0000, 0.0605, 0.0000],
        ...,
        [0.0106, 0.0749, 0.0000,  ..., 0.0000, 0.0626, 0.0000],
        [0.0116, 0.0729, 0.0000,  ..., 0.0000, 0.0610, 0.0000],
        [0.0147, 0.0665, 0.0000,  ..., 0.0000, 0.0557, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646441.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4025.0059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-259.1303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(209.2090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3461.2529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.4207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7806],
        [-0.8607],
        [-0.9214],
        ...,
        [-0.8150],
        [-0.7807],
        [-0.7145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153360.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0073],
        [1.0040],
        ...,
        [1.0015],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369161.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0073],
        [1.0040],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369173.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7306e-02,  1.8525e-02,  3.1945e-02,  ...,  3.1632e-03,
         -1.2355e-02, -2.2390e-02],
        [ 1.0342e-02,  1.0608e-02,  1.9149e-02,  ...,  1.3567e-03,
         -7.3927e-03, -1.3397e-02],
        [ 5.0639e-03,  4.6071e-03,  9.4500e-03,  ..., -1.2453e-05,
         -3.6313e-03, -6.5806e-03],
        ...,
        [-3.1646e-05, -1.1864e-03,  8.6530e-05,  ..., -1.3343e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1646e-05, -1.1864e-03,  8.6530e-05,  ..., -1.3343e-03,
          0.0000e+00,  0.0000e+00],
        [-3.1646e-05, -1.1864e-03,  8.6530e-05,  ..., -1.3343e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2504.1943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.9513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3462, device='cuda:0')



h[100].sum tensor(-5.0788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7943, device='cuda:0')



h[200].sum tensor(-22.2890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8609e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0502, 0.0524, 0.0928,  ..., 0.0077, 0.0000, 0.0000],
        [0.0489, 0.0509, 0.0904,  ..., 0.0073, 0.0000, 0.0000],
        [0.0429, 0.0441, 0.0794,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60746.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2379, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2152, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1955, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0105, 0.0751, 0.0000,  ..., 0.0000, 0.0629, 0.0000],
        [0.0105, 0.0750, 0.0000,  ..., 0.0000, 0.0628, 0.0000],
        [0.0105, 0.0750, 0.0000,  ..., 0.0000, 0.0628, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610268., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3260.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-200.1387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.3304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3527.4836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.0897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3051],
        [ 0.3156],
        [ 0.3210],
        ...,
        [-0.8325],
        [-0.8299],
        [-0.8287]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159705.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0073],
        [1.0040],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369173.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0074],
        [1.0041],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369186.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3959e-02,  1.4707e-02,  2.5792e-02,  ...,  2.2928e-03,
         -9.9518e-03, -1.8045e-02],
        [ 1.4673e-02,  1.5519e-02,  2.7105e-02,  ...,  2.4781e-03,
         -1.0460e-02, -1.8967e-02],
        [ 1.9976e-02,  2.1548e-02,  3.6849e-02,  ...,  3.8536e-03,
         -1.4233e-02, -2.5809e-02],
        ...,
        [-2.5830e-05, -1.1934e-03,  9.2804e-05,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5830e-05, -1.1934e-03,  9.2804e-05,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5830e-05, -1.1934e-03,  9.2804e-05,  ..., -1.3350e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3022.6768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2685, device='cuda:0')



h[100].sum tensor(-7.8913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.8595, device='cuda:0')



h[200].sum tensor(-34.6971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3754e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0776, 0.0835, 0.1431,  ..., 0.0148, 0.0000, 0.0000],
        [0.0579, 0.0623, 0.1069,  ..., 0.0110, 0.0000, 0.0000],
        [0.0312, 0.0331, 0.0578,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0061, 0.0123,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73216.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.3291e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.5362e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.6880e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.1623e-02, 7.2956e-02, 1.6487e-05,  ..., 0.0000e+00, 6.1362e-02,
         0.0000e+00],
        [1.8786e-02, 5.9595e-02, 7.7803e-06,  ..., 0.0000e+00, 5.0401e-02,
         0.0000e+00],
        [4.3510e-02, 2.9267e-02, 0.0000e+00,  ..., 0.0000e+00, 2.4784e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665279.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4346.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-287.8841, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.1069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3188.1633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.4198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2891],
        [ 0.3097],
        [ 0.3249],
        ...,
        [-0.7045],
        [-0.5022],
        [-0.2160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133281.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0074],
        [1.0041],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369186.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0074],
        [1.0041],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369198.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4740e-02,  1.5591e-02,  2.7217e-02,  ...,  2.4947e-03,
         -1.0492e-02, -1.9036e-02],
        [ 8.5480e-03,  8.5513e-03,  1.5838e-02,  ...,  8.8846e-04,
         -6.0922e-03, -1.1053e-02],
        [ 5.0729e-03,  4.6003e-03,  9.4518e-03,  ..., -1.2986e-05,
         -3.6228e-03, -6.5730e-03],
        ...,
        [-2.5636e-05, -1.1963e-03,  8.2714e-05,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5636e-05, -1.1963e-03,  8.2714e-05,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00],
        [-2.5636e-05, -1.1963e-03,  8.2714e-05,  ..., -1.3355e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2633.8101, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.2355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3156, device='cuda:0')



h[100].sum tensor(-5.7513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5506, device='cuda:0')



h[200].sum tensor(-25.3356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0077e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0434, 0.0447, 0.0803,  ..., 0.0059, 0.0000, 0.0000],
        [0.0354, 0.0356, 0.0656,  ..., 0.0041, 0.0000, 0.0000],
        [0.0204, 0.0185, 0.0380,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63826.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1304, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1164, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0934, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0103, 0.0763, 0.0000,  ..., 0.0000, 0.0642, 0.0000],
        [0.0103, 0.0763, 0.0000,  ..., 0.0000, 0.0642, 0.0000],
        [0.0103, 0.0763, 0.0000,  ..., 0.0000, 0.0642, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(626318.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3487.5520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-222.3835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(165.4395, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3583.3452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.9258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3520],
        [ 0.3556],
        [ 0.3509],
        ...,
        [-0.8480],
        [-0.8451],
        [-0.8439]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165724.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0074],
        [1.0041],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369198.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0074],
        [1.0042],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369211.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.2366e-05, -1.1995e-03,  7.4620e-05,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [-2.2366e-05, -1.1995e-03,  7.4620e-05,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3943e-02,  1.4678e-02,  2.5738e-02,  ...,  2.2865e-03,
         -9.9089e-03, -1.7989e-02],
        ...,
        [-2.2366e-05, -1.1995e-03,  7.4620e-05,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [-2.2366e-05, -1.1995e-03,  7.4620e-05,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00],
        [-2.2366e-05, -1.1995e-03,  7.4620e-05,  ..., -1.3360e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2727.4673, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.9763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0095, device='cuda:0')



h[100].sum tensor(-6.2825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.0613, device='cuda:0')



h[200].sum tensor(-27.7278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1124e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0195, 0.0198, 0.0363,  ..., 0.0024, 0.0000, 0.0000],
        [0.0380, 0.0397, 0.0703,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65405.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0339, 0.0389, 0.0000,  ..., 0.0000, 0.0334, 0.0000],
        [0.0922, 0.0124, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.1610, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0103, 0.0772, 0.0000,  ..., 0.0000, 0.0651, 0.0000],
        [0.0102, 0.0772, 0.0000,  ..., 0.0000, 0.0651, 0.0000],
        [0.0102, 0.0771, 0.0000,  ..., 0.0000, 0.0650, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635494.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3639.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-232.4372, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(178.7051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3554.8149, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.2436, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3823],
        [-0.1531],
        [ 0.0605],
        ...,
        [-0.8576],
        [-0.8546],
        [-0.8534]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161861.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0074],
        [1.0042],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369211.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0075],
        [1.0042],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369223.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1772e-02,  1.2187e-02,  2.1732e-02,  ...,  1.7192e-03,
         -8.3470e-03, -1.5162e-02],
        [ 1.0521e-02,  1.0765e-02,  1.9432e-02,  ...,  1.3946e-03,
         -7.4603e-03, -1.3552e-02],
        [ 1.0654e-02,  1.0917e-02,  1.9678e-02,  ...,  1.4293e-03,
         -7.5550e-03, -1.3724e-02],
        ...,
        [-9.5281e-06, -1.2063e-03,  8.0902e-05,  ..., -1.3367e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5281e-06, -1.2063e-03,  8.0902e-05,  ..., -1.3367e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5281e-06, -1.2063e-03,  8.0902e-05,  ..., -1.3367e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2935.6973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0975, device='cuda:0')



h[100].sum tensor(-7.3598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.8152, device='cuda:0')



h[200].sum tensor(-32.5439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3031e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0466, 0.0481, 0.0860,  ..., 0.0068, 0.0000, 0.0000],
        [0.0313, 0.0308, 0.0579,  ..., 0.0032, 0.0000, 0.0000],
        [0.0323, 0.0319, 0.0597,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72618.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1770, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1288, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1094, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0100, 0.0775, 0.0000,  ..., 0.0000, 0.0657, 0.0000],
        [0.0100, 0.0775, 0.0000,  ..., 0.0000, 0.0657, 0.0000],
        [0.0100, 0.0775, 0.0000,  ..., 0.0000, 0.0656, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678166.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4373.8950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-286.2571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.0568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3565.9580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(39.7271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1696],
        [ 0.1424],
        [ 0.1138],
        ...,
        [-0.8637],
        [-0.8608],
        [-0.8596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164604.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0075],
        [1.0042],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369223.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0075],
        [1.0042],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369236.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1606e-03,  5.7888e-03,  1.1403e-02,  ...,  2.5991e-04,
         -4.3565e-03, -7.9182e-03],
        [ 4.0740e-03,  3.4170e-03,  7.5690e-03,  ..., -2.8125e-04,
         -2.8805e-03, -5.2354e-03],
        [ 1.0232e-02,  1.0417e-02,  1.8886e-02,  ...,  1.3160e-03,
         -7.2370e-03, -1.3154e-02],
        ...,
        [ 2.1130e-06, -1.2117e-03,  8.6042e-05,  ..., -1.3373e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1130e-06, -1.2117e-03,  8.6042e-05,  ..., -1.3373e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1130e-06, -1.2117e-03,  8.6042e-05,  ..., -1.3373e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2524.8901, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.0795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3543, device='cuda:0')



h[100].sum tensor(-5.0052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8014, device='cuda:0')



h[200].sum tensor(-22.1744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8659e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7076e-02, 1.6959e-02, 3.1712e-02,  ..., 2.0147e-03, 0.0000e+00,
         0.0000e+00],
        [4.3390e-02, 4.4418e-02, 8.0069e-02,  ..., 5.8491e-03, 0.0000e+00,
         0.0000e+00],
        [2.5415e-02, 2.5218e-02, 4.7036e-02,  ..., 2.8314e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.7793e-06, 0.0000e+00, 3.5750e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.7757e-06, 0.0000e+00, 3.5735e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.7732e-06, 0.0000e+00, 3.5725e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61087.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0888, 0.0050, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.1304, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1130, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0099, 0.0776, 0.0000,  ..., 0.0000, 0.0660, 0.0000],
        [0.0099, 0.0776, 0.0000,  ..., 0.0000, 0.0659, 0.0000],
        [0.0099, 0.0775, 0.0000,  ..., 0.0000, 0.0659, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622774.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3197.1160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-203.2394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.3042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3765.1470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.1791, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2235],
        [ 0.3269],
        [ 0.3372],
        ...,
        [-0.8672],
        [-0.8643],
        [-0.8631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187379.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0075],
        [1.0042],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369236.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0075],
        [1.0043],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369249.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2187e-02,  1.2649e-02,  2.2483e-02,  ...,  1.8233e-03,
         -8.6110e-03, -1.5660e-02],
        [-3.9896e-06, -1.2081e-03,  8.0223e-05,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        [-3.9896e-06, -1.2081e-03,  8.0223e-05,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-3.9896e-06, -1.2081e-03,  8.0223e-05,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        [-3.9896e-06, -1.2081e-03,  8.0223e-05,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00],
        [-3.9896e-06, -1.2081e-03,  8.0223e-05,  ..., -1.3382e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3141.7097, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5115, device='cuda:0')



h[100].sum tensor(-8.2375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.9680, device='cuda:0')



h[200].sum tensor(-36.5634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4522e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0100, 0.0102, 0.0187,  ..., 0.0013, 0.0000, 0.0000],
        [0.0123, 0.0128, 0.0229,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75727.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0860, 0.0080, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0597, 0.0117, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0405, 0.0206, 0.0000,  ..., 0.0000, 0.0186, 0.0000],
        ...,
        [0.0098, 0.0777, 0.0000,  ..., 0.0000, 0.0661, 0.0000],
        [0.0098, 0.0777, 0.0000,  ..., 0.0000, 0.0660, 0.0000],
        [0.0098, 0.0777, 0.0000,  ..., 0.0000, 0.0660, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692281.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4581.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.9735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(275.7293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3522.3184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.4780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3379],
        [ 0.3252],
        [ 0.3163],
        ...,
        [-0.8710],
        [-0.8680],
        [-0.8668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167017.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0075],
        [1.0043],
        ...,
        [1.0016],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369249.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0076],
        [1.0043],
        ...,
        [1.0016],
        [1.0009],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369262.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0558e-05, -1.2024e-03,  7.4352e-05,  ..., -1.3395e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0558e-05, -1.2024e-03,  7.4352e-05,  ..., -1.3395e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0558e-05, -1.2024e-03,  7.4352e-05,  ..., -1.3395e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.0558e-05, -1.2024e-03,  7.4352e-05,  ..., -1.3395e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0558e-05, -1.2024e-03,  7.4352e-05,  ..., -1.3395e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0558e-05, -1.2024e-03,  7.4352e-05,  ..., -1.3395e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2814.4814, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0822, device='cuda:0')



h[100].sum tensor(-6.3325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.1261, device='cuda:0')



h[200].sum tensor(-28.1613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1168e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0049, 0.0044, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67737.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0103, 0.0735, 0.0000,  ..., 0.0000, 0.0621, 0.0000],
        [0.0162, 0.0623, 0.0000,  ..., 0.0000, 0.0531, 0.0000],
        [0.0309, 0.0380, 0.0000,  ..., 0.0000, 0.0330, 0.0000],
        ...,
        [0.0098, 0.0779, 0.0000,  ..., 0.0000, 0.0660, 0.0000],
        [0.0098, 0.0779, 0.0000,  ..., 0.0000, 0.0659, 0.0000],
        [0.0098, 0.0779, 0.0000,  ..., 0.0000, 0.0659, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648329.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3744.5872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-250.0968, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(204.7079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3492.2251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.5524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8260],
        [-0.6267],
        [-0.3525],
        ...,
        [-0.8745],
        [-0.8715],
        [-0.8703]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162593.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0076],
        [1.0043],
        ...,
        [1.0016],
        [1.0009],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369262.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0076],
        [1.0044],
        ...,
        [1.0017],
        [1.0009],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369274.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7158e-05, -1.1960e-03,  6.7463e-05,  ..., -1.3407e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0248e-02,  1.0474e-02,  1.8929e-02,  ...,  1.3214e-03,
         -7.2300e-03, -1.3165e-02],
        [ 5.9852e-03,  5.6280e-03,  1.1097e-02,  ...,  2.1595e-04,
         -4.2278e-03, -7.6980e-03],
        ...,
        [-1.7158e-05, -1.1960e-03,  6.7463e-05,  ..., -1.3407e-03,
          0.0000e+00,  0.0000e+00],
        [-1.7158e-05, -1.1960e-03,  6.7463e-05,  ..., -1.3407e-03,
          0.0000e+00,  0.0000e+00],
        [-1.7158e-05, -1.1960e-03,  6.7463e-05,  ..., -1.3407e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3015.6699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.4303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2304, device='cuda:0')



h[100].sum tensor(-7.2838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.9337, device='cuda:0')



h[200].sum tensor(-32.4534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3113e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0243, 0.0228, 0.0450,  ..., 0.0014, 0.0000, 0.0000],
        [0.0145, 0.0129, 0.0270,  ..., 0.0002, 0.0000, 0.0000],
        [0.0379, 0.0384, 0.0701,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72933.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0723, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0799, 0.0010, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.1352, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0098, 0.0781, 0.0000,  ..., 0.0000, 0.0658, 0.0000],
        [0.0098, 0.0781, 0.0000,  ..., 0.0000, 0.0657, 0.0000],
        [0.0097, 0.0780, 0.0000,  ..., 0.0000, 0.0657, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675919.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4276.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-287.9868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(255.5478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3376.8318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.2089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3513],
        [ 0.3488],
        [ 0.3452],
        ...,
        [-0.8777],
        [-0.8747],
        [-0.8735]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155388.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0076],
        [1.0044],
        ...,
        [1.0017],
        [1.0009],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369274.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(536.2151, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0077],
        [1.0044],
        ...,
        [1.0017],
        [1.0009],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369287.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6348e-05, -1.1942e-03,  6.7333e-05,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6348e-05, -1.1942e-03,  6.7333e-05,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6348e-05, -1.1942e-03,  6.7333e-05,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.6348e-05, -1.1942e-03,  6.7333e-05,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6348e-05, -1.1942e-03,  6.7333e-05,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00],
        [-1.6348e-05, -1.1942e-03,  6.7333e-05,  ..., -1.3418e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2596.3379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.3296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2086, device='cuda:0')



h[100].sum tensor(-4.9122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6716, device='cuda:0')



h[200].sum tensor(-21.9285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.7759e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61296.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0526e-02, 7.2458e-02, 1.1764e-05,  ..., 0.0000e+00, 6.0395e-02,
         0.0000e+00],
        [9.3837e-03, 7.5466e-02, 0.0000e+00,  ..., 0.0000e+00, 6.3113e-02,
         0.0000e+00],
        [9.3672e-03, 7.5259e-02, 0.0000e+00,  ..., 0.0000e+00, 6.2953e-02,
         0.0000e+00],
        ...,
        [9.6180e-03, 7.8259e-02, 0.0000e+00,  ..., 0.0000e+00, 6.5654e-02,
         0.0000e+00],
        [9.6132e-03, 7.8225e-02, 0.0000e+00,  ..., 0.0000e+00, 6.5625e-02,
         0.0000e+00],
        [9.6088e-03, 7.8198e-02, 0.0000e+00,  ..., 0.0000e+00, 6.5600e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621318.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3091.2036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-203.6454, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.5483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3540.9619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.3392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4279],
        [-0.4175],
        [-0.3492],
        ...,
        [-0.8811],
        [-0.8781],
        [-0.8768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173269.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0077],
        [1.0044],
        ...,
        [1.0017],
        [1.0009],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369287.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0077],
        [1.0044],
        ...,
        [1.0017],
        [1.0009],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369300.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3562e-05, -1.1949e-03,  6.6874e-05,  ..., -1.3429e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3562e-05, -1.1949e-03,  6.6874e-05,  ..., -1.3429e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3562e-05, -1.1949e-03,  6.6874e-05,  ..., -1.3429e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.3562e-05, -1.1949e-03,  6.6874e-05,  ..., -1.3429e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3562e-05, -1.1949e-03,  6.6874e-05,  ..., -1.3429e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3562e-05, -1.1949e-03,  6.6874e-05,  ..., -1.3429e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3732.3103, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.6301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1599, device='cuda:0')



h[100].sum tensor(-10.8913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.6809, device='cuda:0')



h[200].sum tensor(-48.7120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9863e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86789.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0091, 0.0760, 0.0000,  ..., 0.0000, 0.0635, 0.0000],
        [0.0091, 0.0762, 0.0000,  ..., 0.0000, 0.0637, 0.0000],
        [0.0091, 0.0760, 0.0000,  ..., 0.0000, 0.0635, 0.0000],
        ...,
        [0.0094, 0.0786, 0.0000,  ..., 0.0000, 0.0659, 0.0000],
        [0.0094, 0.0786, 0.0000,  ..., 0.0000, 0.0658, 0.0000],
        [0.0094, 0.0786, 0.0000,  ..., 0.0000, 0.0658, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(742196.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5354.5635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-393.0307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(388.0871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3450.2153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(51.9305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9087],
        [-0.9334],
        [-0.9248],
        ...,
        [-0.8864],
        [-0.8834],
        [-0.8821]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169746.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0077],
        [1.0044],
        ...,
        [1.0017],
        [1.0009],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369300.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0078],
        [1.0045],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369312.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9605e-06, -1.2002e-03,  6.4827e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-5.9605e-06, -1.2002e-03,  6.4827e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2188e-03,  4.7394e-03,  9.6652e-03,  ...,  1.1124e-05,
         -3.6638e-03, -6.6832e-03],
        ...,
        [-5.9605e-06, -1.2002e-03,  6.4827e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-5.9605e-06, -1.2002e-03,  6.4827e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [-5.9605e-06, -1.2002e-03,  6.4827e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2483.5186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.6226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.3727, device='cuda:0')



h[100].sum tensor(-4.2096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0343, device='cuda:0')



h[200].sum tensor(-18.8638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.6420e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.6131e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.2640e-03, 4.7804e-03, 9.9454e-03,  ..., 1.1220e-05, 0.0000e+00,
         0.0000e+00],
        [1.6282e-02, 1.6106e-02, 3.0201e-02,  ..., 1.6282e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.6958e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6947e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.6939e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58786.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0222, 0.0521, 0.0000,  ..., 0.0000, 0.0443, 0.0000],
        [0.0487, 0.0256, 0.0000,  ..., 0.0000, 0.0218, 0.0000],
        [0.0851, 0.0051, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        ...,
        [0.0093, 0.0792, 0.0000,  ..., 0.0000, 0.0664, 0.0000],
        [0.0093, 0.0792, 0.0000,  ..., 0.0000, 0.0663, 0.0000],
        [0.0093, 0.0791, 0.0000,  ..., 0.0000, 0.0663, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614505.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2838.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-187.0858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.8710, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3616.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(47.4865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0635],
        [ 0.1909],
        [ 0.2714],
        ...,
        [-0.8934],
        [-0.8903],
        [-0.8890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182333.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0078],
        [1.0045],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369312.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0078],
        [1.0045],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369325.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2399e-02,  2.4244e-02,  4.1214e-02,  ...,  4.4616e-03,
         -1.5678e-02, -2.8615e-02],
        [ 1.9036e-02,  2.0421e-02,  3.5035e-02,  ...,  3.5896e-03,
         -1.3323e-02, -2.4318e-02],
        [ 1.8228e-02,  1.9503e-02,  3.3550e-02,  ...,  3.3801e-03,
         -1.2758e-02, -2.3285e-02],
        ...,
        [ 7.3985e-06, -1.2081e-03,  6.8819e-05,  ..., -1.3441e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.3985e-06, -1.2081e-03,  6.8819e-05,  ..., -1.3441e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.3985e-06, -1.2081e-03,  6.8819e-05,  ..., -1.3441e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2764.0693, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.5204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4946, device='cuda:0')



h[100].sum tensor(-5.6527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7102, device='cuda:0')



h[200].sum tensor(-25.3789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0188e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.9668e-02, 6.2918e-02, 1.0986e-01,  ..., 1.0045e-02, 0.0000e+00,
         0.0000e+00],
        [8.2717e-02, 8.9104e-02, 1.5222e-01,  ..., 1.6006e-02, 0.0000e+00,
         0.0000e+00],
        [5.7460e-02, 6.0411e-02, 1.0581e-01,  ..., 9.4744e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.0770e-05, 0.0000e+00, 2.8622e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.0758e-05, 0.0000e+00, 2.8610e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.0750e-05, 0.0000e+00, 2.8602e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65047.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1976, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2325, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1957, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0091, 0.0795, 0.0000,  ..., 0.0000, 0.0668, 0.0000],
        [0.0091, 0.0795, 0.0000,  ..., 0.0000, 0.0668, 0.0000],
        [0.0091, 0.0795, 0.0000,  ..., 0.0000, 0.0667, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639499.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3298.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-232.0959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(186.1508, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3478.2297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(47.3946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3586],
        [ 0.3524],
        [ 0.3461],
        ...,
        [-0.8989],
        [-0.8959],
        [-0.8947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172907.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0078],
        [1.0045],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369325.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0079],
        [1.0046],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369337.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3580e-05, -1.2150e-03,  6.5493e-05,  ..., -1.3444e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3580e-05, -1.2150e-03,  6.5493e-05,  ..., -1.3444e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3580e-05, -1.2150e-03,  6.5493e-05,  ..., -1.3444e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.3580e-05, -1.2150e-03,  6.5493e-05,  ..., -1.3444e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3580e-05, -1.2150e-03,  6.5493e-05,  ..., -1.3444e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3580e-05, -1.2150e-03,  6.5493e-05,  ..., -1.3444e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3127.2971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9782, device='cuda:0')



h[100].sum tensor(-7.5628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.6006, device='cuda:0')



h[200].sum tensor(-34.0198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3575e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.4744e-05, 0.0000e+00, 2.6402e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.4888e-05, 0.0000e+00, 2.6471e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.2528e-02, 1.1729e-02, 2.3185e-02,  ..., 5.2592e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.6487e-05, 0.0000e+00, 2.7242e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.6464e-05, 0.0000e+00, 2.7232e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.6449e-05, 0.0000e+00, 2.7224e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74961.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0576, 0.0000,  ..., 0.0000, 0.0488, 0.0000],
        [0.0353, 0.0300, 0.0000,  ..., 0.0000, 0.0254, 0.0000],
        [0.0673, 0.0115, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        ...,
        [0.0091, 0.0801, 0.0000,  ..., 0.0000, 0.0674, 0.0000],
        [0.0091, 0.0801, 0.0000,  ..., 0.0000, 0.0674, 0.0000],
        [0.0091, 0.0801, 0.0000,  ..., 0.0000, 0.0673, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687049.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4286.7256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-301.8777, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(276.0034, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3174.8608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.1397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0730],
        [ 0.1349],
        [ 0.2670],
        ...,
        [-0.9061],
        [-0.9031],
        [-0.9017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140243.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0079],
        [1.0046],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369337.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0080],
        [1.0046],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369349.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6508e-02,  1.7523e-02,  3.0367e-02,  ...,  2.9314e-03,
         -1.1512e-02, -2.1037e-02],
        [ 9.4307e-03,  9.4803e-03,  1.7362e-02,  ...,  1.0966e-03,
         -6.5716e-03, -1.2009e-02],
        [ 1.3452e-02,  1.4050e-02,  2.4751e-02,  ...,  2.1391e-03,
         -9.3787e-03, -1.7138e-02],
        ...,
        [ 1.6454e-05, -1.2185e-03,  6.2469e-05,  ..., -1.3442e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.1765e-03,  8.0550e-03,  1.5057e-02,  ...,  7.7140e-04,
         -5.6962e-03, -1.0409e-02],
        [ 8.1765e-03,  8.0550e-03,  1.5057e-02,  ...,  7.7140e-04,
         -5.6962e-03, -1.0409e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2793.6353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8660, device='cuda:0')



h[100].sum tensor(-5.7765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0415, device='cuda:0')



h[200].sum tensor(-26.0344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0417e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0231, 0.0225, 0.0426,  ..., 0.0019, 0.0000, 0.0000],
        [0.0497, 0.0515, 0.0915,  ..., 0.0074, 0.0000, 0.0000],
        [0.0284, 0.0285, 0.0522,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0155, 0.0150, 0.0286,  ..., 0.0012, 0.0000, 0.0000],
        [0.0155, 0.0150, 0.0286,  ..., 0.0012, 0.0000, 0.0000],
        [0.0155, 0.0150, 0.0286,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64870.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1091, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1334, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1086, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0523, 0.0198, 0.0000,  ..., 0.0000, 0.0175, 0.0000],
        [0.0614, 0.0068, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0613, 0.0068, 0.0000,  ..., 0.0000, 0.0061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640742.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3254.5425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-230.6736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(183.2090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3637.8491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.5768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3539],
        [ 0.3622],
        [ 0.3542],
        ...,
        [-0.2945],
        [-0.1391],
        [-0.1382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183557.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0080],
        [1.0046],
        ...,
        [1.0018],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369349.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0080],
        [1.0046],
        ...,
        [1.0019],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369362.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8885e-05, -1.2222e-03,  5.9379e-05,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8885e-05, -1.2222e-03,  5.9379e-05,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8885e-05, -1.2222e-03,  5.9379e-05,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.8885e-05, -1.2222e-03,  5.9379e-05,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8885e-05, -1.2222e-03,  5.9379e-05,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8885e-05, -1.2222e-03,  5.9379e-05,  ..., -1.3438e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2629.6699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.6238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2811, device='cuda:0')



h[100].sum tensor(-4.8642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7361, device='cuda:0')



h[200].sum tensor(-21.9649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8207e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.6142e-05, 0.0000e+00, 2.3941e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.6344e-05, 0.0000e+00, 2.4004e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.6107e-05, 0.0000e+00, 2.3929e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.8577e-05, 0.0000e+00, 2.4706e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.8545e-05, 0.0000e+00, 2.4696e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.8524e-05, 0.0000e+00, 2.4689e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61455.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.7694e-03, 7.8097e-02, 0.0000e+00,  ..., 0.0000e+00, 6.5675e-02,
         0.0000e+00],
        [8.9932e-03, 7.7703e-02, 9.1674e-05,  ..., 0.0000e+00, 6.5211e-02,
         0.0000e+00],
        [1.2835e-02, 6.9109e-02, 3.8829e-04,  ..., 0.0000e+00, 5.8010e-02,
         0.0000e+00],
        ...,
        [9.1495e-03, 8.0863e-02, 0.0000e+00,  ..., 0.0000e+00, 6.8150e-02,
         0.0000e+00],
        [9.1450e-03, 8.0828e-02, 0.0000e+00,  ..., 0.0000e+00, 6.8119e-02,
         0.0000e+00],
        [9.1408e-03, 8.0801e-02, 0.0000e+00,  ..., 0.0000e+00, 6.8094e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629792.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2991.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-200.9699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.9929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3814.8379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.5414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8690],
        [-0.7215],
        [-0.4925],
        ...,
        [-0.9193],
        [-0.9162],
        [-0.9148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193303.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0080],
        [1.0046],
        ...,
        [1.0019],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369362.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0081],
        [1.0046],
        ...,
        [1.0019],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369374.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2020e-05, -1.2265e-03,  5.7454e-05,  ..., -1.3430e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.2020e-05, -1.2265e-03,  5.7454e-05,  ..., -1.3430e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.2020e-05, -1.2265e-03,  5.7454e-05,  ..., -1.3430e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.1538e-03,  3.4685e-03,  7.6500e-03,  ..., -2.7181e-04,
         -2.8756e-03, -5.2611e-03],
        [ 6.8519e-03,  6.5344e-03,  1.2608e-02,  ...,  4.2768e-04,
         -4.7533e-03, -8.6966e-03],
        [ 2.2020e-05, -1.2265e-03,  5.7454e-05,  ..., -1.3430e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2813.2354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9408, device='cuda:0')



h[100].sum tensor(-5.7355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.1082, device='cuda:0')



h[200].sum tensor(-25.9492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0463e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.8787e-05, 0.0000e+00, 2.3167e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.9022e-05, 0.0000e+00, 2.3228e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.8742e-05, 0.0000e+00, 2.3155e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.8820e-02, 3.8903e-02, 7.1405e-02,  ..., 4.4518e-03, 0.0000e+00,
         0.0000e+00],
        [1.3702e-02, 1.2914e-02, 2.5250e-02,  ..., 1.0169e-03, 0.0000e+00,
         0.0000e+00],
        [7.1977e-03, 6.7987e-03, 1.3297e-02,  ..., 4.4497e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67670.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0257, 0.0463, 0.0000,  ..., 0.0000, 0.0405, 0.0000],
        [0.0118, 0.0722, 0.0000,  ..., 0.0000, 0.0611, 0.0000],
        [0.0088, 0.0782, 0.0000,  ..., 0.0000, 0.0659, 0.0000],
        ...,
        [0.1208, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0805, 0.0095, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0453, 0.0312, 0.0000,  ..., 0.0000, 0.0269, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664414.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3686.2856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-239.5894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.9030, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3762.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.1932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1244],
        [-0.4349],
        [-0.6991],
        ...,
        [ 0.3023],
        [ 0.1163],
        [-0.1973]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176311.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0081],
        [1.0046],
        ...,
        [1.0019],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369374.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0081],
        [1.0047],
        ...,
        [1.0019],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369387.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9997e-05, -1.2287e-03,  5.4435e-05,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9997e-05, -1.2287e-03,  5.4435e-05,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9997e-05, -1.2287e-03,  5.4435e-05,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.9997e-05, -1.2287e-03,  5.4435e-05,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9997e-05, -1.2287e-03,  5.4435e-05,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9997e-05, -1.2287e-03,  5.4435e-05,  ..., -1.3423e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2548.2222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.0979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.8356, device='cuda:0')



h[100].sum tensor(-4.3331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4470, device='cuda:0')



h[200].sum tensor(-19.6418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.9279e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.0641e-05, 0.0000e+00, 2.1951e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.0853e-05, 0.0000e+00, 2.2009e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.0595e-05, 0.0000e+00, 2.1939e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.3229e-05, 0.0000e+00, 2.2656e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.3194e-05, 0.0000e+00, 2.2646e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.3170e-05, 0.0000e+00, 2.2640e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59956.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0787, 0.0000,  ..., 0.0000, 0.0661, 0.0000],
        [0.0088, 0.0789, 0.0000,  ..., 0.0000, 0.0663, 0.0000],
        [0.0088, 0.0786, 0.0000,  ..., 0.0000, 0.0661, 0.0000],
        ...,
        [0.0196, 0.0609, 0.0000,  ..., 0.0000, 0.0524, 0.0000],
        [0.0126, 0.0746, 0.0000,  ..., 0.0000, 0.0632, 0.0000],
        [0.0092, 0.0814, 0.0000,  ..., 0.0000, 0.0685, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624031.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2876.5364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-186.0154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.1704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3884.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.2802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8416],
        [-0.7437],
        [-0.5715],
        ...,
        [-0.5170],
        [-0.7001],
        [-0.8303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185037.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0081],
        [1.0047],
        ...,
        [1.0019],
        [1.0010],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369387.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0082],
        [1.0047],
        ...,
        [1.0019],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369399.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5931e-06, -1.2278e-03,  4.7683e-05,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5931e-06, -1.2278e-03,  4.7683e-05,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5931e-06, -1.2278e-03,  4.7683e-05,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.5931e-06, -1.2278e-03,  4.7683e-05,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.5931e-06, -1.2278e-03,  4.7683e-05,  ..., -1.3417e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2104e-02,  1.2515e-02,  2.2271e-02,  ...,  1.7940e-03,
         -8.3924e-03, -1.5373e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3660.9968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.4809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8170, device='cuda:0')



h[100].sum tensor(-10.0337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.5914, device='cuda:0')



h[200].sum tensor(-45.5704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8416e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.8688e-05, 0.0000e+00, 1.9230e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.8790e-05, 0.0000e+00, 1.9281e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.8665e-05, 0.0000e+00, 1.9219e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.9933e-05, 0.0000e+00, 1.9849e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.2609e-02, 1.3007e-02, 2.3295e-02,  ..., 1.8645e-03, 0.0000e+00,
         0.0000e+00],
        [2.2896e-02, 2.3420e-02, 4.2197e-02,  ..., 3.1367e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86890.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0791, 0.0000,  ..., 0.0000, 0.0662, 0.0000],
        [0.0157, 0.0660, 0.0000,  ..., 0.0000, 0.0558, 0.0000],
        [0.0410, 0.0360, 0.0000,  ..., 0.0000, 0.0314, 0.0000],
        ...,
        [0.0240, 0.0563, 0.0000,  ..., 0.0000, 0.0482, 0.0000],
        [0.0636, 0.0264, 0.0000,  ..., 0.0000, 0.0227, 0.0000],
        [0.1117, 0.0042, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(754995.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5449.0786, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-390.8817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.0715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3525.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.7938, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6605],
        [-0.4047],
        [-0.0781],
        ...,
        [-0.5909],
        [-0.3012],
        [-0.0066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159985.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0082],
        [1.0047],
        ...,
        [1.0019],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369399.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(522.4877, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0083],
        [1.0047],
        ...,
        [1.0019],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369412.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2526.7454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.0843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.3109, device='cuda:0')



h[100].sum tensor(-4.1452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9792, device='cuda:0')



h[200].sum tensor(-18.8627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.6038e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.1435e-03, 4.4856e-03, 1.1444e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.0986e-03, 2.2547e-03, 5.8489e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7088e-02, 2.8288e-02, 4.9929e-02,  ..., 4.3136e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.1978e-05, 0.0000e+00, 2.0016e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.1968e-05, 0.0000e+00, 2.0007e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.1961e-05, 0.0000e+00, 2.0001e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58790.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0318, 0.0295, 0.0000,  ..., 0.0000, 0.0248, 0.0000],
        [0.0480, 0.0203, 0.0000,  ..., 0.0000, 0.0169, 0.0000],
        [0.1068, 0.0007, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        ...,
        [0.0090, 0.0823, 0.0000,  ..., 0.0000, 0.0687, 0.0000],
        [0.0090, 0.0822, 0.0000,  ..., 0.0000, 0.0687, 0.0000],
        [0.0090, 0.0822, 0.0000,  ..., 0.0000, 0.0686, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623493.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2729.6887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-189.9395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.9610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4051.1514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.8397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1416],
        [ 0.1122],
        [ 0.2873],
        ...,
        [-0.9394],
        [-0.9362],
        [-0.9349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206836.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0083],
        [1.0047],
        ...,
        [1.0019],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369412.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0083],
        [1.0047],
        ...,
        [1.0019],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369412.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2789e-06, -1.2271e-03,  4.8077e-05,  ..., -1.3422e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3517.3792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.9066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5072, device='cuda:0')



h[100].sum tensor(-9.2274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.5315, device='cuda:0')



h[200].sum tensor(-41.9894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6990e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.6595e-02, 1.6363e-02, 3.0648e-02,  ..., 1.5934e-03, 0.0000e+00,
         0.0000e+00],
        [1.7213e-02, 1.7055e-02, 3.1785e-02,  ..., 1.7412e-03, 0.0000e+00,
         0.0000e+00],
        [5.9048e-03, 5.4501e-03, 1.1005e-02,  ..., 1.7318e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.1978e-05, 0.0000e+00, 2.0016e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.1968e-05, 0.0000e+00, 2.0007e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.1961e-05, 0.0000e+00, 2.0001e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81031.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0644, 0.0033, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0651, 0.0029, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0436, 0.0220, 0.0000,  ..., 0.0000, 0.0191, 0.0000],
        ...,
        [0.0090, 0.0823, 0.0000,  ..., 0.0000, 0.0687, 0.0000],
        [0.0090, 0.0822, 0.0000,  ..., 0.0000, 0.0687, 0.0000],
        [0.0090, 0.0822, 0.0000,  ..., 0.0000, 0.0686, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(722693.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4825.2671, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-349.6783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(334.3006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3423.6052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.7749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0573],
        [ 0.0083],
        [-0.1815],
        ...,
        [-0.9394],
        [-0.9362],
        [-0.9348]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145562.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0083],
        [1.0047],
        ...,
        [1.0019],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369412.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0084],
        [1.0048],
        ...,
        [1.0020],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369424.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2872e-03,  4.7921e-03,  9.7720e-03,  ...,  2.9697e-05,
         -3.6625e-03, -6.7170e-03],
        [-5.6587e-06, -1.2240e-03,  4.6640e-05,  ..., -1.3427e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6587e-06, -1.2240e-03,  4.6640e-05,  ..., -1.3427e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.6587e-06, -1.2240e-03,  4.6640e-05,  ..., -1.3427e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6587e-06, -1.2240e-03,  4.6640e-05,  ..., -1.3427e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6587e-06, -1.2240e-03,  4.6640e-05,  ..., -1.3427e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3458.8101, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.3156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0752, device='cuda:0')



h[100].sum tensor(-8.8126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.2544, device='cuda:0')



h[200].sum tensor(-40.1796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6105e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7477e-02, 1.7411e-02, 3.2323e-02,  ..., 1.8478e-03, 0.0000e+00,
         0.0000e+00],
        [5.3441e-03, 4.8437e-03, 1.0019e-02,  ..., 3.0016e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8801e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.9421e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.9412e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.9405e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83841.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0763, 0.0059, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0514, 0.0140, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0429, 0.0188, 0.0000,  ..., 0.0000, 0.0172, 0.0000],
        ...,
        [0.0089, 0.0823, 0.0000,  ..., 0.0000, 0.0683, 0.0000],
        [0.0089, 0.0822, 0.0000,  ..., 0.0000, 0.0683, 0.0000],
        [0.0089, 0.0822, 0.0000,  ..., 0.0000, 0.0683, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(755224.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5276.3584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-376.3721, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(365.1014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3740.5962, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(50.9875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3408],
        [ 0.3088],
        [ 0.2663],
        ...,
        [-0.9380],
        [-0.9349],
        [-0.9335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175808.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0084],
        [1.0048],
        ...,
        [1.0020],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369424.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0084],
        [1.0048],
        ...,
        [1.0020],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369436.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.7377e-03,  9.8497e-03,  1.7959e-02,  ...,  1.1832e-03,
         -6.7331e-03, -1.2356e-02],
        [-6.6613e-06, -1.2270e-03,  5.4228e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6613e-06, -1.2270e-03,  5.4228e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.6613e-06, -1.2270e-03,  5.4228e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6613e-06, -1.2270e-03,  5.4228e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        [-6.6613e-06, -1.2270e-03,  5.4228e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2888.9504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.9317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2137, device='cuda:0')



h[100].sum tensor(-5.8018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.3515, device='cuda:0')



h[200].sum tensor(-26.5034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0632e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0450, 0.0475, 0.0829,  ..., 0.0076, 0.0000, 0.0000],
        [0.0166, 0.0164, 0.0307,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68455.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.0619e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.4486e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0327e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.6229e-03, 8.2242e-02, 8.2345e-05,  ..., 0.0000e+00, 6.8071e-02,
         0.0000e+00],
        [8.6178e-03, 8.2200e-02, 8.0209e-05,  ..., 0.0000e+00, 6.8034e-02,
         0.0000e+00],
        [8.6134e-03, 8.2169e-02, 7.7806e-05,  ..., 0.0000e+00, 6.8006e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663068.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3507.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-261.0250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(224.1144, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3544.2139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.9121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3231],
        [ 0.3237],
        [ 0.3241],
        ...,
        [-0.9416],
        [-0.9386],
        [-0.9373]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149931.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0084],
        [1.0048],
        ...,
        [1.0020],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369436.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0085],
        [1.0048],
        ...,
        [1.0020],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369448.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.8546e-06, -1.2312e-03,  5.3792e-05,  ..., -1.3442e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8546e-06, -1.2312e-03,  5.3792e-05,  ..., -1.3442e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8546e-06, -1.2312e-03,  5.3792e-05,  ..., -1.3442e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.8546e-06, -1.2312e-03,  5.3792e-05,  ..., -1.3442e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8546e-06, -1.2312e-03,  5.3792e-05,  ..., -1.3442e-03,
          0.0000e+00,  0.0000e+00],
        [-7.8546e-06, -1.2312e-03,  5.3792e-05,  ..., -1.3442e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3500.9971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5118, device='cuda:0')



h[100].sum tensor(-8.8787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6438, device='cuda:0')



h[200].sum tensor(-40.6378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6375e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85600.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.6489e-03, 7.8026e-02, 6.4727e-04,  ..., 0.0000e+00, 6.3836e-02,
         0.0000e+00],
        [8.2550e-03, 7.9469e-02, 2.0844e-04,  ..., 0.0000e+00, 6.5302e-02,
         0.0000e+00],
        [8.0361e-03, 7.9829e-02, 3.3695e-05,  ..., 0.0000e+00, 6.5745e-02,
         0.0000e+00],
        ...,
        [8.3895e-03, 8.2739e-02, 1.3592e-04,  ..., 0.0000e+00, 6.8256e-02,
         0.0000e+00],
        [8.3843e-03, 8.2694e-02, 1.3370e-04,  ..., 0.0000e+00, 6.8218e-02,
         0.0000e+00],
        [8.3800e-03, 8.2662e-02, 1.3117e-04,  ..., 0.0000e+00, 6.8190e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(774339.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5478.4424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-391.3347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(385.2681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3755.6514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(53.0512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4561],
        [-0.6407],
        [-0.8012],
        ...,
        [-0.9476],
        [-0.9445],
        [-0.9432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175999.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0085],
        [1.0048],
        ...,
        [1.0020],
        [1.0011],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369448.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0085],
        [1.0049],
        ...,
        [1.0020],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369460.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2808e-03,  3.6349e-03,  7.9307e-03,  ..., -2.3362e-04,
         -2.9525e-03, -5.4248e-03],
        [ 1.1892e-02,  1.2287e-02,  2.1916e-02,  ...,  1.7399e-03,
         -8.1964e-03, -1.5060e-02],
        [-4.4761e-06, -1.2368e-03,  5.6752e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.4761e-06, -1.2368e-03,  5.6752e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [-4.4761e-06, -1.2368e-03,  5.6752e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [-4.4761e-06, -1.2368e-03,  5.6752e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2899.2466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.1127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5730, device='cuda:0')



h[100].sum tensor(-5.8047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6720, device='cuda:0')



h[200].sum tensor(-26.6199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0854e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0640, 0.0678, 0.1179,  ..., 0.0112, 0.0000, 0.0000],
        [0.0177, 0.0176, 0.0328,  ..., 0.0021, 0.0000, 0.0000],
        [0.0120, 0.0124, 0.0223,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66999.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1691, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0997, 0.0055, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0595, 0.0239, 0.0000,  ..., 0.0000, 0.0197, 0.0000],
        ...,
        [0.0082, 0.0832, 0.0003,  ..., 0.0000, 0.0685, 0.0000],
        [0.0082, 0.0831, 0.0002,  ..., 0.0000, 0.0685, 0.0000],
        [0.0082, 0.0831, 0.0002,  ..., 0.0000, 0.0684, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656481.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3200.6660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-251.9471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.6947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3750.7451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(47.7737, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3323],
        [ 0.2413],
        [ 0.0513],
        ...,
        [-0.9481],
        [-0.9491],
        [-0.9486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163360.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0085],
        [1.0049],
        ...,
        [1.0020],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369460.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0086],
        [1.0049],
        ...,
        [1.0020],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369473.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1722e-06, -1.2394e-03,  4.4218e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.5823e-03,  6.2521e-03,  1.2151e-02,  ...,  3.6383e-04,
         -4.5334e-03, -8.3345e-03],
        [-7.1722e-06, -1.2394e-03,  4.4218e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.1722e-06, -1.2394e-03,  4.4218e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [-7.1722e-06, -1.2394e-03,  4.4218e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00],
        [-7.1722e-06, -1.2394e-03,  4.4218e-05,  ..., -1.3448e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3207.7705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.0527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5828, device='cuda:0')



h[100].sum tensor(-7.3841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.1398, device='cuda:0')



h[200].sum tensor(-33.9283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3948e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.4120e-02, 2.2454e-02, 4.4549e-02,  ..., 8.3610e-04, 0.0000e+00,
         0.0000e+00],
        [5.4425e-03, 4.9404e-03, 1.0192e-02,  ..., 5.0945e-05, 0.0000e+00,
         0.0000e+00],
        [2.2279e-02, 2.2847e-02, 4.1139e-02,  ..., 3.0702e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.8423e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8413e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8407e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72228.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0496, 0.0040, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0578, 0.0099, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.1067, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0082, 0.0840, 0.0000,  ..., 0.0000, 0.0688, 0.0000],
        [0.0082, 0.0839, 0.0000,  ..., 0.0000, 0.0688, 0.0000],
        [0.0082, 0.0839, 0.0000,  ..., 0.0000, 0.0688, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676201.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3574.5459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-292.8441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.4162, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3940.5723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(49.9047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2688],
        [-0.0713],
        [ 0.1323],
        ...,
        [-0.9629],
        [-0.9598],
        [-0.9584]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174778.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0086],
        [1.0049],
        ...,
        [1.0020],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369473.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0087],
        [1.0049],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369485.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8520e-03,  5.4051e-03,  1.0797e-02,  ...,  1.7208e-04,
         -4.0193e-03, -7.3938e-03],
        [ 1.2004e-06, -1.2465e-03,  4.7219e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2004e-06, -1.2465e-03,  4.7219e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.2004e-06, -1.2465e-03,  4.7219e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2004e-06, -1.2465e-03,  4.7219e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2004e-06, -1.2465e-03,  4.7219e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2802.1855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.8754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1016, device='cuda:0')



h[100].sum tensor(-5.3214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3598, device='cuda:0')



h[200].sum tensor(-24.4985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9451e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8130e-02, 1.8093e-02, 3.3494e-02,  ..., 2.2330e-03, 0.0000e+00,
         0.0000e+00],
        [5.9206e-03, 5.4651e-03, 1.1061e-02,  ..., 1.7399e-04, 0.0000e+00,
         0.0000e+00],
        [4.8399e-06, 0.0000e+00, 1.9038e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.0020e-06, 0.0000e+00, 1.9676e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.9992e-06, 0.0000e+00, 1.9665e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.9975e-06, 0.0000e+00, 1.9658e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66326.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1254, 0.0047, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0723, 0.0178, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0448, 0.0246, 0.0000,  ..., 0.0000, 0.0206, 0.0000],
        ...,
        [0.0081, 0.0843, 0.0000,  ..., 0.0000, 0.0691, 0.0000],
        [0.0081, 0.0843, 0.0000,  ..., 0.0000, 0.0691, 0.0000],
        [0.0081, 0.0842, 0.0000,  ..., 0.0000, 0.0690, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660299.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3187.0330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-247.8293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.8880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4012.9448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.3735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2574],
        [ 0.2445],
        [ 0.2330],
        ...,
        [-0.9651],
        [-0.9619],
        [-0.9605]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176131.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0087],
        [1.0049],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369485.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0087],
        [1.0050],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369497.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2185e-05, -1.2541e-03,  5.0736e-05,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2185e-05, -1.2541e-03,  5.0736e-05,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2185e-05, -1.2541e-03,  5.0736e-05,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.2185e-05, -1.2541e-03,  5.0736e-05,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2185e-05, -1.2541e-03,  5.0736e-05,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2185e-05, -1.2541e-03,  5.0736e-05,  ..., -1.3451e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3028.7917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3798, device='cuda:0')



h[100].sum tensor(-6.4193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.2833, device='cuda:0')



h[200].sum tensor(-29.6104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1970e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0328e-02, 9.1575e-03, 1.9091e-02,  ..., 5.1083e-04, 0.0000e+00,
         0.0000e+00],
        [3.1471e-03, 2.2511e-03, 5.8973e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.6420e-03, 5.0952e-03, 1.0481e-02,  ..., 9.5737e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.0781e-05, 0.0000e+00, 2.1145e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.0752e-05, 0.0000e+00, 2.1133e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.0735e-05, 0.0000e+00, 2.1125e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72200.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.7609e-02, 1.3780e-02, 0.0000e+00,  ..., 0.0000e+00, 1.1675e-02,
         0.0000e+00],
        [4.1363e-02, 1.5120e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3550e-02,
         0.0000e+00],
        [4.5078e-02, 9.0166e-03, 0.0000e+00,  ..., 0.0000e+00, 8.3097e-03,
         0.0000e+00],
        ...,
        [7.8545e-03, 8.4584e-02, 5.3661e-05,  ..., 0.0000e+00, 6.9496e-02,
         0.0000e+00],
        [7.8493e-03, 8.4534e-02, 5.1271e-05,  ..., 0.0000e+00, 6.9454e-02,
         0.0000e+00],
        [7.8450e-03, 8.4499e-02, 4.8609e-05,  ..., 0.0000e+00, 6.9423e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691293.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3739.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-289.5662, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(259.9067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3975.4731, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.0381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2357],
        [ 0.2972],
        [ 0.3244],
        ...,
        [-0.9742],
        [-0.9708],
        [-0.9694]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169907.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0087],
        [1.0050],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369497.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0088],
        [1.0050],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369508.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6136e-06, -1.2562e-03,  3.9274e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.6136e-06, -1.2562e-03,  3.9274e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.4873e-03,  3.8406e-03,  8.2781e-03,  ..., -1.8206e-04,
         -3.0709e-03, -5.6560e-03],
        ...,
        [ 3.6136e-06, -1.2562e-03,  3.9274e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.6136e-06, -1.2562e-03,  3.9274e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.6136e-06, -1.2562e-03,  3.9274e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3144.5564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0657, device='cuda:0')



h[100].sum tensor(-6.9805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.7868, device='cuda:0')



h[200].sum tensor(-32.2620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3011e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.4582e-05, 0.0000e+00, 1.5848e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.2262e-02, 1.1383e-02, 2.2663e-02,  ..., 6.4104e-04, 0.0000e+00,
         0.0000e+00],
        [1.6222e-02, 1.4626e-02, 2.9939e-02,  ..., 6.3857e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.5062e-05, 0.0000e+00, 1.6370e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5054e-05, 0.0000e+00, 1.6361e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5048e-05, 0.0000e+00, 1.6355e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73213.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0557, 0.0038, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0603, 0.0032, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0743, 0.0038, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        ...,
        [0.0079, 0.0851, 0.0000,  ..., 0.0000, 0.0700, 0.0000],
        [0.0079, 0.0851, 0.0000,  ..., 0.0000, 0.0700, 0.0000],
        [0.0079, 0.0850, 0.0000,  ..., 0.0000, 0.0699, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693127.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3744.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-297.5389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(269.1613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4181.0791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.2311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3667],
        [ 0.3667],
        [ 0.3649],
        ...,
        [-0.9814],
        [-0.9780],
        [-0.9765]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182871.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0088],
        [1.0050],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369508.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(483.1823, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0088],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369520.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2927e-02,  2.4794e-02,  4.2158e-02,  ...,  4.5993e-03,
         -1.5676e-02, -2.8890e-02],
        [ 1.5438e-02,  1.6282e-02,  2.8397e-02,  ...,  2.6577e-03,
         -1.0555e-02, -1.9452e-02],
        [ 8.1877e-03,  8.0404e-03,  1.5074e-02,  ...,  7.7777e-04,
         -5.5966e-03, -1.0314e-02],
        ...,
        [ 3.8768e-06, -1.2619e-03,  3.5547e-05,  ..., -1.3441e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.8768e-06, -1.2619e-03,  3.5547e-05,  ..., -1.3441e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.8768e-06, -1.2619e-03,  3.5547e-05,  ..., -1.3441e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2990.9832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7168, device='cuda:0')



h[100].sum tensor(-6.1673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6921, device='cuda:0')



h[200].sum tensor(-28.5595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1560e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.0146e-02, 7.4622e-02, 1.2901e-01,  ..., 1.2759e-02, 0.0000e+00,
         0.0000e+00],
        [6.9119e-02, 7.3441e-02, 1.2712e-01,  ..., 1.2478e-02, 0.0000e+00,
         0.0000e+00],
        [3.3906e-02, 3.5977e-02, 6.2418e-02,  ..., 6.0761e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.6162e-05, 0.0000e+00, 1.4819e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.6152e-05, 0.0000e+00, 1.4810e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.6146e-05, 0.0000e+00, 1.4805e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70171.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2479, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2323, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1653, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0078, 0.0853, 0.0000,  ..., 0.0000, 0.0704, 0.0000],
        [0.0078, 0.0853, 0.0000,  ..., 0.0000, 0.0703, 0.0000],
        [0.0078, 0.0852, 0.0000,  ..., 0.0000, 0.0703, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681519.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3456.4976, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-271.2629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(239.5407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4212.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.9452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3606],
        [ 0.3576],
        [ 0.3479],
        ...,
        [-0.9861],
        [-0.9826],
        [-0.9811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182433.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0088],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369520.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0089],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369532.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7342e-05, -1.2687e-03,  4.2253e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.7342e-05, -1.2687e-03,  4.2253e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.7342e-05, -1.2687e-03,  4.2253e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.7342e-05, -1.2687e-03,  4.2253e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.7342e-05, -1.2687e-03,  4.2253e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.7342e-05, -1.2687e-03,  4.2253e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3142.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9979, device='cuda:0')



h[100].sum tensor(-6.8104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.7264, device='cuda:0')



h[200].sum tensor(-31.5992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2969e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.9989e-05, 0.0000e+00, 1.7052e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.0176e-05, 0.0000e+00, 1.7098e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.9936e-05, 0.0000e+00, 1.7039e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.2307e-05, 0.0000e+00, 1.7617e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.2264e-05, 0.0000e+00, 1.7607e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.2239e-05, 0.0000e+00, 1.7600e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72669.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0071, 0.0822, 0.0000,  ..., 0.0000, 0.0679, 0.0000],
        [0.0072, 0.0825, 0.0000,  ..., 0.0000, 0.0681, 0.0000],
        [0.0071, 0.0822, 0.0000,  ..., 0.0000, 0.0678, 0.0000],
        ...,
        [0.0075, 0.0853, 0.0000,  ..., 0.0000, 0.0705, 0.0000],
        [0.0074, 0.0852, 0.0000,  ..., 0.0000, 0.0704, 0.0000],
        [0.0074, 0.0852, 0.0000,  ..., 0.0000, 0.0704, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690850.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3622.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-286.6747, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(261.4883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4115.2256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(37.2444, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1134],
        [-1.0610],
        [-0.9656],
        ...,
        [-0.9886],
        [-0.9852],
        [-0.9837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177654.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0089],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369532.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0089],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369544.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2835.4146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.5240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7361, device='cuda:0')



h[100].sum tensor(-5.1875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0338, device='cuda:0')



h[200].sum tensor(-24.1161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7194e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66858.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8208e-02, 6.1202e-02, 0.0000e+00,  ..., 0.0000e+00, 5.1523e-02,
         0.0000e+00],
        [7.8011e-03, 8.0358e-02, 0.0000e+00,  ..., 0.0000e+00, 6.6529e-02,
         0.0000e+00],
        [6.8717e-03, 8.2089e-02, 0.0000e+00,  ..., 0.0000e+00, 6.7875e-02,
         0.0000e+00],
        ...,
        [7.1808e-03, 8.5156e-02, 4.5733e-05,  ..., 0.0000e+00, 7.0533e-02,
         0.0000e+00],
        [7.1756e-03, 8.5102e-02, 4.3183e-05,  ..., 0.0000e+00, 7.0486e-02,
         0.0000e+00],
        [7.1716e-03, 8.5066e-02, 4.0348e-05,  ..., 0.0000e+00, 7.0455e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665669.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3056.3481, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-240.9279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.7263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4061.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.3315, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3641],
        [-0.6054],
        [-0.7464],
        ...,
        [-0.9901],
        [-0.9866],
        [-0.9850]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184065.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0089],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369544.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0089],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369544.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.9464e-03,  7.7306e-03,  1.4602e-02,  ...,  7.0883e-04,
         -5.4001e-03, -9.9642e-03],
        [ 9.2648e-03,  9.2290e-03,  1.7024e-02,  ...,  1.0506e-03,
         -6.2989e-03, -1.1623e-02],
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6081e-05, -1.2713e-03,  4.7201e-05,  ..., -1.3446e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3245.9873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.9697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9279, device='cuda:0')



h[100].sum tensor(-7.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.5557, device='cuda:0')



h[200].sum tensor(-33.4589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3544e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.0083e-02, 8.5769e-02, 1.4716e-01,  ..., 1.5308e-02, 0.0000e+00,
         0.0000e+00],
        [2.8100e-02, 2.9244e-02, 5.1635e-02,  ..., 4.5355e-03, 0.0000e+00,
         0.0000e+00],
        [9.4263e-03, 9.3114e-03, 1.7319e-02,  ..., 1.0600e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0876e-04, 0.0000e+00, 1.9683e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0869e-04, 0.0000e+00, 1.9671e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0865e-04, 0.0000e+00, 1.9664e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74981.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.5506e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.0626e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0497e-01, 1.2944e-02, 0.0000e+00,  ..., 0.0000e+00, 1.0876e-02,
         0.0000e+00],
        ...,
        [7.1808e-03, 8.5156e-02, 4.5733e-05,  ..., 0.0000e+00, 7.0533e-02,
         0.0000e+00],
        [7.1756e-03, 8.5102e-02, 4.3183e-05,  ..., 0.0000e+00, 7.0486e-02,
         0.0000e+00],
        [7.1716e-03, 8.5066e-02, 4.0348e-05,  ..., 0.0000e+00, 7.0455e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698681.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3715.8103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-300.0651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.4910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3900.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.8684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2542],
        [ 0.2800],
        [ 0.2959],
        ...,
        [-0.9851],
        [-0.9819],
        [-0.9807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169106.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0089],
        [1.0051],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369544.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0090],
        [1.0052],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369556.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5874e-05, -1.2689e-03,  4.7166e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.5874e-05, -1.2689e-03,  4.7166e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.5874e-05, -1.2689e-03,  4.7166e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.5874e-05, -1.2689e-03,  4.7166e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.5874e-05, -1.2689e-03,  4.7166e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.5874e-05, -1.2689e-03,  4.7166e-05,  ..., -1.3450e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3002.1929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9807, device='cuda:0')



h[100].sum tensor(-5.8838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.0356, device='cuda:0')



h[200].sum tensor(-27.4072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1106e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0082, 0.0066, 0.0150,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71204.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0578, 0.0205, 0.0000,  ..., 0.0000, 0.0176, 0.0000],
        [0.0786, 0.0118, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.1017, 0.0051, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        ...,
        [0.0070, 0.0851, 0.0001,  ..., 0.0000, 0.0704, 0.0000],
        [0.0070, 0.0851, 0.0001,  ..., 0.0000, 0.0704, 0.0000],
        [0.0070, 0.0850, 0.0001,  ..., 0.0000, 0.0704, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683113.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3397.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-270.6691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(246.7391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3739.7551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(33.7630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3025],
        [ 0.3300],
        [ 0.3482],
        ...,
        [-0.9929],
        [-0.9895],
        [-0.9880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159682.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0090],
        [1.0052],
        ...,
        [1.0021],
        [1.0012],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369556.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0090],
        [1.0052],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369568.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5555e-05, -1.2616e-03,  3.9386e-05,  ..., -1.3453e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5555e-05, -1.2616e-03,  3.9386e-05,  ..., -1.3453e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5838e-02,  1.6724e-02,  2.9115e-02,  ...,  2.7569e-03,
         -1.0757e-02, -1.9872e-02],
        ...,
        [ 1.5555e-05, -1.2616e-03,  3.9386e-05,  ..., -1.3453e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5555e-05, -1.2616e-03,  3.9386e-05,  ..., -1.3453e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5555e-05, -1.2616e-03,  3.9386e-05,  ..., -1.3453e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2864.3943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.0043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8008, device='cuda:0')



h[100].sum tensor(-5.1092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0915, device='cuda:0')



h[200].sum tensor(-23.8458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7593e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.2785e-05, 0.0000e+00, 1.5898e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.6044e-02, 1.6892e-02, 2.9526e-02,  ..., 2.7845e-03, 0.0000e+00,
         0.0000e+00],
        [1.3096e-02, 1.3543e-02, 2.4108e-02,  ..., 2.0218e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.4881e-05, 0.0000e+00, 1.6429e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.4841e-05, 0.0000e+00, 1.6419e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.4818e-05, 0.0000e+00, 1.6413e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65000.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0218, 0.0567, 0.0000,  ..., 0.0000, 0.0477, 0.0000],
        [0.0589, 0.0253, 0.0000,  ..., 0.0000, 0.0224, 0.0000],
        [0.0800, 0.0087, 0.0000,  ..., 0.0000, 0.0084, 0.0000],
        ...,
        [0.0070, 0.0853, 0.0000,  ..., 0.0000, 0.0704, 0.0000],
        [0.0070, 0.0852, 0.0000,  ..., 0.0000, 0.0703, 0.0000],
        [0.0070, 0.0852, 0.0000,  ..., 0.0000, 0.0703, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649409.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2701.5464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-224.0665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(190.1553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3844.4829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(32.9410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7881],
        [-0.5537],
        [-0.3136],
        ...,
        [-0.9970],
        [-0.9936],
        [-0.9921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174365.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0090],
        [1.0052],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369568.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0091],
        [1.0053],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369580.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3156e-02,  1.3694e-02,  2.4195e-02,  ...,  2.0634e-03,
         -8.9262e-03, -1.6501e-02],
        [ 1.0331e-02,  1.0482e-02,  1.9003e-02,  ...,  1.3308e-03,
         -7.0081e-03, -1.2955e-02],
        [ 2.8553e-02,  3.1197e-02,  5.2488e-02,  ...,  6.0552e-03,
         -1.9378e-02, -3.5822e-02],
        ...,
        [ 6.8898e-06, -1.2545e-03,  3.2523e-05,  ..., -1.3457e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.8898e-06, -1.2545e-03,  3.2523e-05,  ..., -1.3457e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.8898e-06, -1.2545e-03,  3.2523e-05,  ..., -1.3457e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3167.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8092, device='cuda:0')



h[100].sum tensor(-6.4737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.6662, device='cuda:0')



h[200].sum tensor(-30.2736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2235e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.9788e-02, 3.1299e-02, 5.4817e-02,  ..., 4.9982e-03, 0.0000e+00,
         0.0000e+00],
        [8.0458e-02, 8.6357e-02, 1.4793e-01,  ..., 1.5406e-02, 0.0000e+00,
         0.0000e+00],
        [7.5185e-02, 8.0380e-02, 1.3823e-01,  ..., 1.4057e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.8742e-05, 0.0000e+00, 1.3568e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.8725e-05, 0.0000e+00, 1.3559e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.8714e-05, 0.0000e+00, 1.3555e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72153.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5268e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1370e-04,
         0.0000e+00],
        [2.5770e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.9501e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.9997e-03, 8.5455e-02, 0.0000e+00,  ..., 0.0000e+00, 7.0349e-02,
         0.0000e+00],
        [6.9944e-03, 8.5399e-02, 0.0000e+00,  ..., 0.0000e+00, 7.0301e-02,
         0.0000e+00],
        [6.9905e-03, 8.5363e-02, 0.0000e+00,  ..., 0.0000e+00, 7.0270e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687118.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3417.4053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-278.9492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(257.7495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3863.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(35.0975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2359],
        [ 0.3480],
        [ 0.3712],
        ...,
        [-1.0018],
        [-0.9983],
        [-0.9968]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183057.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0091],
        [1.0053],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369580.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0091],
        [1.0053],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369592.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7783e-03,  7.5963e-03,  1.4321e-02,  ...,  6.7136e-04,
         -5.2737e-03, -9.7549e-03],
        [-2.0626e-06, -1.2490e-03,  2.4571e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.0726e-03,  3.3834e-03,  7.5118e-03,  ..., -2.8938e-04,
         -2.7619e-03, -5.1088e-03],
        ...,
        [ 1.3526e-02,  1.4131e-02,  2.4882e-02,  ...,  2.1615e-03,
         -9.1697e-03, -1.6961e-02],
        [-2.0626e-06, -1.2490e-03,  2.4571e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [-2.0626e-06, -1.2490e-03,  2.4571e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3572.5046, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4550, device='cuda:0')



h[100].sum tensor(-8.3283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.7013, device='cuda:0')



h[200].sum tensor(-39.0230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5722e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0287, 0.0525,  ..., 0.0033, 0.0000, 0.0000],
        [0.0218, 0.0197, 0.0401,  ..., 0.0010, 0.0000, 0.0000],
        [0.0115, 0.0093, 0.0212,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0115, 0.0118, 0.0213,  ..., 0.0016, 0.0000, 0.0000],
        [0.0141, 0.0147, 0.0260,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79830.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1450, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1063, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0741, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0632, 0.0127, 0.0000,  ..., 0.0000, 0.0117, 0.0000],
        [0.0510, 0.0305, 0.0000,  ..., 0.0000, 0.0266, 0.0000],
        [0.0207, 0.0620, 0.0000,  ..., 0.0000, 0.0519, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(721712.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4072.8127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-335.0620, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(329.2164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3748.6821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(35.0949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3841],
        [ 0.3926],
        [ 0.3987],
        ...,
        [-0.2231],
        [-0.4333],
        [-0.6802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175335.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0091],
        [1.0053],
        ...,
        [1.0021],
        [1.0012],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369592.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0092],
        [1.0054],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369603.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.3475e-06, -1.2450e-03,  1.4006e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3475e-06, -1.2450e-03,  1.4006e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3475e-06, -1.2450e-03,  1.4006e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-8.3475e-06, -1.2450e-03,  1.4006e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3475e-06, -1.2450e-03,  1.4006e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00],
        [-8.3475e-06, -1.2450e-03,  1.4006e-05,  ..., -1.3458e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3212.7683, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.0228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3760, device='cuda:0')



h[100].sum tensor(-6.5535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.1718, device='cuda:0')



h[200].sum tensor(-30.7679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2585e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 5.6541e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.6692e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.6496e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.8444e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.8407e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.8387e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74921.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0068, 0.0830, 0.0000,  ..., 0.0000, 0.0681, 0.0000],
        [0.0068, 0.0832, 0.0000,  ..., 0.0000, 0.0682, 0.0000],
        [0.0068, 0.0830, 0.0000,  ..., 0.0000, 0.0680, 0.0000],
        ...,
        [0.0071, 0.0861, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0071, 0.0861, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0071, 0.0860, 0.0000,  ..., 0.0000, 0.0706, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709417., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3826.5857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-299.2145, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(283.6443, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3755.6274, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(33.5355, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1647],
        [-1.1673],
        [-1.1608],
        ...,
        [-1.0142],
        [-1.0107],
        [-1.0091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177363.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0092],
        [1.0054],
        ...,
        [1.0021],
        [1.0011],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369603.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0093],
        [1.0054],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369614.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4301e-05, -1.2412e-03, -2.9674e-06,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4301e-05, -1.2412e-03, -2.9674e-06,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4301e-05, -1.2412e-03, -2.9674e-06,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.4301e-05, -1.2412e-03, -2.9674e-06,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4301e-05, -1.2412e-03, -2.9674e-06,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4301e-05, -1.2412e-03, -2.9674e-06,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3577.3323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.4814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5509, device='cuda:0')



h[100].sum tensor(-8.2715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.7868, device='cuda:0')



h[200].sum tensor(-38.9103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5781e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81439.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0069, 0.0835, 0.0000,  ..., 0.0000, 0.0687, 0.0000],
        [0.0070, 0.0833, 0.0000,  ..., 0.0000, 0.0686, 0.0000],
        [0.0074, 0.0819, 0.0000,  ..., 0.0000, 0.0673, 0.0000],
        ...,
        [0.0072, 0.0866, 0.0000,  ..., 0.0000, 0.0714, 0.0000],
        [0.0072, 0.0865, 0.0000,  ..., 0.0000, 0.0714, 0.0000],
        [0.0072, 0.0865, 0.0000,  ..., 0.0000, 0.0713, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(736039.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4374.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-345.4761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(342.7092, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3781.7651, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(32.5190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0865],
        [-1.0007],
        [-0.8706],
        ...,
        [-1.0226],
        [-1.0189],
        [-1.0173]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173707.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0093],
        [1.0054],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369614.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(476.2893, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0093],
        [1.0055],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369625.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0833e-05, -1.2427e-03, -1.2501e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0833e-05, -1.2427e-03, -1.2501e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0833e-05, -1.2427e-03, -1.2501e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 7.0115e-03,  6.7392e-03,  1.2892e-02,  ...,  4.7705e-04,
         -4.7381e-03, -8.7803e-03],
        [-1.0833e-05, -1.2427e-03, -1.2501e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0833e-05, -1.2427e-03, -1.2501e-05,  ..., -1.3435e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3396.0522, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0401, device='cuda:0')



h[100].sum tensor(-7.3840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.5476, device='cuda:0')



h[200].sum tensor(-34.8042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4231e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0222, 0.0227, 0.0409,  ..., 0.0032, 0.0000, 0.0000],
        [0.0073, 0.0070, 0.0135,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78488.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0128, 0.0734, 0.0000,  ..., 0.0000, 0.0613, 0.0000],
        [0.0281, 0.0453, 0.0000,  ..., 0.0000, 0.0397, 0.0000],
        [0.0469, 0.0218, 0.0000,  ..., 0.0000, 0.0192, 0.0000],
        ...,
        [0.1261, 0.0023, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0671, 0.0276, 0.0000,  ..., 0.0000, 0.0238, 0.0000],
        [0.0254, 0.0548, 0.0000,  ..., 0.0000, 0.0473, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(732885.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4288.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-323.0213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.6350, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3790.1255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.6916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1512],
        [ 0.0968],
        [ 0.2725],
        ...,
        [ 0.2361],
        [-0.0765],
        [-0.4647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171433.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0093],
        [1.0055],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369625.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0094],
        [1.0056],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369637.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9820e-07, -1.2484e-03, -1.4510e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9820e-07, -1.2484e-03, -1.4510e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9820e-07, -1.2484e-03, -1.4510e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 9.9820e-07, -1.2484e-03, -1.4510e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9820e-07, -1.2484e-03, -1.4510e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.9820e-07, -1.2484e-03, -1.4510e-05,  ..., -1.3436e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2902.0239, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.3725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7695, device='cuda:0')



h[100].sum tensor(-4.9833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0636, device='cuda:0')



h[200].sum tensor(-23.5348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7400e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.5296e-03, 5.0194e-03, 1.0139e-02,  ..., 7.5724e-05, 0.0000e+00,
         0.0000e+00],
        [4.0412e-06, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.0272e-06, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.1671e-06, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.1644e-06, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.1630e-06, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64924.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0386, 0.0000,  ..., 0.0000, 0.0336, 0.0000],
        [0.0139, 0.0698, 0.0000,  ..., 0.0000, 0.0589, 0.0000],
        [0.0087, 0.0798, 0.0000,  ..., 0.0000, 0.0666, 0.0000],
        ...,
        [0.0067, 0.0876, 0.0000,  ..., 0.0000, 0.0728, 0.0000],
        [0.0067, 0.0876, 0.0000,  ..., 0.0000, 0.0728, 0.0000],
        [0.0067, 0.0875, 0.0000,  ..., 0.0000, 0.0727, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658834.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2719.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-221.4566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(187.0862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4030.0884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.0079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0970],
        [-0.3563],
        [-0.4989],
        ...,
        [-1.0398],
        [-1.0360],
        [-1.0344]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198442.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0094],
        [1.0056],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369637.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0095],
        [1.0056],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369649.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3994e-03,  7.1291e-03,  1.3545e-02,  ...,  5.6819e-04,
         -4.9625e-03, -9.2075e-03],
        [ 2.1568e-05, -1.2552e-03, -1.2593e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1037e-02,  2.2627e-02,  3.8607e-02,  ...,  4.1033e-03,
         -1.4136e-02, -2.6227e-02],
        ...,
        [ 2.1568e-05, -1.2552e-03, -1.2593e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1568e-05, -1.2552e-03, -1.2593e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.1568e-05, -1.2552e-03, -1.2593e-05,  ..., -1.3443e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3234.8899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.0187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3629, device='cuda:0')



h[100].sum tensor(-6.4739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.1601, device='cuda:0')



h[200].sum tensor(-30.6356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2577e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.1608e-03, 5.6367e-03, 1.1149e-02,  ..., 2.1905e-04, 0.0000e+00,
         0.0000e+00],
        [4.0995e-02, 4.1406e-02, 7.5123e-02,  ..., 5.1612e-03, 0.0000e+00,
         0.0000e+00],
        [2.0376e-02, 2.0524e-02, 3.7259e-02,  ..., 2.5468e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [9.0051e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.9992e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.9961e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74098.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1017, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1360, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1335, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0062, 0.0879, 0.0000,  ..., 0.0000, 0.0732, 0.0000],
        [0.0061, 0.0878, 0.0000,  ..., 0.0000, 0.0732, 0.0000],
        [0.0061, 0.0878, 0.0000,  ..., 0.0000, 0.0732, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700240.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3437.5933, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-282.5249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(268.9398, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3573.9927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(29.5055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4536],
        [ 0.4565],
        [ 0.4585],
        ...,
        [-1.0465],
        [-1.0428],
        [-1.0411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165901.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0095],
        [1.0056],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369649.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0096],
        [1.0057],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369660.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9530e-03,  1.0030e-02,  1.8226e-02,  ...,  1.2273e-03,
         -6.6671e-03, -1.2378e-02],
        [ 6.2861e-03,  5.8624e-03,  1.1488e-02,  ...,  2.7686e-04,
         -4.2043e-03, -7.8055e-03],
        [ 4.9131e-03,  4.3020e-03,  8.9650e-03,  ..., -7.9011e-05,
         -3.2822e-03, -6.0935e-03],
        ...,
        [ 2.6338e-05, -1.2518e-03, -1.4755e-05,  ..., -1.3457e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6338e-05, -1.2518e-03, -1.4755e-05,  ..., -1.3457e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.6338e-05, -1.2518e-03, -1.4755e-05,  ..., -1.3457e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2892.9658, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.5128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8962, device='cuda:0')



h[100].sum tensor(-4.7372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2848, device='cuda:0')



h[200].sum tensor(-22.4614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.2006e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0303, 0.0571,  ..., 0.0026, 0.0000, 0.0000],
        [0.0284, 0.0270, 0.0519,  ..., 0.0023, 0.0000, 0.0000],
        [0.0105, 0.0092, 0.0190,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64897.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0904, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0754, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0443, 0.0237, 0.0000,  ..., 0.0000, 0.0208, 0.0000],
        ...,
        [0.0057, 0.0880, 0.0000,  ..., 0.0000, 0.0731, 0.0000],
        [0.0057, 0.0879, 0.0000,  ..., 0.0000, 0.0730, 0.0000],
        [0.0057, 0.0879, 0.0000,  ..., 0.0000, 0.0730, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659857.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2507.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-218.1985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(186.7221, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3752.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(31.7084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3995],
        [ 0.2309],
        [-0.0743],
        ...,
        [-1.0501],
        [-1.0466],
        [-1.0452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191711.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0096],
        [1.0057],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369660.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0097],
        [1.0057],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369672.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9130e-05, -1.2473e-03, -1.6757e-05,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.9130e-05, -1.2473e-03, -1.6757e-05,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.9130e-05, -1.2473e-03, -1.6757e-05,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.9130e-05, -1.2473e-03, -1.6757e-05,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.9130e-05, -1.2473e-03, -1.6757e-05,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.9130e-05, -1.2473e-03, -1.6757e-05,  ..., -1.3468e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3239.6892, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.5892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5665, device='cuda:0')



h[100].sum tensor(-6.2312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.4498, device='cuda:0')



h[200].sum tensor(-29.6041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2085e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0216, 0.0219, 0.0394,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73220.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0126, 0.0714, 0.0000,  ..., 0.0000, 0.0599, 0.0000],
        [0.0336, 0.0409, 0.0000,  ..., 0.0000, 0.0354, 0.0000],
        [0.1000, 0.0106, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        ...,
        [0.0053, 0.0880, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0053, 0.0879, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0053, 0.0879, 0.0000,  ..., 0.0000, 0.0728, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695312.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3139.0913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-280.3546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(265.5938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3499.5229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.2259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3852],
        [-0.0384],
        [ 0.2342],
        ...,
        [-1.0558],
        [-1.0521],
        [-1.0504]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175568.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0097],
        [1.0057],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369672.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0097],
        [1.0058],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369684.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2342e-03,  4.6820e-03,  9.5516e-03,  ...,  2.5332e-06,
         -3.4889e-03, -6.4854e-03],
        [ 2.4593e-05, -1.2402e-03, -2.0465e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4593e-05, -1.2402e-03, -2.0465e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.4593e-05, -1.2402e-03, -2.0465e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4593e-05, -1.2402e-03, -2.0465e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4593e-05, -1.2402e-03, -2.0465e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2957.4353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.7816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2660, device='cuda:0')



h[100].sum tensor(-4.8106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.6145, device='cuda:0')



h[200].sum tensor(-22.9005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.4290e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7254e-02, 1.5747e-02, 3.1457e-02,  ..., 7.4063e-04, 0.0000e+00,
         0.0000e+00],
        [1.3509e-02, 1.2731e-02, 2.4597e-02,  ..., 7.4478e-04, 0.0000e+00,
         0.0000e+00],
        [9.9243e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0273e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0267e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0262e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67816.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0627, 0.0047, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0500, 0.0175, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0228, 0.0474, 0.0000,  ..., 0.0000, 0.0413, 0.0000],
        ...,
        [0.0051, 0.0882, 0.0000,  ..., 0.0000, 0.0728, 0.0000],
        [0.0051, 0.0882, 0.0000,  ..., 0.0000, 0.0728, 0.0000],
        [0.0051, 0.0881, 0.0000,  ..., 0.0000, 0.0727, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680797.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2799.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-245.8702, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(219.3914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3641.8694, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(38.0593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3191],
        [ 0.2191],
        [ 0.0286],
        ...,
        [-1.0624],
        [-1.0587],
        [-1.0570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196935.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0097],
        [1.0058],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369684.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0098],
        [1.0058],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369695.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5696e-05, -1.2325e-03, -2.3907e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5696e-05, -1.2325e-03, -2.3907e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5696e-05, -1.2325e-03, -2.3907e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5696e-05, -1.2325e-03, -2.3907e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5696e-05, -1.2325e-03, -2.3907e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5696e-05, -1.2325e-03, -2.3907e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3308.0930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1839, device='cuda:0')



h[100].sum tensor(-6.3468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.0005, device='cuda:0')



h[200].sum tensor(-30.2736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2467e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.4567e-03, 4.7833e-03, 1.1698e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0306e-02, 9.1474e-03, 1.8770e-02,  ..., 4.5623e-04, 0.0000e+00,
         0.0000e+00],
        [1.4069e-02, 1.3437e-02, 2.5684e-02,  ..., 9.0923e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.5576e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5535e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5506e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74094.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0098, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        [0.0467, 0.0115, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0522, 0.0079, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        ...,
        [0.0050, 0.0885, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0050, 0.0885, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0050, 0.0884, 0.0000,  ..., 0.0000, 0.0726, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704800.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3207.9260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-293.7683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(279.8813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3502.7515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.5876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3553],
        [ 0.3292],
        [ 0.2704],
        ...,
        [-1.0697],
        [-1.0659],
        [-1.0642]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187394.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0098],
        [1.0058],
        ...,
        [1.0021],
        [1.0011],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369695.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0099],
        [1.0059],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369706.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0956e-03,  5.6899e-03,  1.1151e-02,  ...,  2.2818e-04,
         -4.0622e-03, -7.5605e-03],
        [ 1.2487e-05, -1.2273e-03, -2.4530e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.8758e-02,  2.0088e-02,  3.4414e-02,  ...,  3.5102e-03,
         -1.2518e-02, -2.3297e-02],
        ...,
        [ 1.2487e-05, -1.2273e-03, -2.4530e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2487e-05, -1.2273e-03, -2.4530e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2487e-05, -1.2273e-03, -2.4530e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3078.8696, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5053, device='cuda:0')



h[100].sum tensor(-5.2080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7198, device='cuda:0')



h[200].sum tensor(-24.8914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0194e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.0593e-03, 4.4580e-03, 9.1775e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.5153e-02, 2.6061e-02, 4.6068e-02,  ..., 3.7785e-03, 0.0000e+00,
         0.0000e+00],
        [1.5500e-02, 1.6329e-02, 2.8358e-02,  ..., 2.6432e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.2176e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.2143e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.2119e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67693.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0534, 0.0022, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0836, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0841, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        ...,
        [0.0048, 0.0888, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0048, 0.0888, 0.0000,  ..., 0.0000, 0.0726, 0.0000],
        [0.0048, 0.0887, 0.0000,  ..., 0.0000, 0.0726, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669629.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2477.0217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-247.5982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.3479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3504.1948, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.8276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3899],
        [ 0.3762],
        [ 0.2898],
        ...,
        [-1.0769],
        [-1.0732],
        [-1.0714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188636.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0099],
        [1.0059],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369706.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0099],
        [1.0059],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369718.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8456e-03,  5.4050e-03,  1.0687e-02,  ...,  1.6210e-04,
         -3.8877e-03, -7.2401e-03],
        [ 1.5337e-05, -1.2251e-03, -2.3669e-05,  ..., -1.3491e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5337e-05, -1.2251e-03, -2.3669e-05,  ..., -1.3491e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5337e-05, -1.2251e-03, -2.3669e-05,  ..., -1.3491e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5337e-05, -1.2251e-03, -2.3669e-05,  ..., -1.3491e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5337e-05, -1.2251e-03, -2.3669e-05,  ..., -1.3491e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3090.1189, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.8906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5619, device='cuda:0')



h[100].sum tensor(-5.1975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7703, device='cuda:0')



h[200].sum tensor(-24.8908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0229e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.2516e-02, 1.1689e-02, 2.2831e-02,  ..., 7.7455e-04, 0.0000e+00,
         0.0000e+00],
        [5.9649e-03, 5.4722e-03, 1.0820e-02,  ..., 1.6411e-04, 0.0000e+00,
         0.0000e+00],
        [6.1899e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.4095e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.4054e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.4024e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68901.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0784, 0.0150, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0365, 0.0361, 0.0000,  ..., 0.0000, 0.0308, 0.0000],
        [0.0133, 0.0667, 0.0004,  ..., 0.0000, 0.0549, 0.0000],
        ...,
        [0.0046, 0.0892, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0046, 0.0891, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0046, 0.0891, 0.0000,  ..., 0.0000, 0.0726, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677580., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2583.9834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-256.7559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(233.9064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3386.0493, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.6936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2353],
        [ 0.0216],
        [-0.2541],
        ...,
        [-1.0847],
        [-1.0810],
        [-1.0792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181144.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0099],
        [1.0059],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369718.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0100],
        [1.0060],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369729.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3939e-03,  4.8925e-03,  9.8568e-03,  ...,  4.4018e-05,
         -3.5810e-03, -6.6730e-03],
        [ 1.5570e-05, -1.2239e-03, -2.3564e-05,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5570e-05, -1.2239e-03, -2.3564e-05,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5570e-05, -1.2239e-03, -2.3564e-05,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5570e-05, -1.2239e-03, -2.3564e-05,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5570e-05, -1.2239e-03, -2.3564e-05,  ..., -1.3500e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2986.7480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.3175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0249, device='cuda:0')



h[100].sum tensor(-4.7113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3995, device='cuda:0')



h[200].sum tensor(-22.6078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.2801e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.3193e-02, 1.2460e-02, 2.4073e-02,  ..., 7.9907e-04, 0.0000e+00,
         0.0000e+00],
        [5.5087e-03, 4.9536e-03, 9.9800e-03,  ..., 4.4568e-05, 0.0000e+00,
         0.0000e+00],
        [6.2844e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.5080e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5039e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.5008e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66962.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0591, 0.0136, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0371, 0.0281, 0.0000,  ..., 0.0000, 0.0248, 0.0000],
        [0.0242, 0.0480, 0.0000,  ..., 0.0000, 0.0408, 0.0000],
        ...,
        [0.0044, 0.0899, 0.0000,  ..., 0.0000, 0.0731, 0.0000],
        [0.0044, 0.0899, 0.0000,  ..., 0.0000, 0.0730, 0.0000],
        [0.0044, 0.0898, 0.0000,  ..., 0.0000, 0.0730, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671526.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2394.0232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-242.8827, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.9914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3348.6948, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.8733, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3105],
        [ 0.2490],
        [ 0.2033],
        ...,
        [-1.0950],
        [-1.0914],
        [-1.0897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180604.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0100],
        [1.0060],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369729.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(524.8451, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0100],
        [1.0060],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369740.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1690e-02,  2.3430e-02,  3.9797e-02,  ...,  4.2675e-03,
         -1.4411e-02, -2.6872e-02],
        [ 1.2718e-05, -1.2223e-03, -2.3721e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.3471e-02,  1.4083e-02,  2.4698e-02,  ...,  2.1372e-03,
         -8.9470e-03, -1.6683e-02],
        ...,
        [ 1.2718e-05, -1.2223e-03, -2.3721e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2718e-05, -1.2223e-03, -2.3721e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2718e-05, -1.2223e-03, -2.3721e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3468.7168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2661, device='cuda:0')



h[100].sum tensor(-6.9153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.8574, device='cuda:0')



h[200].sum tensor(-33.2504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3753e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.9861e-02, 6.4317e-02, 1.0980e-01,  ..., 1.1411e-02, 0.0000e+00,
         0.0000e+00],
        [7.1186e-02, 7.5945e-02, 1.3058e-01,  ..., 1.2965e-02, 0.0000e+00,
         0.0000e+00],
        [1.6471e-02, 1.6208e-02, 3.0115e-02,  ..., 1.5313e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.3166e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3132e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3106e-05, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76381.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3073, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2429, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1331, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0909, 0.0000,  ..., 0.0000, 0.0736, 0.0000],
        [0.0043, 0.0908, 0.0000,  ..., 0.0000, 0.0735, 0.0000],
        [0.0043, 0.0908, 0.0000,  ..., 0.0000, 0.0735, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716027.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3164.9158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-315.3148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(302.8802, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3480.8950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.9619, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2559],
        [ 0.2868],
        [ 0.3174],
        ...,
        [-1.1077],
        [-1.1039],
        [-1.1021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199865.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0100],
        [1.0060],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369740.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0101],
        [1.0060],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369752.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4632e-05, -1.2234e-03, -1.8629e-05,  ..., -1.3512e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2453e-02,  1.2910e-02,  2.2811e-02,  ...,  1.8698e-03,
         -8.2497e-03, -1.5392e-02],
        [ 1.2433e-02,  1.2888e-02,  2.2776e-02,  ...,  1.8647e-03,
         -8.2368e-03, -1.5368e-02],
        ...,
        [ 2.4632e-05, -1.2234e-03, -1.8629e-05,  ..., -1.3512e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4632e-05, -1.2234e-03, -1.8629e-05,  ..., -1.3512e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4632e-05, -1.2234e-03, -1.8629e-05,  ..., -1.3512e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3207.3105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3426, device='cuda:0')



h[100].sum tensor(-5.6630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.3583, device='cuda:0')



h[200].sum tensor(-27.2835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1329e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.2677e-02, 1.3065e-02, 2.3085e-02,  ..., 1.8922e-03, 0.0000e+00,
         0.0000e+00],
        [2.2921e-02, 2.3477e-02, 4.1884e-02,  ..., 3.1801e-03, 0.0000e+00,
         0.0000e+00],
        [7.0251e-02, 7.4839e-02, 1.2879e-01,  ..., 1.2726e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0299e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0293e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0288e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69567.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0581, 0.0270, 0.0000,  ..., 0.0000, 0.0227, 0.0000],
        [0.1059, 0.0061, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.1895, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0911, 0.0000,  ..., 0.0000, 0.0736, 0.0000],
        [0.0041, 0.0910, 0.0000,  ..., 0.0000, 0.0735, 0.0000],
        [0.0041, 0.0909, 0.0000,  ..., 0.0000, 0.0735, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683317.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2531.2993, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-262.2373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(238.2808, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3420.5317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(43.1155, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1292],
        [ 0.0651],
        [ 0.2531],
        ...,
        [-1.1140],
        [-1.1102],
        [-1.1083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185531.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0101],
        [1.0060],
        ...,
        [1.0021],
        [1.0011],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369752.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0101],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369764.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.0260e-03,  7.8651e-03,  1.4667e-02,  ...,  7.1980e-04,
         -5.2972e-03, -9.8897e-03],
        [ 2.6479e-02,  2.8849e-02,  4.8563e-02,  ...,  5.5018e-03,
         -1.7527e-02, -3.2722e-02],
        [ 1.2374e-02,  1.2809e-02,  2.2654e-02,  ...,  1.8466e-03,
         -8.1789e-03, -1.5270e-02],
        ...,
        [ 3.3421e-05, -1.2238e-03, -1.4704e-05,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.3421e-05, -1.2238e-03, -1.4704e-05,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.3421e-05, -1.2238e-03, -1.4704e-05,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2952.7314, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.0850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3618, device='cuda:0')



h[100].sum tensor(-4.4236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8081, device='cuda:0')



h[200].sum tensor(-21.3554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8705e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.9706e-02, 8.5542e-02, 1.4611e-01,  ..., 1.5160e-02, 0.0000e+00,
         0.0000e+00],
        [5.9490e-02, 6.2539e-02, 1.0897e-01,  ..., 9.9069e-03, 0.0000e+00,
         0.0000e+00],
        [6.0386e-02, 6.3576e-02, 1.1062e-01,  ..., 1.0158e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.3976e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.3967e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.3960e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65655.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2085, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2151, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2117, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0910, 0.0000,  ..., 0.0000, 0.0735, 0.0000],
        [0.0039, 0.0909, 0.0000,  ..., 0.0000, 0.0735, 0.0000],
        [0.0039, 0.0909, 0.0000,  ..., 0.0000, 0.0734, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671802.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2243.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-233.2786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(201.6967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3592.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.8595, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3468],
        [ 0.3396],
        [ 0.3297],
        ...,
        [-1.1186],
        [-1.1147],
        [-1.1129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202185.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0101],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369764.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0102],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369775.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.0340e-05, -1.2229e-03, -1.0417e-05,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.0340e-05, -1.2229e-03, -1.0417e-05,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.0340e-05, -1.2229e-03, -1.0417e-05,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 4.0340e-05, -1.2229e-03, -1.0417e-05,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.0340e-05, -1.2229e-03, -1.0417e-05,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.0340e-05, -1.2229e-03, -1.0417e-05,  ..., -1.3516e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3679.6553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.5912, device='cuda:0')



h[100].sum tensor(-7.5630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.9309, device='cuda:0')



h[200].sum tensor(-36.5843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5189e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0052, 0.0104,  ..., 0.0001, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0220, 0.0224, 0.0401,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80981.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0545, 0.0056, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0717, 0.0093, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.1589, 0.0037, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        ...,
        [0.0040, 0.0906, 0.0000,  ..., 0.0000, 0.0731, 0.0000],
        [0.0040, 0.0905, 0.0000,  ..., 0.0000, 0.0731, 0.0000],
        [0.0040, 0.0904, 0.0000,  ..., 0.0000, 0.0730, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(746544.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3686.9163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-345.0262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(344.4738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3472.1587, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.7455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3736],
        [ 0.3488],
        [ 0.3173],
        ...,
        [-1.1198],
        [-1.1159],
        [-1.1140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191063.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0102],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369775.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0102],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369787., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4518e-03,  9.4789e-03,  1.7278e-02,  ...,  1.0871e-03,
         -6.2174e-03, -1.1622e-02],
        [ 4.6123e-03,  3.9760e-03,  8.3881e-03,  ..., -1.6694e-04,
         -3.0198e-03, -5.6448e-03],
        [ 1.4929e-02,  1.5706e-02,  2.7338e-02,  ...,  2.5063e-03,
         -9.8360e-03, -1.8386e-02],
        ...,
        [ 4.1971e-05, -1.2209e-03, -7.0890e-06,  ..., -1.3513e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.1971e-05, -1.2209e-03, -7.0890e-06,  ..., -1.3513e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.1971e-05, -1.2209e-03, -7.0890e-06,  ..., -1.3513e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3357.1768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5946, device='cuda:0')



h[100].sum tensor(-5.9899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.4749, device='cuda:0')



h[200].sum tensor(-29.0333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2103e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0303, 0.0569,  ..., 0.0031, 0.0000, 0.0000],
        [0.0402, 0.0406, 0.0736,  ..., 0.0052, 0.0000, 0.0000],
        [0.0291, 0.0280, 0.0531,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72187.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1283, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1367, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1288, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0902, 0.0000,  ..., 0.0000, 0.0728, 0.0000],
        [0.0041, 0.0902, 0.0000,  ..., 0.0000, 0.0728, 0.0000],
        [0.0041, 0.0901, 0.0000,  ..., 0.0000, 0.0727, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691532.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2674.4468, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-277.8248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.7805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3583.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.1850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4135],
        [ 0.3948],
        [ 0.3714],
        ...,
        [-1.1213],
        [-1.1176],
        [-1.1158]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193380.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0102],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369787., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0103],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369797.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3307e-02,  1.3869e-02,  2.4369e-02,  ...,  2.0880e-03,
         -8.7540e-03, -1.6374e-02],
        [ 1.8933e-02,  2.0267e-02,  3.4705e-02,  ...,  3.5460e-03,
         -1.2466e-02, -2.3317e-02],
        [ 1.6616e-02,  1.7633e-02,  3.0449e-02,  ...,  2.9457e-03,
         -1.0938e-02, -2.0458e-02],
        ...,
        [ 3.7128e-05, -1.2186e-03, -5.2126e-06,  ..., -1.3505e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.7128e-05, -1.2186e-03, -5.2126e-06,  ..., -1.3505e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.7128e-05, -1.2186e-03, -5.2126e-06,  ..., -1.3505e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3483.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.2886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1986, device='cuda:0')



h[100].sum tensor(-6.4629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.9054, device='cuda:0')



h[200].sum tensor(-31.3891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3093e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0532, 0.0554, 0.0975,  ..., 0.0083, 0.0000, 0.0000],
        [0.0642, 0.0679, 0.1176,  ..., 0.0111, 0.0000, 0.0000],
        [0.0668, 0.0708, 0.1223,  ..., 0.0118, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73396.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1752, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2116, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2218, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0069, 0.0848, 0.0000,  ..., 0.0000, 0.0687, 0.0000],
        [0.0081, 0.0821, 0.0000,  ..., 0.0000, 0.0666, 0.0000],
        [0.0069, 0.0847, 0.0000,  ..., 0.0000, 0.0686, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695964.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2803.4966, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-286.2187, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(275.4031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3690.8293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(33.5272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4012],
        [ 0.3982],
        [ 0.3906],
        ...,
        [-0.9414],
        [-0.8974],
        [-0.8960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197613.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0103],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369797.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0103],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369808.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7225e-02,  2.9694e-02,  4.9939e-02,  ...,  5.6954e-03,
         -1.7908e-02, -3.3517e-02],
        [ 1.0562e-02,  1.0749e-02,  1.9332e-02,  ...,  1.3777e-03,
         -6.9328e-03, -1.2976e-02],
        [ 1.4586e-02,  1.5323e-02,  2.6723e-02,  ...,  2.4203e-03,
         -9.5831e-03, -1.7936e-02],
        ...,
        [ 3.6957e-05, -1.2180e-03, -7.9975e-07,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.6957e-05, -1.2180e-03, -7.9975e-07,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.6957e-05, -1.2180e-03, -7.9975e-07,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3287.5681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0160, device='cuda:0')



h[100].sum tensor(-5.5230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.0670, device='cuda:0')



h[200].sum tensor(-26.8785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1128e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0528, 0.0550, 0.0968,  ..., 0.0082, 0.0000, 0.0000],
        [0.0650, 0.0688, 0.1191,  ..., 0.0113, 0.0000, 0.0000],
        [0.0587, 0.0617, 0.1076,  ..., 0.0097, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70734.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2169, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2123, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1889, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0046, 0.0903, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0046, 0.0903, 0.0000,  ..., 0.0000, 0.0726, 0.0000],
        [0.0046, 0.0902, 0.0000,  ..., 0.0000, 0.0726, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689388.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2735.0044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-264.6849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(250.1752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3791.7861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.3055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4318],
        [ 0.4385],
        [ 0.4423],
        ...,
        [-1.1293],
        [-1.1254],
        [-1.1236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196237.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0103],
        [1.0061],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369808.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0104],
        [1.0062],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369819.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4892e-02,  1.5677e-02,  2.7298e-02,  ...,  2.5020e-03,
         -9.7731e-03, -1.8303e-02],
        [ 1.2287e-02,  1.2715e-02,  2.2514e-02,  ...,  1.8271e-03,
         -8.0601e-03, -1.5095e-02],
        [ 1.4702e-02,  1.5461e-02,  2.6950e-02,  ...,  2.4529e-03,
         -9.6484e-03, -1.8070e-02],
        ...,
        [ 3.1775e-05, -1.2180e-03,  2.9649e-06,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.1775e-05, -1.2180e-03,  2.9649e-06,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.1775e-05, -1.2180e-03,  2.9649e-06,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3122.1028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.4130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6836, device='cuda:0')



h[100].sum tensor(-4.7652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9869, device='cuda:0')



h[200].sum tensor(-23.2376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.6869e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.4716e-02, 4.5769e-02, 8.1912e-02,  ..., 6.1031e-03, 0.0000e+00,
         0.0000e+00],
        [5.8186e-02, 6.1071e-02, 1.0665e-01,  ..., 9.5797e-03, 0.0000e+00,
         0.0000e+00],
        [6.4999e-02, 6.8833e-02, 1.1917e-01,  ..., 1.1364e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.3297e-04, 0.0000e+00, 1.2408e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.3289e-04, 0.0000e+00, 1.2400e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.3282e-04, 0.0000e+00, 1.2393e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66822.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2349, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2814, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2899, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0048, 0.0908, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0048, 0.0908, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0048, 0.0907, 0.0000,  ..., 0.0000, 0.0728, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671629.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2432.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-234.2051, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.3997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3874.4680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(26.5703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3581],
        [ 0.3518],
        [ 0.3508],
        ...,
        [-1.1376],
        [-1.1336],
        [-1.1316]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191993.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0104],
        [1.0062],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369819.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0104],
        [1.0062],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369829.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4482e-03,  4.9456e-03,  9.9643e-03,  ...,  5.7612e-05,
         -3.5612e-03, -6.6736e-03],
        [ 2.4802e-05, -1.2197e-03,  2.6038e-06,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4802e-05, -1.2197e-03,  2.6038e-06,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 2.4802e-05, -1.2197e-03,  2.6038e-06,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4802e-05, -1.2197e-03,  2.6038e-06,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.4802e-05, -1.2197e-03,  2.6038e-06,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3977.8633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.2519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4719, device='cuda:0')



h[100].sum tensor(-8.5557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.3919, device='cuda:0')



h[200].sum tensor(-41.8060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7586e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.2201e-02, 1.1292e-02, 2.2238e-02,  ..., 6.8578e-04, 0.0000e+00,
         0.0000e+00],
        [5.5946e-03, 5.0101e-03, 1.0102e-02,  ..., 5.8363e-05, 0.0000e+00,
         0.0000e+00],
        [1.0015e-04, 0.0000e+00, 1.0514e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.0381e-04, 0.0000e+00, 1.0898e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0374e-04, 0.0000e+00, 1.0891e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0369e-04, 0.0000e+00, 1.0886e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90426.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0545, 0.0185, 0.0000,  ..., 0.0000, 0.0162, 0.0000],
        [0.0308, 0.0390, 0.0000,  ..., 0.0000, 0.0327, 0.0000],
        [0.0144, 0.0676, 0.0000,  ..., 0.0000, 0.0551, 0.0000],
        ...,
        [0.0051, 0.0917, 0.0000,  ..., 0.0000, 0.0735, 0.0000],
        [0.0086, 0.0856, 0.0000,  ..., 0.0000, 0.0689, 0.0000],
        [0.0203, 0.0654, 0.0000,  ..., 0.0000, 0.0536, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(806718.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5037.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-409.1793, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(429.1318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3865.5579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(31.0799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2240],
        [-0.0546],
        [-0.3724],
        ...,
        [-1.1047],
        [-0.9932],
        [-0.7832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187113.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0104],
        [1.0062],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369829.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0104],
        [1.0062],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369840.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4373e-05, -1.2187e-03,  5.7800e-07,  ..., -1.3471e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4373e-05, -1.2187e-03,  5.7800e-07,  ..., -1.3471e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4373e-05, -1.2187e-03,  5.7800e-07,  ..., -1.3471e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.4373e-05, -1.2187e-03,  5.7800e-07,  ..., -1.3471e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4373e-05, -1.2187e-03,  5.7800e-07,  ..., -1.3471e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.4373e-05, -1.2187e-03,  5.7800e-07,  ..., -1.3471e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3133.0591, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.0887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9077, device='cuda:0')



h[100].sum tensor(-4.8158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1868, device='cuda:0')



h[200].sum tensor(-23.5793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.8253e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.8100e-05, 0.0000e+00, 2.3364e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8241e-05, 0.0000e+00, 2.3421e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.8041e-05, 0.0000e+00, 2.3341e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.0164e-05, 0.0000e+00, 2.4195e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.0126e-05, 0.0000e+00, 2.4179e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.0096e-05, 0.0000e+00, 2.4167e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67196.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0051, 0.0889, 0.0000,  ..., 0.0000, 0.0714, 0.0000],
        [0.0051, 0.0892, 0.0000,  ..., 0.0000, 0.0715, 0.0000],
        [0.0051, 0.0889, 0.0000,  ..., 0.0000, 0.0713, 0.0000],
        ...,
        [0.0053, 0.0924, 0.0000,  ..., 0.0000, 0.0743, 0.0000],
        [0.0053, 0.0924, 0.0000,  ..., 0.0000, 0.0742, 0.0000],
        [0.0053, 0.0923, 0.0000,  ..., 0.0000, 0.0742, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679555.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2627.0342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-229.9118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(217.7332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4144.6807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(19.2033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3271],
        [-1.3377],
        [-1.3411],
        ...,
        [-1.1569],
        [-1.1529],
        [-1.1509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211306.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0104],
        [1.0062],
        ...,
        [1.0020],
        [1.0010],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369840.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(501.5971, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0105],
        [1.0063],
        ...,
        [1.0020],
        [1.0010],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369851.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1631e-05, -1.2177e-03,  6.1376e-06,  ..., -1.3469e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.2731e-03,  3.6267e-03,  7.8335e-03,  ..., -2.4262e-04,
         -2.7898e-03, -5.2346e-03],
        [ 4.2731e-03,  3.6267e-03,  7.8335e-03,  ..., -2.4262e-04,
         -2.7898e-03, -5.2346e-03],
        ...,
        [ 1.1631e-05, -1.2177e-03,  6.1376e-06,  ..., -1.3469e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1631e-05, -1.2177e-03,  6.1376e-06,  ..., -1.3469e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.1631e-05, -1.2177e-03,  6.1376e-06,  ..., -1.3469e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3120.6289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.7675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7511, device='cuda:0')



h[100].sum tensor(-4.7363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0471, device='cuda:0')



h[200].sum tensor(-23.2372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7286e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.8734e-03, 6.4351e-03, 1.4400e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.8869e-03, 6.4452e-03, 1.4425e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [7.8674e-03, 6.4299e-03, 1.4389e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.8691e-05, 0.0000e+00, 2.5695e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.8659e-05, 0.0000e+00, 2.5678e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.8635e-05, 0.0000e+00, 2.5666e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67689.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0346, 0.0270, 0.0000,  ..., 0.0000, 0.0250, 0.0000],
        [0.0286, 0.0362, 0.0000,  ..., 0.0000, 0.0319, 0.0000],
        [0.0270, 0.0390, 0.0000,  ..., 0.0000, 0.0340, 0.0000],
        ...,
        [0.0054, 0.0929, 0.0000,  ..., 0.0000, 0.0743, 0.0000],
        [0.0054, 0.0928, 0.0000,  ..., 0.0000, 0.0743, 0.0000],
        [0.0054, 0.0928, 0.0000,  ..., 0.0000, 0.0742, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690006.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2788.4880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.4335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(216.8520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4369.4126, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(26.7656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1718],
        [-0.4210],
        [-0.6597],
        ...,
        [-1.1640],
        [-1.1598],
        [-1.1578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227461.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0105],
        [1.0063],
        ...,
        [1.0020],
        [1.0010],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369851.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0105],
        [1.0063],
        ...,
        [1.0019],
        [1.0009],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369862.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3642e-05, -1.2193e-03,  3.3505e-05,  ..., -1.3476e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.3642e-05, -1.2193e-03,  3.3505e-05,  ..., -1.3476e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.3642e-05, -1.2193e-03,  3.3505e-05,  ..., -1.3476e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 3.3642e-05, -1.2193e-03,  3.3505e-05,  ..., -1.3476e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.3642e-05, -1.2193e-03,  3.3505e-05,  ..., -1.3476e-03,
          0.0000e+00,  0.0000e+00],
        [ 3.3642e-05, -1.2193e-03,  3.3505e-05,  ..., -1.3476e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3469.2058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2473, device='cuda:0')



h[100].sum tensor(-6.1337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.0569, device='cuda:0')



h[200].sum tensor(-30.1545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2506e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76164.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0381, 0.0367, 0.0000,  ..., 0.0000, 0.0296, 0.0000],
        [0.0352, 0.0394, 0.0000,  ..., 0.0000, 0.0327, 0.0000],
        [0.0299, 0.0457, 0.0000,  ..., 0.0000, 0.0384, 0.0000],
        ...,
        [0.0052, 0.0923, 0.0000,  ..., 0.0000, 0.0739, 0.0000],
        [0.0052, 0.0923, 0.0000,  ..., 0.0000, 0.0739, 0.0000],
        [0.0052, 0.0922, 0.0000,  ..., 0.0000, 0.0738, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(729486.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3573.5552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-306.2297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(295.4617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4095.7185, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.2586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2789],
        [ 0.2547],
        [ 0.2036],
        ...,
        [-1.0072],
        [-0.9920],
        [-0.9351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206113.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0105],
        [1.0063],
        ...,
        [1.0019],
        [1.0009],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369862.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0105],
        [1.0064],
        ...,
        [1.0019],
        [1.0009],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369873.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1102e-03,  3.3890e-03,  7.5111e-03,  ..., -2.9794e-04,
         -2.6467e-03, -4.9722e-03],
        [ 6.0664e-03,  5.6129e-03,  1.1104e-02,  ...,  2.0889e-04,
         -3.9234e-03, -7.3708e-03],
        [ 5.5239e-05, -1.2211e-03,  6.2609e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 5.5239e-05, -1.2211e-03,  6.2609e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5239e-05, -1.2211e-03,  6.2609e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5239e-05, -1.2211e-03,  6.2609e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3759.2627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8069, device='cuda:0')



h[100].sum tensor(-7.2551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.2314, device='cuda:0')



h[200].sum tensor(-35.7402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4704e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0338, 0.0333, 0.0620,  ..., 0.0033, 0.0000, 0.0000],
        [0.0127, 0.0117, 0.0231,  ..., 0.0008, 0.0000, 0.0000],
        [0.0063, 0.0057, 0.0114,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82496.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1020, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0670, 0.0142, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0352, 0.0379, 0.0000,  ..., 0.0000, 0.0319, 0.0000],
        ...,
        [0.0074, 0.0864, 0.0000,  ..., 0.0000, 0.0694, 0.0000],
        [0.0086, 0.0836, 0.0000,  ..., 0.0000, 0.0673, 0.0000],
        [0.0074, 0.0863, 0.0000,  ..., 0.0000, 0.0693, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(753874.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3983.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-351.9724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(355.1492, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3785.4302, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(29.2810, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4239],
        [ 0.2640],
        [-0.0230],
        ...,
        [-0.9687],
        [-0.9307],
        [-0.9628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184637.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0105],
        [1.0064],
        ...,
        [1.0019],
        [1.0009],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369873.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0106],
        [1.0064],
        ...,
        [1.0018],
        [1.0009],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369884.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9393e-05, -1.2177e-03,  7.9985e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.4983e-03,  7.2407e-03,  1.3744e-02,  ...,  5.7819e-04,
         -4.8483e-03, -9.1141e-03],
        [ 1.3179e-02,  1.3699e-02,  2.4178e-02,  ...,  2.0500e-03,
         -8.5503e-03, -1.6074e-02],
        ...,
        [ 5.9393e-05, -1.2177e-03,  7.9985e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.9393e-05, -1.2177e-03,  7.9985e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.9393e-05, -1.2177e-03,  7.9985e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3342.6196, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.6550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8414, device='cuda:0')



h[100].sum tensor(-5.3465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.9113, device='cuda:0')



h[200].sum tensor(-26.3920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1020e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0139, 0.0131, 0.0254,  ..., 0.0008, 0.0000, 0.0000],
        [0.0243, 0.0236, 0.0445,  ..., 0.0023, 0.0000, 0.0000],
        [0.0417, 0.0422, 0.0764,  ..., 0.0057, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70930.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0667, 0.0227, 0.0000,  ..., 0.0000, 0.0191, 0.0000],
        [0.1160, 0.0034, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.1681, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0046, 0.0916, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0046, 0.0916, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0046, 0.0915, 0.0000,  ..., 0.0000, 0.0728, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693577.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2704.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-270.3307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.4597, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4037.4736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.5106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0540],
        [ 0.3099],
        [ 0.4373],
        ...,
        [-1.1624],
        [-1.1585],
        [-1.1566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209030.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0106],
        [1.0064],
        ...,
        [1.0018],
        [1.0009],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369884.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0106],
        [1.0065],
        ...,
        [1.0018],
        [1.0008],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369894.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0706e-02,  1.0898e-02,  1.9654e-02,  ...,  1.4106e-03,
         -6.9315e-03, -1.3038e-02],
        [ 5.5537e-05, -1.2142e-03,  8.9571e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        [ 8.3628e-03,  8.2326e-03,  1.5349e-02,  ...,  8.0337e-04,
         -5.4063e-03, -1.0170e-02],
        ...,
        [ 5.5537e-05, -1.2142e-03,  8.9571e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5537e-05, -1.2142e-03,  8.9571e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.5537e-05, -1.2142e-03,  8.9571e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3028.0269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.6091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.0177, device='cuda:0')



h[100].sum tensor(-3.9257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6095, device='cuda:0')



h[200].sum tensor(-19.4179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.0404e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0156, 0.0315,  ..., 0.0008, 0.0000, 0.0000],
        [0.0371, 0.0370, 0.0680,  ..., 0.0044, 0.0000, 0.0000],
        [0.0518, 0.0549, 0.0950,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65338.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1228, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1964, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2826, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0918, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0044, 0.0917, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0044, 0.0917, 0.0000,  ..., 0.0000, 0.0726, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671286.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2254.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-233.4071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.5114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4051.7119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(32.0467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4051],
        [ 0.3839],
        [ 0.3586],
        ...,
        [-1.1645],
        [-1.1608],
        [-1.1590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201949.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0106],
        [1.0065],
        ...,
        [1.0018],
        [1.0008],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369894.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0106],
        [1.0065],
        ...,
        [1.0018],
        [1.0008],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369904.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.8455e-05, -1.2064e-03,  8.2748e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 7.6901e-03,  7.5075e-03,  1.4156e-02,  ...,  6.3684e-04,
         -4.9789e-03, -9.3716e-03],
        [ 5.2075e-03,  4.6839e-03,  9.5957e-03,  ..., -6.5236e-06,
         -3.3656e-03, -6.3349e-03],
        ...,
        [ 2.8455e-05, -1.2064e-03,  8.2748e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.8455e-05, -1.2064e-03,  8.2748e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00],
        [ 2.8455e-05, -1.2064e-03,  8.2748e-05,  ..., -1.3486e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3172.4490, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.8060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1850, device='cuda:0')



h[100].sum tensor(-4.5190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5423, device='cuda:0')



h[200].sum tensor(-22.3983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3790e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0076, 0.0146,  ..., 0.0006, 0.0000, 0.0000],
        [0.0117, 0.0107, 0.0216,  ..., 0.0003, 0.0000, 0.0000],
        [0.0454, 0.0467, 0.0836,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66305.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0451, 0.0294, 0.0000,  ..., 0.0000, 0.0237, 0.0000],
        [0.0786, 0.0130, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        [0.1404, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0923, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        [0.0045, 0.0923, 0.0000,  ..., 0.0000, 0.0726, 0.0000],
        [0.0045, 0.0922, 0.0000,  ..., 0.0000, 0.0726, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672261.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2257.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-245.7415, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(214.3425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4106.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.2192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2990],
        [ 0.3383],
        [ 0.3650],
        ...,
        [-1.1712],
        [-1.1674],
        [-1.1641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200038.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0106],
        [1.0065],
        ...,
        [1.0018],
        [1.0008],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369904.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0106],
        [1.0066],
        ...,
        [1.0017],
        [1.0008],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369914.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0411e-06, -1.2015e-03,  7.6020e-05,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0411e-06, -1.2015e-03,  7.6020e-05,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0411e-06, -1.2015e-03,  7.6020e-05,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.0411e-06, -1.2015e-03,  7.6020e-05,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0411e-06, -1.2015e-03,  7.6020e-05,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.0411e-06, -1.2015e-03,  7.6020e-05,  ..., -1.3482e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3146.5227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.7229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6613, device='cuda:0')



h[100].sum tensor(-4.4150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0753, device='cuda:0')



h[200].sum tensor(-21.9277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.0555e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.4433e-05, 0.0000e+00, 3.0746e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4486e-05, 0.0000e+00, 3.0813e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4402e-05, 0.0000e+00, 3.0706e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.5310e-05, 0.0000e+00, 3.1849e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.5293e-05, 0.0000e+00, 3.1828e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.5282e-05, 0.0000e+00, 3.1814e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69759.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0896, 0.0000,  ..., 0.0000, 0.0702, 0.0000],
        [0.0058, 0.0869, 0.0000,  ..., 0.0000, 0.0682, 0.0000],
        [0.0173, 0.0663, 0.0000,  ..., 0.0000, 0.0526, 0.0000],
        ...,
        [0.0045, 0.0931, 0.0000,  ..., 0.0000, 0.0730, 0.0000],
        [0.0045, 0.0931, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0045, 0.0930, 0.0000,  ..., 0.0000, 0.0729, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699313., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2784.0732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-276.7758, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(247.8709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3997.1528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(37.3731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6775],
        [-0.5109],
        [-0.2432],
        ...,
        [-1.1802],
        [-1.1767],
        [-1.1749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184463.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0106],
        [1.0066],
        ...,
        [1.0017],
        [1.0008],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369914.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0107],
        [1.0066],
        ...,
        [1.0017],
        [1.0007],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369924.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.7463e-06, -1.2007e-03,  7.2808e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7463e-06, -1.2007e-03,  7.2808e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7463e-06, -1.2007e-03,  7.2808e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.7463e-06, -1.2007e-03,  7.2808e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7463e-06, -1.2007e-03,  7.2808e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00],
        [-5.7463e-06, -1.2007e-03,  7.2808e-05,  ..., -1.3478e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3327.1689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.1575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7735, device='cuda:0')



h[100].sum tensor(-5.2071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.8508, device='cuda:0')



h[200].sum tensor(-25.9148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0978e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72173.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0904, 0.0000,  ..., 0.0000, 0.0706, 0.0000],
        [0.0043, 0.0906, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0043, 0.0903, 0.0000,  ..., 0.0000, 0.0705, 0.0000],
        ...,
        [0.0045, 0.0940, 0.0000,  ..., 0.0000, 0.0734, 0.0000],
        [0.0045, 0.0939, 0.0000,  ..., 0.0000, 0.0733, 0.0000],
        [0.0045, 0.0939, 0.0000,  ..., 0.0000, 0.0733, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708532., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2936.2214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-299.1408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(270.6505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4167.8188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.9879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2835],
        [-1.2587],
        [-1.2074],
        ...,
        [-1.1914],
        [-1.1874],
        [-1.1854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198391.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0107],
        [1.0066],
        ...,
        [1.0017],
        [1.0007],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369924.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0107],
        [1.0066],
        ...,
        [1.0017],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369934.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2740e-05, -1.2025e-03,  6.8988e-05,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2740e-05, -1.2025e-03,  6.8988e-05,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2740e-05, -1.2025e-03,  6.8988e-05,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.2740e-05, -1.2025e-03,  6.8988e-05,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2740e-05, -1.2025e-03,  6.8988e-05,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00],
        [-1.2740e-05, -1.2025e-03,  6.8988e-05,  ..., -1.3474e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3157.8977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.5683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2294, device='cuda:0')



h[100].sum tensor(-4.5271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5818, device='cuda:0')



h[200].sum tensor(-22.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.4064e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68628.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0044, 0.0913, 0.0000,  ..., 0.0000, 0.0710, 0.0000],
        [0.0044, 0.0914, 0.0000,  ..., 0.0000, 0.0711, 0.0000],
        [0.0043, 0.0911, 0.0000,  ..., 0.0000, 0.0709, 0.0000],
        ...,
        [0.0046, 0.0949, 0.0000,  ..., 0.0000, 0.0738, 0.0000],
        [0.0046, 0.0948, 0.0000,  ..., 0.0000, 0.0738, 0.0000],
        [0.0046, 0.0948, 0.0000,  ..., 0.0000, 0.0738, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691279.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2617.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-272.8807, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(236.7025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4141.2686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.8037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1940],
        [-1.2667],
        [-1.3231],
        ...,
        [-1.2007],
        [-1.1965],
        [-1.1944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189448.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0107],
        [1.0066],
        ...,
        [1.0017],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369934.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0108],
        [1.0067],
        ...,
        [1.0017],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369944.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1401e-05, -1.2064e-03,  7.1682e-05,  ..., -1.3473e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1401e-05, -1.2064e-03,  7.1682e-05,  ..., -1.3473e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1401e-05, -1.2064e-03,  7.1682e-05,  ..., -1.3473e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.1401e-05, -1.2064e-03,  7.1682e-05,  ..., -1.3473e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1401e-05, -1.2064e-03,  7.1682e-05,  ..., -1.3473e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1401e-05, -1.2064e-03,  7.1682e-05,  ..., -1.3473e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3222.2222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.6961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3101, device='cuda:0')



h[100].sum tensor(-4.8195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5457, device='cuda:0')



h[200].sum tensor(-24.0844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0074e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70920.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0044, 0.0919, 0.0000,  ..., 0.0000, 0.0714, 0.0000],
        [0.0044, 0.0921, 0.0000,  ..., 0.0000, 0.0716, 0.0000],
        [0.0044, 0.0917, 0.0000,  ..., 0.0000, 0.0713, 0.0000],
        ...,
        [0.0046, 0.0955, 0.0000,  ..., 0.0000, 0.0743, 0.0000],
        [0.0046, 0.0955, 0.0000,  ..., 0.0000, 0.0743, 0.0000],
        [0.0046, 0.0954, 0.0000,  ..., 0.0000, 0.0742, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711641.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2942.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-289.8743, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(255.5662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4257.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.1850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2240],
        [-1.3094],
        [-1.3535],
        ...,
        [-1.2116],
        [-1.2074],
        [-1.2054]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202517.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0108],
        [1.0067],
        ...,
        [1.0017],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369944.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(483.0526, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0108],
        [1.0068],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369954.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2816e-06, -1.2174e-03,  8.7579e-05,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.7040e-03,  5.2641e-03,  1.0556e-02,  ...,  1.2908e-04,
         -3.6752e-03, -6.9396e-03],
        [ 4.2497e-03,  3.6101e-03,  7.8841e-03,  ..., -2.4780e-04,
         -2.7373e-03, -5.1686e-03],
        ...,
        [ 5.2816e-06, -1.2174e-03,  8.7579e-05,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2816e-06, -1.2174e-03,  8.7579e-05,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2816e-06, -1.2174e-03,  8.7579e-05,  ..., -1.3477e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3014.8445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.3025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3099, device='cuda:0')



h[100].sum tensor(-3.9727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8701, device='cuda:0')



h[200].sum tensor(-19.8937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.2209e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.7956e-03, 5.3339e-03, 1.0961e-02,  ..., 1.3080e-04, 0.0000e+00,
         0.0000e+00],
        [1.2543e-02, 1.1776e-02, 2.3356e-02,  ..., 7.6648e-04, 0.0000e+00,
         0.0000e+00],
        [4.2899e-02, 4.3850e-02, 7.9117e-02,  ..., 5.6670e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.2138e-05, 0.0000e+00, 3.6710e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.2123e-05, 0.0000e+00, 3.6685e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.2114e-05, 0.0000e+00, 3.6670e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67134.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0379, 0.0371, 0.0000,  ..., 0.0000, 0.0301, 0.0000],
        [0.0754, 0.0156, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.1416, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0045, 0.0960, 0.0000,  ..., 0.0000, 0.0749, 0.0000],
        [0.0045, 0.0959, 0.0000,  ..., 0.0000, 0.0749, 0.0000],
        [0.0045, 0.0959, 0.0000,  ..., 0.0000, 0.0749, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697125.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2586.7463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-263.1127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.7144, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4432.0083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.3850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2264],
        [ 0.3155],
        [ 0.3895],
        ...,
        [-1.2209],
        [-1.2167],
        [-1.2147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231886.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0108],
        [1.0068],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369954.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0109],
        [1.0068],
        ...,
        [1.0016],
        [1.0006],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369964.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5418e-03,  5.0631e-03,  1.0254e-02,  ...,  8.4398e-05,
         -3.5601e-03, -6.7265e-03],
        [ 1.2889e-05, -1.2247e-03,  9.7462e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2889e-05, -1.2247e-03,  9.7462e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.2889e-05, -1.2247e-03,  9.7462e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2889e-05, -1.2247e-03,  9.7462e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.2889e-05, -1.2247e-03,  9.7462e-05,  ..., -1.3483e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3568.6084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.8390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5559, device='cuda:0')



h[100].sum tensor(-6.2830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.2240, device='cuda:0')



h[200].sum tensor(-31.5275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3314e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.6835e-02, 1.6612e-02, 3.1223e-02,  ..., 1.8053e-03, 0.0000e+00,
         0.0000e+00],
        [3.0217e-02, 3.0577e-02, 5.5806e-02,  ..., 3.7129e-03, 0.0000e+00,
         0.0000e+00],
        [2.4911e-02, 2.5799e-02, 4.6059e-02,  ..., 3.7196e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.4033e-05, 0.0000e+00, 4.0857e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3997e-05, 0.0000e+00, 4.0830e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3975e-05, 0.0000e+00, 4.0813e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76794.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0516e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.3658e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.4276e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.4748e-03, 9.6222e-02, 5.7047e-07,  ..., 0.0000e+00, 7.5370e-02,
         0.0000e+00],
        [4.4708e-03, 9.6154e-02, 0.0000e+00,  ..., 0.0000e+00, 7.5317e-02,
         0.0000e+00],
        [4.4672e-03, 9.6108e-02, 0.0000e+00,  ..., 0.0000e+00, 7.5280e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(737874.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3421.7947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-332.2771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(300.4892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4083.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.1039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4243],
        [ 0.4147],
        [ 0.4056],
        ...,
        [-1.2290],
        [-1.2248],
        [-1.2228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201838.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0109],
        [1.0068],
        ...,
        [1.0016],
        [1.0006],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369964.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0110],
        [1.0069],
        ...,
        [1.0016],
        [1.0006],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369975.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6095e-02,  1.7049e-02,  2.9635e-02,  ...,  2.8163e-03,
         -1.0335e-02, -1.9539e-02],
        [ 2.2945e-02,  2.4839e-02,  4.2218e-02,  ...,  4.5912e-03,
         -1.4738e-02, -2.7865e-02],
        [ 1.6507e-02,  1.7518e-02,  3.0393e-02,  ...,  2.9233e-03,
         -1.0600e-02, -2.0040e-02],
        ...,
        [ 1.9775e-05, -1.2312e-03,  1.0507e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9775e-05, -1.2312e-03,  1.0507e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.9775e-05, -1.2312e-03,  1.0507e-04,  ..., -1.3490e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3601.1714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7689, device='cuda:0')



h[100].sum tensor(-6.4112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.4140, device='cuda:0')



h[200].sum tensor(-32.2369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3446e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.6193e-02, 9.2944e-02, 1.5862e-01,  ..., 1.6855e-02, 0.0000e+00,
         0.0000e+00],
        [7.5221e-02, 8.0456e-02, 1.3846e-01,  ..., 1.4000e-02, 0.0000e+00,
         0.0000e+00],
        [6.2166e-02, 6.5629e-02, 1.1448e-01,  ..., 1.0637e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.2910e-05, 0.0000e+00, 4.4050e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.2855e-05, 0.0000e+00, 4.4021e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [8.2822e-05, 0.0000e+00, 4.4003e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79050.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.3383e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.1080e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.7440e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.4960e-03, 9.6474e-02, 1.6040e-04,  ..., 0.0000e+00, 7.5720e-02,
         0.0000e+00],
        [4.4920e-03, 9.6406e-02, 1.5708e-04,  ..., 0.0000e+00, 7.5666e-02,
         0.0000e+00],
        [4.4884e-03, 9.6361e-02, 1.5288e-04,  ..., 0.0000e+00, 7.5629e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744195.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3542.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-349.7181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(317.7048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3998.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.5747, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3360],
        [ 0.3347],
        [ 0.3365],
        ...,
        [-1.1219],
        [-1.0776],
        [-1.0755]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199259.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0110],
        [1.0069],
        ...,
        [1.0016],
        [1.0006],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369975.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0111],
        [1.0069],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369985.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5011e-05, -1.2345e-03,  1.0486e-04,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5011e-05, -1.2345e-03,  1.0486e-04,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5011e-05, -1.2345e-03,  1.0486e-04,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 1.5011e-05, -1.2345e-03,  1.0486e-04,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5011e-05, -1.2345e-03,  1.0486e-04,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5011e-05, -1.2345e-03,  1.0486e-04,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3525.4316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6800, device='cuda:0')



h[100].sum tensor(-6.0888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.4429, device='cuda:0')



h[200].sum tensor(-30.6794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2773e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.0746e-05, 0.0000e+00, 4.2435e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.0879e-05, 0.0000e+00, 4.2528e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.0660e-05, 0.0000e+00, 4.2375e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.2942e-05, 0.0000e+00, 4.3969e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.2900e-05, 0.0000e+00, 4.3940e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [6.2875e-05, 0.0000e+00, 4.3923e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76483.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0440, 0.0287, 0.0000,  ..., 0.0000, 0.0232, 0.0000],
        [0.0469, 0.0274, 0.0000,  ..., 0.0000, 0.0223, 0.0000],
        [0.0517, 0.0225, 0.0000,  ..., 0.0000, 0.0187, 0.0000],
        ...,
        [0.0046, 0.0969, 0.0001,  ..., 0.0000, 0.0760, 0.0000],
        [0.0046, 0.0968, 0.0001,  ..., 0.0000, 0.0759, 0.0000],
        [0.0046, 0.0967, 0.0001,  ..., 0.0000, 0.0759, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(726305.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3288.5164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-329.6074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(291.6599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3801.0930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.2645, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3214],
        [ 0.3264],
        [ 0.3367],
        ...,
        [-1.2443],
        [-1.2404],
        [-1.2385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177041.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0111],
        [1.0069],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369985.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0112],
        [1.0070],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369996.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1004e-05, -1.2300e-03,  8.9684e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1004e-05, -1.2300e-03,  8.9684e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1004e-05, -1.2300e-03,  8.9684e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.1004e-05, -1.2300e-03,  8.9684e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1004e-05, -1.2300e-03,  8.9684e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1004e-05, -1.2300e-03,  8.9684e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2947.2056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.7718, device='cuda:0')



h[100].sum tensor(-3.7267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3902, device='cuda:0')



h[200].sum tensor(-18.8165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.8885e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0065, 0.0062, 0.0123,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63359.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0913, 0.0000,  ..., 0.0000, 0.0714, 0.0000],
        [0.0137, 0.0768, 0.0000,  ..., 0.0000, 0.0608, 0.0000],
        [0.0360, 0.0407, 0.0000,  ..., 0.0000, 0.0331, 0.0000],
        ...,
        [0.0050, 0.0976, 0.0000,  ..., 0.0000, 0.0762, 0.0000],
        [0.0050, 0.0975, 0.0000,  ..., 0.0000, 0.0762, 0.0000],
        [0.0050, 0.0975, 0.0000,  ..., 0.0000, 0.0761, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677568.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2261.6550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-237.9049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.0150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4495.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(47.7488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9880],
        [-0.6686],
        [-0.2751],
        ...,
        [-1.2552],
        [-1.2509],
        [-1.2489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250518.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0112],
        [1.0070],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369996.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0113],
        [1.0070],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370007., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4407e-05, -1.2284e-03,  9.2554e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00],
        [-2.4407e-05, -1.2284e-03,  9.2554e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.5454e-02,  1.6375e-02,  2.8526e-02,  ...,  2.6599e-03,
         -9.9061e-03, -1.8764e-02],
        ...,
        [-2.4407e-05, -1.2284e-03,  9.2554e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00],
        [-2.4407e-05, -1.2284e-03,  9.2554e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00],
        [-2.4407e-05, -1.2284e-03,  9.2554e-05,  ..., -1.3508e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3257.8936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.0105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1484, device='cuda:0')



h[100].sum tensor(-4.9471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2933, device='cuda:0')



h[200].sum tensor(-25.0298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0592e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0156, 0.0166, 0.0291,  ..., 0.0027, 0.0000, 0.0000],
        [0.0128, 0.0133, 0.0238,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70134.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0686, 0.0000,  ..., 0.0000, 0.0545, 0.0000],
        [0.0523, 0.0338, 0.0000,  ..., 0.0000, 0.0278, 0.0000],
        [0.0654, 0.0141, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        ...,
        [0.0050, 0.0975, 0.0000,  ..., 0.0000, 0.0762, 0.0000],
        [0.0050, 0.0975, 0.0000,  ..., 0.0000, 0.0761, 0.0000],
        [0.0050, 0.0974, 0.0000,  ..., 0.0000, 0.0761, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(705515.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2851.5171, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-287.9009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(233.2229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4157.4834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(48.4196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8910],
        [-0.5069],
        [-0.1280],
        ...,
        [-1.2570],
        [-1.2531],
        [-1.2515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223641.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0113],
        [1.0070],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370007., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0114],
        [1.0070],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370017.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.3637e-05, -1.2187e-03,  8.7766e-05,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [ 5.2565e-03,  4.8096e-03,  9.8237e-03,  ...,  2.1832e-05,
         -3.3870e-03, -6.4198e-03],
        [ 4.7270e-03,  4.2074e-03,  8.8510e-03,  ..., -1.1538e-04,
         -3.0486e-03, -5.7784e-03],
        ...,
        [-4.3637e-05, -1.2187e-03,  8.7766e-05,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3637e-05, -1.2187e-03,  8.7766e-05,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3637e-05, -1.2187e-03,  8.7766e-05,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3136.6863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.2764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1447, device='cuda:0')



h[100].sum tensor(-4.3578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5064, device='cuda:0')



h[200].sum tensor(-22.0942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3541e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.3295e-03, 4.8764e-03, 1.0226e-02,  ..., 2.2135e-05, 0.0000e+00,
         0.0000e+00],
        [9.1272e-03, 8.0127e-03, 1.7284e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.3220e-02, 2.1685e-02, 4.3333e-02,  ..., 8.3182e-04, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.6815e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6791e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6777e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68564.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0458, 0.0252, 0.0000,  ..., 0.0000, 0.0207, 0.0000],
        [0.0648, 0.0106, 0.0000,  ..., 0.0000, 0.0089, 0.0000],
        [0.0842, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0054, 0.0973, 0.0000,  ..., 0.0000, 0.0759, 0.0000],
        [0.0054, 0.0972, 0.0000,  ..., 0.0000, 0.0758, 0.0000],
        [0.0054, 0.0972, 0.0000,  ..., 0.0000, 0.0758, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702218.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2866.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-277.6357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(221.2003, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4153.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(49.7740, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2589],
        [ 0.3569],
        [ 0.4118],
        ...,
        [-1.2599],
        [-1.2559],
        [-1.2540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227849.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0114],
        [1.0070],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370017.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0115],
        [1.0071],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370028.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1497e-02,  2.3303e-02,  3.9683e-02,  ...,  4.2317e-03,
         -1.3752e-02, -2.6084e-02],
        [ 2.9038e-02,  3.1881e-02,  5.3535e-02,  ...,  6.1857e-03,
         -1.8564e-02, -3.5210e-02],
        [ 2.2349e-02,  2.4272e-02,  4.1247e-02,  ...,  4.4523e-03,
         -1.4296e-02, -2.7114e-02],
        ...,
        [-5.6135e-05, -1.2124e-03,  9.1242e-05,  ..., -1.3529e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6135e-05, -1.2124e-03,  9.1242e-05,  ..., -1.3529e-03,
          0.0000e+00,  0.0000e+00],
        [-5.6135e-05, -1.2124e-03,  9.1242e-05,  ..., -1.3529e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3498.6694, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.8425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7963, device='cuda:0')



h[100].sum tensor(-5.7216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.6547, device='cuda:0')



h[200].sum tensor(-29.0688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2227e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0986, 0.1075, 0.1819,  ..., 0.0201, 0.0000, 0.0000],
        [0.1110, 0.1216, 0.2047,  ..., 0.0233, 0.0000, 0.0000],
        [0.1085, 0.1188, 0.2001,  ..., 0.0227, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76408.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3711, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4143, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.4041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0055, 0.0969, 0.0000,  ..., 0.0000, 0.0757, 0.0000],
        [0.0055, 0.0969, 0.0000,  ..., 0.0000, 0.0756, 0.0000],
        [0.0055, 0.0968, 0.0000,  ..., 0.0000, 0.0756, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739015.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3653.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-335.5671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(295.1057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3870.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(51.8427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2947],
        [ 0.2856],
        [ 0.2753],
        ...,
        [-1.2594],
        [-1.2554],
        [-1.2535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206329.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0115],
        [1.0071],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370028.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0148],
        [1.0116],
        [1.0071],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370039.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3515e-05, -1.2085e-03,  9.7346e-05,  ..., -1.3539e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3515e-05, -1.2085e-03,  9.7346e-05,  ..., -1.3539e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3515e-05, -1.2085e-03,  9.7346e-05,  ..., -1.3539e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.3515e-05, -1.2085e-03,  9.7346e-05,  ..., -1.3539e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3515e-05, -1.2085e-03,  9.7346e-05,  ..., -1.3539e-03,
          0.0000e+00,  0.0000e+00],
        [-6.3515e-05, -1.2085e-03,  9.7346e-05,  ..., -1.3539e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3239.8071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.4368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0038, device='cuda:0')



h[100].sum tensor(-4.5628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2725, device='cuda:0')



h[200].sum tensor(-23.2298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.8847e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.3744e-03, 6.0913e-03, 1.4176e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.7078e-03, 3.0626e-03, 7.3245e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [5.3667e-03, 4.9568e-03, 1.0370e-02,  ..., 3.9861e-05, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.0846e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.0820e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.0803e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69815.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0302, 0.0348, 0.0000,  ..., 0.0000, 0.0299, 0.0000],
        [0.0264, 0.0437, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0306, 0.0346, 0.0000,  ..., 0.0000, 0.0296, 0.0000],
        ...,
        [0.0056, 0.0966, 0.0000,  ..., 0.0000, 0.0755, 0.0000],
        [0.0056, 0.0965, 0.0000,  ..., 0.0000, 0.0754, 0.0000],
        [0.0056, 0.0965, 0.0000,  ..., 0.0000, 0.0754, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703347.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3004.6743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-286.0927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(234.7225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3814.9998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(50.5310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0355],
        [ 0.0540],
        [ 0.1584],
        ...,
        [-1.2582],
        [-1.2543],
        [-1.2525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209572.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0148],
        [1.0116],
        [1.0071],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370039.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0117],
        [1.0071],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370050.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2692e-03,  9.4131e-03,  1.7257e-02,  ...,  1.0644e-03,
         -5.9393e-03, -1.1279e-02],
        [ 6.4807e-03,  6.2414e-03,  1.2135e-02,  ...,  3.4193e-04,
         -4.1656e-03, -7.9107e-03],
        [-6.7876e-05, -1.2071e-03,  1.0512e-04,  ..., -1.3547e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.7876e-05, -1.2071e-03,  1.0512e-04,  ..., -1.3547e-03,
          0.0000e+00,  0.0000e+00],
        [-6.7876e-05, -1.2071e-03,  1.0512e-04,  ..., -1.3547e-03,
          0.0000e+00,  0.0000e+00],
        [-6.7876e-05, -1.2071e-03,  1.0512e-04,  ..., -1.3547e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3375.2954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.9759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4379, device='cuda:0')



h[100].sum tensor(-5.0122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5515, device='cuda:0')



h[200].sum tensor(-25.5712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0770e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0369, 0.0374, 0.0688,  ..., 0.0042, 0.0000, 0.0000],
        [0.0148, 0.0145, 0.0278,  ..., 0.0011, 0.0000, 0.0000],
        [0.0066, 0.0063, 0.0126,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71529.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1133, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0770, 0.0054, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0493, 0.0214, 0.0000,  ..., 0.0000, 0.0181, 0.0000],
        ...,
        [0.0056, 0.0964, 0.0000,  ..., 0.0000, 0.0755, 0.0000],
        [0.0056, 0.0963, 0.0000,  ..., 0.0000, 0.0754, 0.0000],
        [0.0056, 0.0963, 0.0000,  ..., 0.0000, 0.0754, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706017.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3090.1113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-297.8388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(250.3077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3671.6519, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(51.5020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4300],
        [ 0.3865],
        [ 0.2914],
        ...,
        [-1.2601],
        [-1.2562],
        [-1.2544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196526.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0117],
        [1.0071],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370050.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(501.2889, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0118],
        [1.0072],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370061., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.9460e-05, -1.2007e-03,  9.7898e-05,  ..., -1.3553e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9460e-05, -1.2007e-03,  9.7898e-05,  ..., -1.3553e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9460e-05, -1.2007e-03,  9.7898e-05,  ..., -1.3553e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-7.9460e-05, -1.2007e-03,  9.7898e-05,  ..., -1.3553e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9460e-05, -1.2007e-03,  9.7898e-05,  ..., -1.3553e-03,
          0.0000e+00,  0.0000e+00],
        [-7.9460e-05, -1.2007e-03,  9.7898e-05,  ..., -1.3553e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3232.8501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.3702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2169, device='cuda:0')



h[100].sum tensor(-4.3932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5708, device='cuda:0')



h[200].sum tensor(-22.4597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3987e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68798.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0055, 0.0929, 0.0000,  ..., 0.0000, 0.0728, 0.0000],
        [0.0055, 0.0931, 0.0000,  ..., 0.0000, 0.0730, 0.0000],
        [0.0069, 0.0899, 0.0000,  ..., 0.0000, 0.0706, 0.0000],
        ...,
        [0.0059, 0.0966, 0.0000,  ..., 0.0000, 0.0758, 0.0000],
        [0.0059, 0.0966, 0.0000,  ..., 0.0000, 0.0757, 0.0000],
        [0.0059, 0.0965, 0.0000,  ..., 0.0000, 0.0757, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698356.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2947.0173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-278.3878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(225.0096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3781.7073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.7605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3108],
        [-1.1953],
        [-1.0093],
        ...,
        [-1.1117],
        [-1.2225],
        [-1.2490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214935.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0118],
        [1.0072],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370061., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0118],
        [1.0072],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370071.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.6722e-05, -1.1962e-03,  9.3573e-05,  ..., -1.3557e-03,
          0.0000e+00,  0.0000e+00],
        [-8.6722e-05, -1.1962e-03,  9.3573e-05,  ..., -1.3557e-03,
          0.0000e+00,  0.0000e+00],
        [-8.6722e-05, -1.1962e-03,  9.3573e-05,  ..., -1.3557e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-8.6722e-05, -1.1962e-03,  9.3573e-05,  ..., -1.3557e-03,
          0.0000e+00,  0.0000e+00],
        [-8.6722e-05, -1.1962e-03,  9.3573e-05,  ..., -1.3557e-03,
          0.0000e+00,  0.0000e+00],
        [-8.6722e-05, -1.1962e-03,  9.3573e-05,  ..., -1.3557e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3704.2383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0007, device='cuda:0')



h[100].sum tensor(-6.2410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.6207, device='cuda:0')



h[200].sum tensor(-31.9732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3589e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79051.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0280, 0.0467, 0.0000,  ..., 0.0000, 0.0391, 0.0000],
        [0.0165, 0.0683, 0.0000,  ..., 0.0000, 0.0547, 0.0000],
        [0.0127, 0.0760, 0.0000,  ..., 0.0000, 0.0602, 0.0000],
        ...,
        [0.0060, 0.0970, 0.0000,  ..., 0.0000, 0.0763, 0.0000],
        [0.0060, 0.0969, 0.0000,  ..., 0.0000, 0.0762, 0.0000],
        [0.0060, 0.0968, 0.0000,  ..., 0.0000, 0.0762, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744370.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3931.6003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-352.7375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(318.6756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3464.4058, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(54.5241, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1666],
        [-0.0526],
        [-0.2331],
        ...,
        [-1.2745],
        [-1.2668],
        [-1.2453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184611.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0118],
        [1.0072],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370071.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0119],
        [1.0073],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370080.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0675e-02,  1.1052e-02,  1.9863e-02,  ...,  1.4351e-03,
         -6.8183e-03, -1.2973e-02],
        [ 6.3517e-03,  6.1357e-03,  1.1921e-02,  ...,  3.1507e-04,
         -4.0811e-03, -7.7654e-03],
        [ 1.4256e-02,  1.5125e-02,  2.6442e-02,  ...,  2.3629e-03,
         -9.0856e-03, -1.7288e-02],
        ...,
        [-9.4325e-05, -1.1950e-03,  7.8754e-05,  ..., -1.3549e-03,
          0.0000e+00,  0.0000e+00],
        [-9.4325e-05, -1.1950e-03,  7.8754e-05,  ..., -1.3549e-03,
          0.0000e+00,  0.0000e+00],
        [-9.4325e-05, -1.1950e-03,  7.8754e-05,  ..., -1.3549e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3848.7905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7120, device='cuda:0')



h[100].sum tensor(-6.8319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.1468, device='cuda:0')



h[200].sum tensor(-35.0735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4646e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.0251, 0.0487,  ..., 0.0016, 0.0000, 0.0000],
        [0.0351, 0.0366, 0.0653,  ..., 0.0050, 0.0000, 0.0000],
        [0.0182, 0.0185, 0.0341,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80998.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1031, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1176, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0994, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0065, 0.0977, 0.0000,  ..., 0.0000, 0.0772, 0.0000],
        [0.0065, 0.0976, 0.0000,  ..., 0.0000, 0.0771, 0.0000],
        [0.0065, 0.0976, 0.0000,  ..., 0.0000, 0.0771, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(754483.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4149.3115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-367.6006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(334.4230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3636.1475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(54.9549, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5019],
        [ 0.5159],
        [ 0.5148],
        ...,
        [-1.2764],
        [-1.2671],
        [-1.2629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211566.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0119],
        [1.0073],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370080.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0120],
        [1.0073],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370090.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9976e-03,  4.6010e-03,  9.4330e-03,  ..., -3.3782e-05,
         -3.2216e-03, -6.1339e-03],
        [-9.8854e-05, -1.1946e-03,  7.0060e-05,  ..., -1.3542e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8854e-05, -1.1946e-03,  7.0060e-05,  ..., -1.3542e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-9.8854e-05, -1.1946e-03,  7.0060e-05,  ..., -1.3542e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8854e-05, -1.1946e-03,  7.0060e-05,  ..., -1.3542e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8854e-05, -1.1946e-03,  7.0060e-05,  ..., -1.3542e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3157.3833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.7302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3810, device='cuda:0')



h[100].sum tensor(-4.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8253, device='cuda:0')



h[200].sum tensor(-21.0365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8824e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0174, 0.0322,  ..., 0.0019, 0.0000, 0.0000],
        [0.0051, 0.0047, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68082.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0959, 0.0092, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0529, 0.0302, 0.0000,  ..., 0.0000, 0.0253, 0.0000],
        [0.0238, 0.0595, 0.0000,  ..., 0.0000, 0.0497, 0.0000],
        ...,
        [0.0067, 0.0983, 0.0000,  ..., 0.0000, 0.0779, 0.0000],
        [0.0067, 0.0982, 0.0000,  ..., 0.0000, 0.0779, 0.0000],
        [0.0067, 0.0982, 0.0000,  ..., 0.0000, 0.0778, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698856.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3149.6060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-269.2846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.7717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3666.7317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(52.0857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4165],
        [ 0.2895],
        [ 0.0534],
        ...,
        [-1.3001],
        [-1.2958],
        [-1.2938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202198.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0120],
        [1.0073],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370090.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0121],
        [1.0074],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370100.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.8262e-05, -1.1962e-03,  7.2310e-05,  ..., -1.3537e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8262e-05, -1.1962e-03,  7.2310e-05,  ..., -1.3537e-03,
          0.0000e+00,  0.0000e+00],
        [ 9.8289e-03,  1.0092e-02,  1.8311e-02,  ...,  1.2182e-03,
         -6.2653e-03, -1.1937e-02],
        ...,
        [-9.8262e-05, -1.1962e-03,  7.2310e-05,  ..., -1.3537e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8262e-05, -1.1962e-03,  7.2310e-05,  ..., -1.3537e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8262e-05, -1.1962e-03,  7.2310e-05,  ..., -1.3537e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3082.3643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.5609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.2758, device='cuda:0')



h[100].sum tensor(-3.7558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8396, device='cuda:0')



h[200].sum tensor(-19.3621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.1998e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0140, 0.0137, 0.0263,  ..., 0.0012, 0.0000, 0.0000],
        [0.0129, 0.0114, 0.0246,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66109.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0349, 0.0416, 0.0000,  ..., 0.0000, 0.0368, 0.0000],
        [0.0519, 0.0228, 0.0000,  ..., 0.0000, 0.0200, 0.0000],
        [0.0644, 0.0111, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        ...,
        [0.0067, 0.0986, 0.0000,  ..., 0.0000, 0.0784, 0.0000],
        [0.0067, 0.0986, 0.0000,  ..., 0.0000, 0.0784, 0.0000],
        [0.0067, 0.0985, 0.0000,  ..., 0.0000, 0.0783, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696755.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3026.3882, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-256.5174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(193.8694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4083.7761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(54.3068, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2983],
        [ 0.3369],
        [ 0.4125],
        ...,
        [-1.2978],
        [-1.2992],
        [-1.2980]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244658.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0121],
        [1.0074],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370100.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0122],
        [1.0074],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370110.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0083e-04, -1.1939e-03,  7.4636e-05,  ..., -1.3535e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0083e-04, -1.1939e-03,  7.4636e-05,  ..., -1.3535e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0083e-04, -1.1939e-03,  7.4636e-05,  ..., -1.3535e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.0083e-04, -1.1939e-03,  7.4636e-05,  ..., -1.3535e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0083e-04, -1.1939e-03,  7.4636e-05,  ..., -1.3535e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0083e-04, -1.1939e-03,  7.4636e-05,  ..., -1.3535e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3329.6792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.9242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7133, device='cuda:0')



h[100].sum tensor(-4.6745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.9053, device='cuda:0')



h[200].sum tensor(-24.1491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0323e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70130.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0886, 0.0000,  ..., 0.0000, 0.0710, 0.0000],
        [0.0092, 0.0883, 0.0000,  ..., 0.0000, 0.0708, 0.0000],
        [0.0110, 0.0841, 0.0000,  ..., 0.0000, 0.0679, 0.0000],
        ...,
        [0.0090, 0.0946, 0.0000,  ..., 0.0000, 0.0757, 0.0000],
        [0.0066, 0.0988, 0.0000,  ..., 0.0000, 0.0788, 0.0000],
        [0.0066, 0.0988, 0.0000,  ..., 0.0000, 0.0787, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714194.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3328.8545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-286.5347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(231.1634, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4109.3389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.0240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5984],
        [-0.5736],
        [-0.4269],
        ...,
        [-1.1847],
        [-1.2708],
        [-1.2980]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244313.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0122],
        [1.0074],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370110.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0157],
        [1.0123],
        [1.0075],
        ...,
        [1.0010],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370120., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.8012e-05, -1.1950e-03,  7.9886e-05,  ..., -1.3522e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8012e-05, -1.1950e-03,  7.9886e-05,  ..., -1.3522e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8012e-05, -1.1950e-03,  7.9886e-05,  ..., -1.3522e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-9.8012e-05, -1.1950e-03,  7.9886e-05,  ..., -1.3522e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8012e-05, -1.1950e-03,  7.9886e-05,  ..., -1.3522e-03,
          0.0000e+00,  0.0000e+00],
        [-9.8012e-05, -1.1950e-03,  7.9886e-05,  ..., -1.3522e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3480.5171, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.0613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5074, device='cuda:0')



h[100].sum tensor(-5.2090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5052, device='cuda:0')



h[200].sum tensor(-26.9672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1431e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73775.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0076, 0.0913, 0.0000,  ..., 0.0000, 0.0732, 0.0000],
        [0.0066, 0.0941, 0.0000,  ..., 0.0000, 0.0753, 0.0000],
        [0.0061, 0.0951, 0.0000,  ..., 0.0000, 0.0760, 0.0000],
        ...,
        [0.0066, 0.0990, 0.0000,  ..., 0.0000, 0.0792, 0.0000],
        [0.0065, 0.0990, 0.0000,  ..., 0.0000, 0.0792, 0.0000],
        [0.0065, 0.0989, 0.0000,  ..., 0.0000, 0.0791, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(725814.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3568.7429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-311.3659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.9732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4013.5889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.2458, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7139],
        [-0.9745],
        [-1.1414],
        ...,
        [-1.3170],
        [-1.3126],
        [-1.3106]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224646.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0157],
        [1.0123],
        [1.0075],
        ...,
        [1.0010],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370120., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0124],
        [1.0076],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370129.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0376e-04, -1.1885e-03,  8.0123e-05,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0376e-04, -1.1885e-03,  8.0123e-05,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0376e-04, -1.1885e-03,  8.0123e-05,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.0376e-04, -1.1885e-03,  8.0123e-05,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0376e-04, -1.1885e-03,  8.0123e-05,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00],
        [-1.0376e-04, -1.1885e-03,  8.0123e-05,  ..., -1.3510e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3288.3948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.8738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7248, device='cuda:0')



h[100].sum tensor(-4.3926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0237, device='cuda:0')



h[200].sum tensor(-22.7884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.7124e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69257.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0954, 0.0000,  ..., 0.0000, 0.0762, 0.0000],
        [0.0060, 0.0956, 0.0000,  ..., 0.0000, 0.0764, 0.0000],
        [0.0059, 0.0953, 0.0000,  ..., 0.0000, 0.0761, 0.0000],
        ...,
        [0.0063, 0.0992, 0.0000,  ..., 0.0000, 0.0794, 0.0000],
        [0.0063, 0.0992, 0.0000,  ..., 0.0000, 0.0793, 0.0000],
        [0.0063, 0.0991, 0.0000,  ..., 0.0000, 0.0793, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(707315.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3160.6802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-279.8758, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(224.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4105.7544, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.5414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4500],
        [-1.3885],
        [-1.2778],
        ...,
        [-1.3217],
        [-1.3173],
        [-1.3153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220743.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0124],
        [1.0076],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370129.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0125],
        [1.0076],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370139.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [ 1.0334e-02,  1.0699e-02,  1.9272e-02,  ...,  1.3571e-03,
         -6.5521e-03, -1.2515e-02],
        [ 1.0985e-02,  1.1439e-02,  2.0469e-02,  ...,  1.5258e-03,
         -6.9605e-03, -1.3295e-02],
        ...,
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3521.8357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.8497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8790, device='cuda:0')



h[100].sum tensor(-5.2552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.8367, device='cuda:0')



h[200].sum tensor(-27.3208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1661e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0109, 0.0198,  ..., 0.0014, 0.0000, 0.0000],
        [0.0197, 0.0202, 0.0369,  ..., 0.0024, 0.0000, 0.0000],
        [0.0586, 0.0623, 0.1087,  ..., 0.0098, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74344.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0385, 0.0000,  ..., 0.0000, 0.0336, 0.0000],
        [0.0902, 0.0130, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.1616, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0061, 0.0997, 0.0000,  ..., 0.0000, 0.0796, 0.0000],
        [0.0060, 0.0996, 0.0000,  ..., 0.0000, 0.0796, 0.0000],
        [0.0060, 0.0996, 0.0000,  ..., 0.0000, 0.0795, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(735594.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3645.4683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-322.0707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.7100, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4186.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(63.4690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3088],
        [-0.0734],
        [ 0.1913],
        ...,
        [-1.3285],
        [-1.3241],
        [-1.3221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214545.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0125],
        [1.0076],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370139.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0125],
        [1.0076],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370139.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1260e-04, -1.1784e-03,  7.5280e-05,  ..., -1.3498e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3850.4150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.6130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4278, device='cuda:0')



h[100].sum tensor(-6.5274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.8934, device='cuda:0')



h[200].sum tensor(-33.9348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4470e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83060.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0057, 0.0958, 0.0000,  ..., 0.0000, 0.0765, 0.0000],
        [0.0068, 0.0935, 0.0000,  ..., 0.0000, 0.0748, 0.0000],
        [0.0113, 0.0828, 0.0000,  ..., 0.0000, 0.0672, 0.0000],
        ...,
        [0.0061, 0.0997, 0.0000,  ..., 0.0000, 0.0796, 0.0000],
        [0.0060, 0.0996, 0.0000,  ..., 0.0000, 0.0796, 0.0000],
        [0.0060, 0.0996, 0.0000,  ..., 0.0000, 0.0795, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(773393.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4346.1191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-384.9092, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(354.0588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3917.2375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(63.3047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7360],
        [-0.6261],
        [-0.3813],
        ...,
        [-1.3244],
        [-1.3223],
        [-1.3209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192113.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0125],
        [1.0076],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370139.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(461.7298, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0126],
        [1.0077],
        ...,
        [1.0009],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370149.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3167e-04, -1.1596e-03,  5.9943e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3167e-04, -1.1596e-03,  5.9943e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3167e-04, -1.1596e-03,  5.9943e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.3167e-04, -1.1596e-03,  5.9943e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3167e-04, -1.1596e-03,  5.9943e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3167e-04, -1.1596e-03,  5.9943e-05,  ..., -1.3493e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3449.9097, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.1820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8924, device='cuda:0')



h[100].sum tensor(-4.9691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.9568, device='cuda:0')



h[200].sum tensor(-25.8882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1051e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75516.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0054, 0.0967, 0.0000,  ..., 0.0000, 0.0767, 0.0000],
        [0.0054, 0.0969, 0.0000,  ..., 0.0000, 0.0768, 0.0000],
        [0.0054, 0.0966, 0.0000,  ..., 0.0000, 0.0765, 0.0000],
        ...,
        [0.0057, 0.1007, 0.0000,  ..., 0.0000, 0.0798, 0.0000],
        [0.0057, 0.1006, 0.0000,  ..., 0.0000, 0.0798, 0.0000],
        [0.0057, 0.1006, 0.0000,  ..., 0.0000, 0.0797, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(757671.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3841.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-336.5491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(289.7610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4605.7471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.8203, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1180],
        [-1.2086],
        [-1.2616],
        ...,
        [-1.3394],
        [-1.3350],
        [-1.3329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251197.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0126],
        [1.0077],
        ...,
        [1.0009],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370149.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0162],
        [1.0127],
        [1.0077],
        ...,
        [1.0009],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370159.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4389e-04, -1.1462e-03,  5.2835e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4389e-04, -1.1462e-03,  5.2835e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4389e-04, -1.1462e-03,  5.2835e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-1.4389e-04, -1.1462e-03,  5.2835e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4389e-04, -1.1462e-03,  5.2835e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00],
        [-1.4389e-04, -1.1462e-03,  5.2835e-05,  ..., -1.3492e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3489.8867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.3007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2048, device='cuda:0')



h[100].sum tensor(-5.0971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.2354, device='cuda:0')



h[200].sum tensor(-26.6112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1244e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73570.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0055, 0.0957, 0.0000,  ..., 0.0000, 0.0754, 0.0000],
        [0.0090, 0.0878, 0.0000,  ..., 0.0000, 0.0696, 0.0000],
        [0.0192, 0.0671, 0.0000,  ..., 0.0000, 0.0549, 0.0000],
        ...,
        [0.0053, 0.1013, 0.0000,  ..., 0.0000, 0.0800, 0.0000],
        [0.0053, 0.1013, 0.0000,  ..., 0.0000, 0.0799, 0.0000],
        [0.0053, 0.1012, 0.0000,  ..., 0.0000, 0.0799, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727739.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3226.6091, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-322.3739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.9912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4292.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(76.1270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9486],
        [-0.6761],
        [-0.3491],
        ...,
        [-1.3473],
        [-1.3429],
        [-1.3409]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207768.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0162],
        [1.0127],
        [1.0077],
        ...,
        [1.0009],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370159.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0129],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370169.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1043e-02,  1.1576e-02,  2.0614e-02,  ...,  1.5485e-03,
         -6.9827e-03, -1.3364e-02],
        [ 2.0398e-02,  2.2219e-02,  3.7807e-02,  ...,  3.9734e-03,
         -1.2825e-02, -2.4545e-02],
        [ 2.4701e-02,  2.7114e-02,  4.5715e-02,  ...,  5.0887e-03,
         -1.5513e-02, -2.9688e-02],
        ...,
        [-1.3772e-04, -1.1440e-03,  6.6640e-05,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3772e-04, -1.1440e-03,  6.6640e-05,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00],
        [-1.3772e-04, -1.1440e-03,  6.6640e-05,  ..., -1.3496e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3341.1223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.1919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1152, device='cuda:0')



h[100].sum tensor(-4.4686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3719, device='cuda:0')



h[200].sum tensor(-23.3792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.9535e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0723, 0.0782, 0.1341,  ..., 0.0134, 0.0000, 0.0000],
        [0.0961, 0.1053, 0.1779,  ..., 0.0196, 0.0000, 0.0000],
        [0.0952, 0.1044, 0.1763,  ..., 0.0194, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71492.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3052, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3538, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3674, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0048, 0.1014, 0.0000,  ..., 0.0000, 0.0799, 0.0000],
        [0.0048, 0.1013, 0.0000,  ..., 0.0000, 0.0799, 0.0000],
        [0.0048, 0.1013, 0.0000,  ..., 0.0000, 0.0798, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(725345.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3032.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-308.8722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(255.5385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4393.7935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(80.3147, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4058],
        [ 0.3971],
        [ 0.3933],
        ...,
        [-1.3506],
        [-1.3460],
        [-1.3439]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215001.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0129],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370169.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0130],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370179.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5803e-03,  9.8881e-03,  1.7921e-02,  ...,  1.1635e-03,
         -6.0482e-03, -1.1583e-02],
        [ 7.7540e-03,  7.8104e-03,  1.4565e-02,  ...,  6.9014e-04,
         -4.9094e-03, -9.4019e-03],
        [ 1.7774e-02,  1.9210e-02,  3.2980e-02,  ...,  3.2873e-03,
         -1.1157e-02, -2.1367e-02],
        ...,
        [-1.1918e-04, -1.1467e-03,  9.4737e-05,  ..., -1.3505e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1918e-04, -1.1467e-03,  9.4737e-05,  ..., -1.3505e-03,
          0.0000e+00,  0.0000e+00],
        [-1.1918e-04, -1.1467e-03,  9.4737e-05,  ..., -1.3505e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2956.7039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.9582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(10.5922, device='cuda:0')



h[100].sum tensor(-2.9071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4463, device='cuda:0')



h[200].sum tensor(-15.2417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-6.5422e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0158, 0.0159, 0.0298,  ..., 0.0014, 0.0000, 0.0000],
        [0.0501, 0.0529, 0.0934,  ..., 0.0076, 0.0000, 0.0000],
        [0.0518, 0.0548, 0.0965,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62466.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0730, 0.0187, 0.0000,  ..., 0.0000, 0.0171, 0.0000],
        [0.1334, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1555, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0042, 0.1008, 0.0000,  ..., 0.0000, 0.0797, 0.0000],
        [0.0042, 0.1008, 0.0000,  ..., 0.0000, 0.0796, 0.0000],
        [0.0042, 0.1007, 0.0000,  ..., 0.0000, 0.0796, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684498., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2045.3318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-242.2889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.7763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4653.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(80.7612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2573],
        [ 0.1304],
        [ 0.3632],
        ...,
        [-1.3494],
        [-1.3453],
        [-1.3435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246783.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0130],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370179.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0131],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370189.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9708e-03,  4.6091e-03,  9.4346e-03,  ..., -3.8870e-05,
         -3.1539e-03, -6.0440e-03],
        [-9.5244e-05, -1.1541e-03,  1.2309e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        [ 4.9708e-03,  4.6091e-03,  9.4346e-03,  ..., -3.8870e-05,
         -3.1539e-03, -6.0440e-03],
        ...,
        [-9.5244e-05, -1.1541e-03,  1.2309e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5244e-05, -1.1541e-03,  1.2309e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00],
        [-9.5244e-05, -1.1541e-03,  1.2309e-04,  ..., -1.3518e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3294.3027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.4468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0364, device='cuda:0')



h[100].sum tensor(-4.1296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.4098, device='cuda:0')



h[200].sum tensor(-21.6973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.2872e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0036, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0183, 0.0166, 0.0349,  ..., 0.0000, 0.0000, 0.0000],
        [0.0041, 0.0036, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68817.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0193, 0.0606, 0.0000,  ..., 0.0000, 0.0512, 0.0000],
        [0.0310, 0.0348, 0.0000,  ..., 0.0000, 0.0331, 0.0000],
        [0.0192, 0.0606, 0.0000,  ..., 0.0000, 0.0512, 0.0000],
        ...,
        [0.0036, 0.1005, 0.0006,  ..., 0.0000, 0.0797, 0.0000],
        [0.0036, 0.1005, 0.0006,  ..., 0.0000, 0.0797, 0.0000],
        [0.0036, 0.1004, 0.0006,  ..., 0.0000, 0.0796, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706089.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2354.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-287.3865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(227.8182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4327.9932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(80.1576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8087],
        [-0.8462],
        [-0.9806],
        ...,
        [-1.3489],
        [-1.3448],
        [-1.3430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225424.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0131],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370189.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0132],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370198.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.7460e-05, -1.1582e-03,  1.4246e-04,  ..., -1.3529e-03,
          0.0000e+00,  0.0000e+00],
        [ 6.8906e-03,  6.7686e-03,  1.2951e-02,  ...,  4.5296e-04,
         -4.3313e-03, -8.3056e-03],
        [ 2.5881e-02,  2.8372e-02,  4.7857e-02,  ...,  5.3744e-03,
         -1.6135e-02, -3.0941e-02],
        ...,
        [-7.7460e-05, -1.1582e-03,  1.4246e-04,  ..., -1.3529e-03,
          0.0000e+00,  0.0000e+00],
        [-7.7460e-05, -1.1582e-03,  1.4246e-04,  ..., -1.3529e-03,
          0.0000e+00,  0.0000e+00],
        [-7.7460e-05, -1.1582e-03,  1.4246e-04,  ..., -1.3529e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4890.4463, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.8951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2724, device='cuda:0')



h[100].sum tensor(-10.1152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2403, device='cuda:0')



h[200].sum tensor(-53.2589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.3021e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0189, 0.0349,  ..., 0.0021, 0.0000, 0.0000],
        [0.0546, 0.0589, 0.1014,  ..., 0.0101, 0.0000, 0.0000],
        [0.0728, 0.0785, 0.1349,  ..., 0.0135, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(104715.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1466, 0.0102, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        [0.2721, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3720, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0031, 0.1004, 0.0011,  ..., 0.0000, 0.0798, 0.0000],
        [0.0031, 0.1004, 0.0011,  ..., 0.0000, 0.0798, 0.0000],
        [0.0031, 0.1003, 0.0011,  ..., 0.0000, 0.0797, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(886962.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5641.9229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-553.9410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(557.9490, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3905.8677, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(86.7757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3375],
        [ 0.3418],
        [ 0.3224],
        ...,
        [-1.3513],
        [-1.3472],
        [-1.3454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200904.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0132],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370198.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0132],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370208.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9660e-05, -1.1629e-03,  1.5558e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-5.9660e-05, -1.1629e-03,  1.5558e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-5.9660e-05, -1.1629e-03,  1.5558e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-5.9660e-05, -1.1629e-03,  1.5558e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-5.9660e-05, -1.1629e-03,  1.5558e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-5.9660e-05, -1.1629e-03,  1.5558e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3082.1682, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.5018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0264, device='cuda:0')



h[100].sum tensor(-3.2797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7254, device='cuda:0')



h[200].sum tensor(-17.3052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.4281e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65142.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0968, 0.0011,  ..., 0.0000, 0.0770, 0.0000],
        [0.0025, 0.0970, 0.0011,  ..., 0.0000, 0.0771, 0.0000],
        [0.0048, 0.0925, 0.0012,  ..., 0.0000, 0.0739, 0.0000],
        ...,
        [0.0027, 0.1007, 0.0014,  ..., 0.0000, 0.0801, 0.0000],
        [0.0027, 0.1006, 0.0014,  ..., 0.0000, 0.0801, 0.0000],
        [0.0027, 0.1006, 0.0014,  ..., 0.0000, 0.0801, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696003.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1896.6434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-259.4720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(188.4109, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4499.7598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(80.2966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3691],
        [-1.2352],
        [-1.0109],
        ...,
        [-1.3572],
        [-1.3531],
        [-1.3513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251038.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0132],
        [1.0078],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370208.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0133],
        [1.0078],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370217.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.7631e-05, -1.1657e-03,  1.5839e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-4.7631e-05, -1.1657e-03,  1.5839e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-4.7631e-05, -1.1657e-03,  1.5839e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.7631e-05, -1.1657e-03,  1.5839e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-4.7631e-05, -1.1657e-03,  1.5839e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00],
        [-4.7631e-05, -1.1657e-03,  1.5839e-04,  ..., -1.3531e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3415.9583, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.8188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8257, device='cuda:0')



h[100].sum tensor(-4.5510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0055, device='cuda:0')



h[200].sum tensor(-24.0645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0392e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71545.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0974, 0.0012,  ..., 0.0000, 0.0775, 0.0000],
        [0.0022, 0.0976, 0.0012,  ..., 0.0000, 0.0776, 0.0000],
        [0.0022, 0.0973, 0.0012,  ..., 0.0000, 0.0774, 0.0000],
        ...,
        [0.0024, 0.1014, 0.0015,  ..., 0.0000, 0.0807, 0.0000],
        [0.0024, 0.1013, 0.0015,  ..., 0.0000, 0.0806, 0.0000],
        [0.0024, 0.1013, 0.0015,  ..., 0.0000, 0.0806, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718771.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2329.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-304.3877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(244.5009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4127.5537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(79.4994, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3951],
        [-1.4220],
        [-1.4134],
        ...,
        [-1.3686],
        [-1.3644],
        [-1.3626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212563.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0133],
        [1.0078],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370217.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0134],
        [1.0079],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370226.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7063e-02,  2.9669e-02,  4.9985e-02,  ...,  5.6722e-03,
         -1.6769e-02, -3.2219e-02],
        [ 1.7860e-02,  1.9202e-02,  3.3066e-02,  ...,  3.2872e-03,
         -1.1076e-02, -2.1281e-02],
        [ 9.2778e-03,  9.4405e-03,  1.7289e-02,  ...,  1.0632e-03,
         -5.7669e-03, -1.1080e-02],
        ...,
        [-4.3980e-05, -1.1622e-03,  1.5127e-04,  ..., -1.3526e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3980e-05, -1.1622e-03,  1.5127e-04,  ..., -1.3526e-03,
          0.0000e+00,  0.0000e+00],
        [-4.3980e-05, -1.1622e-03,  1.5127e-04,  ..., -1.3526e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4032.3691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9972, device='cuda:0')



h[100].sum tensor(-6.8746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.2930, device='cuda:0')



h[200].sum tensor(-36.4282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5439e-13, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0812, 0.0879, 0.1503,  ..., 0.0156, 0.0000, 0.0000],
        [0.0594, 0.0631, 0.1102,  ..., 0.0100, 0.0000, 0.0000],
        [0.0330, 0.0342, 0.0616,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85718.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2831, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.2396, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.1824, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0023, 0.1024, 0.0013,  ..., 0.0000, 0.0812, 0.0000],
        [0.0023, 0.1023, 0.0013,  ..., 0.0000, 0.0812, 0.0000],
        [0.0023, 0.1023, 0.0013,  ..., 0.0000, 0.0812, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(789829.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3666.8416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-410.5647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(374.0853, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3925.2588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(82.3700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4830],
        [ 0.4889],
        [ 0.4965],
        ...,
        [-1.3832],
        [-1.3790],
        [-1.3771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190289.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0134],
        [1.0079],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370226.3750, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:35.241665
evaluation loss: 557.3297119140625
epoch: 0 mean loss: 514.8673706054688
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



training loss:
 [514.86737061] 

\evaluation loss:
 [557.32971191]



eval_efficiency:
 [0.93818136 0.93455329 0.93023134 0.92607097 0.918953   0.91222906
 0.90550933 0.89843825 0.89386623 0.88816377 0.87903221 0.87004119
 0.8568506  0.84028762 0.82560969 0.81473739 0.80497413 0.79010606
 0.77734139 0.76672796 0.75739698 0.74851898 0.74172552 0.73664163
 0.72741862 0.71874527 0.71158951 0.70234714 0.69445368 0.68591608
 0.677879   0.66638072 0.65357167 0.6445006  0.63335214 0.62325498
 0.61409038 0.60337809 0.59313286 0.5809467  0.57061919 0.56034386
 0.55067157 0.53887468 0.52934509 0.51951696 0.50873896 0.496243
 0.4839487  0.47082269 0.46026835 0.44907649 0.43690332 0.42539091
 0.41275131 0.39975422 0.3889948  0.37811898 0.36810333 0.35779642
 0.34847087 0.33993131 0.32919536 0.31856636 0.31035784 0.301163
 0.29200479 0.2839106  0.27589901 0.2679156  0.26025075 0.25088665
 0.24201596 0.23478235 0.22782738 0.2201271  0.21304624 0.20583205
 0.20016504 0.19458985 0.1889361  0.18355999 0.17678095 0.17249384
 0.16883459 0.16479279 0.16029047 0.15670194 0.15316874 0.14965936
 0.14669531 0.14327757 0.14146602 0.13915696 0.13700833 0.13466992
 0.13111757 0.12905931 0.12626715 0.12312046] 


eval_purity:
 [0.61917985 0.62102392 0.622525   0.62415037 0.6276357  0.63032289
 0.63226311 0.6353569  0.63772508 0.63983777 0.64086756 0.64294061
 0.64235806 0.6412276  0.64054861 0.64123435 0.64218692 0.64155058
 0.64139965 0.64198552 0.64291658 0.64349463 0.6458647  0.6466904
 0.64766566 0.64865295 0.65123559 0.65256755 0.65251292 0.65224895
 0.65291124 0.65419967 0.65435715 0.65514058 0.65590652 0.65612634
 0.65699594 0.65654044 0.65722363 0.65955946 0.66015115 0.66186985
 0.66363933 0.66233398 0.66329174 0.66385843 0.66315391 0.66178394
 0.65930125 0.65726168 0.65545118 0.65368958 0.65317054 0.6533911
 0.64984736 0.64760566 0.64437158 0.64475094 0.64425007 0.64330087
 0.64275487 0.64089265 0.63672752 0.63500376 0.63489098 0.6335683
 0.63043444 0.62920605 0.6290022  0.62788281 0.62696405 0.62479191
 0.62130108 0.61997216 0.61758972 0.61056041 0.60853904 0.60464349
 0.60098303 0.59659841 0.59458943 0.5907763  0.58305503 0.58398658
 0.58384045 0.58023668 0.57723645 0.57698104 0.57404371 0.57102567
 0.56954124 0.57151557 0.57560536 0.57591987 0.57969006 0.5824082
 0.58190849 0.58420028 0.58420621 0.5860952 ]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0134],
        [1.0079],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(74047.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(613.6219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.2473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(2.4673, device='cuda:0')



h[100].sum tensor(-0.6678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2004, device='cuda:0')



h[200].sum tensor(-3.5461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5239e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(12684.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0997, 0.0005,  ..., 0.0000, 0.0787, 0.0000],
        [0.0033, 0.0976, 0.0005,  ..., 0.0000, 0.0772, 0.0000],
        [0.0076, 0.0876, 0.0003,  ..., 0.0000, 0.0702, 0.0000],
        ...,
        [0.0024, 0.1038, 0.0008,  ..., 0.0000, 0.0819, 0.0000],
        [0.0024, 0.1037, 0.0008,  ..., 0.0000, 0.0819, 0.0000],
        [0.0024, 0.1036, 0.0008,  ..., 0.0000, 0.0818, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(138055.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(323.3657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-49.2564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(33.4836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(974.2594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(17.0428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0064],
        [-0.8883],
        [-0.5928],
        ...,
        [-1.2640],
        [-1.2595],
        [-1.2910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-55692.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0134],
        [1.0079],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(74047.1250, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network after training 
result1: tensor([[-1.0064],
        [-0.8883],
        [-0.5928],
        ...,
        [-1.2640],
        [-1.2595],
        [-1.2910]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0134],
        [1.0079],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(148094.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        ...,
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00],
        [-4.8008e-05, -1.1547e-03,  1.3114e-04,  ..., -1.3515e-03,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(1610.5637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.9773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(9.8491, device='cuda:0')



h[100].sum tensor(-2.7582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7836, device='cuda:0')



h[200].sum tensor(-14.6467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-6.0833e-14, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(35106.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8686e-02, 6.7203e-02, 9.2612e-05,  ..., 0.0000e+00, 5.5829e-02,
         0.0000e+00],
        [5.3870e-03, 9.2911e-02, 3.5504e-04,  ..., 0.0000e+00, 7.3947e-02,
         0.0000e+00],
        [2.1891e-03, 9.9509e-02, 5.7626e-04,  ..., 0.0000e+00, 7.8536e-02,
         0.0000e+00],
        ...,
        [2.3551e-03, 1.0375e-01, 8.2298e-04,  ..., 0.0000e+00, 8.1934e-02,
         0.0000e+00],
        [2.3529e-03, 1.0368e-01, 8.1989e-04,  ..., 0.0000e+00, 8.1881e-02,
         0.0000e+00],
        [2.3509e-03, 1.0364e-01, 8.1522e-04,  ..., 0.0000e+00, 8.1849e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(327789.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1614.1177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-170.9377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.2054, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1844.4546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(35.7935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3985],
        [-0.8169],
        [-1.1072],
        ...,
        [-1.4019],
        [-1.3977],
        [-1.3958]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-103452.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0134],
        [1.0079],
        ...,
        [1.0008],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(148094.2500, device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network after training 
result1: tensor([[-1.0064],
        [-0.8883],
        [-0.5928],
        ...,
        [-1.2640],
        [-1.2595],
        [-1.2910]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



total time: 0:00:39.191531 hpmesh elements: 44 to 45

real	1m25.404s
user	0m49.898s
sys	0m17.751s
