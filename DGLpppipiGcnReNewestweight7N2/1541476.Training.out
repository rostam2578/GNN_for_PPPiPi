0: gpu022.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-d5f273be-27af-8c76-0b01-74d509810fbb)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Fri Aug 12 22:11:48 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B5:00.0 Off |                    0 |
| N/A   39C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b27c51478e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.258s
user	0m2.420s
sys	0m1.012s
[22:11:55] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.0068],
        [-1.1405],
        [ 1.3145],
        ...,
        [ 0.0587],
        [ 0.6507],
        [-0.3484]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-7.5147, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0345,  0.0091, -0.0080,  0.0438,  0.1298, -0.0460,  0.0564,  0.1195,
          0.1280, -0.1483, -0.0368,  0.0307,  0.0605, -0.0377, -0.0653, -0.0633,
          0.0289,  0.0489, -0.1482, -0.0254,  0.1501,  0.1499,  0.0267,  0.1195,
          0.1050,  0.0684,  0.0372, -0.0508, -0.0593, -0.0786, -0.1036, -0.1483,
         -0.0019,  0.0668,  0.0254,  0.1003,  0.0705,  0.1008, -0.1048, -0.0340,
          0.1215, -0.0543, -0.1227,  0.0723, -0.0562, -0.0864,  0.1512,  0.0882,
          0.0511,  0.1193, -0.0462,  0.0344,  0.1209, -0.1183,  0.0144,  0.0467,
          0.0464, -0.0557, -0.0315,  0.0924,  0.0558,  0.0105,  0.0693, -0.0879,
         -0.0529, -0.1122,  0.1318, -0.0993,  0.0926, -0.0307, -0.0480,  0.1200,
          0.0543, -0.1450,  0.1448, -0.0509, -0.1409, -0.1463,  0.0935,  0.1244,
          0.0361,  0.0370,  0.0622, -0.0700, -0.1179, -0.0674,  0.1392,  0.1183,
         -0.0385,  0.1467,  0.0798,  0.0853, -0.0233, -0.0763,  0.0377,  0.1000,
          0.1063, -0.0769,  0.0319, -0.0742, -0.1330, -0.0468,  0.1232,  0.1263,
          0.0911, -0.0888, -0.0995, -0.1426,  0.1214, -0.0451, -0.0926, -0.1138,
          0.1313,  0.1369, -0.0673,  0.0737,  0.0569,  0.0738,  0.1319, -0.1190,
          0.0275,  0.0759,  0.1269,  0.0207, -0.1526,  0.0497,  0.0518, -0.0249,
         -0.0995,  0.0112,  0.0306, -0.1525,  0.0373,  0.1301, -0.0097,  0.1203,
         -0.0412,  0.0036, -0.0433,  0.1361,  0.0498, -0.1516, -0.0538, -0.0183,
         -0.1162,  0.1452, -0.1054,  0.0436, -0.1326,  0.0593, -0.0376, -0.0580,
         -0.1204,  0.1184,  0.0970, -0.1284,  0.1470, -0.0526, -0.0460, -0.0331,
         -0.0849,  0.1527, -0.1116,  0.0136, -0.0515,  0.1162, -0.0790, -0.1295,
          0.0396,  0.1339, -0.0053, -0.1366,  0.1008,  0.0102,  0.1084, -0.1289,
         -0.1316,  0.1378, -0.1424, -0.1021, -0.0427,  0.1081,  0.1085,  0.1298,
         -0.0676,  0.0270, -0.0567, -0.0264, -0.0986, -0.1256,  0.0488,  0.1294,
          0.0288,  0.0236, -0.0478,  0.0608,  0.0464, -0.0956, -0.0310, -0.0238,
          0.0111,  0.0225,  0.1129,  0.1204,  0.0154,  0.1419, -0.0166,  0.0371,
          0.0254,  0.0088, -0.0407,  0.0622,  0.0326,  0.0041, -0.0246,  0.0717,
          0.0904, -0.0724, -0.0478, -0.1320, -0.1009,  0.0995,  0.0467, -0.1490,
         -0.0138, -0.0019,  0.0716, -0.0829,  0.0479,  0.1097,  0.0986, -0.1466,
         -0.0572,  0.0797,  0.1376, -0.1020, -0.1312, -0.1467,  0.0614, -0.1337,
         -0.1392,  0.1274,  0.1473,  0.0470,  0.0043, -0.0769, -0.0729,  0.1495,
          0.0249, -0.0021, -0.1350, -0.1381,  0.1163,  0.1130, -0.1468,  0.0464]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0345,  0.0091, -0.0080,  0.0438,  0.1298, -0.0460,  0.0564,  0.1195,
          0.1280, -0.1483, -0.0368,  0.0307,  0.0605, -0.0377, -0.0653, -0.0633,
          0.0289,  0.0489, -0.1482, -0.0254,  0.1501,  0.1499,  0.0267,  0.1195,
          0.1050,  0.0684,  0.0372, -0.0508, -0.0593, -0.0786, -0.1036, -0.1483,
         -0.0019,  0.0668,  0.0254,  0.1003,  0.0705,  0.1008, -0.1048, -0.0340,
          0.1215, -0.0543, -0.1227,  0.0723, -0.0562, -0.0864,  0.1512,  0.0882,
          0.0511,  0.1193, -0.0462,  0.0344,  0.1209, -0.1183,  0.0144,  0.0467,
          0.0464, -0.0557, -0.0315,  0.0924,  0.0558,  0.0105,  0.0693, -0.0879,
         -0.0529, -0.1122,  0.1318, -0.0993,  0.0926, -0.0307, -0.0480,  0.1200,
          0.0543, -0.1450,  0.1448, -0.0509, -0.1409, -0.1463,  0.0935,  0.1244,
          0.0361,  0.0370,  0.0622, -0.0700, -0.1179, -0.0674,  0.1392,  0.1183,
         -0.0385,  0.1467,  0.0798,  0.0853, -0.0233, -0.0763,  0.0377,  0.1000,
          0.1063, -0.0769,  0.0319, -0.0742, -0.1330, -0.0468,  0.1232,  0.1263,
          0.0911, -0.0888, -0.0995, -0.1426,  0.1214, -0.0451, -0.0926, -0.1138,
          0.1313,  0.1369, -0.0673,  0.0737,  0.0569,  0.0738,  0.1319, -0.1190,
          0.0275,  0.0759,  0.1269,  0.0207, -0.1526,  0.0497,  0.0518, -0.0249,
         -0.0995,  0.0112,  0.0306, -0.1525,  0.0373,  0.1301, -0.0097,  0.1203,
         -0.0412,  0.0036, -0.0433,  0.1361,  0.0498, -0.1516, -0.0538, -0.0183,
         -0.1162,  0.1452, -0.1054,  0.0436, -0.1326,  0.0593, -0.0376, -0.0580,
         -0.1204,  0.1184,  0.0970, -0.1284,  0.1470, -0.0526, -0.0460, -0.0331,
         -0.0849,  0.1527, -0.1116,  0.0136, -0.0515,  0.1162, -0.0790, -0.1295,
          0.0396,  0.1339, -0.0053, -0.1366,  0.1008,  0.0102,  0.1084, -0.1289,
         -0.1316,  0.1378, -0.1424, -0.1021, -0.0427,  0.1081,  0.1085,  0.1298,
         -0.0676,  0.0270, -0.0567, -0.0264, -0.0986, -0.1256,  0.0488,  0.1294,
          0.0288,  0.0236, -0.0478,  0.0608,  0.0464, -0.0956, -0.0310, -0.0238,
          0.0111,  0.0225,  0.1129,  0.1204,  0.0154,  0.1419, -0.0166,  0.0371,
          0.0254,  0.0088, -0.0407,  0.0622,  0.0326,  0.0041, -0.0246,  0.0717,
          0.0904, -0.0724, -0.0478, -0.1320, -0.1009,  0.0995,  0.0467, -0.1490,
         -0.0138, -0.0019,  0.0716, -0.0829,  0.0479,  0.1097,  0.0986, -0.1466,
         -0.0572,  0.0797,  0.1376, -0.1020, -0.1312, -0.1467,  0.0614, -0.1337,
         -0.1392,  0.1274,  0.1473,  0.0470,  0.0043, -0.0769, -0.0729,  0.1495,
          0.0249, -0.0021, -0.1350, -0.1381,  0.1163,  0.1130, -0.1468,  0.0464]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.1062, -0.0149,  0.0072,  ...,  0.0057, -0.0565, -0.0502],
        [ 0.0865, -0.0623,  0.0349,  ...,  0.0232,  0.0220,  0.0971],
        [-0.0402,  0.1079, -0.0068,  ..., -0.0074,  0.1010, -0.0034],
        ...,
        [ 0.0917, -0.0124, -0.1177,  ...,  0.0967,  0.0165, -0.1139],
        [-0.0523,  0.1174, -0.0290,  ...,  0.0759, -0.0514, -0.0442],
        [ 0.0690, -0.0759,  0.0921,  ..., -0.0261,  0.0600,  0.1022]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1062, -0.0149,  0.0072,  ...,  0.0057, -0.0565, -0.0502],
        [ 0.0865, -0.0623,  0.0349,  ...,  0.0232,  0.0220,  0.0971],
        [-0.0402,  0.1079, -0.0068,  ..., -0.0074,  0.1010, -0.0034],
        ...,
        [ 0.0917, -0.0124, -0.1177,  ...,  0.0967,  0.0165, -0.1139],
        [-0.0523,  0.1174, -0.0290,  ...,  0.0759, -0.0514, -0.0442],
        [ 0.0690, -0.0759,  0.0921,  ..., -0.0261,  0.0600,  0.1022]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0858, -0.1629, -0.1288,  ...,  0.1723, -0.1275, -0.0498],
        [-0.0144,  0.1080,  0.1712,  ...,  0.0994, -0.0155,  0.0513],
        [-0.1061,  0.1743,  0.0882,  ..., -0.0851,  0.0887,  0.1705],
        ...,
        [-0.1216, -0.1744,  0.0776,  ...,  0.1015, -0.0609, -0.1611],
        [-0.0363,  0.1182, -0.1089,  ..., -0.1555, -0.0310,  0.1452],
        [ 0.0545, -0.0347, -0.1698,  ...,  0.1412, -0.0706,  0.0169]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0858, -0.1629, -0.1288,  ...,  0.1723, -0.1275, -0.0498],
        [-0.0144,  0.1080,  0.1712,  ...,  0.0994, -0.0155,  0.0513],
        [-0.1061,  0.1743,  0.0882,  ..., -0.0851,  0.0887,  0.1705],
        ...,
        [-0.1216, -0.1744,  0.0776,  ...,  0.1015, -0.0609, -0.1611],
        [-0.0363,  0.1182, -0.1089,  ..., -0.1555, -0.0310,  0.1452],
        [ 0.0545, -0.0347, -0.1698,  ...,  0.1412, -0.0706,  0.0169]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0492, -0.1471, -0.1310,  ...,  0.2237, -0.1285, -0.1838],
        [ 0.1323, -0.2457, -0.0740,  ...,  0.2270,  0.1519, -0.0684],
        [-0.0045, -0.2290,  0.0714,  ..., -0.2370, -0.1162,  0.0775],
        ...,
        [ 0.2496, -0.0785, -0.2359,  ...,  0.0573, -0.0245,  0.1018],
        [-0.0703,  0.1197, -0.2364,  ..., -0.0765, -0.1041,  0.1681],
        [ 0.0752,  0.0695,  0.1819,  ...,  0.0316, -0.1216,  0.2193]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0492, -0.1471, -0.1310,  ...,  0.2237, -0.1285, -0.1838],
        [ 0.1323, -0.2457, -0.0740,  ...,  0.2270,  0.1519, -0.0684],
        [-0.0045, -0.2290,  0.0714,  ..., -0.2370, -0.1162,  0.0775],
        ...,
        [ 0.2496, -0.0785, -0.2359,  ...,  0.0573, -0.0245,  0.1018],
        [-0.0703,  0.1197, -0.2364,  ..., -0.0765, -0.1041,  0.1681],
        [ 0.0752,  0.0695,  0.1819,  ...,  0.0316, -0.1216,  0.2193]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.0206],
        [-0.4045],
        [-0.1158],
        [-0.2202],
        [ 0.1964],
        [-0.2146],
        [-0.2237],
        [ 0.3286],
        [ 0.1965],
        [-0.1187],
        [ 0.0247],
        [ 0.0642],
        [-0.3559],
        [ 0.2053],
        [-0.2649],
        [-0.3167],
        [ 0.1901],
        [-0.1940],
        [ 0.1580],
        [-0.4206],
        [ 0.3008],
        [-0.0847],
        [ 0.2638],
        [ 0.3831],
        [ 0.3601],
        [-0.1656],
        [ 0.3781],
        [-0.2827],
        [ 0.3538],
        [-0.1615],
        [ 0.2389],
        [-0.3935]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.0206],
        [-0.4045],
        [-0.1158],
        [-0.2202],
        [ 0.1964],
        [-0.2146],
        [-0.2237],
        [ 0.3286],
        [ 0.1965],
        [-0.1187],
        [ 0.0247],
        [ 0.0642],
        [-0.3559],
        [ 0.2053],
        [-0.2649],
        [-0.3167],
        [ 0.1901],
        [-0.1940],
        [ 0.1580],
        [-0.4206],
        [ 0.3008],
        [-0.0847],
        [ 0.2638],
        [ 0.3831],
        [ 0.3601],
        [-0.1656],
        [ 0.3781],
        [-0.2827],
        [ 0.3538],
        [-0.1615],
        [ 0.2389],
        [-0.3935]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1092, -0.1041,  0.0056, -0.0643, -0.0937, -0.0844, -0.1327,  0.0217,
         -0.1436, -0.1399, -0.0129,  0.0093,  0.0635, -0.0020, -0.0117,  0.0637,
         -0.0979, -0.0413,  0.0114, -0.1406,  0.0559, -0.1502, -0.1276, -0.1428,
         -0.0563, -0.0152, -0.0150, -0.0043,  0.0018,  0.1183,  0.0558, -0.1072,
          0.0215,  0.0835, -0.0082, -0.0607,  0.0877,  0.0701, -0.1228, -0.0445,
         -0.1088, -0.0057,  0.1445,  0.1129, -0.0184, -0.0220,  0.0170, -0.0549,
         -0.0263, -0.1359, -0.0767,  0.0176, -0.1467, -0.1052, -0.0335,  0.1358,
         -0.1363, -0.1013, -0.0967, -0.1332, -0.0249, -0.0223, -0.0937, -0.0443,
         -0.1135, -0.0346,  0.0548, -0.0675,  0.0827, -0.0554,  0.1404,  0.0269,
         -0.1177,  0.0634,  0.1243, -0.0886,  0.0325, -0.0576,  0.0286,  0.0173,
          0.0762,  0.1383, -0.0861, -0.0650, -0.0508, -0.1272, -0.0664,  0.0778,
         -0.1317, -0.0023,  0.0258,  0.1122, -0.1176,  0.0626, -0.1380,  0.0119,
          0.1375,  0.1463, -0.1312,  0.1240, -0.0483, -0.0683,  0.1200,  0.0494,
         -0.0788,  0.1369,  0.0930,  0.1474,  0.1419,  0.1038,  0.1482, -0.1230,
          0.1002, -0.1038,  0.0386, -0.1363,  0.0761, -0.0862, -0.0917, -0.1482,
          0.1360, -0.0768,  0.1042, -0.0448, -0.0879, -0.1367, -0.1225,  0.1470,
          0.0805, -0.0100,  0.1135, -0.0391,  0.1415, -0.0279,  0.1011,  0.0163,
          0.0157,  0.0991,  0.1150, -0.1439,  0.1197, -0.0247,  0.0214, -0.0192,
         -0.1286, -0.0365,  0.0559,  0.1232,  0.1159,  0.0817, -0.0672,  0.0471,
         -0.1220,  0.0147, -0.1506, -0.0297,  0.0362,  0.1204, -0.1234, -0.1364,
         -0.0592, -0.0880, -0.0297, -0.0599,  0.1401,  0.0996,  0.0403,  0.1110,
          0.0219, -0.1041, -0.0525,  0.0563, -0.0919, -0.1060,  0.0754, -0.1045,
          0.0607, -0.0399, -0.0624,  0.0010, -0.0287, -0.0367,  0.0332,  0.0411,
         -0.0664,  0.0945, -0.1336,  0.1506, -0.0404,  0.0452, -0.1138,  0.1041,
          0.1467,  0.1391,  0.0923,  0.0291, -0.0689,  0.0598, -0.1332, -0.0418,
          0.0484, -0.0097,  0.0341,  0.0468, -0.0243, -0.1423, -0.0711, -0.0617,
          0.0038, -0.0467, -0.1327, -0.0754,  0.0634, -0.1136,  0.0252,  0.0898,
          0.1413,  0.0410, -0.0167,  0.0711,  0.0703,  0.1486,  0.0959, -0.1069,
         -0.0346,  0.1058, -0.0093, -0.0864, -0.1375, -0.1518, -0.1207, -0.0927,
         -0.1315,  0.0462,  0.1001,  0.1471, -0.0479,  0.1511,  0.0655, -0.0130,
         -0.0906,  0.0606,  0.0811, -0.0044, -0.0451, -0.0582, -0.0695,  0.0451,
         -0.0892, -0.0215,  0.1235, -0.0671,  0.0035, -0.1474, -0.0052,  0.0676]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1092, -0.1041,  0.0056, -0.0643, -0.0937, -0.0844, -0.1327,  0.0217,
         -0.1436, -0.1399, -0.0129,  0.0093,  0.0635, -0.0020, -0.0117,  0.0637,
         -0.0979, -0.0413,  0.0114, -0.1406,  0.0559, -0.1502, -0.1276, -0.1428,
         -0.0563, -0.0152, -0.0150, -0.0043,  0.0018,  0.1183,  0.0558, -0.1072,
          0.0215,  0.0835, -0.0082, -0.0607,  0.0877,  0.0701, -0.1228, -0.0445,
         -0.1088, -0.0057,  0.1445,  0.1129, -0.0184, -0.0220,  0.0170, -0.0549,
         -0.0263, -0.1359, -0.0767,  0.0176, -0.1467, -0.1052, -0.0335,  0.1358,
         -0.1363, -0.1013, -0.0967, -0.1332, -0.0249, -0.0223, -0.0937, -0.0443,
         -0.1135, -0.0346,  0.0548, -0.0675,  0.0827, -0.0554,  0.1404,  0.0269,
         -0.1177,  0.0634,  0.1243, -0.0886,  0.0325, -0.0576,  0.0286,  0.0173,
          0.0762,  0.1383, -0.0861, -0.0650, -0.0508, -0.1272, -0.0664,  0.0778,
         -0.1317, -0.0023,  0.0258,  0.1122, -0.1176,  0.0626, -0.1380,  0.0119,
          0.1375,  0.1463, -0.1312,  0.1240, -0.0483, -0.0683,  0.1200,  0.0494,
         -0.0788,  0.1369,  0.0930,  0.1474,  0.1419,  0.1038,  0.1482, -0.1230,
          0.1002, -0.1038,  0.0386, -0.1363,  0.0761, -0.0862, -0.0917, -0.1482,
          0.1360, -0.0768,  0.1042, -0.0448, -0.0879, -0.1367, -0.1225,  0.1470,
          0.0805, -0.0100,  0.1135, -0.0391,  0.1415, -0.0279,  0.1011,  0.0163,
          0.0157,  0.0991,  0.1150, -0.1439,  0.1197, -0.0247,  0.0214, -0.0192,
         -0.1286, -0.0365,  0.0559,  0.1232,  0.1159,  0.0817, -0.0672,  0.0471,
         -0.1220,  0.0147, -0.1506, -0.0297,  0.0362,  0.1204, -0.1234, -0.1364,
         -0.0592, -0.0880, -0.0297, -0.0599,  0.1401,  0.0996,  0.0403,  0.1110,
          0.0219, -0.1041, -0.0525,  0.0563, -0.0919, -0.1060,  0.0754, -0.1045,
          0.0607, -0.0399, -0.0624,  0.0010, -0.0287, -0.0367,  0.0332,  0.0411,
         -0.0664,  0.0945, -0.1336,  0.1506, -0.0404,  0.0452, -0.1138,  0.1041,
          0.1467,  0.1391,  0.0923,  0.0291, -0.0689,  0.0598, -0.1332, -0.0418,
          0.0484, -0.0097,  0.0341,  0.0468, -0.0243, -0.1423, -0.0711, -0.0617,
          0.0038, -0.0467, -0.1327, -0.0754,  0.0634, -0.1136,  0.0252,  0.0898,
          0.1413,  0.0410, -0.0167,  0.0711,  0.0703,  0.1486,  0.0959, -0.1069,
         -0.0346,  0.1058, -0.0093, -0.0864, -0.1375, -0.1518, -0.1207, -0.0927,
         -0.1315,  0.0462,  0.1001,  0.1471, -0.0479,  0.1511,  0.0655, -0.0130,
         -0.0906,  0.0606,  0.0811, -0.0044, -0.0451, -0.0582, -0.0695,  0.0451,
         -0.0892, -0.0215,  0.1235, -0.0671,  0.0035, -0.1474, -0.0052,  0.0676]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.1203, -0.1248, -0.1228,  ..., -0.0555,  0.0611,  0.0600],
        [-0.1139, -0.0046,  0.0154,  ..., -0.0661,  0.0008,  0.0059],
        [ 0.1078,  0.0128, -0.0849,  ..., -0.0002, -0.0172,  0.0883],
        ...,
        [-0.0258, -0.1183, -0.0166,  ...,  0.0737,  0.0044,  0.0541],
        [ 0.0033,  0.0679, -0.1212,  ...,  0.0029, -0.0766, -0.0170],
        [-0.0481,  0.0707,  0.1102,  ...,  0.1075,  0.0704, -0.0834]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1203, -0.1248, -0.1228,  ..., -0.0555,  0.0611,  0.0600],
        [-0.1139, -0.0046,  0.0154,  ..., -0.0661,  0.0008,  0.0059],
        [ 0.1078,  0.0128, -0.0849,  ..., -0.0002, -0.0172,  0.0883],
        ...,
        [-0.0258, -0.1183, -0.0166,  ...,  0.0737,  0.0044,  0.0541],
        [ 0.0033,  0.0679, -0.1212,  ...,  0.0029, -0.0766, -0.0170],
        [-0.0481,  0.0707,  0.1102,  ...,  0.1075,  0.0704, -0.0834]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0166, -0.0778, -0.1627,  ...,  0.0542,  0.0579, -0.1337],
        [-0.0856,  0.1501,  0.0768,  ...,  0.1017, -0.1264, -0.1712],
        [-0.1321, -0.0859, -0.1673,  ...,  0.1216, -0.0638,  0.0755],
        ...,
        [-0.1523, -0.1758, -0.1002,  ..., -0.1391,  0.0426, -0.0660],
        [ 0.1468, -0.0558,  0.0728,  ..., -0.0026,  0.0498, -0.0797],
        [ 0.1299, -0.0708, -0.0604,  ...,  0.0251, -0.0963,  0.0113]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0166, -0.0778, -0.1627,  ...,  0.0542,  0.0579, -0.1337],
        [-0.0856,  0.1501,  0.0768,  ...,  0.1017, -0.1264, -0.1712],
        [-0.1321, -0.0859, -0.1673,  ...,  0.1216, -0.0638,  0.0755],
        ...,
        [-0.1523, -0.1758, -0.1002,  ..., -0.1391,  0.0426, -0.0660],
        [ 0.1468, -0.0558,  0.0728,  ..., -0.0026,  0.0498, -0.0797],
        [ 0.1299, -0.0708, -0.0604,  ...,  0.0251, -0.0963,  0.0113]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1325,  0.0838, -0.0118,  ...,  0.2490, -0.0940,  0.1993],
        [ 0.1272,  0.0628,  0.1011,  ...,  0.0193, -0.1789,  0.0914],
        [-0.1068,  0.1218,  0.2330,  ...,  0.0074, -0.2403,  0.0211],
        ...,
        [-0.0340, -0.2386, -0.2204,  ..., -0.0506,  0.1067, -0.1991],
        [ 0.0878,  0.0555, -0.0512,  ..., -0.1355,  0.0529, -0.0755],
        [-0.1919, -0.0521,  0.0199,  ...,  0.2378,  0.0139,  0.0576]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1325,  0.0838, -0.0118,  ...,  0.2490, -0.0940,  0.1993],
        [ 0.1272,  0.0628,  0.1011,  ...,  0.0193, -0.1789,  0.0914],
        [-0.1068,  0.1218,  0.2330,  ...,  0.0074, -0.2403,  0.0211],
        ...,
        [-0.0340, -0.2386, -0.2204,  ..., -0.0506,  0.1067, -0.1991],
        [ 0.0878,  0.0555, -0.0512,  ..., -0.1355,  0.0529, -0.0755],
        [-0.1919, -0.0521,  0.0199,  ...,  0.2378,  0.0139,  0.0576]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2401],
        [-0.3195],
        [-0.2526],
        [-0.1487],
        [ 0.3397],
        [-0.4132],
        [-0.1726],
        [ 0.2488],
        [-0.0457],
        [ 0.0675],
        [-0.1602],
        [ 0.1579],
        [ 0.1501],
        [-0.0941],
        [-0.4149],
        [ 0.2322],
        [-0.0785],
        [ 0.1847],
        [ 0.2288],
        [ 0.0925],
        [ 0.2322],
        [-0.0886],
        [ 0.2182],
        [-0.0063],
        [-0.4114],
        [-0.3722],
        [-0.1442],
        [ 0.3545],
        [-0.3918],
        [ 0.1591],
        [-0.0756],
        [-0.1450]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2401],
        [-0.3195],
        [-0.2526],
        [-0.1487],
        [ 0.3397],
        [-0.4132],
        [-0.1726],
        [ 0.2488],
        [-0.0457],
        [ 0.0675],
        [-0.1602],
        [ 0.1579],
        [ 0.1501],
        [-0.0941],
        [-0.4149],
        [ 0.2322],
        [-0.0785],
        [ 0.1847],
        [ 0.2288],
        [ 0.0925],
        [ 0.2322],
        [-0.0886],
        [ 0.2182],
        [-0.0063],
        [-0.4114],
        [-0.3722],
        [-0.1442],
        [ 0.3545],
        [-0.3918],
        [ 0.1591],
        [-0.0756],
        [-0.1450]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-60.9621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-3.0852, device='cuda:0')



h[100].sum tensor(-2.4468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.5112, device='cuda:0')



h[200].sum tensor(2.5170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(2.5832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2518.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.5801e-05, 1.5349e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         4.5088e-04],
        [1.8680e-04, 8.0086e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.3526e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(12660.2275, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7.2763, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(0.5821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(241.6857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(19.3342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-3.8389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1153],
        [-0.1412],
        [-0.2035],
        ...,
        [-0.0326],
        [-0.0326],
        [-0.0258]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-2909.5566, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.1153],
        [-0.1412],
        [-0.2035],
        ...,
        [-0.0326],
        [-0.0326],
        [-0.0258]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-5.4860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.0857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9310, device='cuda:0')



h[100].sum tensor(-17.6365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.4935, device='cuda:0')



h[200].sum tensor(11.9083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8118, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15259.6182, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0173, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0066],
        [0.0036, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(92474.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1397.9028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(98.3490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1830.7063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.7987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-27.6419, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3150],
        [-0.1931],
        [-0.1182],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-18502.0137, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.1153],
        [-0.1412],
        [-0.2035],
        ...,
        [-0.0326],
        [-0.0326],
        [-0.0258]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/checkpoint_dir/100016284saved_checkpoint.tar



load_model True 
TraEvN 10001 
BatchSize 30 
EpochNum 6 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 6.5320e-02,  1.0181e-01,  7.7903e-02, -1.4600e-01, -4.3572e-02,
          8.7323e-02,  6.1367e-02,  6.8252e-03,  1.5011e-01,  3.0484e-02,
          9.8042e-02, -2.2791e-02, -1.0692e-01,  8.8814e-05,  4.6891e-02,
         -1.0057e-01, -3.6017e-02,  5.3465e-02, -1.1657e-01, -7.1127e-02,
         -1.4378e-01,  6.1435e-02, -7.2882e-02,  4.2995e-02, -1.3643e-01,
          1.3640e-01,  9.0186e-02,  5.3037e-03,  1.3307e-01,  2.0841e-02,
         -9.9000e-02, -7.7175e-02,  1.1318e-01,  4.0370e-02, -9.5113e-02,
          1.3355e-01,  4.7370e-02, -3.3772e-02,  3.4349e-02,  2.3070e-02,
         -1.1799e-01,  6.8780e-02, -1.4752e-01,  1.1090e-01,  1.4430e-01,
          8.3771e-02,  4.0702e-02, -1.3604e-02, -6.3380e-02,  5.2267e-02,
          9.2418e-03, -6.4968e-02, -1.4224e-01,  5.6899e-02,  1.1670e-01,
          6.7457e-02,  2.8506e-02,  1.3511e-01, -1.3263e-01,  5.1032e-03,
         -8.1742e-02, -1.0755e-01,  1.8807e-02, -8.8458e-02,  1.4454e-01,
          7.7570e-03,  7.0031e-02,  1.3322e-01,  5.1202e-03,  8.5038e-02,
         -9.5406e-02,  4.1124e-02,  7.3718e-02, -7.3250e-02, -7.8510e-02,
          1.3648e-01, -3.8644e-02, -5.9758e-02,  4.2668e-02,  3.7067e-02,
          2.1780e-02,  9.0435e-02, -8.5697e-02,  1.4106e-01, -1.4863e-01,
         -4.0607e-02,  1.2539e-02,  1.1635e-01, -2.9013e-02, -3.3969e-02,
         -1.0041e-01, -2.2505e-02, -4.3770e-02, -1.2142e-01, -7.5965e-03,
         -8.0608e-02,  1.0952e-01,  9.1776e-02,  3.9711e-02, -7.6285e-03,
          1.4626e-01,  1.2131e-01, -1.3239e-01, -2.4071e-02, -1.3341e-01,
          5.7662e-02, -1.0812e-01, -9.6267e-02, -1.3736e-01, -1.5650e-02,
          8.4463e-02,  8.1897e-02,  1.3349e-01,  1.3813e-01, -1.3961e-01,
          9.7126e-02, -1.3322e-02, -1.2697e-01, -1.2040e-01, -7.6959e-03,
         -5.0588e-02,  2.5234e-02,  1.4276e-01,  6.0558e-02,  2.7517e-02,
          9.5785e-02,  3.3276e-02,  8.3802e-02,  9.0318e-02,  1.4609e-02,
         -1.0055e-01,  1.5036e-01, -1.4964e-01, -1.4319e-01, -1.2591e-01,
         -5.3744e-02,  9.1233e-02, -1.1197e-02,  5.4623e-02, -3.2091e-02,
         -7.4419e-02,  1.6500e-02, -1.3021e-01, -1.3832e-02,  1.3406e-01,
         -7.8179e-02, -9.3269e-03, -1.2956e-01,  4.3121e-02,  2.5552e-02,
         -1.0122e-01, -1.4670e-01, -5.0054e-02,  3.7624e-02,  7.2612e-02,
         -2.2943e-02,  3.5052e-02, -1.4247e-01, -9.9177e-03, -4.9478e-03,
         -1.5277e-01, -6.0029e-02, -6.9180e-03, -9.6907e-02,  6.3530e-02,
          1.2865e-01,  1.0276e-01,  7.0664e-02, -1.0914e-01, -9.9896e-02,
          1.5651e-02, -3.5871e-02, -3.0548e-02, -1.4544e-01,  1.1284e-01,
         -6.2944e-02, -1.3796e-01, -1.2551e-01,  7.6147e-02, -2.7632e-02,
         -8.2619e-02, -6.1456e-02, -7.6753e-02, -1.2193e-01, -1.7246e-02,
         -1.1007e-01, -4.3622e-02, -7.1907e-02,  1.4858e-01, -2.8724e-02,
         -1.4080e-01, -1.1493e-01,  1.1055e-01, -1.5103e-01, -8.4363e-02,
         -9.4768e-02,  1.4033e-01, -1.4749e-01,  1.2793e-01, -8.7863e-03,
         -9.4301e-02,  4.2799e-02, -1.2130e-01,  9.2468e-02,  8.5338e-02,
          1.7445e-02, -6.6981e-02,  2.4236e-02, -1.1762e-01,  1.3288e-01,
         -1.0897e-01, -1.3084e-01,  4.0134e-02,  1.0040e-01,  7.3381e-02,
          1.5185e-01, -1.0410e-01,  7.3424e-02,  7.3819e-02, -7.2305e-02,
         -1.4091e-01,  1.4118e-01,  5.9405e-02, -8.6681e-02,  4.4945e-02,
         -4.8203e-02, -9.6928e-02, -1.3909e-02,  1.4363e-01, -8.8163e-03,
          4.9525e-02,  1.3201e-02,  7.4475e-02, -1.5028e-01, -4.1728e-03,
          1.0109e-01,  1.0631e-02,  2.9087e-02,  6.6718e-02,  1.2589e-01,
          1.7914e-02,  1.5014e-01,  3.1768e-02,  4.1997e-02,  7.6421e-02,
          4.0088e-03,  2.1495e-02, -1.1133e-01,  1.2904e-01,  1.1537e-01,
         -1.2158e-01,  6.0158e-02,  1.0649e-01, -1.3671e-01,  1.5074e-01,
         -1.4565e-01]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0549,  0.1066, -0.0258,  ...,  0.0463, -0.1011, -0.0468],
        [ 0.0345, -0.0973, -0.1060,  ..., -0.1043,  0.0904,  0.0420],
        [ 0.0818, -0.1122, -0.1048,  ..., -0.0883,  0.0457, -0.0399],
        ...,
        [-0.0597,  0.0182,  0.0788,  ..., -0.0174,  0.0254,  0.0835],
        [-0.0494,  0.0548, -0.0887,  ..., -0.1088,  0.1233,  0.0360],
        [ 0.0123, -0.0277,  0.1083,  ..., -0.0715,  0.0680, -0.1147]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1290, -0.0370,  0.1140,  ..., -0.0021, -0.0115,  0.0288],
        [-0.0314,  0.0820,  0.0682,  ...,  0.0406,  0.0747,  0.1578],
        [-0.0762, -0.1275,  0.0470,  ...,  0.0132, -0.1074,  0.1517],
        ...,
        [ 0.1098,  0.0640, -0.1441,  ...,  0.1499,  0.0172,  0.0326],
        [ 0.1090, -0.1489,  0.0308,  ...,  0.1030, -0.1366, -0.1307],
        [-0.1088,  0.1470,  0.1163,  ..., -0.0369,  0.1110, -0.1223]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1910,  0.0908,  0.2291,  ...,  0.1086,  0.2212,  0.1080],
        [-0.0657,  0.1305,  0.0345,  ...,  0.1100,  0.1880, -0.2194],
        [-0.0988,  0.0804,  0.1814,  ..., -0.2276,  0.0191,  0.1817],
        ...,
        [-0.1003,  0.0284, -0.1762,  ...,  0.1032,  0.1745, -0.1287],
        [ 0.0102,  0.1638,  0.1566,  ..., -0.1131,  0.2066, -0.1182],
        [ 0.1648,  0.0677,  0.1914,  ...,  0.2366, -0.2475, -0.2044]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0286],
        [ 0.3978],
        [-0.4181],
        [-0.0154],
        [ 0.3237],
        [-0.0057],
        [-0.1712],
        [ 0.1107],
        [ 0.1949],
        [-0.3678],
        [ 0.0081],
        [-0.1936],
        [-0.0981],
        [-0.1243],
        [-0.1083],
        [ 0.2809],
        [ 0.2562],
        [ 0.3129],
        [-0.1749],
        [ 0.0061],
        [-0.0825],
        [ 0.4026],
        [-0.2210],
        [-0.2470],
        [-0.1987],
        [ 0.0141],
        [ 0.4149],
        [ 0.4150],
        [-0.2610],
        [-0.2093],
        [-0.1946],
        [-0.0462]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 6.5320e-02,  1.0181e-01,  7.7903e-02, -1.4600e-01, -4.3572e-02,
          8.7323e-02,  6.1367e-02,  6.8252e-03,  1.5011e-01,  3.0484e-02,
          9.8042e-02, -2.2791e-02, -1.0692e-01,  8.8814e-05,  4.6891e-02,
         -1.0057e-01, -3.6017e-02,  5.3465e-02, -1.1657e-01, -7.1127e-02,
         -1.4378e-01,  6.1435e-02, -7.2882e-02,  4.2995e-02, -1.3643e-01,
          1.3640e-01,  9.0186e-02,  5.3037e-03,  1.3307e-01,  2.0841e-02,
         -9.9000e-02, -7.7175e-02,  1.1318e-01,  4.0370e-02, -9.5113e-02,
          1.3355e-01,  4.7370e-02, -3.3772e-02,  3.4349e-02,  2.3070e-02,
         -1.1799e-01,  6.8780e-02, -1.4752e-01,  1.1090e-01,  1.4430e-01,
          8.3771e-02,  4.0702e-02, -1.3604e-02, -6.3380e-02,  5.2267e-02,
          9.2418e-03, -6.4968e-02, -1.4224e-01,  5.6899e-02,  1.1670e-01,
          6.7457e-02,  2.8506e-02,  1.3511e-01, -1.3263e-01,  5.1032e-03,
         -8.1742e-02, -1.0755e-01,  1.8807e-02, -8.8458e-02,  1.4454e-01,
          7.7570e-03,  7.0031e-02,  1.3322e-01,  5.1202e-03,  8.5038e-02,
         -9.5406e-02,  4.1124e-02,  7.3718e-02, -7.3250e-02, -7.8510e-02,
          1.3648e-01, -3.8644e-02, -5.9758e-02,  4.2668e-02,  3.7067e-02,
          2.1780e-02,  9.0435e-02, -8.5697e-02,  1.4106e-01, -1.4863e-01,
         -4.0607e-02,  1.2539e-02,  1.1635e-01, -2.9013e-02, -3.3969e-02,
         -1.0041e-01, -2.2505e-02, -4.3770e-02, -1.2142e-01, -7.5965e-03,
         -8.0608e-02,  1.0952e-01,  9.1776e-02,  3.9711e-02, -7.6285e-03,
          1.4626e-01,  1.2131e-01, -1.3239e-01, -2.4071e-02, -1.3341e-01,
          5.7662e-02, -1.0812e-01, -9.6267e-02, -1.3736e-01, -1.5650e-02,
          8.4463e-02,  8.1897e-02,  1.3349e-01,  1.3813e-01, -1.3961e-01,
          9.7126e-02, -1.3322e-02, -1.2697e-01, -1.2040e-01, -7.6959e-03,
         -5.0588e-02,  2.5234e-02,  1.4276e-01,  6.0558e-02,  2.7517e-02,
          9.5785e-02,  3.3276e-02,  8.3802e-02,  9.0318e-02,  1.4609e-02,
         -1.0055e-01,  1.5036e-01, -1.4964e-01, -1.4319e-01, -1.2591e-01,
         -5.3744e-02,  9.1233e-02, -1.1197e-02,  5.4623e-02, -3.2091e-02,
         -7.4419e-02,  1.6500e-02, -1.3021e-01, -1.3832e-02,  1.3406e-01,
         -7.8179e-02, -9.3269e-03, -1.2956e-01,  4.3121e-02,  2.5552e-02,
         -1.0122e-01, -1.4670e-01, -5.0054e-02,  3.7624e-02,  7.2612e-02,
         -2.2943e-02,  3.5052e-02, -1.4247e-01, -9.9177e-03, -4.9478e-03,
         -1.5277e-01, -6.0029e-02, -6.9180e-03, -9.6907e-02,  6.3530e-02,
          1.2865e-01,  1.0276e-01,  7.0664e-02, -1.0914e-01, -9.9896e-02,
          1.5651e-02, -3.5871e-02, -3.0548e-02, -1.4544e-01,  1.1284e-01,
         -6.2944e-02, -1.3796e-01, -1.2551e-01,  7.6147e-02, -2.7632e-02,
         -8.2619e-02, -6.1456e-02, -7.6753e-02, -1.2193e-01, -1.7246e-02,
         -1.1007e-01, -4.3622e-02, -7.1907e-02,  1.4858e-01, -2.8724e-02,
         -1.4080e-01, -1.1493e-01,  1.1055e-01, -1.5103e-01, -8.4363e-02,
         -9.4768e-02,  1.4033e-01, -1.4749e-01,  1.2793e-01, -8.7863e-03,
         -9.4301e-02,  4.2799e-02, -1.2130e-01,  9.2468e-02,  8.5338e-02,
          1.7445e-02, -6.6981e-02,  2.4236e-02, -1.1762e-01,  1.3288e-01,
         -1.0897e-01, -1.3084e-01,  4.0134e-02,  1.0040e-01,  7.3381e-02,
          1.5185e-01, -1.0410e-01,  7.3424e-02,  7.3819e-02, -7.2305e-02,
         -1.4091e-01,  1.4118e-01,  5.9405e-02, -8.6681e-02,  4.4945e-02,
         -4.8203e-02, -9.6928e-02, -1.3909e-02,  1.4363e-01, -8.8163e-03,
          4.9525e-02,  1.3201e-02,  7.4475e-02, -1.5028e-01, -4.1728e-03,
          1.0109e-01,  1.0631e-02,  2.9087e-02,  6.6718e-02,  1.2589e-01,
          1.7914e-02,  1.5014e-01,  3.1768e-02,  4.1997e-02,  7.6421e-02,
          4.0088e-03,  2.1495e-02, -1.1133e-01,  1.2904e-01,  1.1537e-01,
         -1.2158e-01,  6.0158e-02,  1.0649e-01, -1.3671e-01,  1.5074e-01,
         -1.4565e-01]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0549,  0.1066, -0.0258,  ...,  0.0463, -0.1011, -0.0468],
        [ 0.0345, -0.0973, -0.1060,  ..., -0.1043,  0.0904,  0.0420],
        [ 0.0818, -0.1122, -0.1048,  ..., -0.0883,  0.0457, -0.0399],
        ...,
        [-0.0597,  0.0182,  0.0788,  ..., -0.0174,  0.0254,  0.0835],
        [-0.0494,  0.0548, -0.0887,  ..., -0.1088,  0.1233,  0.0360],
        [ 0.0123, -0.0277,  0.1083,  ..., -0.0715,  0.0680, -0.1147]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1290, -0.0370,  0.1140,  ..., -0.0021, -0.0115,  0.0288],
        [-0.0314,  0.0820,  0.0682,  ...,  0.0406,  0.0747,  0.1578],
        [-0.0762, -0.1275,  0.0470,  ...,  0.0132, -0.1074,  0.1517],
        ...,
        [ 0.1098,  0.0640, -0.1441,  ...,  0.1499,  0.0172,  0.0326],
        [ 0.1090, -0.1489,  0.0308,  ...,  0.1030, -0.1366, -0.1307],
        [-0.1088,  0.1470,  0.1163,  ..., -0.0369,  0.1110, -0.1223]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1910,  0.0908,  0.2291,  ...,  0.1086,  0.2212,  0.1080],
        [-0.0657,  0.1305,  0.0345,  ...,  0.1100,  0.1880, -0.2194],
        [-0.0988,  0.0804,  0.1814,  ..., -0.2276,  0.0191,  0.1817],
        ...,
        [-0.1003,  0.0284, -0.1762,  ...,  0.1032,  0.1745, -0.1287],
        [ 0.0102,  0.1638,  0.1566,  ..., -0.1131,  0.2066, -0.1182],
        [ 0.1648,  0.0677,  0.1914,  ...,  0.2366, -0.2475, -0.2044]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0286],
        [ 0.3978],
        [-0.4181],
        [-0.0154],
        [ 0.3237],
        [-0.0057],
        [-0.1712],
        [ 0.1107],
        [ 0.1949],
        [-0.3678],
        [ 0.0081],
        [-0.1936],
        [-0.0981],
        [-0.1243],
        [-0.1083],
        [ 0.2809],
        [ 0.2562],
        [ 0.3129],
        [-0.1749],
        [ 0.0061],
        [-0.0825],
        [ 0.4026],
        [-0.2210],
        [-0.2470],
        [-0.1987],
        [ 0.0141],
        [ 0.4149],
        [ 0.4150],
        [-0.2610],
        [-0.2093],
        [-0.1946],
        [-0.0462]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1478.1433, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1478.1433, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0096,  0.0149,  0.0114,  ..., -0.0201,  0.0221, -0.0214],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-618.9943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(94.3408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.6495, device='cuda:0')



h[100].sum tensor(211.2454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(222.5016, device='cuda:0')



h[200].sum tensor(-136.1961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0362, 0.0565, 0.0432,  ..., 0.0000, 0.0836, 0.0000],
        [0.0298, 0.0464, 0.0355,  ..., 0.0000, 0.0688, 0.0000],
        [0.0070, 0.0109, 0.0083,  ..., 0.0000, 0.0161, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(153294.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0411, 0.0566],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0352, 0.0485],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0283, 0.0390],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(786583.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(374.0587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1828.7927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-490.6493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22141.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(976.9661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1965],
        [0.2126],
        [0.2346],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(20601.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(59.0245, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1290.4813, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1290.4813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
          1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
          1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
          1.0000e-04,  0.0000e+00],
        ...,
        [ 1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
          1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
          1.0000e-04,  0.0000e+00],
        [ 1.0000e-04,  1.0000e-04, -1.0000e-04,  ...,  0.0000e+00,
          1.0000e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-650.7855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(103.3242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.3148, device='cuda:0')



h[100].sum tensor(164.9097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(194.2533, device='cuda:0')



h[200].sum tensor(-119.4221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        ...,
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0000, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(145705.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.8425e-04, 9.0288e-05,  ..., 0.0000e+00, 2.5369e-03,
         6.4019e-03],
        [0.0000e+00, 0.0000e+00, 2.0622e-04,  ..., 0.0000e+00, 2.6931e-04,
         2.1262e-03],
        [0.0000e+00, 0.0000e+00, 4.1406e-04,  ..., 0.0000e+00, 0.0000e+00,
         9.7762e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 4.0918e-04,  ..., 0.0000e+00, 0.0000e+00,
         9.7081e-04],
        [0.0000e+00, 0.0000e+00, 4.0918e-04,  ..., 0.0000e+00, 0.0000e+00,
         9.7081e-04],
        [0.0000e+00, 0.0000e+00, 4.0918e-04,  ..., 0.0000e+00, 0.0000e+00,
         9.7081e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(773752.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(357.4275, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(813.2034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-548.2639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21904.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(873.5869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1207],
        [0.0904],
        [0.0706],
        ...,
        [0.0127],
        [0.0127],
        [0.0127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(126534.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1739.2471, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1739.2471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9435e-04,  4.0674e-05, -1.1593e-04,  ...,  0.0000e+00,
          3.6093e-05,  0.0000e+00],
        [ 1.9435e-04,  4.0674e-05, -1.1593e-04,  ...,  0.0000e+00,
          3.6093e-05,  0.0000e+00],
        [ 1.9435e-04,  4.0674e-05, -1.1593e-04,  ...,  0.0000e+00,
          3.6093e-05,  0.0000e+00],
        ...,
        [ 1.9435e-04,  4.0674e-05, -1.1593e-04,  ...,  0.0000e+00,
          3.6093e-05,  0.0000e+00],
        [ 1.9435e-04,  4.0674e-05, -1.1593e-04,  ...,  0.0000e+00,
          3.6093e-05,  0.0000e+00],
        [ 1.9435e-04,  4.0674e-05, -1.1593e-04,  ...,  0.0000e+00,
          3.6093e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1448.2664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(150.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.2461, device='cuda:0')



h[100].sum tensor(227.1989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(261.8049, device='cuda:0')



h[200].sum tensor(-159.9395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0402, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0002, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0008, 0.0002, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0080, 0.0114, 0.0085,  ..., 0.0000, 0.0168, 0.0000],
        ...,
        [0.0008, 0.0002, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0008, 0.0002, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0008, 0.0002, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(186332.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.9681e-04, 0.0000e+00, 9.4634e-04,  ..., 0.0000e+00, 0.0000e+00,
         1.1800e-03],
        [2.3475e-04, 0.0000e+00, 6.1171e-04,  ..., 0.0000e+00, 7.1097e-04,
         3.3528e-03],
        [3.0087e-05, 4.4063e-04, 0.0000e+00,  ..., 0.0000e+00, 3.8746e-03,
         9.5254e-03],
        ...,
        [4.6580e-04, 0.0000e+00, 1.2161e-03,  ..., 0.0000e+00, 0.0000e+00,
         6.3308e-04],
        [4.6580e-04, 0.0000e+00, 1.2161e-03,  ..., 0.0000e+00, 0.0000e+00,
         6.3308e-04],
        [4.6580e-04, 0.0000e+00, 1.2161e-03,  ..., 0.0000e+00, 0.0000e+00,
         6.3308e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(973999.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(450.7540, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(501.3425, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1305.2943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-775.1340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24384.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1113.9098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0374],
        [0.0546],
        [0.0766],
        ...,
        [0.0005],
        [0.0005],
        [0.0005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(76880.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1748.9614, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1748.9614, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6653e-03,  6.8196e-03,  5.1333e-03,  ..., -9.1416e-03,
          1.0075e-02, -9.7409e-03],
        [ 4.3783e-03,  6.3724e-03,  4.7909e-03,  ..., -8.5421e-03,
          9.4115e-03, -9.1022e-03],
        [ 2.8904e-04, -9.3360e-07, -8.9477e-05,  ...,  0.0000e+00,
         -4.4479e-05,  0.0000e+00],
        ...,
        [ 2.8904e-04, -9.3360e-07, -8.9477e-05,  ...,  0.0000e+00,
         -4.4479e-05,  0.0000e+00],
        [ 2.8904e-04, -9.3360e-07, -8.9477e-05,  ...,  0.0000e+00,
         -4.4479e-05,  0.0000e+00],
        [ 2.8904e-04, -9.3360e-07, -8.9477e-05,  ...,  0.0000e+00,
         -4.4479e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1646.0524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(170.6779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.6775, device='cuda:0')



h[100].sum tensor(237.2370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(263.2672, device='cuda:0')



h[200].sum tensor(-160.8632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0350, 0.0528, 0.0401,  ..., 0.0000, 0.0781, 0.0000],
        [0.0089, 0.0120, 0.0090,  ..., 0.0000, 0.0178, 0.0000],
        [0.0179, 0.0261, 0.0197,  ..., 0.0000, 0.0385, 0.0000],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0012, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(190032.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0256, 0.0362],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0140, 0.0213],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0157, 0.0234],
        ...,
        [0.0017, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0002],
        [0.0017, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0002],
        [0.0017, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1028399.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1697.3405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(538.2194, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1477.4655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-800.6161, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24176.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1117.9164, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0753],
        [ 0.0714],
        [ 0.0721],
        ...,
        [-0.0067],
        [-0.0067],
        [-0.0067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(10483.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1448.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1448.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.9363e-03,  5.5200e-03,  4.1435e-03,  ..., -7.4314e-03,
          8.1243e-03, -7.9190e-03],
        [ 4.9068e-03,  7.0332e-03,  5.3039e-03,  ..., -9.4593e-03,
          1.0372e-02, -1.0080e-02],
        [ 3.8003e-04, -2.5330e-05, -1.0876e-04,  ...,  0.0000e+00,
         -1.1150e-04,  0.0000e+00],
        ...,
        [ 3.8003e-04, -2.5330e-05, -1.0876e-04,  ...,  0.0000e+00,
         -1.1150e-04,  0.0000e+00],
        [ 3.8003e-04, -2.5330e-05, -1.0876e-04,  ...,  0.0000e+00,
         -1.1150e-04,  0.0000e+00],
        [ 3.8003e-04, -2.5330e-05, -1.0876e-04,  ...,  0.0000e+00,
         -1.1150e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1792.1516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(169.6451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.3364, device='cuda:0')



h[100].sum tensor(189.3652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(218.0512, device='cuda:0')



h[200].sum tensor(-132.6746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0352, 0.0524, 0.0398,  ..., 0.0000, 0.0775, 0.0000],
        [0.0150, 0.0210, 0.0159,  ..., 0.0000, 0.0311, 0.0000],
        [0.0060, 0.0070, 0.0053,  ..., 0.0000, 0.0104, 0.0000],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0015, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0073, 0.0090, 0.0068,  ..., 0.0000, 0.0133, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(156135.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0082, 0.0000,  ..., 0.0000, 0.0227, 0.0320],
        [0.0018, 0.0050, 0.0000,  ..., 0.0000, 0.0137, 0.0214],
        [0.0022, 0.0021, 0.0000,  ..., 0.0000, 0.0056, 0.0111],
        ...,
        [0.0027, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0010],
        [0.0025, 0.0006, 0.0012,  ..., 0.0000, 0.0017, 0.0049],
        [0.0022, 0.0027, 0.0000,  ..., 0.0000, 0.0074, 0.0133]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(850721., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3387.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.1440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1203.8093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-717.6752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18324.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(892.5923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0245],
        [ 0.0209],
        [ 0.0146],
        ...,
        [-0.0049],
        [ 0.0028],
        [ 0.0104]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-24244.1055, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1411.6823, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.6823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5435e-04, -2.9137e-05, -1.5585e-04,  ...,  0.0000e+00,
         -1.6959e-04,  0.0000e+00],
        [ 4.5435e-04, -2.9137e-05, -1.5585e-04,  ...,  0.0000e+00,
         -1.6959e-04,  0.0000e+00],
        [ 4.5435e-04, -2.9137e-05, -1.5585e-04,  ...,  0.0000e+00,
         -1.6959e-04,  0.0000e+00],
        ...,
        [ 4.5435e-04, -2.9137e-05, -1.5585e-04,  ...,  0.0000e+00,
         -1.6959e-04,  0.0000e+00],
        [ 4.5435e-04, -2.9137e-05, -1.5585e-04,  ...,  0.0000e+00,
         -1.6959e-04,  0.0000e+00],
        [ 4.5435e-04, -2.9137e-05, -1.5585e-04,  ...,  0.0000e+00,
         -1.6959e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2071.2366, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(183.4333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.6978, device='cuda:0')



h[100].sum tensor(176.1436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(212.4973, device='cuda:0')



h[200].sum tensor(-130.7269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0018, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(158738.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0000, 0.0025,  ..., 0.0000, 0.0000, 0.0018],
        [0.0035, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0015],
        [0.0036, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0035, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0015],
        [0.0035, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0015],
        [0.0035, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(895966.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4121.8926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(430.6179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1049.1399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-774.3484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18837.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(929.3521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0015],
        [-0.0106],
        [-0.0183],
        ...,
        [-0.0188],
        [-0.0187],
        [-0.0186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-36684.2773, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1633.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1633.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5378e-03,  1.4057e-02,  1.0590e-02,  ..., -1.8843e-02,
          2.0702e-02, -2.0081e-02],
        [ 1.0829e-02,  1.6073e-02,  1.2138e-02,  ..., -2.1543e-02,
          2.3700e-02, -2.2958e-02],
        [ 2.9375e-02,  4.5041e-02,  3.4379e-02,  ..., -6.0329e-02,
          6.6772e-02, -6.4293e-02],
        ...,
        [ 5.2787e-04, -1.6091e-05, -2.1549e-04,  ...,  0.0000e+00,
         -2.2323e-04,  0.0000e+00],
        [ 5.2787e-04, -1.6091e-05, -2.1549e-04,  ...,  0.0000e+00,
         -2.2323e-04,  0.0000e+00],
        [ 5.2787e-04, -1.6091e-05, -2.1549e-04,  ...,  0.0000e+00,
         -2.2323e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2386.2690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(211.9375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.5599, device='cuda:0')



h[100].sum tensor(193.9582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(245.9226, device='cuda:0')



h[200].sum tensor(-150.1890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0377, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0724, 0.1098, 0.0835,  ..., 0.0000, 0.1625, 0.0000],
        [0.0818, 0.1245, 0.0948,  ..., 0.0000, 0.1843, 0.0000],
        [0.0506, 0.0757, 0.0573,  ..., 0.0000, 0.1118, 0.0000],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0021, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(184099.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0277, 0.0000,  ..., 0.0000, 0.0675, 0.0825],
        [0.0000, 0.0297, 0.0000,  ..., 0.0000, 0.0729, 0.0883],
        [0.0000, 0.0245, 0.0000,  ..., 0.0000, 0.0592, 0.0737],
        ...,
        [0.0043, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0026],
        [0.0043, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0026],
        [0.0043, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1037606.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4549.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(470.4248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1039.9862, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-912.1193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21780.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1117.1554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0186],
        [ 0.0178],
        [ 0.0200],
        ...,
        [-0.0257],
        [-0.0256],
        [-0.0255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-41613.3242, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1654.8737, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1654.8737, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0362e-04,  6.7176e-06, -2.7855e-04,  ...,  0.0000e+00,
         -2.7574e-04,  0.0000e+00],
        [ 1.6088e-02,  2.4214e-02,  1.8312e-02,  ..., -3.2392e-02,
          3.5727e-02, -3.4522e-02],
        [ 6.0362e-04,  6.7176e-06, -2.7855e-04,  ...,  0.0000e+00,
         -2.7574e-04,  0.0000e+00],
        ...,
        [ 6.0362e-04,  6.7176e-06, -2.7855e-04,  ...,  0.0000e+00,
         -2.7574e-04,  0.0000e+00],
        [ 6.0362e-04,  6.7176e-06, -2.7855e-04,  ...,  0.0000e+00,
         -2.7574e-04,  0.0000e+00],
        [ 6.0362e-04,  6.7176e-06, -2.7855e-04,  ...,  0.0000e+00,
         -2.7574e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2586.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(228.7259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.4987, device='cuda:0')



h[100].sum tensor(184.1187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(249.1044, device='cuda:0')



h[200].sum tensor(-152.1107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.6720e-02, 3.8024e-02, 2.8623e-02,  ..., 0.0000e+00, 5.5960e-02,
         0.0000e+00],
        [3.2698e-02, 4.7371e-02, 3.5801e-02,  ..., 0.0000e+00, 6.9861e-02,
         0.0000e+00],
        [6.7490e-02, 1.0176e-01, 7.7013e-02,  ..., 0.0000e+00, 1.5020e-01,
         0.0000e+00],
        ...,
        [2.4145e-03, 2.6870e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4145e-03, 2.6870e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.4145e-03, 2.6870e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(186799.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0084, 0.0000,  ..., 0.0000, 0.0165, 0.0282],
        [0.0002, 0.0118, 0.0000,  ..., 0.0000, 0.0244, 0.0364],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0317, 0.0440],
        ...,
        [0.0052, 0.0000, 0.0043,  ..., 0.0000, 0.0000, 0.0035],
        [0.0052, 0.0000, 0.0043,  ..., 0.0000, 0.0000, 0.0035],
        [0.0052, 0.0000, 0.0043,  ..., 0.0000, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1064060.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5294.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.1414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(784.1433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-976.3715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21333.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1166.0031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0329],
        [ 0.0348],
        [ 0.0339],
        ...,
        [-0.0328],
        [-0.0327],
        [-0.0327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-44869.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1763.7823, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1763.7823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7424e-04,  2.4423e-05, -3.4047e-04,  ...,  0.0000e+00,
         -3.2664e-04,  0.0000e+00],
        [ 6.7424e-04,  2.4423e-05, -3.4047e-04,  ...,  0.0000e+00,
         -3.2664e-04,  0.0000e+00],
        [ 5.4202e-03,  7.4504e-03,  5.3628e-03,  ..., -9.9293e-03,
          1.0719e-02, -1.0583e-02],
        ...,
        [ 6.7424e-04,  2.4423e-05, -3.4047e-04,  ...,  0.0000e+00,
         -3.2664e-04,  0.0000e+00],
        [ 6.7424e-04,  2.4423e-05, -3.4047e-04,  ...,  0.0000e+00,
         -3.2664e-04,  0.0000e+00],
        [ 6.7424e-04,  2.4423e-05, -3.4047e-04,  ...,  0.0000e+00,
         -3.2664e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2778.4917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(249.9415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.3358, device='cuda:0')



h[100].sum tensor(187.2746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(265.4982, device='cuda:0')



h[200].sum tensor(-161.8979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1872e-02, 1.4453e-02, 1.0685e-02,  ..., 0.0000e+00, 2.1026e-02,
         0.0000e+00],
        [7.4429e-03, 7.5237e-03, 5.3628e-03,  ..., 0.0000e+00, 1.0719e-02,
         0.0000e+00],
        [1.3164e-02, 1.6476e-02, 1.1898e-02,  ..., 0.0000e+00, 2.3708e-02,
         0.0000e+00],
        ...,
        [2.6970e-03, 9.7693e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.6970e-03, 9.7693e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [2.6970e-03, 9.7693e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(203902.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0031, 0.0000,  ..., 0.0000, 0.0023, 0.0147],
        [0.0020, 0.0035, 0.0000,  ..., 0.0000, 0.0025, 0.0151],
        [0.0011, 0.0061, 0.0000,  ..., 0.0000, 0.0067, 0.0200],
        ...,
        [0.0063, 0.0000, 0.0046,  ..., 0.0000, 0.0000, 0.0044],
        [0.0063, 0.0000, 0.0046,  ..., 0.0000, 0.0000, 0.0044],
        [0.0063, 0.0000, 0.0046,  ..., 0.0000, 0.0000, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1193883.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6272.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(487.7300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(806.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1100.3096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23629.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1318.5677, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0723],
        [ 0.0794],
        [ 0.0873],
        ...,
        [-0.0407],
        [-0.0405],
        [-0.0405]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-48327.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1875.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1875.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2864e-02,  1.9012e-02,  1.4183e-02,  ..., -2.5363e-02,
          2.7863e-02, -2.7033e-02],
        [ 9.1431e-03,  1.3185e-02,  9.7065e-03,  ..., -1.7577e-02,
          1.9194e-02, -1.8734e-02],
        [ 1.2616e-02,  1.8624e-02,  1.3884e-02,  ..., -2.4843e-02,
          2.7285e-02, -2.6480e-02],
        ...,
        [ 7.4346e-04,  3.0874e-05, -3.9791e-04,  ...,  0.0000e+00,
         -3.7495e-04,  0.0000e+00],
        [ 7.4346e-04,  3.0874e-05, -3.9791e-04,  ...,  0.0000e+00,
         -3.7495e-04,  0.0000e+00],
        [ 7.4346e-04,  3.0874e-05, -3.9791e-04,  ...,  0.0000e+00,
         -3.7495e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2925.3127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(271.0400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.2929, device='cuda:0')



h[100].sum tensor(192.1627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(282.2992, device='cuda:0')



h[200].sum tensor(-171.9257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0330, 0.0472, 0.0346,  ..., 0.0000, 0.0686, 0.0000],
        [0.0367, 0.0529, 0.0390,  ..., 0.0000, 0.0770, 0.0000],
        [0.0240, 0.0330, 0.0241,  ..., 0.0000, 0.0479, 0.0000],
        ...,
        [0.0030, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0030, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0030, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(215680.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0249, 0.0388],
        [0.0000, 0.0151, 0.0000,  ..., 0.0000, 0.0241, 0.0380],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0183, 0.0324],
        ...,
        [0.0074, 0.0000, 0.0049,  ..., 0.0000, 0.0000, 0.0053],
        [0.0074, 0.0000, 0.0049,  ..., 0.0000, 0.0000, 0.0053],
        [0.0074, 0.0000, 0.0049,  ..., 0.0000, 0.0000, 0.0053]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1266961., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7343.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(491.6917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(739.5359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1200.9659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24304.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1433.7871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1041],
        [ 0.1072],
        [ 0.1116],
        ...,
        [-0.0484],
        [-0.0481],
        [-0.0481]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-60488.5039, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1562.0170, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(1562.0170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.0637e-04,  3.2306e-05, -4.5337e-04,  ...,  0.0000e+00,
         -4.1608e-04,  0.0000e+00],
        [ 8.0637e-04,  3.2306e-05, -4.5337e-04,  ...,  0.0000e+00,
         -4.1608e-04,  0.0000e+00],
        [ 8.0637e-04,  3.2306e-05, -4.5337e-04,  ...,  0.0000e+00,
         -4.1608e-04,  0.0000e+00],
        ...,
        [ 8.4008e-03,  1.1935e-02,  8.6926e-03,  ..., -1.5895e-02,
          1.7297e-02, -1.6943e-02],
        [ 6.1371e-03,  8.3874e-03,  5.9664e-03,  ..., -1.1157e-02,
          1.2017e-02, -1.1892e-02],
        [ 8.4008e-03,  1.1935e-02,  8.6926e-03,  ..., -1.5895e-02,
          1.7297e-02, -1.6943e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2893.9089, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(263.1030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.3746, device='cuda:0')



h[100].sum tensor(135.2122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(235.1269, device='cuda:0')



h[200].sum tensor(-142.0294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0032, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0032, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0148, 0.0182, 0.0130,  ..., 0.0000, 0.0261, 0.0000],
        [0.0352, 0.0502, 0.0367,  ..., 0.0000, 0.0728, 0.0000],
        [0.0288, 0.0402, 0.0290,  ..., 0.0000, 0.0580, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(178220.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0084, 0.0000, 0.0053,  ..., 0.0000, 0.0000, 0.0062],
        [0.0084, 0.0000, 0.0053,  ..., 0.0000, 0.0000, 0.0062],
        [0.0085, 0.0000, 0.0053,  ..., 0.0000, 0.0000, 0.0063],
        ...,
        [0.0028, 0.0043, 0.0000,  ..., 0.0000, 0.0042, 0.0183],
        [0.0004, 0.0084, 0.0000,  ..., 0.0000, 0.0109, 0.0259],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0130, 0.0279]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1060790.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8695.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(370.9299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(413.7442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1141.8811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17318.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1218.1110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0854],
        [-0.0805],
        [-0.0691],
        ...,
        [ 0.0263],
        [ 0.0404],
        [ 0.0449]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-96708.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 300 loss: tensor(602.9163, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1436.0972, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1436.0972, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6351e-04,  3.0946e-05, -5.0693e-04,  ...,  0.0000e+00,
         -4.5101e-04,  0.0000e+00],
        [ 8.6351e-04,  3.0946e-05, -5.0693e-04,  ...,  0.0000e+00,
         -4.5101e-04,  0.0000e+00],
        [ 8.6351e-04,  3.0946e-05, -5.0693e-04,  ...,  0.0000e+00,
         -4.5101e-04,  0.0000e+00],
        ...,
        [ 8.6351e-04,  3.0946e-05, -5.0693e-04,  ...,  0.0000e+00,
         -4.5101e-04,  0.0000e+00],
        [ 8.6351e-04,  3.0946e-05, -5.0693e-04,  ...,  0.0000e+00,
         -4.5101e-04,  0.0000e+00],
        [ 8.6351e-04,  3.0946e-05, -5.0693e-04,  ...,  0.0000e+00,
         -4.5101e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2902.5898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(267.1838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.7821, device='cuda:0')



h[100].sum tensor(108.2234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(216.1725, device='cuda:0')



h[200].sum tensor(-131.1005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0035, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0035, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0035, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0035, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0035, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0035, 0.0001, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(172492.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.0016, 0.0014,  ..., 0.0000, 0.0009, 0.0136],
        [0.0080, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0095],
        [0.0085, 0.0000, 0.0017,  ..., 0.0000, 0.0000, 0.0082],
        ...,
        [0.0094, 0.0000, 0.0057,  ..., 0.0000, 0.0000, 0.0071],
        [0.0094, 0.0000, 0.0057,  ..., 0.0000, 0.0000, 0.0071],
        [0.0094, 0.0000, 0.0057,  ..., 0.0000, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1067103.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9745.0215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.9342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(259.8261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1181.6998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16239.4482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1203.0387, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0715],
        [ 0.0643],
        [ 0.0631],
        ...,
        [-0.0638],
        [-0.0635],
        [-0.0634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-117041.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1459.6827, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1459.6827, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0954e-04,  1.9056e-05, -5.5504e-04,  ...,  0.0000e+00,
         -4.7488e-04,  0.0000e+00],
        [ 9.0954e-04,  1.9056e-05, -5.5504e-04,  ...,  0.0000e+00,
         -4.7488e-04,  0.0000e+00],
        [ 9.0954e-04,  1.9056e-05, -5.5504e-04,  ...,  0.0000e+00,
         -4.7488e-04,  0.0000e+00],
        ...,
        [ 9.0954e-04,  1.9056e-05, -5.5504e-04,  ...,  0.0000e+00,
         -4.7488e-04,  0.0000e+00],
        [ 9.0954e-04,  1.9056e-05, -5.5504e-04,  ...,  0.0000e+00,
         -4.7488e-04,  0.0000e+00],
        [ 9.0954e-04,  1.9056e-05, -5.5504e-04,  ...,  0.0000e+00,
         -4.7488e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2949.9182, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(277.5423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.8296, device='cuda:0')



h[100].sum tensor(100.8457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(219.7227, device='cuda:0')



h[200].sum tensor(-132.4623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.4220e-02, 1.6684e-02, 1.1658e-02,  ..., 0.0000e+00, 2.3776e-02,
         0.0000e+00],
        [3.6382e-03, 7.6225e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.6382e-03, 7.6225e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.6382e-03, 7.6225e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.6382e-03, 7.6225e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.6382e-03, 7.6225e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(182891.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0041, 0.0000,  ..., 0.0000, 0.0032, 0.0196],
        [0.0065, 0.0012, 0.0015,  ..., 0.0000, 0.0000, 0.0132],
        [0.0087, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0102],
        ...,
        [0.0103, 0.0000, 0.0063,  ..., 0.0000, 0.0000, 0.0080],
        [0.0103, 0.0000, 0.0063,  ..., 0.0000, 0.0000, 0.0080],
        [0.0103, 0.0000, 0.0063,  ..., 0.0000, 0.0000, 0.0080]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1172003.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10626.6035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.2123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(399.2649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1260.1110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17747.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1284.4474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0965],
        [ 0.0693],
        [ 0.0354],
        ...,
        [-0.0717],
        [-0.0714],
        [-0.0713]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-133165.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1485.6726, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.6726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4492e-04, -6.4846e-06, -5.9628e-04,  ...,  0.0000e+00,
         -4.8523e-04,  0.0000e+00],
        [ 9.4492e-04, -6.4846e-06, -5.9628e-04,  ...,  0.0000e+00,
         -4.8523e-04,  0.0000e+00],
        [ 9.4492e-04, -6.4846e-06, -5.9628e-04,  ...,  0.0000e+00,
         -4.8523e-04,  0.0000e+00],
        ...,
        [ 9.4492e-04, -6.4846e-06, -5.9628e-04,  ...,  0.0000e+00,
         -4.8523e-04,  0.0000e+00],
        [ 9.4492e-04, -6.4846e-06, -5.9628e-04,  ...,  0.0000e+00,
         -4.8523e-04,  0.0000e+00],
        [ 9.4492e-04, -6.4846e-06, -5.9628e-04,  ...,  0.0000e+00,
         -4.8523e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2996.2366, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(286.7330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.9839, device='cuda:0')



h[100].sum tensor(96.0187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(223.6349, device='cuda:0')



h[200].sum tensor(-135.2709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0181, 0.0225, 0.0162,  ..., 0.0000, 0.0326, 0.0000],
        ...,
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0038, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(184478.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0105, 0.0000, 0.0042,  ..., 0.0000, 0.0000, 0.0100],
        [0.0081, 0.0012, 0.0024,  ..., 0.0000, 0.0007, 0.0142],
        [0.0035, 0.0056, 0.0000,  ..., 0.0000, 0.0083, 0.0246],
        ...,
        [0.0112, 0.0000, 0.0069,  ..., 0.0000, 0.0000, 0.0087],
        [0.0112, 0.0000, 0.0069,  ..., 0.0000, 0.0000, 0.0087],
        [0.0112, 0.0000, 0.0069,  ..., 0.0000, 0.0000, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1178458., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11658.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(390.5389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(414.3511, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1309.9252, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16623.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1299.8195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0265],
        [ 0.0189],
        [ 0.0573],
        ...,
        [-0.0795],
        [-0.0792],
        [-0.0790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-169555.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1741.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1741.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.7280e-04, -3.2492e-05, -6.3430e-04,  ...,  0.0000e+00,
         -4.8238e-04,  0.0000e+00],
        [ 9.7280e-04, -3.2492e-05, -6.3430e-04,  ...,  0.0000e+00,
         -4.8238e-04,  0.0000e+00],
        [ 9.7280e-04, -3.2492e-05, -6.3430e-04,  ...,  0.0000e+00,
         -4.8238e-04,  0.0000e+00],
        ...,
        [ 9.7280e-04, -3.2492e-05, -6.3430e-04,  ...,  0.0000e+00,
         -4.8238e-04,  0.0000e+00],
        [ 9.7280e-04, -3.2492e-05, -6.3430e-04,  ...,  0.0000e+00,
         -4.8238e-04,  0.0000e+00],
        [ 9.7280e-04, -3.2492e-05, -6.3430e-04,  ...,  0.0000e+00,
         -4.8238e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3078.7573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(308.2285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.3487, device='cuda:0')



h[100].sum tensor(122.1863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(262.1529, device='cuda:0')



h[200].sum tensor(-157.9533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0402, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0039, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(209506.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.2710e-03, 2.5546e-03, 5.9068e-04,  ..., 0.0000e+00, 2.7448e-03,
         1.9926e-02],
        [7.7258e-03, 4.7134e-04, 1.8300e-03,  ..., 0.0000e+00, 0.0000e+00,
         1.5834e-02],
        [9.2068e-03, 3.4785e-05, 5.9068e-04,  ..., 0.0000e+00, 0.0000e+00,
         1.4072e-02],
        ...,
        [1.1966e-02, 0.0000e+00, 7.7084e-03,  ..., 0.0000e+00, 0.0000e+00,
         9.5104e-03],
        [1.1966e-02, 0.0000e+00, 7.7084e-03,  ..., 0.0000e+00, 0.0000e+00,
         9.5104e-03],
        [1.1966e-02, 0.0000e+00, 7.7084e-03,  ..., 0.0000e+00, 0.0000e+00,
         9.5104e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1317620.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11953.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(479.9952, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(507.1045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1441.3656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19297.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1462.7362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0564],
        [ 0.0535],
        [ 0.0534],
        ...,
        [-0.0869],
        [-0.0865],
        [-0.0864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-176230.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1642.3500, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1642.3500, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9996e-04, -6.0699e-05, -6.6515e-04,  ...,  0.0000e+00,
         -4.6402e-04,  0.0000e+00],
        [ 9.9996e-04, -6.0699e-05, -6.6515e-04,  ...,  0.0000e+00,
         -4.6402e-04,  0.0000e+00],
        [ 5.1676e-03,  6.4890e-03,  4.3786e-03,  ..., -8.7229e-03,
          9.3011e-03, -9.2999e-03],
        ...,
        [ 9.9996e-04, -6.0699e-05, -6.6515e-04,  ...,  0.0000e+00,
         -4.6402e-04,  0.0000e+00],
        [ 9.9996e-04, -6.0699e-05, -6.6515e-04,  ...,  0.0000e+00,
         -4.6402e-04,  0.0000e+00],
        [ 9.9996e-04, -6.0699e-05, -6.6515e-04,  ...,  0.0000e+00,
         -4.6402e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3035.9324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(307.6219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.9425, device='cuda:0')



h[100].sum tensor(98.1617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(247.2192, device='cuda:0')



h[200].sum tensor(-149.0474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0082, 0.0065, 0.0044,  ..., 0.0000, 0.0093, 0.0000],
        [0.0181, 0.0220, 0.0157,  ..., 0.0000, 0.0321, 0.0000],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(206506.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0131],
        [0.0056, 0.0013, 0.0005,  ..., 0.0000, 0.0017, 0.0185],
        [0.0024, 0.0039, 0.0000,  ..., 0.0000, 0.0065, 0.0258],
        ...,
        [0.0125, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0100],
        [0.0125, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0100],
        [0.0125, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1346251.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12432.3994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.7978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(535.4985, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1490.6692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19101.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1441.3278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0065],
        [ 0.0524],
        [ 0.0848],
        ...,
        [-0.0936],
        [-0.0932],
        [-0.0931]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-202417.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1593.8136, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1593.8136, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0219e-03, -8.9022e-05, -6.9488e-04,  ...,  0.0000e+00,
         -4.3486e-04,  0.0000e+00],
        [ 1.0219e-03, -8.9022e-05, -6.9488e-04,  ...,  0.0000e+00,
         -4.3486e-04,  0.0000e+00],
        [ 1.0219e-03, -8.9022e-05, -6.9488e-04,  ...,  0.0000e+00,
         -4.3486e-04,  0.0000e+00],
        ...,
        [ 1.0219e-03, -8.9022e-05, -6.9488e-04,  ...,  0.0000e+00,
         -4.3486e-04,  0.0000e+00],
        [ 1.0219e-03, -8.9022e-05, -6.9488e-04,  ...,  0.0000e+00,
         -4.3486e-04,  0.0000e+00],
        [ 1.0219e-03, -8.9022e-05, -6.9488e-04,  ...,  0.0000e+00,
         -4.3486e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2990.5730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(309.7830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.7868, device='cuda:0')



h[100].sum tensor(82.5004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(239.9132, device='cuda:0')



h[200].sum tensor(-145.6391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(201528.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0105],
        [0.0130, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0105],
        [0.0131, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0106],
        ...,
        [0.0130, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0105],
        [0.0130, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0105],
        [0.0128, 0.0000, 0.0085,  ..., 0.0000, 0.0000, 0.0106]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1316119.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12928.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(495.3408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(638.6553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1520.7644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17441.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1422.1935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1565],
        [-0.1565],
        [-0.1528],
        ...,
        [-0.1004],
        [-0.0992],
        [-0.0954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-236465.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1495.2291, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1495.2291, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0071,  0.0048,  ..., -0.0096,  0.0104, -0.0103],
        [ 0.0010, -0.0001, -0.0007,  ...,  0.0000, -0.0004,  0.0000],
        [ 0.0056,  0.0071,  0.0048,  ..., -0.0096,  0.0104, -0.0103],
        ...,
        [ 0.0010, -0.0001, -0.0007,  ...,  0.0000, -0.0004,  0.0000],
        [ 0.0010, -0.0001, -0.0007,  ...,  0.0000, -0.0004,  0.0000],
        [ 0.0010, -0.0001, -0.0007,  ...,  0.0000, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2934.6150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(304.3625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.4084, device='cuda:0')



h[100].sum tensor(55.0281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(225.0735, device='cuda:0')



h[200].sum tensor(-135.1421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0183, 0.0220, 0.0157,  ..., 0.0000, 0.0324, 0.0000],
        [0.0275, 0.0363, 0.0255,  ..., 0.0000, 0.0534, 0.0000],
        [0.0112, 0.0111, 0.0079,  ..., 0.0000, 0.0163, 0.0000],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0041, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(193523.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0022, 0.0000,  ..., 0.0000, 0.0083, 0.0295],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0082, 0.0295],
        [0.0032, 0.0005, 0.0000,  ..., 0.0000, 0.0024, 0.0223],
        ...,
        [0.0132, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0111],
        [0.0132, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0111],
        [0.0132, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0111]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1295970., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12728.0869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(498.0726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(481.8503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1542.8615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16117.3574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1394.5031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0909],
        [ 0.0855],
        [ 0.0686],
        ...,
        [-0.1048],
        [-0.1043],
        [-0.1042]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-235570.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1916.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1916.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0053,  0.0034,  ..., -0.0073,  0.0078, -0.0078],
        [ 0.0045,  0.0053,  0.0034,  ..., -0.0073,  0.0078, -0.0078],
        [ 0.0010, -0.0001, -0.0008,  ...,  0.0000, -0.0004,  0.0000],
        ...,
        [ 0.0010, -0.0001, -0.0008,  ...,  0.0000, -0.0004,  0.0000],
        [ 0.0010, -0.0001, -0.0008,  ...,  0.0000, -0.0004,  0.0000],
        [ 0.0010, -0.0001, -0.0008,  ...,  0.0000, -0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2985.2886, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(333.1963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(85.1017, device='cuda:0')



h[100].sum tensor(105.3427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(288.4295, device='cuda:0')



h[200].sum tensor(-173.7796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0443, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0342, 0.0238,  ..., 0.0000, 0.0505, 0.0000],
        [0.0172, 0.0201, 0.0136,  ..., 0.0000, 0.0296, 0.0000],
        [0.0303, 0.0405, 0.0286,  ..., 0.0000, 0.0599, 0.0000],
        ...,
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(242994.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0155, 0.0366],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0125, 0.0342],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0170, 0.0376],
        ...,
        [0.0134, 0.0000, 0.0106,  ..., 0.0000, 0.0000, 0.0118],
        [0.0134, 0.0000, 0.0106,  ..., 0.0000, 0.0000, 0.0118],
        [0.0134, 0.0000, 0.0106,  ..., 0.0000, 0.0000, 0.0118]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1562746.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12380.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(640.4736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1075.2131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1745.0270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22767.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1744.2375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0934],
        [ 0.0915],
        [ 0.0838],
        ...,
        [-0.1085],
        [-0.1080],
        [-0.1079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-234582.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1644.5442, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1644.5442, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0053,  0.0065,  0.0043,  ..., -0.0088,  0.0096, -0.0094],
        ...,
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2872.3911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(318.2474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.0400, device='cuda:0')



h[100].sum tensor(54.2538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(247.5495, device='cuda:0')



h[200].sum tensor(-148.0478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0084, 0.0065, 0.0043,  ..., 0.0000, 0.0096, 0.0000],
        [0.0105, 0.0098, 0.0068,  ..., 0.0000, 0.0145, 0.0000],
        ...,
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(209859.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0112, 0.0000, 0.0057,  ..., 0.0000, 0.0000, 0.0148],
        [0.0066, 0.0000, 0.0012,  ..., 0.0000, 0.0010, 0.0196],
        [0.0031, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0241],
        ...,
        [0.0135, 0.0000, 0.0114,  ..., 0.0000, 0.0000, 0.0127],
        [0.0135, 0.0000, 0.0114,  ..., 0.0000, 0.0000, 0.0127],
        [0.0135, 0.0000, 0.0114,  ..., 0.0000, 0.0000, 0.0127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1389161., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12232.1074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(556.0051, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(545.4443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1676.0059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17328.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1560.1163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0633],
        [-0.0077],
        [ 0.0382],
        ...,
        [-0.1119],
        [-0.1114],
        [-0.1112]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-230665.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1516.4635, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1516.4635, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0053,  0.0064,  0.0043,  ..., -0.0088,  0.0096, -0.0094],
        ...,
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000],
        [ 0.0011, -0.0002, -0.0008,  ...,  0.0000, -0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2788.8838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(314.8681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.3515, device='cuda:0')



h[100].sum tensor(26.7809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(228.2698, device='cuda:0')



h[200].sum tensor(-136.7549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0085, 0.0064, 0.0043,  ..., 0.0000, 0.0096, 0.0000],
        [0.0183, 0.0216, 0.0153,  ..., 0.0000, 0.0323, 0.0000],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0043, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(203090.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0194],
        [0.0041, 0.0000, 0.0014,  ..., 0.0000, 0.0019, 0.0238],
        [0.0014, 0.0008, 0.0000,  ..., 0.0000, 0.0072, 0.0309],
        ...,
        [0.0138, 0.0000, 0.0122,  ..., 0.0000, 0.0000, 0.0134],
        [0.0138, 0.0000, 0.0122,  ..., 0.0000, 0.0000, 0.0134],
        [0.0138, 0.0000, 0.0122,  ..., 0.0000, 0.0000, 0.0134]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1383229.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12125.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(540.0471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(445.5921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1692.0309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16482.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1537.7430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0013],
        [ 0.0342],
        [ 0.0600],
        ...,
        [-0.1160],
        [-0.1154],
        [-0.1153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-224800.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 600 loss: tensor(575.6323, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1694.9645, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1694.9645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0011, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0011, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0011, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0011, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0011, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2749.7183, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(332.8789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.2793, device='cuda:0')



h[100].sum tensor(43.4389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(255.1392, device='cuda:0')



h[200].sum tensor(-152.6519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(217440.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0000, 0.0016,  ..., 0.0000, 0.0000, 0.0195],
        [0.0098, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0179],
        [0.0114, 0.0000, 0.0046,  ..., 0.0000, 0.0000, 0.0164],
        ...,
        [0.0145, 0.0000, 0.0128,  ..., 0.0000, 0.0000, 0.0137],
        [0.0145, 0.0000, 0.0128,  ..., 0.0000, 0.0000, 0.0137],
        [0.0145, 0.0000, 0.0128,  ..., 0.0000, 0.0000, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1457859.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13025.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(567.8079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(642.7233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1777.0255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17833.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1628.5739, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0595],
        [ 0.0592],
        [ 0.0586],
        ...,
        [-0.1216],
        [-0.1210],
        [-0.1209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-252902.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1562.5605, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1562.5605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0100,  0.0137,  0.0099,  ..., -0.0185,  0.0207, -0.0197],
        [ 0.0012, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0254,  0.0380,  0.0286,  ..., -0.0507,  0.0570, -0.0541],
        ...,
        [ 0.0012, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0012, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0012, -0.0002, -0.0009,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2651.8018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(333.1037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.3988, device='cuda:0')



h[100].sum tensor(17.0511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(235.2087, device='cuda:0')



h[200].sum tensor(-141.0167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0112, 0.0079,  ..., 0.0000, 0.0168, 0.0000],
        [0.0579, 0.0830, 0.0611,  ..., 0.0000, 0.1247, 0.0000],
        [0.0542, 0.0775, 0.0576,  ..., 0.0000, 0.1163, 0.0000],
        ...,
        [0.0046, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0046, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0046, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(205461.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0012, 0.0000,  ..., 0.0000, 0.0105, 0.0321],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0361, 0.0541],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0495, 0.0642],
        ...,
        [0.0153, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0140],
        [0.0153, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0140],
        [0.0153, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0140]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1408138.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(14309.8262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(525.6475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(510.0893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1775.8823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15670.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1541.7869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0349],
        [ 0.0250],
        [ 0.0074],
        ...,
        [-0.1283],
        [-0.1276],
        [-0.1273]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-282548.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1498.5149, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1498.5149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0118,  0.0084,  ..., -0.0160,  0.0179, -0.0171],
        [ 0.0099,  0.0134,  0.0096,  ..., -0.0181,  0.0203, -0.0193],
        [ 0.0101,  0.0137,  0.0098,  ..., -0.0185,  0.0207, -0.0197],
        ...,
        [ 0.0012, -0.0002, -0.0009,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0012, -0.0002, -0.0009,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0012, -0.0002, -0.0009,  ...,  0.0000, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2553.6582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(338.6818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.5543, device='cuda:0')



h[100].sum tensor(-0.0867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(225.5681, device='cuda:0')



h[200].sum tensor(-134.8840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0328, 0.0432, 0.0304,  ..., 0.0000, 0.0654, 0.0000],
        [0.0374, 0.0504, 0.0359,  ..., 0.0000, 0.0762, 0.0000],
        [0.0460, 0.0639, 0.0464,  ..., 0.0000, 0.0965, 0.0000],
        ...,
        [0.0048, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0048, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(204077.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0435, 0.0613],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0383, 0.0576],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0321, 0.0530],
        ...,
        [0.0163, 0.0000, 0.0139,  ..., 0.0000, 0.0000, 0.0141],
        [0.0163, 0.0000, 0.0139,  ..., 0.0000, 0.0000, 0.0141],
        [0.0163, 0.0000, 0.0139,  ..., 0.0000, 0.0000, 0.0141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1435540.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(15761.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(507.8760, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(532.7737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1808.1534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15612.0732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1516.3501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0568],
        [-0.0240],
        [ 0.0044],
        ...,
        [-0.1348],
        [-0.1342],
        [-0.1340]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-317658.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1297.9454, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1297.9454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2471e-03, -2.4878e-04, -9.5466e-04,  ...,  0.0000e+00,
         -9.8750e-05,  0.0000e+00],
        [ 1.2471e-03, -2.4878e-04, -9.5466e-04,  ...,  0.0000e+00,
         -9.8750e-05,  0.0000e+00],
        [ 5.7951e-03,  6.9137e-03,  4.5793e-03,  ..., -9.4931e-03,
          1.0622e-02, -1.0125e-02],
        ...,
        [ 1.2471e-03, -2.4878e-04, -9.5466e-04,  ...,  0.0000e+00,
         -9.8750e-05,  0.0000e+00],
        [ 1.2471e-03, -2.4878e-04, -9.5466e-04,  ...,  0.0000e+00,
         -9.8750e-05,  0.0000e+00],
        [ 1.2471e-03, -2.4878e-04, -9.5466e-04,  ...,  0.0000e+00,
         -9.8750e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2432.6055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(336.0679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.6463, device='cuda:0')



h[100].sum tensor(-36.1646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(195.3768, device='cuda:0')



h[200].sum tensor(-116.8449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0095, 0.0069, 0.0046,  ..., 0.0000, 0.0106, 0.0000],
        [0.0298, 0.0386, 0.0283,  ..., 0.0000, 0.0583, 0.0000],
        ...,
        [0.0050, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0050, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(185669.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0108, 0.0000, 0.0034,  ..., 0.0000, 0.0003, 0.0207],
        [0.0053, 0.0001, 0.0000,  ..., 0.0000, 0.0061, 0.0290],
        [0.0009, 0.0032, 0.0000,  ..., 0.0000, 0.0202, 0.0437],
        ...,
        [0.0172, 0.0000, 0.0144,  ..., 0.0000, 0.0000, 0.0143],
        [0.0172, 0.0000, 0.0144,  ..., 0.0000, 0.0000, 0.0143],
        [0.0172, 0.0000, 0.0144,  ..., 0.0000, 0.0000, 0.0143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1366295., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(16824.9082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(452.7313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(316.3254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1785.4077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12979.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1386.9985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0012],
        [ 0.0008],
        [-0.0188],
        ...,
        [-0.1399],
        [-0.1393],
        [-0.1391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-334551.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1355.1281, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1355.1281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4917e-03,  6.3519e-03,  4.1270e-03,  ..., -8.7570e-03,
          9.8499e-03, -9.3405e-03],
        [ 5.3819e-03,  6.1789e-03,  3.9933e-03,  ..., -8.5279e-03,
          9.5909e-03, -9.0961e-03],
        [ 9.5794e-03,  1.2790e-02,  9.1022e-03,  ..., -1.7285e-02,
          1.9489e-02, -1.8437e-02],
        ...,
        [ 1.2943e-03, -2.5932e-04, -9.8187e-04,  ...,  0.0000e+00,
         -4.8729e-05,  0.0000e+00],
        [ 1.2943e-03, -2.5932e-04, -9.8187e-04,  ...,  0.0000e+00,
         -4.8729e-05,  0.0000e+00],
        [ 1.2943e-03, -2.5932e-04, -9.8187e-04,  ...,  0.0000e+00,
         -4.8729e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2335.0498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(349.0289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.1860, device='cuda:0')



h[100].sum tensor(-36.1825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(203.9844, device='cuda:0')



h[200].sum tensor(-121.5216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0189, 0.0210, 0.0147,  ..., 0.0000, 0.0321, 0.0000],
        [0.0335, 0.0436, 0.0305,  ..., 0.0000, 0.0666, 0.0000],
        [0.0189, 0.0208, 0.0137,  ..., 0.0000, 0.0321, 0.0000],
        ...,
        [0.0052, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0052, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(192009.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0004, 0.0000,  ..., 0.0000, 0.0085, 0.0347],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0173, 0.0432],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0137, 0.0405],
        ...,
        [0.0179, 0.0000, 0.0148,  ..., 0.0000, 0.0000, 0.0145],
        [0.0179, 0.0000, 0.0148,  ..., 0.0000, 0.0000, 0.0145],
        [0.0179, 0.0000, 0.0148,  ..., 0.0000, 0.0000, 0.0145]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1408561., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(17591.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.4750, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(403.9811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1839.9469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13478.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1424.3931, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0191],
        [ 0.0283],
        [ 0.0376],
        ...,
        [-0.1447],
        [-0.1441],
        [-0.1439]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-350012.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1678.5708, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1678.5708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3327e-03, -2.6665e-04, -1.0151e-03,  ...,  0.0000e+00,
          8.2207e-06,  0.0000e+00],
        [ 1.3327e-03, -2.6665e-04, -1.0151e-03,  ...,  0.0000e+00,
          8.2207e-06,  0.0000e+00],
        [ 1.3327e-03, -2.6665e-04, -1.0151e-03,  ...,  0.0000e+00,
          8.2207e-06,  0.0000e+00],
        ...,
        [ 1.3327e-03, -2.6665e-04, -1.0151e-03,  ...,  0.0000e+00,
          8.2207e-06,  0.0000e+00],
        [ 1.3327e-03, -2.6665e-04, -1.0151e-03,  ...,  0.0000e+00,
          8.2207e-06,  0.0000e+00],
        [ 1.3327e-03, -2.6665e-04, -1.0151e-03,  ...,  0.0000e+00,
          8.2207e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2262.3591, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(377.8539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.5512, device='cuda:0')



h[100].sum tensor(3.4119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(252.6715, device='cuda:0')



h[200].sum tensor(-151.3820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0388, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.3306e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2883e-05,
         0.0000e+00],
        [5.3306e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2883e-05,
         0.0000e+00],
        [5.3306e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2883e-05,
         0.0000e+00],
        ...,
        [5.3306e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2883e-05,
         0.0000e+00],
        [5.3306e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2883e-05,
         0.0000e+00],
        [5.3306e-03, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2883e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(224084.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0110, 0.0000, 0.0059,  ..., 0.0000, 0.0000, 0.0211],
        [0.0152, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0173],
        [0.0175, 0.0000, 0.0146,  ..., 0.0000, 0.0000, 0.0156],
        ...,
        [0.0181, 0.0000, 0.0154,  ..., 0.0000, 0.0000, 0.0150],
        [0.0181, 0.0000, 0.0154,  ..., 0.0000, 0.0000, 0.0150],
        [0.0181, 0.0000, 0.0154,  ..., 0.0000, 0.0000, 0.0150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1556334.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(17287.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(537.1146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(721.8581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1974.7479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16926.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1654.7958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0398],
        [-0.0031],
        [-0.0546],
        ...,
        [-0.1484],
        [-0.1477],
        [-0.1475]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-348451.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1653.8173, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1653.8173, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0956e-02,  1.4842e-02,  1.0628e-02,  ..., -1.9991e-02,
          2.2708e-02, -2.1325e-02],
        [ 1.0006e-02,  1.3346e-02,  9.4715e-03,  ..., -1.8012e-02,
          2.0467e-02, -1.9213e-02],
        [ 1.0779e-02,  1.4564e-02,  1.0413e-02,  ..., -1.9623e-02,
          2.2291e-02, -2.0932e-02],
        ...,
        [ 1.3666e-03, -2.7067e-04, -1.0511e-03,  ...,  0.0000e+00,
          6.9410e-05,  0.0000e+00],
        [ 1.3666e-03, -2.7067e-04, -1.0511e-03,  ...,  0.0000e+00,
          6.9410e-05,  0.0000e+00],
        [ 1.3666e-03, -2.7067e-04, -1.0511e-03,  ...,  0.0000e+00,
          6.9410e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2138.5332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(382.6230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.4518, device='cuda:0')



h[100].sum tensor(-9.8576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(248.9454, device='cuda:0')



h[200].sum tensor(-148.2124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0386, 0.0511, 0.0361,  ..., 0.0000, 0.0784, 0.0000],
        [0.0394, 0.0524, 0.0371,  ..., 0.0000, 0.0804, 0.0000],
        [0.0383, 0.0507, 0.0358,  ..., 0.0000, 0.0778, 0.0000],
        ...,
        [0.0055, 0.0000, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0055, 0.0000, 0.0000,  ..., 0.0000, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(220179.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0241, 0.0502],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0238, 0.0498],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0231, 0.0493],
        ...,
        [0.0179, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0158],
        [0.0179, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0158],
        [0.0179, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0158]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1530650.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(16447.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(515.8982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(463.2166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2005.5179, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15704.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1677.1476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1083],
        [ 0.1056],
        [ 0.1052],
        ...,
        [-0.1512],
        [-0.1506],
        [-0.1504]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-317140.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1819.8193, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1819.8193, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014, -0.0003, -0.0011,  ...,  0.0000,  0.0001,  0.0000],
        [ 0.0014, -0.0003, -0.0011,  ...,  0.0000,  0.0001,  0.0000],
        [ 0.0014, -0.0003, -0.0011,  ...,  0.0000,  0.0001,  0.0000],
        ...,
        [ 0.0014, -0.0003, -0.0011,  ...,  0.0000,  0.0001,  0.0000],
        [ 0.0014, -0.0003, -0.0011,  ...,  0.0000,  0.0001,  0.0000],
        [ 0.0014, -0.0003, -0.0011,  ...,  0.0000,  0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2026.1940, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(402.3795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.8246, device='cuda:0')



h[100].sum tensor(7.1105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(273.9333, device='cuda:0')



h[200].sum tensor(-163.5671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0056, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        ...,
        [0.0056, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0056, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(254597.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0000, 0.0164,  ..., 0.0000, 0.0000, 0.0163],
        [0.0182, 0.0000, 0.0164,  ..., 0.0000, 0.0000, 0.0163],
        [0.0182, 0.0000, 0.0164,  ..., 0.0000, 0.0000, 0.0163],
        ...,
        [0.0181, 0.0000, 0.0163,  ..., 0.0000, 0.0000, 0.0162],
        [0.0181, 0.0000, 0.0163,  ..., 0.0000, 0.0000, 0.0162],
        [0.0181, 0.0000, 0.0163,  ..., 0.0000, 0.0000, 0.0162]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1795071.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(16116.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(591.0065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(948.9030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2154.8372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22506.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1923.3539, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2186],
        [-0.2331],
        [-0.2424],
        ...,
        [-0.1548],
        [-0.1542],
        [-0.1539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-308881.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1359.9628, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1359.9628, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0090,  0.0117,  0.0081,  ..., -0.0158,  0.0180, -0.0168],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        ...,
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1832.8489, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(385.2715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.4007, device='cuda:0')



h[100].sum tensor(-66.7551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(204.7121, device='cuda:0')



h[200].sum tensor(-121.6808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0212, 0.0235, 0.0164,  ..., 0.0000, 0.0367, 0.0000],
        [0.0135, 0.0117, 0.0081,  ..., 0.0000, 0.0185, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        ...,
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0059, 0.0000, 0.0000,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(198842.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0007, 0.0000,  ..., 0.0000, 0.0094, 0.0359],
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0277],
        [0.0124, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0214],
        ...,
        [0.0187, 0.0000, 0.0164,  ..., 0.0000, 0.0000, 0.0160],
        [0.0187, 0.0000, 0.0164,  ..., 0.0000, 0.0000, 0.0160],
        [0.0187, 0.0000, 0.0164,  ..., 0.0000, 0.0000, 0.0160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1472979.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(17857.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(417.8210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(214.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1996.6464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13142.1523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1556.6476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0277],
        [ 0.0272],
        [ 0.0172],
        ...,
        [-0.1592],
        [-0.1585],
        [-0.1583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-359069.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1772.9882, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1772.9882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0149,  0.0208,  0.0152,  ..., -0.0278,  0.0317, -0.0297],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        ...,
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0015, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1704.8684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(423.7741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(78.7446, device='cuda:0')



h[100].sum tensor(-13.7349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(266.8839, device='cuda:0')



h[200].sum tensor(-158.7160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0553, 0.0766, 0.0566,  ..., 0.0000, 0.1168, 0.0000],
        [0.0395, 0.0517, 0.0373,  ..., 0.0000, 0.0795, 0.0000],
        [0.0260, 0.0305, 0.0210,  ..., 0.0000, 0.0477, 0.0000],
        ...,
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0061, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(244706.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0564, 0.0735],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0445, 0.0652],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0328, 0.0574],
        ...,
        [0.0194, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0158],
        [0.0194, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0158],
        [0.0194, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0158]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1721417., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(18461.5137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(515.9066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(795.8362, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2175.2332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19175.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1843.7830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0560],
        [-0.0430],
        [-0.0243],
        ...,
        [-0.1637],
        [-0.1630],
        [-0.1628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-381571.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 900 loss: tensor(570.4051, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1195.3223, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1195.3223, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0056,  0.0034,  ..., -0.0078,  0.0090, -0.0083],
        [ 0.0107,  0.0141,  0.0100,  ..., -0.0190,  0.0218, -0.0202],
        [ 0.0065,  0.0074,  0.0049,  ..., -0.0102,  0.0118, -0.0109],
        ...,
        [ 0.0016, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0016, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0016, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1513.1025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(399.5359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.0884, device='cuda:0')



h[100].sum tensor(-102.6688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(179.9292, device='cuda:0')



h[200].sum tensor(-107.2626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0327, 0.0403, 0.0276,  ..., 0.0000, 0.0631, 0.0000],
        [0.0225, 0.0242, 0.0152,  ..., 0.0000, 0.0389, 0.0000],
        [0.0331, 0.0409, 0.0281,  ..., 0.0000, 0.0640, 0.0000],
        ...,
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0064, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(186854.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0002, 0.0000,  ..., 0.0000, 0.0066, 0.0386],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0075, 0.0400],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0136, 0.0448],
        ...,
        [0.0200, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0157],
        [0.0200, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0157],
        [0.0200, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1452266., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(19667.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.8385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(135.5963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2011.4282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11316.7480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1465.8190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0407],
        [-0.0134],
        [ 0.0026],
        ...,
        [-0.1679],
        [-0.1672],
        [-0.1670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-403012.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1411.9515, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.9515, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0154,  0.0214,  0.0156,  ..., -0.0286,  0.0327, -0.0305],
        [ 0.0078,  0.0094,  0.0064,  ..., -0.0128,  0.0148, -0.0137],
        [ 0.0096,  0.0122,  0.0085,  ..., -0.0165,  0.0190, -0.0176],
        ...,
        [ 0.0016, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0016, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000],
        [ 0.0016, -0.0003, -0.0011,  ...,  0.0000,  0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1376.0668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(421.8782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.7097, device='cuda:0')



h[100].sum tensor(-79.7199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(212.5379, device='cuda:0')



h[200].sum tensor(-125.8286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0461, 0.0611, 0.0437,  ..., 0.0000, 0.0945, 0.0000],
        [0.0462, 0.0613, 0.0438,  ..., 0.0000, 0.0947, 0.0000],
        [0.0192, 0.0193, 0.0132,  ..., 0.0000, 0.0309, 0.0000],
        ...,
        [0.0065, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0065, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0065, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(207124.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0517, 0.0714],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0380, 0.0615],
        [0.0009, 0.0026, 0.0000,  ..., 0.0000, 0.0178, 0.0464],
        ...,
        [0.0202, 0.0000, 0.0167,  ..., 0.0000, 0.0000, 0.0159],
        [0.0202, 0.0000, 0.0167,  ..., 0.0000, 0.0000, 0.0159],
        [0.0202, 0.0000, 0.0167,  ..., 0.0000, 0.0000, 0.0159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1551827., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(19563.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(382.5035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(276.7218, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2107.1882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13502.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1614.8768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1652],
        [-0.1380],
        [-0.1147],
        ...,
        [-0.1716],
        [-0.1708],
        [-0.1706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-398936.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1753.9113, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1753.9113, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017, -0.0003, -0.0011,  ...,  0.0000,  0.0003,  0.0000],
        [ 0.0059,  0.0063,  0.0039,  ..., -0.0087,  0.0102, -0.0093],
        [ 0.0017, -0.0003, -0.0011,  ...,  0.0000,  0.0003,  0.0000],
        ...,
        [ 0.0017, -0.0003, -0.0011,  ...,  0.0000,  0.0003,  0.0000],
        [ 0.0017, -0.0003, -0.0011,  ...,  0.0000,  0.0003,  0.0000],
        [ 0.0017, -0.0003, -0.0011,  ...,  0.0000,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1237.7009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(450.8947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.8973, device='cuda:0')



h[100].sum tensor(-38.9199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(264.0123, device='cuda:0')



h[200].sum tensor(-155.5757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0340, 0.0417, 0.0286,  ..., 0.0000, 0.0656, 0.0000],
        [0.0101, 0.0051, 0.0030,  ..., 0.0000, 0.0092, 0.0000],
        [0.0329, 0.0405, 0.0285,  ..., 0.0000, 0.0632, 0.0000],
        ...,
        [0.0067, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0067, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(245237.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0247, 0.0527],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0198, 0.0483],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0361, 0.0602],
        ...,
        [0.0203, 0.0000, 0.0168,  ..., 0.0000, 0.0000, 0.0161],
        [0.0203, 0.0000, 0.0168,  ..., 0.0000, 0.0000, 0.0161],
        [0.0203, 0.0000, 0.0168,  ..., 0.0000, 0.0000, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1740691.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(18793.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(473.9105, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(346.4399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2265.0740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18182.6270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1888.6691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1630],
        [-0.1889],
        [-0.2146],
        ...,
        [-0.1747],
        [-0.1740],
        [-0.1738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-370981.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1505.4855, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1505.4855, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017, -0.0003, -0.0012,  ...,  0.0000,  0.0003,  0.0000],
        [ 0.0017, -0.0003, -0.0012,  ...,  0.0000,  0.0003,  0.0000],
        [ 0.0017, -0.0003, -0.0012,  ...,  0.0000,  0.0003,  0.0000],
        ...,
        [ 0.0017, -0.0003, -0.0012,  ...,  0.0000,  0.0003,  0.0000],
        [ 0.0017, -0.0003, -0.0012,  ...,  0.0000,  0.0003,  0.0000],
        [ 0.0017, -0.0003, -0.0012,  ...,  0.0000,  0.0003,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1072.1034, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(443.7648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.8639, device='cuda:0')



h[100].sum tensor(-81.2231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(226.6173, device='cuda:0')



h[200].sum tensor(-133.5405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0348, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0069, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(217764.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0000, 0.0155,  ..., 0.0000, 0.0000, 0.0168],
        [0.0202, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0167],
        [0.0193, 0.0000, 0.0152,  ..., 0.0000, 0.0000, 0.0173],
        ...,
        [0.0204, 0.0000, 0.0169,  ..., 0.0000, 0.0000, 0.0163],
        [0.0204, 0.0000, 0.0169,  ..., 0.0000, 0.0000, 0.0163],
        [0.0204, 0.0000, 0.0169,  ..., 0.0000, 0.0000, 0.0163]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1608245.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(19498.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(383.3903, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(263.5908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2193.2642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14342.3799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1737.3909, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1112],
        [-0.1407],
        [-0.1462],
        ...,
        [-0.1777],
        [-0.1770],
        [-0.1768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-398201.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1377.7816, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1377.7816, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0106,  0.0136,  0.0096,  ..., -0.0183,  0.0213, -0.0196],
        [ 0.0094,  0.0118,  0.0082,  ..., -0.0159,  0.0185, -0.0170],
        ...,
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-918.1760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(443.8271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.1921, device='cuda:0')



h[100].sum tensor(-106.0416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(207.3944, device='cuda:0')



h[200].sum tensor(-122.1522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0136, 0.0096,  ..., 0.0000, 0.0224, 0.0000],
        [0.0219, 0.0229, 0.0158,  ..., 0.0000, 0.0367, 0.0000],
        [0.0537, 0.0724, 0.0522,  ..., 0.0000, 0.1119, 0.0000],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(207837.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0454e-02, 9.8778e-05, 8.8217e-04,  ..., 0.0000e+00, 3.1626e-03,
         2.7885e-02],
        [5.1124e-03, 1.2632e-03, 0.0000e+00,  ..., 0.0000e+00, 9.3067e-03,
         3.6880e-02],
        [0.0000e+00, 5.5877e-03, 0.0000e+00,  ..., 0.0000e+00, 2.7082e-02,
         5.3149e-02],
        ...,
        [2.0669e-02, 0.0000e+00, 1.6994e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6543e-02],
        [2.0669e-02, 0.0000e+00, 1.6994e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6543e-02],
        [2.0669e-02, 0.0000e+00, 1.6994e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6543e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1570010.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(19719.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(346.6104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(119.9107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2185.8914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12890.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1698.2715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0273],
        [ 0.0296],
        [ 0.0233],
        ...,
        [-0.1809],
        [-0.1801],
        [-0.1799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-392115.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1737.2067, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1737.2067, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0238,  0.0344,  0.0256,  ..., -0.0456,  0.0525, -0.0487],
        [ 0.0176,  0.0246,  0.0181,  ..., -0.0328,  0.0378, -0.0350],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-769.5194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(476.2503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.1554, device='cuda:0')



h[100].sum tensor(-58.7777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(261.4978, device='cuda:0')



h[200].sum tensor(-154.9983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0401, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0905, 0.1301, 0.0967,  ..., 0.0000, 0.1986, 0.0000],
        [0.0475, 0.0626, 0.0456,  ..., 0.0000, 0.0969, 0.0000],
        [0.0291, 0.0339, 0.0243,  ..., 0.0000, 0.0534, 0.0000],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0072, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(248965.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0271, 0.0000,  ..., 0.0000, 0.0844, 0.0940],
        [0.0004, 0.0128, 0.0000,  ..., 0.0000, 0.0442, 0.0652],
        [0.0057, 0.0045, 0.0000,  ..., 0.0000, 0.0177, 0.0430],
        ...,
        [0.0211, 0.0000, 0.0171,  ..., 0.0000, 0.0000, 0.0167],
        [0.0211, 0.0000, 0.0171,  ..., 0.0000, 0.0000, 0.0167],
        [0.0211, 0.0000, 0.0171,  ..., 0.0000, 0.0000, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1783828.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(19413.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(449.4711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(322.8838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2352.3274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18227.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1977.3475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0883],
        [-0.0557],
        [-0.0502],
        ...,
        [-0.1845],
        [-0.1838],
        [-0.1835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-374109.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1414.2482, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1414.2482, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0164,  0.0227,  0.0166,  ..., -0.0302,  0.0349, -0.0322],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-645.2401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(465.3066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.8117, device='cuda:0')



h[100].sum tensor(-111.5726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(212.8836, device='cuda:0')



h[200].sum tensor(-125.3622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0613, 0.0841, 0.0621,  ..., 0.0000, 0.1292, 0.0000],
        [0.0422, 0.0540, 0.0389,  ..., 0.0000, 0.0841, 0.0000],
        [0.0231, 0.0242, 0.0168,  ..., 0.0000, 0.0389, 0.0000],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(212951.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0235, 0.0000,  ..., 0.0000, 0.0725, 0.0863],
        [0.0000, 0.0154, 0.0000,  ..., 0.0000, 0.0506, 0.0712],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0371, 0.0616],
        ...,
        [0.0218, 0.0000, 0.0172,  ..., 0.0000, 0.0000, 0.0167],
        [0.0218, 0.0000, 0.0172,  ..., 0.0000, 0.0000, 0.0167],
        [0.0218, 0.0000, 0.0172,  ..., 0.0000, 0.0000, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1608211., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(20953.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.2924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75.3859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2250.3306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13086.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1747.5100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2133],
        [-0.1901],
        [-0.1714],
        ...,
        [-0.1887],
        [-0.1879],
        [-0.1877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-410206.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1577.3439, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1577.3439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0018, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-642.1395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(475.4353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.0554, device='cuda:0')



h[100].sum tensor(-88.5092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(237.4340, device='cuda:0')



h[200].sum tensor(-139.6419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(237748.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0000, 0.0120,  ..., 0.0000, 0.0000, 0.0182],
        [0.0196, 0.0000, 0.0105,  ..., 0.0000, 0.0000, 0.0186],
        [0.0192, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0190],
        ...,
        [0.0218, 0.0000, 0.0172,  ..., 0.0000, 0.0000, 0.0167],
        [0.0218, 0.0000, 0.0172,  ..., 0.0000, 0.0000, 0.0167],
        [0.0218, 0.0000, 0.0172,  ..., 0.0000, 0.0000, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1756618.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(20282.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.0004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(142.1102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2337.4966, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17031.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1907.0161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0910],
        [-0.0572],
        [-0.0264],
        ...,
        [-0.1887],
        [-0.1879],
        [-0.1877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-385557.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1326.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1326.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-527.1938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(470.8922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.9098, device='cuda:0')



h[100].sum tensor(-129.1743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(199.6592, device='cuda:0')



h[200].sum tensor(-117.1556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0076, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(210167.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0229, 0.0000, 0.0174,  ..., 0.0000, 0.0000, 0.0167],
        [0.0229, 0.0000, 0.0174,  ..., 0.0000, 0.0000, 0.0167],
        [0.0229, 0.0000, 0.0174,  ..., 0.0000, 0.0000, 0.0167],
        ...,
        [0.0228, 0.0000, 0.0173,  ..., 0.0000, 0.0000, 0.0166],
        [0.0228, 0.0000, 0.0173,  ..., 0.0000, 0.0000, 0.0166],
        [0.0228, 0.0000, 0.0173,  ..., 0.0000, 0.0000, 0.0166]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1630783., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(21944.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.1754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(127.7910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2259.4326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13026.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1722.2683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2883],
        [-0.2912],
        [-0.2939],
        ...,
        [-0.1934],
        [-0.1926],
        [-0.1923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-424537.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1815.7734, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1815.7734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0019, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-386.2119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(510.7286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(80.6449, device='cuda:0')



h[100].sum tensor(-62.9198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(273.3243, device='cuda:0')



h[200].sum tensor(-161.1992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(267397., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0214, 0.0000, 0.0118,  ..., 0.0000, 0.0000, 0.0186],
        [0.0229, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0172],
        [0.0218, 0.0000, 0.0118,  ..., 0.0000, 0.0000, 0.0182],
        ...,
        [0.0232, 0.0000, 0.0175,  ..., 0.0000, 0.0000, 0.0167],
        [0.0232, 0.0000, 0.0175,  ..., 0.0000, 0.0000, 0.0167],
        [0.0232, 0.0000, 0.0175,  ..., 0.0000, 0.0000, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1941467., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(21899.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(463.9828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(376.2437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2475.5364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20991.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2101.6492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0806],
        [-0.1233],
        [-0.1462],
        ...,
        [-0.1971],
        [-0.1962],
        [-0.1960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-423700., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 1200 loss: tensor(502.8658, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1665.8350, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1665.8350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-284.4307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(507.4042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.9856, device='cuda:0')



h[100].sum tensor(-90.9476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(250.7544, device='cuda:0')



h[200].sum tensor(-147.2590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0079, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(244456.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0237, 0.0000, 0.0178,  ..., 0.0000, 0.0000, 0.0172],
        [0.0237, 0.0000, 0.0178,  ..., 0.0000, 0.0000, 0.0172],
        [0.0237, 0.0000, 0.0178,  ..., 0.0000, 0.0000, 0.0172],
        ...,
        [0.0236, 0.0000, 0.0178,  ..., 0.0000, 0.0000, 0.0171],
        [0.0236, 0.0000, 0.0178,  ..., 0.0000, 0.0000, 0.0171],
        [0.0236, 0.0000, 0.0178,  ..., 0.0000, 0.0000, 0.0171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1802676.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(21921.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.9931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(100.6418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2431.8330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16799.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1974.6156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2039],
        [-0.2543],
        [-0.2880],
        ...,
        [-0.2004],
        [-0.1995],
        [-0.1993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-407711.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1393.8000, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1393.8000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0096,  0.0117,  0.0081,  ..., -0.0158,  0.0186, -0.0168],
        [ 0.0166,  0.0227,  0.0166,  ..., -0.0301,  0.0350, -0.0322],
        ...,
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-191.9474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(495.6111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.9035, device='cuda:0')



h[100].sum tensor(-135.8482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.8056, device='cuda:0')



h[200].sum tensor(-123.0521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0117, 0.0081,  ..., 0.0000, 0.0201, 0.0000],
        [0.0346, 0.0413, 0.0300,  ..., 0.0000, 0.0649, 0.0000],
        [0.0737, 0.1022, 0.0751,  ..., 0.0000, 0.1572, 0.0000],
        ...,
        [0.0080, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0080, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(216120.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0117, 0.0016, 0.0015,  ..., 0.0000, 0.0059, 0.0321],
        [0.0045, 0.0079, 0.0000,  ..., 0.0000, 0.0231, 0.0503],
        [0.0000, 0.0199, 0.0000,  ..., 0.0000, 0.0558, 0.0757],
        ...,
        [0.0237, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0175],
        [0.0237, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0175],
        [0.0237, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1671084.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(22727.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.7731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30.9532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2357.8560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13018.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1817.5598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0431],
        [-0.0206],
        [-0.0215],
        ...,
        [-0.2031],
        [-0.2023],
        [-0.2020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-428025.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1521.9532, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1521.9532, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0020, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-55.7986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(509.6887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.5953, device='cuda:0')



h[100].sum tensor(-123.3882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(229.0962, device='cuda:0')



h[200].sum tensor(-134.1576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0062, 0.0038,  ..., 0.0000, 0.0119, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0081, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0081, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(234592.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0691e-02, 7.5056e-05, 2.9270e-03,  ..., 0.0000e+00, 1.5147e-03,
         2.9508e-02],
        [1.7822e-02, 0.0000e+00, 4.6931e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.2812e-02],
        [1.9321e-02, 0.0000e+00, 6.5321e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.1657e-02],
        ...,
        [2.3832e-02, 0.0000e+00, 1.8182e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7977e-02],
        [2.3832e-02, 0.0000e+00, 1.8182e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7977e-02],
        [2.3832e-02, 0.0000e+00, 1.8182e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7977e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1784306.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(22864.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.2496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(120.7006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2440.4619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15854.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1957.5111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0728],
        [ 0.0499],
        [ 0.0295],
        ...,
        [-0.2059],
        [-0.2051],
        [-0.2048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-439444.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1588.5226, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1588.5226, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(76.6727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(520.0999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.5518, device='cuda:0')



h[100].sum tensor(-119.9277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(239.1167, device='cuda:0')



h[200].sum tensor(-139.5961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        ...,
        [0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0082, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(237982.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0242, 0.0000, 0.0185,  ..., 0.0000, 0.0000, 0.0185],
        [0.0242, 0.0000, 0.0185,  ..., 0.0000, 0.0000, 0.0185],
        [0.0242, 0.0000, 0.0185,  ..., 0.0000, 0.0000, 0.0185],
        ...,
        [0.0240, 0.0000, 0.0184,  ..., 0.0000, 0.0000, 0.0184],
        [0.0240, 0.0000, 0.0184,  ..., 0.0000, 0.0000, 0.0184],
        [0.0240, 0.0000, 0.0184,  ..., 0.0000, 0.0000, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1791585.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(22348.4023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(389.4551, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(71.7786, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2482.6440, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15608.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1994.3014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2761],
        [-0.2909],
        [-0.2977],
        ...,
        [-0.2076],
        [-0.2067],
        [-0.2063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-413266.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1511.5735, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1511.5735, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0152,  0.0203,  0.0147,  ..., -0.0269,  0.0315, -0.0287],
        [ 0.0226,  0.0320,  0.0237,  ..., -0.0422,  0.0490, -0.0450],
        [ 0.0254,  0.0364,  0.0272,  ..., -0.0480,  0.0556, -0.0512],
        ...,
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(200.4888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(523.0684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.1343, device='cuda:0')



h[100].sum tensor(-136.6465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(227.5338, device='cuda:0')



h[200].sum tensor(-132.3648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0349, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0707, 0.0969, 0.0711,  ..., 0.0000, 0.1496, 0.0000],
        [0.0729, 0.1002, 0.0736,  ..., 0.0000, 0.1546, 0.0000],
        [0.0812, 0.1134, 0.0838,  ..., 0.0000, 0.1743, 0.0000],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(230075.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0292, 0.0000,  ..., 0.0000, 0.0710, 0.0894],
        [0.0000, 0.0282, 0.0000,  ..., 0.0000, 0.0685, 0.0877],
        [0.0000, 0.0261, 0.0000,  ..., 0.0000, 0.0640, 0.0840],
        ...,
        [0.0245, 0.0000, 0.0186,  ..., 0.0000, 0.0000, 0.0185],
        [0.0245, 0.0000, 0.0186,  ..., 0.0000, 0.0000, 0.0185],
        [0.0245, 0.0000, 0.0186,  ..., 0.0000, 0.0000, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1760546.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(23229.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(365.3374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2478.9634, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14438.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1941.3270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0337],
        [-0.0205],
        [-0.0098],
        ...,
        [-0.2126],
        [-0.2117],
        [-0.2114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-440168.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1517.3489, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1517.3489, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0172,  0.0234,  0.0171,  ..., -0.0310,  0.0362, -0.0331],
        [ 0.0120,  0.0153,  0.0109,  ..., -0.0204,  0.0240, -0.0218],
        [ 0.0137,  0.0180,  0.0129,  ..., -0.0239,  0.0280, -0.0255],
        ...,
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(200.5232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(523.0897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.3908, device='cuda:0')



h[100].sum tensor(-136.5983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(228.4031, device='cuda:0')



h[200].sum tensor(-132.3944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0503, 0.0648, 0.0462,  ..., 0.0000, 0.1013, 0.0000],
        [0.0565, 0.0745, 0.0537,  ..., 0.0000, 0.1159, 0.0000],
        [0.0520, 0.0673, 0.0482,  ..., 0.0000, 0.1052, 0.0000],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0084, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(232415.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0384, 0.0679],
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0382, 0.0675],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0333, 0.0640],
        ...,
        [0.0245, 0.0000, 0.0186,  ..., 0.0000, 0.0000, 0.0185],
        [0.0245, 0.0000, 0.0186,  ..., 0.0000, 0.0000, 0.0185],
        [0.0245, 0.0000, 0.0186,  ..., 0.0000, 0.0000, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1777088.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(23226.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(368.5809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(62.1754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2484.0349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14805.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1957.1918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0451],
        [ 0.0420],
        [ 0.0379],
        ...,
        [-0.2049],
        [-0.2079],
        [-0.2098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-442944.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1486.4899, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1486.4899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        ...,
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0021, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(317.6398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(529.1106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.0202, device='cuda:0')



h[100].sum tensor(-144.5065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(223.7580, device='cuda:0')



h[200].sum tensor(-130.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0085, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0085, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(230454.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0155, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0270],
        [0.0138, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0282],
        [0.0140, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0282],
        ...,
        [0.0250, 0.0000, 0.0189,  ..., 0.0000, 0.0000, 0.0188],
        [0.0250, 0.0000, 0.0189,  ..., 0.0000, 0.0000, 0.0188],
        [0.0250, 0.0000, 0.0189,  ..., 0.0000, 0.0000, 0.0188]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1771491.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(24010.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.7834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66.8447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2499.2124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14341.1104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1945.9755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0330],
        [ 0.0382],
        [ 0.0394],
        ...,
        [-0.2162],
        [-0.2153],
        [-0.2150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-464250.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1295.7305, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1295.7305, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0109,  0.0134,  0.0094,  ..., -0.0179,  0.0212, -0.0191],
        [ 0.0072,  0.0076,  0.0049,  ..., -0.0103,  0.0125, -0.0110],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        ...,
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(407.6570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(523.5343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.5479, device='cuda:0')



h[100].sum tensor(-177.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(195.0434, device='cuda:0')



h[200].sum tensor(-113.4161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0378, 0.0445, 0.0306,  ..., 0.0000, 0.0710, 0.0000],
        [0.0245, 0.0243, 0.0169,  ..., 0.0000, 0.0397, 0.0000],
        [0.0137, 0.0076, 0.0049,  ..., 0.0000, 0.0141, 0.0000],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0087, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(212252.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0097, 0.0480],
        [0.0055, 0.0026, 0.0000,  ..., 0.0000, 0.0054, 0.0396],
        [0.0119, 0.0002, 0.0028,  ..., 0.0000, 0.0006, 0.0300],
        ...,
        [0.0255, 0.0000, 0.0193,  ..., 0.0000, 0.0000, 0.0191],
        [0.0255, 0.0000, 0.0193,  ..., 0.0000, 0.0000, 0.0191],
        [0.0255, 0.0000, 0.0193,  ..., 0.0000, 0.0000, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1704392.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(24370.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.7874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5.9111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2463.6416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11994.3623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1832.6389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1212],
        [ 0.1100],
        [ 0.0910],
        ...,
        [-0.2196],
        [-0.2187],
        [-0.2184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-466129.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1563.3381, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1563.3381, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0156,  0.0208,  0.0151,  ..., -0.0275,  0.0323, -0.0294],
        [ 0.0082,  0.0092,  0.0062,  ..., -0.0124,  0.0149, -0.0133],
        [ 0.0089,  0.0102,  0.0069,  ..., -0.0137,  0.0163, -0.0146],
        ...,
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(541.2149, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(545.1444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.4333, device='cuda:0')



h[100].sum tensor(-144.7084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(235.3258, device='cuda:0')



h[200].sum tensor(-136.5857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0417, 0.0505, 0.0353,  ..., 0.0000, 0.0801, 0.0000],
        [0.0447, 0.0552, 0.0389,  ..., 0.0000, 0.0872, 0.0000],
        [0.0300, 0.0324, 0.0222,  ..., 0.0000, 0.0524, 0.0000],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0088, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(241842.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0144, 0.0508],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0147, 0.0506],
        [0.0013, 0.0045, 0.0000,  ..., 0.0000, 0.0090, 0.0456],
        ...,
        [0.0258, 0.0000, 0.0196,  ..., 0.0000, 0.0000, 0.0197],
        [0.0258, 0.0000, 0.0196,  ..., 0.0000, 0.0000, 0.0197],
        [0.0258, 0.0000, 0.0196,  ..., 0.0000, 0.0000, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1858949.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(24020.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(400.8580, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13.5650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2594.0378, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15814.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2033.7050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0743],
        [ 0.0651],
        [ 0.0502],
        ...,
        [-0.2226],
        [-0.2217],
        [-0.2215]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-446326.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1529.6823, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1529.6823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0100,  0.0119,  0.0083,  ..., -0.0160,  0.0190, -0.0170],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        ...,
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(637.9596, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(548.3182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.9385, device='cuda:0')



h[100].sum tensor(-154.4747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(230.2596, device='cuda:0')



h[200].sum tensor(-133.6595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0353, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0371, 0.0432, 0.0296,  ..., 0.0000, 0.0691, 0.0000],
        [0.0152, 0.0097, 0.0065,  ..., 0.0000, 0.0173, 0.0000],
        [0.0166, 0.0119, 0.0083,  ..., 0.0000, 0.0207, 0.0000],
        ...,
        [0.0089, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0089, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(236696.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0025, 0.0000,  ..., 0.0000, 0.0030, 0.0425],
        [0.0080, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0341],
        [0.0044, 0.0009, 0.0000,  ..., 0.0000, 0.0010, 0.0376],
        ...,
        [0.0261, 0.0000, 0.0200,  ..., 0.0000, 0.0000, 0.0202],
        [0.0261, 0.0000, 0.0200,  ..., 0.0000, 0.0000, 0.0202],
        [0.0261, 0.0000, 0.0200,  ..., 0.0000, 0.0000, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1833642.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(24041.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(391.7991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11.7653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2604.1228, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14793.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2012.9047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0490],
        [ 0.0612],
        [ 0.0668],
        ...,
        [-0.2255],
        [-0.2246],
        [-0.2243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-439638.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 1500 loss: tensor(504.7017, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1513.0721, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1513.0721, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0118,  0.0146,  0.0104,  ..., -0.0195,  0.0231, -0.0208],
        [ 0.0107,  0.0131,  0.0092,  ..., -0.0174,  0.0207, -0.0186],
        [ 0.0109,  0.0132,  0.0093,  ..., -0.0177,  0.0209, -0.0189],
        ...,
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0022, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(747.1703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(552.5756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.2008, device='cuda:0')



h[100].sum tensor(-161.5674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(227.7593, device='cuda:0')



h[200].sum tensor(-132.2731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0463, 0.0575, 0.0407,  ..., 0.0000, 0.0907, 0.0000],
        [0.0340, 0.0381, 0.0258,  ..., 0.0000, 0.0616, 0.0000],
        [0.0349, 0.0394, 0.0268,  ..., 0.0000, 0.0636, 0.0000],
        ...,
        [0.0090, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0090, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0090, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(241818.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0275, 0.0610],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0146, 0.0525],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0094, 0.0486],
        ...,
        [0.0263, 0.0000, 0.0204,  ..., 0.0000, 0.0000, 0.0207],
        [0.0263, 0.0000, 0.0204,  ..., 0.0000, 0.0000, 0.0207],
        [0.0263, 0.0000, 0.0204,  ..., 0.0000, 0.0000, 0.0207]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1888829.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(24598.9082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(402.4625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10.4923, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2642.4607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16154.8975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2061.1580, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0097],
        [ 0.0218],
        [ 0.0338],
        ...,
        [-0.2282],
        [-0.2273],
        [-0.2270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-456863.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1535.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1535.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        ...,
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(867.4990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(560.7139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.1973, device='cuda:0')



h[100].sum tensor(-163.8458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(231.1366, device='cuda:0')



h[200].sum tensor(-133.5964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0091, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0091, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        ...,
        [0.0091, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0091, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0091, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(239974.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0269, 0.0000, 0.0207,  ..., 0.0000, 0.0000, 0.0210],
        [0.0269, 0.0000, 0.0207,  ..., 0.0000, 0.0000, 0.0210],
        [0.0269, 0.0000, 0.0208,  ..., 0.0000, 0.0000, 0.0211],
        ...,
        [0.0267, 0.0000, 0.0207,  ..., 0.0000, 0.0000, 0.0209],
        [0.0267, 0.0000, 0.0207,  ..., 0.0000, 0.0000, 0.0209],
        [0.0267, 0.0000, 0.0207,  ..., 0.0000, 0.0000, 0.0209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1868237.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(25461.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(390.7653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44.2551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2651.8772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15213.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2052.7080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2827],
        [-0.2957],
        [-0.2914],
        ...,
        [-0.2318],
        [-0.2309],
        [-0.2306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-479566.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1687.9701, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1687.9701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        ...,
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(993.1836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(577.9503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.9687, device='cuda:0')



h[100].sum tensor(-145.6277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(254.0863, device='cuda:0')



h[200].sum tensor(-147.2817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0390, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        ...,
        [0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0093, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(262557.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0202, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0263],
        [0.0256, 0.0000, 0.0163,  ..., 0.0000, 0.0000, 0.0223],
        [0.0275, 0.0000, 0.0211,  ..., 0.0000, 0.0000, 0.0212],
        ...,
        [0.0273, 0.0000, 0.0210,  ..., 0.0000, 0.0000, 0.0210],
        [0.0273, 0.0000, 0.0210,  ..., 0.0000, 0.0000, 0.0210],
        [0.0273, 0.0000, 0.0210,  ..., 0.0000, 0.0000, 0.0210]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2019395.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(25573.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(453.5927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20.5427, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2749.9116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18973.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2199.1975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0027],
        [-0.0874],
        [-0.1768],
        ...,
        [-0.2356],
        [-0.2346],
        [-0.2343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-474375.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1605.5924, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1605.5924, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0102,  0.0120,  0.0084,  ..., -0.0161,  0.0192, -0.0172],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        ...,
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0023, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1068.0472, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(579.3304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.3100, device='cuda:0')



h[100].sum tensor(-162.2256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(241.6862, device='cuda:0')



h[200].sum tensor(-139.7101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0371, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0455, 0.0558, 0.0404,  ..., 0.0000, 0.0878, 0.0000],
        [0.0226, 0.0202, 0.0138,  ..., 0.0000, 0.0339, 0.0000],
        [0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        ...,
        [0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(254642.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0217, 0.0000,  ..., 0.0000, 0.0358, 0.0676],
        [0.0019, 0.0128, 0.0000,  ..., 0.0000, 0.0193, 0.0544],
        [0.0024, 0.0065, 0.0000,  ..., 0.0000, 0.0077, 0.0447],
        ...,
        [0.0278, 0.0000, 0.0214,  ..., 0.0000, 0.0000, 0.0212],
        [0.0278, 0.0000, 0.0214,  ..., 0.0000, 0.0000, 0.0212],
        [0.0278, 0.0000, 0.0214,  ..., 0.0000, 0.0000, 0.0212]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1986878.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(25706.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.3026, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6.7696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2746.9907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17675.1914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2154.1919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0334],
        [ 0.0355],
        [ 0.0411],
        ...,
        [-0.2391],
        [-0.2382],
        [-0.2379]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-463885.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1566.0785, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1566.0785, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0024, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0060,  0.0054,  0.0033,  ..., -0.0074,  0.0092, -0.0079],
        ...,
        [ 0.0024, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0024, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0024, -0.0003, -0.0012,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1150.8142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(585.2354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.5550, device='cuda:0')



h[100].sum tensor(-171.0457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(235.7383, device='cuda:0')



h[200].sum tensor(-136.7837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0362, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0095, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0193, 0.0147, 0.0096,  ..., 0.0000, 0.0257, 0.0000],
        [0.0225, 0.0194, 0.0124,  ..., 0.0000, 0.0333, 0.0000],
        ...,
        [0.0095, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0095, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0095, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(250506.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0074, 0.0008, 0.0000,  ..., 0.0000, 0.0004, 0.0361],
        [0.0038, 0.0023, 0.0000,  ..., 0.0000, 0.0004, 0.0396],
        [0.0019, 0.0050, 0.0000,  ..., 0.0000, 0.0023, 0.0444],
        ...,
        [0.0284, 0.0000, 0.0218,  ..., 0.0000, 0.0000, 0.0212],
        [0.0284, 0.0000, 0.0218,  ..., 0.0000, 0.0000, 0.0212],
        [0.0284, 0.0000, 0.0218,  ..., 0.0000, 0.0000, 0.0212]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1963106., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(26594.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.8046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3.5646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2751.3740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16745.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2124.7334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1321],
        [ 0.1398],
        [ 0.1421],
        ...,
        [-0.2429],
        [-0.2419],
        [-0.2416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-490326.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1749.1080, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1749.1080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0152,  0.0198,  0.0144,  ..., -0.0261,  0.0309, -0.0279],
        ...,
        [ 0.0081,  0.0086,  0.0058,  ..., -0.0116,  0.0141, -0.0124],
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1292.4287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(603.7367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(77.6840, device='cuda:0')



h[100].sum tensor(-148.9643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(263.2893, device='cuda:0')



h[200].sum tensor(-152.8694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0225, 0.0198, 0.0144,  ..., 0.0000, 0.0329, 0.0000],
        [0.0201, 0.0161, 0.0116,  ..., 0.0000, 0.0273, 0.0000],
        ...,
        [0.0269, 0.0264, 0.0187,  ..., 0.0000, 0.0434, 0.0000],
        [0.0154, 0.0086, 0.0058,  ..., 0.0000, 0.0161, 0.0000],
        [0.0097, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(267944.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0246, 0.0000, 0.0110,  ..., 0.0000, 0.0000, 0.0250],
        [0.0162, 0.0029, 0.0004,  ..., 0.0000, 0.0040, 0.0333],
        [0.0097, 0.0045, 0.0000,  ..., 0.0000, 0.0052, 0.0382],
        ...,
        [0.0046, 0.0105, 0.0000,  ..., 0.0000, 0.0133, 0.0486],
        [0.0125, 0.0040, 0.0032,  ..., 0.0000, 0.0037, 0.0356],
        [0.0229, 0.0000, 0.0110,  ..., 0.0000, 0.0000, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2056537.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(26807.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(467.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14.9547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2826.2661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18953.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2247.3030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2487],
        [-0.1819],
        [-0.1207],
        ...,
        [ 0.0293],
        [-0.0152],
        [-0.0887]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-501346.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1426.4529, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1426.4529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        ...,
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0024, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1319.9890, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(587.8887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.3538, device='cuda:0')



h[100].sum tensor(-201.3290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(214.7207, device='cuda:0')



h[200].sum tensor(-123.7382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0142, 0.0066, 0.0042,  ..., 0.0000, 0.0132, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        ...,
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(237659.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0154, 0.0007, 0.0051,  ..., 0.0000, 0.0000, 0.0305],
        [0.0248, 0.0000, 0.0125,  ..., 0.0000, 0.0000, 0.0243],
        [0.0277, 0.0000, 0.0188,  ..., 0.0000, 0.0000, 0.0226],
        ...,
        [0.0286, 0.0000, 0.0222,  ..., 0.0000, 0.0000, 0.0216],
        [0.0286, 0.0000, 0.0222,  ..., 0.0000, 0.0000, 0.0216],
        [0.0286, 0.0000, 0.0222,  ..., 0.0000, 0.0000, 0.0216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1914304.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(27049.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(383.1495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1.2730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2741.0828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14868.9463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2072.0483, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0335],
        [-0.0287],
        [-0.0708],
        ...,
        [-0.2485],
        [-0.2475],
        [-0.2472]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-511480.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1386.3345, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1386.3345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0074,  0.0074,  0.0048,  ..., -0.0100,  0.0123, -0.0107],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0176,  0.0235,  0.0173,  ..., -0.0309,  0.0366, -0.0330],
        ...,
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1404.2922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(589.1052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.5720, device='cuda:0')



h[100].sum tensor(-211.5643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(208.6818, device='cuda:0')



h[200].sum tensor(-120.1805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0139, 0.0060, 0.0037,  ..., 0.0000, 0.0124, 0.0000],
        [0.0299, 0.0308, 0.0221,  ..., 0.0000, 0.0503, 0.0000],
        [0.0222, 0.0191, 0.0139,  ..., 0.0000, 0.0321, 0.0000],
        ...,
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0098, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(232734., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0051, 0.0020, 0.0000,  ..., 0.0000, 0.0000, 0.0381],
        [0.0033, 0.0062, 0.0000,  ..., 0.0000, 0.0049, 0.0429],
        [0.0059, 0.0057, 0.0000,  ..., 0.0000, 0.0046, 0.0415],
        ...,
        [0.0284, 0.0000, 0.0225,  ..., 0.0000, 0.0000, 0.0221],
        [0.0284, 0.0000, 0.0225,  ..., 0.0000, 0.0000, 0.0221],
        [0.0284, 0.0000, 0.0225,  ..., 0.0000, 0.0000, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1884335.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(26409.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(370.0282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.3855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2744.0312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13804.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2071.1455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1220],
        [ 0.1063],
        [ 0.0818],
        ...,
        [-0.2492],
        [-0.2482],
        [-0.2479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-491043.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1715.8019, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1715.8019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        ...,
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1568.3738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(613.3981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.2048, device='cuda:0')



h[100].sum tensor(-170.1614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(258.2758, device='cuda:0')



h[200].sum tensor(-147.7032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0396, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0145, 0.0068, 0.0044,  ..., 0.0000, 0.0137, 0.0000],
        [0.0099, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0275, 0.0268, 0.0190,  ..., 0.0000, 0.0444, 0.0000],
        ...,
        [0.0099, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0099, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0099, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(265150.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0066, 0.0024, 0.0000,  ..., 0.0000, 0.0008, 0.0380],
        [0.0061, 0.0049, 0.0000,  ..., 0.0000, 0.0044, 0.0400],
        [0.0033, 0.0186, 0.0000,  ..., 0.0000, 0.0259, 0.0581],
        ...,
        [0.0282, 0.0000, 0.0227,  ..., 0.0000, 0.0000, 0.0224],
        [0.0273, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0232],
        [0.0240, 0.0000, 0.0113,  ..., 0.0000, 0.0000, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2047503.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(26140.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(446.6310, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22.5900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2867.3323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18042.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2308.0813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0737],
        [ 0.0361],
        [-0.0160],
        ...,
        [-0.2313],
        [-0.1951],
        [-0.1349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-495958.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1433.5391, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1433.5391, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        ...,
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0004, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1538.1182, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(601.5522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.6685, device='cuda:0')



h[100].sum tensor(-212.0466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.7874, device='cuda:0')



h[200].sum tensor(-124.3755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0100, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        ...,
        [0.0100, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0100, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(240750.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0286, 0.0000, 0.0231,  ..., 0.0000, 0.0000, 0.0229],
        [0.0286, 0.0000, 0.0231,  ..., 0.0000, 0.0000, 0.0229],
        [0.0287, 0.0000, 0.0231,  ..., 0.0000, 0.0000, 0.0229],
        ...,
        [0.0285, 0.0000, 0.0231,  ..., 0.0000, 0.0000, 0.0227],
        [0.0285, 0.0000, 0.0231,  ..., 0.0000, 0.0000, 0.0227],
        [0.0285, 0.0000, 0.0231,  ..., 0.0000, 0.0000, 0.0227]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1941774.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(27152.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.7600, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.6142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2805.1626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14960.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2166.4604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3559],
        [-0.3623],
        [-0.3640],
        ...,
        [-0.2527],
        [-0.2517],
        [-0.2514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-526968.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 1800 loss: tensor(552.4133, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1243.8391, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1243.8391, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0087,  0.0094,  0.0064,  ..., -0.0126,  0.0154, -0.0135],
        [ 0.0067,  0.0062,  0.0040,  ..., -0.0085,  0.0107, -0.0091],
        ...,
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1503.3877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(595.9570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.2433, device='cuda:0')



h[100].sum tensor(-242.5024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.2323, device='cuda:0')



h[200].sum tensor(-107.7171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0164, 0.0094, 0.0064,  ..., 0.0000, 0.0177, 0.0000],
        [0.0194, 0.0138, 0.0090,  ..., 0.0000, 0.0249, 0.0000],
        [0.0465, 0.0556, 0.0396,  ..., 0.0000, 0.0888, 0.0000],
        ...,
        [0.0102, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0102, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0102, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(224726.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0124, 0.0030, 0.0033,  ..., 0.0000, 0.0013, 0.0350],
        [0.0061, 0.0084, 0.0000,  ..., 0.0000, 0.0067, 0.0442],
        [0.0000, 0.0190, 0.0000,  ..., 0.0000, 0.0201, 0.0589],
        ...,
        [0.0291, 0.0000, 0.0235,  ..., 0.0000, 0.0000, 0.0229],
        [0.0291, 0.0000, 0.0235,  ..., 0.0000, 0.0000, 0.0229],
        [0.0291, 0.0000, 0.0235,  ..., 0.0000, 0.0000, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1890093.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(27414.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.3849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2781.8447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13020.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2067.9417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0790],
        [ 0.0741],
        [ 0.0659],
        ...,
        [-0.2576],
        [-0.2566],
        [-0.2562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-519892.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1467.1385, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1467.1385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070,  0.0067,  0.0043,  ..., -0.0091,  0.0113, -0.0097],
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        ...,
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0025, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1564.3685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(609.9034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.1608, device='cuda:0')



h[100].sum tensor(-210.8479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(220.8451, device='cuda:0')



h[200].sum tensor(-126.8687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0236, 0.0204, 0.0141,  ..., 0.0000, 0.0348, 0.0000],
        [0.0342, 0.0367, 0.0258,  ..., 0.0000, 0.0599, 0.0000],
        [0.0301, 0.0306, 0.0219,  ..., 0.0000, 0.0501, 0.0000],
        ...,
        [0.0102, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0102, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0102, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(242098., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0101, 0.0512],
        [0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.0187, 0.0568],
        [0.0000, 0.0184, 0.0000,  ..., 0.0000, 0.0204, 0.0571],
        ...,
        [0.0291, 0.0000, 0.0235,  ..., 0.0000, 0.0000, 0.0229],
        [0.0291, 0.0000, 0.0235,  ..., 0.0000, 0.0000, 0.0229],
        [0.0291, 0.0000, 0.0235,  ..., 0.0000, 0.0000, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1953901.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(27021.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(396.8430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.3122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2841.8936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14757.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2180.8555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0744],
        [ 0.0595],
        [ 0.0459],
        ...,
        [-0.2577],
        [-0.2566],
        [-0.2563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-507435.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1535.7112, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1535.7112, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0200,  0.0270,  0.0201,  ..., -0.0354,  0.0420, -0.0378],
        [ 0.0261,  0.0366,  0.0275,  ..., -0.0478,  0.0564, -0.0511],
        [ 0.0207,  0.0281,  0.0209,  ..., -0.0368,  0.0436, -0.0393],
        ...,
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1602.1912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(622.0945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.2063, device='cuda:0')



h[100].sum tensor(-203.3142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(231.1671, device='cuda:0')



h[200].sum tensor(-132.9251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0893, 0.1227, 0.0915,  ..., 0.0000, 0.1897, 0.0000],
        [0.0991, 0.1380, 0.1033,  ..., 0.0000, 0.2127, 0.0000],
        [0.0974, 0.1353, 0.1013,  ..., 0.0000, 0.2088, 0.0000],
        ...,
        [0.0103, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0103, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(248743.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0560, 0.0000,  ..., 0.0000, 0.0823, 0.1012],
        [0.0000, 0.0625, 0.0000,  ..., 0.0000, 0.0938, 0.1086],
        [0.0000, 0.0613, 0.0000,  ..., 0.0000, 0.0916, 0.1074],
        ...,
        [0.0300, 0.0000, 0.0239,  ..., 0.0000, 0.0000, 0.0228],
        [0.0300, 0.0000, 0.0239,  ..., 0.0000, 0.0000, 0.0228],
        [0.0300, 0.0000, 0.0239,  ..., 0.0000, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1993211.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(27894.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(422.6178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2894.8684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15339.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2217.5303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1082],
        [-0.1221],
        [-0.1372],
        ...,
        [-0.2608],
        [-0.2580],
        [-0.2567]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-526910.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1311.2579, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1311.2579, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0067,  0.0061,  0.0039,  ..., -0.0084,  0.0105, -0.0089],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        ...,
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1549.3610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(614.1182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.2376, device='cuda:0')



h[100].sum tensor(-238.8857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.3807, device='cuda:0')



h[200].sum tensor(-112.8844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0243, 0.0212, 0.0146,  ..., 0.0000, 0.0358, 0.0000],
        [0.0146, 0.0061, 0.0039,  ..., 0.0000, 0.0127, 0.0000],
        [0.0104, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        ...,
        [0.0104, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0104, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0104, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(226149.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0050, 0.0112, 0.0000,  ..., 0.0000, 0.0088, 0.0471],
        [0.0119, 0.0043, 0.0043,  ..., 0.0000, 0.0015, 0.0365],
        [0.0220, 0.0000, 0.0079,  ..., 0.0000, 0.0000, 0.0288],
        ...,
        [0.0308, 0.0000, 0.0244,  ..., 0.0000, 0.0000, 0.0228],
        [0.0308, 0.0000, 0.0244,  ..., 0.0000, 0.0000, 0.0228],
        [0.0308, 0.0000, 0.0244,  ..., 0.0000, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1904411.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(29599.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(361.6053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.0343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2841.2888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12417.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2069.7126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0887],
        [ 0.0753],
        [ 0.0478],
        ...,
        [-0.2680],
        [-0.2670],
        [-0.2666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-565989.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1362.2043, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.2043, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        ...,
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0026, -0.0003, -0.0011,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1569.2914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(622.2083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.5003, device='cuda:0')



h[100].sum tensor(-234.1593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.0495, device='cuda:0')



h[200].sum tensor(-117.2624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0105, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0105, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        ...,
        [0.0105, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0105, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0105, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(236048.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0000, 0.0250,  ..., 0.0000, 0.0000, 0.0232],
        [0.0307, 0.0000, 0.0232,  ..., 0.0000, 0.0000, 0.0236],
        [0.0272, 0.0000, 0.0156,  ..., 0.0000, 0.0000, 0.0257],
        ...,
        [0.0314, 0.0000, 0.0249,  ..., 0.0000, 0.0000, 0.0230],
        [0.0314, 0.0000, 0.0249,  ..., 0.0000, 0.0000, 0.0230],
        [0.0314, 0.0000, 0.0249,  ..., 0.0000, 0.0000, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1979459.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(29850.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(397.2034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2904.3259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14109.4023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2140.3196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0952],
        [-0.0647],
        [-0.0049],
        ...,
        [-0.2717],
        [-0.2707],
        [-0.2703]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-562292.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1784.9873, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1784.9873, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0083,  0.0085,  0.0057,  ..., -0.0114,  0.0141, -0.0122],
        [ 0.0237,  0.0327,  0.0244,  ..., -0.0426,  0.0504, -0.0455],
        ...,
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1734.3328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(653.3934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.2775, device='cuda:0')



h[100].sum tensor(-177.3067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(268.6901, device='cuda:0')



h[200].sum tensor(-153.0628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0255, 0.0228, 0.0159,  ..., 0.0000, 0.0383, 0.0000],
        [0.0544, 0.0677, 0.0498,  ..., 0.0000, 0.1065, 0.0000],
        [0.0691, 0.0905, 0.0665,  ..., 0.0000, 0.1412, 0.0000],
        ...,
        [0.0106, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0106, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0106, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(276496.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.4881e-03, 2.0086e-02, 0.0000e+00,  ..., 0.0000e+00, 2.3449e-02,
         5.6411e-02],
        [9.4022e-05, 4.1677e-02, 0.0000e+00,  ..., 0.0000e+00, 5.5822e-02,
         8.1420e-02],
        [0.0000e+00, 5.9287e-02, 0.0000e+00,  ..., 0.0000e+00, 8.3844e-02,
         1.0164e-01],
        ...,
        [3.1798e-02, 0.0000e+00, 2.5307e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3150e-02],
        [3.1798e-02, 0.0000e+00, 2.5307e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3150e-02],
        [3.1798e-02, 0.0000e+00, 2.5307e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3150e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2177489., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30241.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(498.3428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2.2535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3061.3831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19230.0605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2409.2004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0470],
        [-0.1030],
        [-0.1607],
        ...,
        [-0.2753],
        [-0.2742],
        [-0.2739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-575435.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1641.8480, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1641.8480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0080,  0.0080,  0.0053,  ..., -0.0107,  0.0132, -0.0114],
        [ 0.0071,  0.0067,  0.0043,  ..., -0.0090,  0.0113, -0.0096],
        [ 0.0124,  0.0149,  0.0107,  ..., -0.0197,  0.0238, -0.0210],
        ...,
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1764.9744, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(649.9904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.9202, device='cuda:0')



h[100].sum tensor(-198.5391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(247.1437, device='cuda:0')



h[200].sum tensor(-141.7688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0231, 0.0188, 0.0128,  ..., 0.0000, 0.0324, 0.0000],
        [0.0419, 0.0477, 0.0334,  ..., 0.0000, 0.0768, 0.0000],
        [0.0347, 0.0366, 0.0257,  ..., 0.0000, 0.0597, 0.0000],
        ...,
        [0.0107, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0107, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0107, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(262162.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0053, 0.0087, 0.0000,  ..., 0.0000, 0.0059, 0.0451],
        [0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.0152, 0.0558],
        [0.0000, 0.0192, 0.0000,  ..., 0.0000, 0.0177, 0.0575],
        ...,
        [0.0319, 0.0000, 0.0256,  ..., 0.0000, 0.0000, 0.0232],
        [0.0319, 0.0000, 0.0256,  ..., 0.0000, 0.0000, 0.0232],
        [0.0319, 0.0000, 0.0256,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2098985., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(29565.4980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.3274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0.0005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3043.6594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16664.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2328.3027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1323],
        [ 0.1272],
        [ 0.1206],
        ...,
        [-0.2782],
        [-0.2771],
        [-0.2768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-553358.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.6771, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.6771, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        ...,
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1783.4969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(640.7543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.9640, device='cuda:0')



h[100].sum tensor(-233.7937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.3997, device='cuda:0')



h[200].sum tensor(-122.0839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0108, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0108, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0157, 0.0073, 0.0048,  ..., 0.0000, 0.0146, 0.0000],
        ...,
        [0.0108, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0108, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0108, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(242679.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0293, 0.0000, 0.0219,  ..., 0.0000, 0.0000, 0.0248],
        [0.0255, 0.0000, 0.0124,  ..., 0.0000, 0.0000, 0.0272],
        [0.0152, 0.0023, 0.0051,  ..., 0.0000, 0.0006, 0.0342],
        ...,
        [0.0318, 0.0000, 0.0257,  ..., 0.0000, 0.0000, 0.0233],
        [0.0318, 0.0000, 0.0257,  ..., 0.0000, 0.0000, 0.0233],
        [0.0318, 0.0000, 0.0257,  ..., 0.0000, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2021817.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(29533.1914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(405.3801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2999.9844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14182.7354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2210.8728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0802],
        [ 0.0086],
        [ 0.0799],
        ...,
        [-0.2810],
        [-0.2798],
        [-0.2794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-553654.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1685.3417, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1685.3417, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097,  0.0106,  0.0074,  ..., -0.0141,  0.0173, -0.0150],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0097,  0.0106,  0.0074,  ..., -0.0141,  0.0173, -0.0150],
        ...,
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0027, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1998.1194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(663.7463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.8519, device='cuda:0')



h[100].sum tensor(-197.6992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(253.6907, device='cuda:0')



h[200].sum tensor(-145.1809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0166, 0.0086, 0.0058,  ..., 0.0000, 0.0167, 0.0000],
        [0.0362, 0.0384, 0.0263,  ..., 0.0000, 0.0631, 0.0000],
        [0.0166, 0.0086, 0.0058,  ..., 0.0000, 0.0167, 0.0000],
        ...,
        [0.0109, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0109, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0109, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(269288.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7986e-02, 6.4877e-04, 6.9021e-05,  ..., 0.0000e+00, 0.0000e+00,
         3.2326e-02],
        [8.3088e-03, 2.9936e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.8667e-02],
        [1.8062e-02, 6.4877e-04, 6.9021e-05,  ..., 0.0000e+00, 0.0000e+00,
         3.2399e-02],
        ...,
        [3.1729e-02, 0.0000e+00, 2.5773e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3212e-02],
        [3.1729e-02, 0.0000e+00, 2.5773e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3212e-02],
        [3.1729e-02, 0.0000e+00, 2.5773e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3212e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2143587.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(28959.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(464.7944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1.7200, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3109.9175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17221.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2388.1514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1492],
        [-0.1559],
        [-0.2040],
        ...,
        [-0.2838],
        [-0.2826],
        [-0.2823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-553474.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1514.9167, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1514.9167, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0049,  0.0030,  ..., -0.0067,  0.0087, -0.0072],
        [ 0.0271,  0.0378,  0.0284,  ..., -0.0490,  0.0583, -0.0524],
        [ 0.0211,  0.0284,  0.0211,  ..., -0.0369,  0.0441, -0.0395],
        ...,
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2064.3638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(658.3508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.2828, device='cuda:0')



h[100].sum tensor(-225.5314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(228.0370, device='cuda:0')



h[200].sum tensor(-129.7444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0932, 0.1276, 0.0955,  ..., 0.0000, 0.1976, 0.0000],
        [0.0620, 0.0787, 0.0576,  ..., 0.0000, 0.1239, 0.0000],
        [0.0896, 0.1220, 0.0911,  ..., 0.0000, 0.1892, 0.0000],
        ...,
        [0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0110, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(256403.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0620, 0.0000,  ..., 0.0000, 0.0855, 0.1023],
        [0.0000, 0.0572, 0.0000,  ..., 0.0000, 0.0775, 0.0975],
        [0.0000, 0.0603, 0.0000,  ..., 0.0000, 0.0827, 0.1005],
        ...,
        [0.0316, 0.0000, 0.0258,  ..., 0.0000, 0.0000, 0.0232],
        [0.0316, 0.0000, 0.0258,  ..., 0.0000, 0.0000, 0.0232],
        [0.0316, 0.0000, 0.0258,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2100324., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(28260.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(425.6506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3089.8674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15673.1504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2311.9846, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1063],
        [-0.1138],
        [-0.1068],
        ...,
        [-0.2862],
        [-0.2851],
        [-0.2847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-536955.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 2100 loss: tensor(504.5981, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1619.7068, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1619.7068, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2223.2241, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(671.1526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.9369, device='cuda:0')



h[100].sum tensor(-210.9468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(243.8108, device='cuda:0')



h[200].sum tensor(-139.6307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        ...,
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(271786.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0000, 0.0259,  ..., 0.0000, 0.0000, 0.0234],
        [0.0314, 0.0000, 0.0257,  ..., 0.0000, 0.0000, 0.0235],
        [0.0306, 0.0000, 0.0250,  ..., 0.0000, 0.0000, 0.0240],
        ...,
        [0.0315, 0.0000, 0.0259,  ..., 0.0000, 0.0000, 0.0232],
        [0.0315, 0.0000, 0.0259,  ..., 0.0000, 0.0000, 0.0232],
        [0.0315, 0.0000, 0.0259,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2188241.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(27882.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(459.6156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3164.3606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17925.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2421.5950, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3503],
        [-0.2946],
        [-0.2127],
        ...,
        [-0.2887],
        [-0.2876],
        [-0.2872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-540218.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1487.3138, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1487.3138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0307,  0.0435,  0.0329,  ..., -0.0563,  0.0669, -0.0601],
        [ 0.0227,  0.0309,  0.0231,  ..., -0.0401,  0.0480, -0.0429],
        [ 0.0259,  0.0360,  0.0270,  ..., -0.0466,  0.0556, -0.0498],
        ...,
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2252.1167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(666.9430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.0568, device='cuda:0')



h[100].sum tensor(-232.8636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(223.8820, device='cuda:0')



h[200].sum tensor(-127.4866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0566, 0.0702, 0.0519,  ..., 0.0000, 0.1108, 0.0000],
        [0.1036, 0.1436, 0.1079,  ..., 0.0000, 0.2220, 0.0000],
        [0.0758, 0.1000, 0.0742,  ..., 0.0000, 0.1563, 0.0000],
        ...,
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0112, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(255454.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0360, 0.0000,  ..., 0.0000, 0.0450, 0.0741],
        [0.0000, 0.0491, 0.0000,  ..., 0.0000, 0.0650, 0.0875],
        [0.0000, 0.0418, 0.0000,  ..., 0.0000, 0.0532, 0.0804],
        ...,
        [0.0314, 0.0000, 0.0261,  ..., 0.0000, 0.0000, 0.0235],
        [0.0314, 0.0000, 0.0261,  ..., 0.0000, 0.0000, 0.0235],
        [0.0314, 0.0000, 0.0261,  ..., 0.0000, 0.0000, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2105355.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(28331.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.7617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3129.2832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15340.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2331.7981, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0606],
        [ 0.0424],
        [ 0.0330],
        ...,
        [-0.2911],
        [-0.2899],
        [-0.2896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-559709.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1329.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1329.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0180,  0.0235,  0.0173,  ..., -0.0305,  0.0368, -0.0326],
        [ 0.0181,  0.0236,  0.0174,  ..., -0.0307,  0.0370, -0.0328],
        [ 0.0187,  0.0245,  0.0182,  ..., -0.0319,  0.0384, -0.0341],
        ...,
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0028, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2230.5183, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(660.9989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.0352, device='cuda:0')



h[100].sum tensor(-257.8052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(200.0841, device='cuda:0')



h[200].sum tensor(-113.4485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0668, 0.0857, 0.0631,  ..., 0.0000, 0.1348, 0.0000],
        [0.0676, 0.0871, 0.0641,  ..., 0.0000, 0.1369, 0.0000],
        [0.0539, 0.0655, 0.0474,  ..., 0.0000, 0.1043, 0.0000],
        ...,
        [0.0113, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0113, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0113, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(238005.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0393, 0.0000,  ..., 0.0000, 0.0478, 0.0781],
        [0.0000, 0.0396, 0.0000,  ..., 0.0000, 0.0482, 0.0784],
        [0.0000, 0.0309, 0.0000,  ..., 0.0000, 0.0353, 0.0695],
        ...,
        [0.0316, 0.0000, 0.0264,  ..., 0.0000, 0.0000, 0.0239],
        [0.0316, 0.0000, 0.0264,  ..., 0.0000, 0.0000, 0.0239],
        [0.0316, 0.0000, 0.0264,  ..., 0.0000, 0.0000, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2027330.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(28581.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(363.2681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3093.1487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12833.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2237.5447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0390],
        [ 0.0322],
        [ 0.0238],
        ...,
        [-0.2927],
        [-0.2917],
        [-0.2914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-566132.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1225.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1225.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0216,  0.0292,  0.0218,  ..., -0.0378,  0.0454, -0.0404],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0103,  0.0114,  0.0080,  ..., -0.0150,  0.0185, -0.0160],
        ...,
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2217.0723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(659.8414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.4330, device='cuda:0')



h[100].sum tensor(-273.5699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(184.4861, device='cuda:0')



h[200].sum tensor(-104.5094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0381, 0.0413, 0.0303,  ..., 0.0000, 0.0668, 0.0000],
        [0.0591, 0.0736, 0.0536,  ..., 0.0000, 0.1164, 0.0000],
        [0.0262, 0.0225, 0.0158,  ..., 0.0000, 0.0385, 0.0000],
        ...,
        [0.0114, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0114, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0114, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(228383.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0165, 0.0000,  ..., 0.0000, 0.0155, 0.0531],
        [0.0000, 0.0208, 0.0000,  ..., 0.0000, 0.0210, 0.0583],
        [0.0006, 0.0109, 0.0000,  ..., 0.0000, 0.0070, 0.0485],
        ...,
        [0.0321, 0.0000, 0.0268,  ..., 0.0000, 0.0000, 0.0240],
        [0.0321, 0.0000, 0.0268,  ..., 0.0000, 0.0000, 0.0240],
        [0.0321, 0.0000, 0.0268,  ..., 0.0000, 0.0000, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1995308.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30161.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.3419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3075.0823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11554.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2179.9255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0452],
        [ 0.0752],
        [ 0.0989],
        ...,
        [-0.2980],
        [-0.2968],
        [-0.2965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-620933.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1495.7053, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1495.7053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2324.2505, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(680.9807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.4295, device='cuda:0')



h[100].sum tensor(-236.2005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(225.1452, device='cuda:0')



h[200].sum tensor(-127.2317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0274, 0.0240, 0.0160,  ..., 0.0000, 0.0411, 0.0000],
        [0.0153, 0.0056, 0.0035,  ..., 0.0000, 0.0125, 0.0000],
        [0.0115, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        ...,
        [0.0115, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0115, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0115, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(254088.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0097, 0.0000,  ..., 0.0000, 0.0055, 0.0479],
        [0.0116, 0.0028, 0.0028,  ..., 0.0000, 0.0000, 0.0373],
        [0.0222, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0304],
        ...,
        [0.0327, 0.0000, 0.0273,  ..., 0.0000, 0.0000, 0.0244],
        [0.0327, 0.0000, 0.0273,  ..., 0.0000, 0.0000, 0.0244],
        [0.0327, 0.0000, 0.0273,  ..., 0.0000, 0.0000, 0.0244]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2118109.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(29565.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(416.6924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3191.5117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14246.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2359.6477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0782],
        [ 0.0656],
        [ 0.0287],
        ...,
        [-0.3022],
        [-0.3010],
        [-0.3007]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-581420.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1670.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1670.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0104,  0.0116,  0.0081,  ..., -0.0152,  0.0188, -0.0162],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0111,  0.0127,  0.0089,  ..., -0.0166,  0.0204, -0.0177],
        ...,
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2405.8184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(696.2300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.1910, device='cuda:0')



h[100].sum tensor(-211.3561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(251.4506, device='cuda:0')



h[200].sum tensor(-142.3607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0386, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0247, 0.0200, 0.0138,  ..., 0.0000, 0.0347, 0.0000],
        [0.0544, 0.0661, 0.0478,  ..., 0.0000, 0.1051, 0.0000],
        [0.0574, 0.0711, 0.0525,  ..., 0.0000, 0.1122, 0.0000],
        ...,
        [0.0116, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0116, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0116, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(274267.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0230, 0.0000,  ..., 0.0000, 0.0223, 0.0617],
        [0.0000, 0.0421, 0.0000,  ..., 0.0000, 0.0496, 0.0803],
        [0.0000, 0.0535, 0.0000,  ..., 0.0000, 0.0671, 0.0908],
        ...,
        [0.0334, 0.0000, 0.0277,  ..., 0.0000, 0.0000, 0.0250],
        [0.0334, 0.0000, 0.0277,  ..., 0.0000, 0.0000, 0.0250],
        [0.0334, 0.0000, 0.0277,  ..., 0.0000, 0.0000, 0.0250]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2250342.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30200.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(476.0397, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3279.7725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17616.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2506.0027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0201],
        [-0.0340],
        [-0.0883],
        ...,
        [-0.3061],
        [-0.3049],
        [-0.3045]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-587435.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1463.7250, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1463.7250, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0159,  0.0201,  0.0147,  ..., -0.0261,  0.0316, -0.0279],
        [ 0.0097,  0.0103,  0.0071,  ..., -0.0135,  0.0168, -0.0144],
        ...,
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0029, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2305.2432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(688.9304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.0092, device='cuda:0')



h[100].sum tensor(-242.5896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(220.3312, device='cuda:0')



h[200].sum tensor(-123.9741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0466, 0.0537, 0.0381,  ..., 0.0000, 0.0862, 0.0000],
        [0.0290, 0.0266, 0.0189,  ..., 0.0000, 0.0446, 0.0000],
        [0.0302, 0.0284, 0.0203,  ..., 0.0000, 0.0473, 0.0000],
        ...,
        [0.0117, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0117, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0117, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(251601.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0113, 0.0000,  ..., 0.0000, 0.0064, 0.0488],
        [0.0033, 0.0091, 0.0000,  ..., 0.0000, 0.0040, 0.0468],
        [0.0027, 0.0100, 0.0000,  ..., 0.0000, 0.0047, 0.0484],
        ...,
        [0.0341, 0.0000, 0.0282,  ..., 0.0000, 0.0000, 0.0249],
        [0.0341, 0.0000, 0.0282,  ..., 0.0000, 0.0000, 0.0249],
        [0.0341, 0.0000, 0.0282,  ..., 0.0000, 0.0000, 0.0249]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2143757.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31436.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.8701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3224.2520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14112.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2356.4272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1265],
        [ 0.1378],
        [ 0.1494],
        ...,
        [-0.3119],
        [-0.3107],
        [-0.3103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-622965.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1668.0586, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1668.0586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2426.7009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(707.9670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.0843, device='cuda:0')



h[100].sum tensor(-214.3943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(251.0891, device='cuda:0')



h[200].sum tensor(-141.2301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0118, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0339, 0.0340, 0.0247,  ..., 0.0000, 0.0557, 0.0000],
        ...,
        [0.0118, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0118, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0118, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(276552.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0186, 0.0012, 0.0000,  ..., 0.0000, 0.0000, 0.0357],
        [0.0167, 0.0026, 0.0014,  ..., 0.0000, 0.0000, 0.0372],
        [0.0071, 0.0114, 0.0000,  ..., 0.0000, 0.0083, 0.0475],
        ...,
        [0.0345, 0.0000, 0.0285,  ..., 0.0000, 0.0000, 0.0248],
        [0.0345, 0.0000, 0.0285,  ..., 0.0000, 0.0000, 0.0248],
        [0.0345, 0.0000, 0.0285,  ..., 0.0000, 0.0000, 0.0248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2273728., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30886.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(496.4219, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3333.2507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17215.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2516.4116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0975],
        [ 0.0993],
        [ 0.1022],
        ...,
        [-0.3161],
        [-0.3148],
        [-0.3145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-604796.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1458.3992, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1458.3992, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2396.9126, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(700.2031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.7726, device='cuda:0')



h[100].sum tensor(-243.3012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(219.5295, device='cuda:0')



h[200].sum tensor(-124.6326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        ...,
        [0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0119, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(255391.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0331, 0.0000, 0.0255,  ..., 0.0000, 0.0000, 0.0258],
        [0.0270, 0.0000, 0.0121,  ..., 0.0000, 0.0000, 0.0291],
        [0.0191, 0.0004, 0.0059,  ..., 0.0000, 0.0000, 0.0336],
        ...,
        [0.0346, 0.0000, 0.0287,  ..., 0.0000, 0.0000, 0.0249],
        [0.0346, 0.0000, 0.0287,  ..., 0.0000, 0.0000, 0.0249],
        [0.0346, 0.0000, 0.0287,  ..., 0.0000, 0.0000, 0.0249]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2178531.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31552.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(432.3492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3281.1187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14313.6465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2385.6978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0723],
        [ 0.0157],
        [ 0.0823],
        ...,
        [-0.3190],
        [-0.3177],
        [-0.3174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-632547.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1389.6379, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1389.6379, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2426.2451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(698.8625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.7187, device='cuda:0')



h[100].sum tensor(-254.7066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.1791, device='cuda:0')



h[200].sum tensor(-118.6052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0165, 0.0069, 0.0045,  ..., 0.0000, 0.0145, 0.0000],
        ...,
        [0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(248783.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0339, 0.0000, 0.0271,  ..., 0.0000, 0.0000, 0.0256],
        [0.0304, 0.0000, 0.0173,  ..., 0.0000, 0.0000, 0.0277],
        [0.0202, 0.0009, 0.0081,  ..., 0.0000, 0.0000, 0.0337],
        ...,
        [0.0344, 0.0000, 0.0288,  ..., 0.0000, 0.0000, 0.0251],
        [0.0344, 0.0000, 0.0288,  ..., 0.0000, 0.0000, 0.0251],
        [0.0344, 0.0000, 0.0288,  ..., 0.0000, 0.0000, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2141383.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31107.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.7756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3287.1587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13070.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2357.3728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2262],
        [-0.1172],
        [ 0.0007],
        ...,
        [-0.3213],
        [-0.3201],
        [-0.3197]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-621599.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 2400 loss: tensor(547.5891, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1511.4043, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1511.4043, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0209,  0.0278,  0.0206,  ..., -0.0357,  0.0433, -0.0382],
        [ 0.0206,  0.0274,  0.0204,  ..., -0.0353,  0.0428, -0.0378],
        [ 0.0216,  0.0289,  0.0215,  ..., -0.0372,  0.0450, -0.0398],
        ...,
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2549.0764, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(706.6739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.1268, device='cuda:0')



h[100].sum tensor(-241.5392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(227.5083, device='cuda:0')



h[200].sum tensor(-127.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0349, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0898, 0.1210, 0.0903,  ..., 0.0000, 0.1882, 0.0000],
        [0.0865, 0.1159, 0.0863,  ..., 0.0000, 0.1804, 0.0000],
        [0.0710, 0.0915, 0.0675,  ..., 0.0000, 0.1436, 0.0000],
        ...,
        [0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(260922.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0663, 0.0000,  ..., 0.0000, 0.0812, 0.1006],
        [0.0000, 0.0625, 0.0000,  ..., 0.0000, 0.0760, 0.0971],
        [0.0000, 0.0540, 0.0000,  ..., 0.0000, 0.0640, 0.0893],
        ...,
        [0.0339, 0.0000, 0.0290,  ..., 0.0000, 0.0000, 0.0256],
        [0.0339, 0.0000, 0.0290,  ..., 0.0000, 0.0000, 0.0256],
        [0.0339, 0.0000, 0.0290,  ..., 0.0000, 0.0000, 0.0256]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2213552.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30733.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(443.9642, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3349.2454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15108.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2461.3003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0751],
        [-0.0674],
        [-0.0565],
        ...,
        [-0.3221],
        [-0.3208],
        [-0.3204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-620921.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1620.7303, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1620.7303, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0116,  0.0081,  ..., -0.0151,  0.0188, -0.0161],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0105,  0.0116,  0.0081,  ..., -0.0151,  0.0188, -0.0161],
        ...,
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2674.2786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(717.8411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.9823, device='cuda:0')



h[100].sum tensor(-224.7917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(243.9649, device='cuda:0')



h[200].sum tensor(-137.5934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0182, 0.0094, 0.0064,  ..., 0.0000, 0.0185, 0.0000],
        [0.0394, 0.0419, 0.0289,  ..., 0.0000, 0.0688, 0.0000],
        [0.0182, 0.0094, 0.0064,  ..., 0.0000, 0.0185, 0.0000],
        ...,
        [0.0121, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0121, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0121, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(278515.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0006, 0.0001,  ..., 0.0000, 0.0000, 0.0353],
        [0.0083, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0418],
        [0.0189, 0.0006, 0.0001,  ..., 0.0000, 0.0000, 0.0353],
        ...,
        [0.0337, 0.0000, 0.0292,  ..., 0.0000, 0.0000, 0.0259],
        [0.0337, 0.0000, 0.0292,  ..., 0.0000, 0.0000, 0.0259],
        [0.0337, 0.0000, 0.0292,  ..., 0.0000, 0.0000, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2320162.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(29785.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(497.0505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3435.1506, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17755.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2594.1455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0933],
        [-0.0876],
        [-0.1173],
        ...,
        [-0.3227],
        [-0.3215],
        [-0.3211]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-590101., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1465.4219, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1465.4219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0030, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2641.5835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(712.6766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.0845, device='cuda:0')



h[100].sum tensor(-249.1098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(220.5867, device='cuda:0')



h[200].sum tensor(-123.6049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        ...,
        [0.0122, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0122, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(261225.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0300, 0.0000, 0.0178,  ..., 0.0000, 0.0000, 0.0285],
        [0.0334, 0.0000, 0.0277,  ..., 0.0000, 0.0000, 0.0265],
        [0.0341, 0.0000, 0.0296,  ..., 0.0000, 0.0000, 0.0262],
        ...,
        [0.0339, 0.0000, 0.0295,  ..., 0.0000, 0.0000, 0.0260],
        [0.0339, 0.0000, 0.0295,  ..., 0.0000, 0.0000, 0.0260],
        [0.0330, 0.0000, 0.0260,  ..., 0.0000, 0.0000, 0.0265]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2227785.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(29547.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(456.7128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3405.4641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14879.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2484.6560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1112],
        [-0.1955],
        [-0.2322],
        ...,
        [-0.3204],
        [-0.2965],
        [-0.2397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-571899.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1372.7136, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1372.7136, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0071,  0.0061,  0.0039,  ..., -0.0081,  0.0106, -0.0087],
        [ 0.0156,  0.0194,  0.0142,  ..., -0.0251,  0.0308, -0.0268],
        [ 0.0156,  0.0194,  0.0142,  ..., -0.0251,  0.0308, -0.0268],
        ...,
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2628.6528, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(714.1199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.9670, device='cuda:0')



h[100].sum tensor(-262.4077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(206.6315, device='cuda:0')



h[200].sum tensor(-115.9821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0488, 0.0561, 0.0400,  ..., 0.0000, 0.0905, 0.0000],
        [0.0392, 0.0413, 0.0294,  ..., 0.0000, 0.0677, 0.0000],
        [0.0407, 0.0437, 0.0313,  ..., 0.0000, 0.0713, 0.0000],
        ...,
        [0.0123, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0123, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0123, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(250136.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0262, 0.0000,  ..., 0.0000, 0.0227, 0.0654],
        [0.0000, 0.0228, 0.0000,  ..., 0.0000, 0.0189, 0.0619],
        [0.0000, 0.0211, 0.0000,  ..., 0.0000, 0.0169, 0.0602],
        ...,
        [0.0345, 0.0000, 0.0297,  ..., 0.0000, 0.0000, 0.0259],
        [0.0345, 0.0000, 0.0297,  ..., 0.0000, 0.0000, 0.0259],
        [0.0345, 0.0000, 0.0297,  ..., 0.0000, 0.0000, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2174153.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30583.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.8992, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3389.8044, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13135.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2399.7952, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1492],
        [ 0.1495],
        [ 0.1510],
        ...,
        [-0.3323],
        [-0.3310],
        [-0.3307]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-618343.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1355.4871, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1355.4871, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0148,  0.0180,  0.0131,  ..., -0.0232,  0.0286, -0.0249],
        [ 0.0064,  0.0049,  0.0029,  ..., -0.0065,  0.0088, -0.0070],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2645.8823, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(719.5847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.2019, device='cuda:0')



h[100].sum tensor(-265.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(204.0384, device='cuda:0')



h[200].sum tensor(-114.5196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0325, 0.0303, 0.0200,  ..., 0.0000, 0.0515, 0.0000],
        [0.0295, 0.0260, 0.0175,  ..., 0.0000, 0.0445, 0.0000],
        [0.0297, 0.0260, 0.0167,  ..., 0.0000, 0.0450, 0.0000],
        ...,
        [0.0124, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0124, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0124, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(250055.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0071, 0.0551],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0048, 0.0522],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0067, 0.0540],
        ...,
        [0.0351, 0.0000, 0.0301,  ..., 0.0000, 0.0000, 0.0259],
        [0.0351, 0.0000, 0.0301,  ..., 0.0000, 0.0000, 0.0259],
        [0.0351, 0.0000, 0.0301,  ..., 0.0000, 0.0000, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2189821.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32396.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.0022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3401.3159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13434.0654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2391.4907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1271],
        [ 0.1273],
        [ 0.1255],
        ...,
        [-0.3369],
        [-0.3356],
        [-0.3352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-694474.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1409.5927, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1409.5927, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0077,  0.0069,  0.0045,  ..., -0.0091,  0.0118, -0.0098],
        [ 0.0120,  0.0136,  0.0096,  ..., -0.0176,  0.0219, -0.0188],
        ...,
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2690.3704, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(727.2617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.6049, device='cuda:0')



h[100].sum tensor(-258.9667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(212.1828, device='cuda:0')



h[200].sum tensor(-118.2762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0069, 0.0045,  ..., 0.0000, 0.0148, 0.0000],
        [0.0283, 0.0242, 0.0170,  ..., 0.0000, 0.0414, 0.0000],
        [0.0428, 0.0464, 0.0324,  ..., 0.0000, 0.0757, 0.0000],
        ...,
        [0.0125, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0125, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0125, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(254322.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9713e-02, 1.7035e-03, 8.6411e-03,  ..., 0.0000e+00, 2.2792e-05,
         3.5929e-02],
        [9.1204e-03, 8.1612e-03, 0.0000e+00,  ..., 0.0000e+00, 4.8352e-03,
         4.6333e-02],
        [7.3300e-04, 1.5414e-02, 0.0000e+00,  ..., 0.0000e+00, 8.5857e-03,
         5.5316e-02],
        ...,
        [3.5455e-02, 0.0000e+00, 3.0463e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6185e-02],
        [3.5455e-02, 0.0000e+00, 3.0463e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6185e-02],
        [3.5455e-02, 0.0000e+00, 3.0463e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6185e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2222016.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31058.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(449.8396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3448.4380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13635.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2427.0574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0109],
        [ 0.0785],
        [ 0.1240],
        ...,
        [-0.3410],
        [-0.3397],
        [-0.3393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-632580.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1370.2076, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1370.2076, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111,  0.0122,  0.0086,  ..., -0.0158,  0.0199, -0.0169],
        [ 0.0111,  0.0122,  0.0086,  ..., -0.0158,  0.0199, -0.0169],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0031, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2695.0793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(728.3853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.8557, device='cuda:0')



h[100].sum tensor(-264.2397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(206.2543, device='cuda:0')



h[200].sum tensor(-115.2702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0336, 0.0324, 0.0233,  ..., 0.0000, 0.0538, 0.0000],
        [0.0319, 0.0298, 0.0213,  ..., 0.0000, 0.0498, 0.0000],
        [0.0368, 0.0372, 0.0261,  ..., 0.0000, 0.0614, 0.0000],
        ...,
        [0.0126, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0126, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0126, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(250873.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0230, 0.0000,  ..., 0.0000, 0.0192, 0.0614],
        [0.0010, 0.0189, 0.0000,  ..., 0.0000, 0.0141, 0.0580],
        [0.0000, 0.0173, 0.0000,  ..., 0.0000, 0.0112, 0.0572],
        ...,
        [0.0357, 0.0000, 0.0309,  ..., 0.0000, 0.0000, 0.0267],
        [0.0357, 0.0000, 0.0309,  ..., 0.0000, 0.0000, 0.0267],
        [0.0357, 0.0000, 0.0309,  ..., 0.0000, 0.0000, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2210318.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31597.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.2529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3457.1211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13113.3428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2419.3645, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0940],
        [ 0.1062],
        [ 0.1198],
        ...,
        [-0.3446],
        [-0.3433],
        [-0.3429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-651023., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1536.8972, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1536.8972, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0076,  0.0067,  0.0043,  ..., -0.0088,  0.0115, -0.0094],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2823.3977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(741.9884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.2590, device='cuda:0')



h[100].sum tensor(-240.0555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(231.3457, device='cuda:0')



h[200].sum tensor(-129.6461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0067, 0.0043,  ..., 0.0000, 0.0145, 0.0000],
        [0.0189, 0.0095, 0.0065,  ..., 0.0000, 0.0188, 0.0000],
        [0.0340, 0.0325, 0.0215,  ..., 0.0000, 0.0546, 0.0000],
        ...,
        [0.0126, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0126, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0126, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(270220.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.0004, 0.0072,  ..., 0.0000, 0.0000, 0.0365],
        [0.0128, 0.0017, 0.0007,  ..., 0.0000, 0.0000, 0.0405],
        [0.0025, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0469],
        ...,
        [0.0358, 0.0000, 0.0313,  ..., 0.0000, 0.0000, 0.0272],
        [0.0358, 0.0000, 0.0313,  ..., 0.0000, 0.0000, 0.0272],
        [0.0358, 0.0000, 0.0313,  ..., 0.0000, 0.0000, 0.0272]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2309063.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30745.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.8445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3550.4229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15355.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2563.6389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0648],
        [ 0.1226],
        [ 0.1487],
        ...,
        [-0.3482],
        [-0.3469],
        [-0.3465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-619891., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1283.8840, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1283.8840, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2728.9731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(728.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.0218, device='cuda:0')



h[100].sum tensor(-277.9902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.2602, device='cuda:0')



h[200].sum tensor(-107.6374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0297, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        ...,
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(244282.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0339, 0.0000, 0.0293,  ..., 0.0000, 0.0000, 0.0288],
        [0.0332, 0.0000, 0.0286,  ..., 0.0000, 0.0000, 0.0291],
        [0.0327, 0.0000, 0.0281,  ..., 0.0000, 0.0000, 0.0295],
        ...,
        [0.0360, 0.0000, 0.0317,  ..., 0.0000, 0.0000, 0.0276],
        [0.0360, 0.0000, 0.0317,  ..., 0.0000, 0.0000, 0.0276],
        [0.0360, 0.0000, 0.0317,  ..., 0.0000, 0.0000, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2199262.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32167.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(420.3359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3476.4548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12152.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2405.2507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1488],
        [-0.1664],
        [-0.1698],
        ...,
        [-0.3519],
        [-0.3506],
        [-0.3502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-678665.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1454.0378, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1454.0378, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0110,  0.0121,  0.0084,  ..., -0.0156,  0.0196, -0.0167],
        [ 0.0110,  0.0121,  0.0084,  ..., -0.0156,  0.0196, -0.0167],
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2911.2764, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(740.6749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.5789, device='cuda:0')



h[100].sum tensor(-254.5312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(218.8730, device='cuda:0')



h[200].sum tensor(-121.7615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0173, 0.0069, 0.0044,  ..., 0.0000, 0.0148, 0.0000],
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        ...,
        [0.0270, 0.0219, 0.0151,  ..., 0.0000, 0.0378, 0.0000],
        [0.0270, 0.0219, 0.0151,  ..., 0.0000, 0.0378, 0.0000],
        [0.0270, 0.0219, 0.0151,  ..., 0.0000, 0.0378, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(261013.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0241, 0.0004, 0.0105,  ..., 0.0000, 0.0000, 0.0349],
        [0.0323, 0.0000, 0.0223,  ..., 0.0000, 0.0000, 0.0301],
        [0.0347, 0.0000, 0.0272,  ..., 0.0000, 0.0000, 0.0290],
        ...,
        [0.0095, 0.0043, 0.0000,  ..., 0.0000, 0.0003, 0.0444],
        [0.0095, 0.0043, 0.0000,  ..., 0.0000, 0.0003, 0.0444],
        [0.0141, 0.0035, 0.0000,  ..., 0.0000, 0.0003, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2279865.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31372.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.5571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3555.1116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14168.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2533.4336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1893],
        [-0.2298],
        [-0.2026],
        ...,
        [ 0.0092],
        [ 0.0105],
        [-0.0496]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-648756., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 2700 loss: tensor(545.3024, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1392.3887, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1392.3887, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0068,  0.0053,  0.0032,  ..., -0.0071,  0.0094, -0.0075],
        [ 0.0140,  0.0167,  0.0120,  ..., -0.0214,  0.0266, -0.0229],
        ...,
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2981.0459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(740.7716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.8409, device='cuda:0')



h[100].sum tensor(-264.0748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.5931, device='cuda:0')



h[200].sum tensor(-116.6517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0279, 0.0229, 0.0150,  ..., 0.0000, 0.0399, 0.0000],
        [0.0347, 0.0336, 0.0232,  ..., 0.0000, 0.0560, 0.0000],
        [0.0320, 0.0291, 0.0188,  ..., 0.0000, 0.0496, 0.0000],
        ...,
        [0.0128, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0128, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0128, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(258222.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0086, 0.0000,  ..., 0.0000, 0.0025, 0.0508],
        [0.0012, 0.0142, 0.0000,  ..., 0.0000, 0.0066, 0.0562],
        [0.0000, 0.0178, 0.0000,  ..., 0.0000, 0.0092, 0.0601],
        ...,
        [0.0361, 0.0000, 0.0322,  ..., 0.0000, 0.0000, 0.0278],
        [0.0361, 0.0000, 0.0322,  ..., 0.0000, 0.0000, 0.0278],
        [0.0361, 0.0000, 0.0322,  ..., 0.0000, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2277616.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31316.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.3558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3564.9521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13734.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2515.3806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1578],
        [ 0.1583],
        [ 0.1568],
        ...,
        [-0.3578],
        [-0.3565],
        [-0.3561]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-654246.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1233.3845, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1233.3845, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2975.6064, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(734.6121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.7789, device='cuda:0')



h[100].sum tensor(-288.2221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(185.6586, device='cuda:0')



h[200].sum tensor(-102.8578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0195, 0.0098, 0.0057,  ..., 0.0000, 0.0197, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0231, 0.0155, 0.0102,  ..., 0.0000, 0.0284, 0.0000],
        ...,
        [0.0129, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(239345.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0092, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0430],
        [0.0155, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0388],
        [0.0078, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0441],
        ...,
        [0.0365, 0.0000, 0.0324,  ..., 0.0000, 0.0000, 0.0276],
        [0.0365, 0.0000, 0.0324,  ..., 0.0000, 0.0000, 0.0276],
        [0.0365, 0.0000, 0.0324,  ..., 0.0000, 0.0000, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2187076.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32378.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(396.5471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3514.4436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10975.6035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2390.5691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1642],
        [ 0.1681],
        [ 0.1691],
        ...,
        [-0.3616],
        [-0.3602],
        [-0.3598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-701789.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1235.7814, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1235.7814, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111,  0.0121,  0.0084,  ..., -0.0156,  0.0197, -0.0167],
        [ 0.0123,  0.0140,  0.0099,  ..., -0.0180,  0.0226, -0.0193],
        [ 0.0119,  0.0134,  0.0094,  ..., -0.0172,  0.0217, -0.0184],
        ...,
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0032, -0.0003, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3087.0815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(737.0775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.8854, device='cuda:0')



h[100].sum tensor(-288.3856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(186.0194, device='cuda:0')



h[200].sum tensor(-103.1535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0428, 0.0459, 0.0319,  ..., 0.0000, 0.0752, 0.0000],
        [0.0438, 0.0475, 0.0331,  ..., 0.0000, 0.0776, 0.0000],
        [0.0320, 0.0292, 0.0199,  ..., 0.0000, 0.0496, 0.0000],
        ...,
        [0.0129, 0.0000, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0129, 0.0000, 0.0000,  ..., 0.0000, 0.0042, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(246383.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0125, 0.0000,  ..., 0.0000, 0.0039, 0.0534],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0052, 0.0554],
        [0.0023, 0.0098, 0.0000,  ..., 0.0000, 0.0029, 0.0512],
        ...,
        [0.0363, 0.0000, 0.0325,  ..., 0.0000, 0.0000, 0.0277],
        [0.0363, 0.0000, 0.0325,  ..., 0.0000, 0.0000, 0.0277],
        [0.0363, 0.0000, 0.0325,  ..., 0.0000, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2242842.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32030.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.4893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3553.9702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12272.7451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2453.4905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0762],
        [ 0.1144],
        [ 0.1357],
        ...,
        [-0.3629],
        [-0.3614],
        [-0.3610]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-691875.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1693.0553, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1693.0553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        ...,
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0032, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3464.7954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(768.2159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.1945, device='cuda:0')



h[100].sum tensor(-222.1193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(254.8518, device='cuda:0')



h[200].sum tensor(-142.1325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0391, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        ...,
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(294046.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0312, 0.0000, 0.0254,  ..., 0.0000, 0.0000, 0.0306],
        [0.0346, 0.0000, 0.0316,  ..., 0.0000, 0.0000, 0.0289],
        [0.0360, 0.0000, 0.0326,  ..., 0.0000, 0.0000, 0.0285],
        ...,
        [0.0361, 0.0000, 0.0327,  ..., 0.0000, 0.0000, 0.0281],
        [0.0361, 0.0000, 0.0327,  ..., 0.0000, 0.0000, 0.0281],
        [0.0361, 0.0000, 0.0327,  ..., 0.0000, 0.0000, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2477596.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30679.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(544.5188, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3733.5576, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18497.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2790.2463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0687],
        [-0.1681],
        [-0.2387],
        ...,
        [-0.3652],
        [-0.3638],
        [-0.3634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-651596.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1622.2264, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1622.2264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0193,  0.0250,  0.0185,  ..., -0.0319,  0.0394, -0.0341],
        [ 0.0143,  0.0171,  0.0123,  ..., -0.0219,  0.0273, -0.0234],
        ...,
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3490.9438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(765.5762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.0488, device='cuda:0')



h[100].sum tensor(-235.0452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(244.1901, device='cuda:0')



h[200].sum tensor(-134.8309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0375, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0757, 0.0975, 0.0718,  ..., 0.0000, 0.1535, 0.0000],
        [0.0372, 0.0375, 0.0272,  ..., 0.0000, 0.0619, 0.0000],
        [0.0381, 0.0389, 0.0283,  ..., 0.0000, 0.0641, 0.0000],
        ...,
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0130, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(281076.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0436, 0.0000,  ..., 0.0000, 0.0413, 0.0785],
        [0.0002, 0.0239, 0.0000,  ..., 0.0000, 0.0184, 0.0617],
        [0.0044, 0.0161, 0.0000,  ..., 0.0000, 0.0112, 0.0547],
        ...,
        [0.0361, 0.0000, 0.0329,  ..., 0.0000, 0.0000, 0.0283],
        [0.0361, 0.0000, 0.0329,  ..., 0.0000, 0.0000, 0.0283],
        [0.0361, 0.0000, 0.0329,  ..., 0.0000, 0.0000, 0.0283]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2405405., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30659.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(514.5043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3709.2412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16351.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2721.3352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1052],
        [ 0.0967],
        [ 0.0567],
        ...,
        [-0.3678],
        [-0.3664],
        [-0.3660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-645356.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1556.0378, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1556.0378, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        ...,
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3509.8770, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(765.8447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.1091, device='cuda:0')



h[100].sum tensor(-244.6573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(234.2269, device='cuda:0')



h[200].sum tensor(-129.4353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0062, 0.0039,  ..., 0.0000, 0.0142, 0.0000],
        [0.0131, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0131, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        ...,
        [0.0131, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0131, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0131, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(276562.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0023, 0.0103,  ..., 0.0000, 0.0005, 0.0385],
        [0.0320, 0.0000, 0.0210,  ..., 0.0000, 0.0000, 0.0309],
        [0.0361, 0.0000, 0.0317,  ..., 0.0000, 0.0000, 0.0289],
        ...,
        [0.0365, 0.0000, 0.0332,  ..., 0.0000, 0.0000, 0.0283],
        [0.0365, 0.0000, 0.0332,  ..., 0.0000, 0.0000, 0.0283],
        [0.0365, 0.0000, 0.0332,  ..., 0.0000, 0.0000, 0.0283]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2385295.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30149.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(517.1073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3721.5078, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15248.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2694.4954, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0534],
        [-0.0344],
        [-0.1338],
        ...,
        [-0.3720],
        [-0.3706],
        [-0.3702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-609736.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1504.6777, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1504.6777, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0200,  0.0260,  0.0192,  ..., -0.0330,  0.0408, -0.0353],
        [ 0.0157,  0.0193,  0.0140,  ..., -0.0246,  0.0307, -0.0263],
        [ 0.0083,  0.0075,  0.0049,  ..., -0.0098,  0.0129, -0.0105],
        ...,
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3544.4260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(768.7239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.8280, device='cuda:0')



h[100].sum tensor(-251.6187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(226.4957, device='cuda:0')



h[200].sum tensor(-125.5911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0348, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0817, 0.1066, 0.0789,  ..., 0.0000, 0.1675, 0.0000],
        [0.0637, 0.0783, 0.0570,  ..., 0.0000, 0.1245, 0.0000],
        [0.0360, 0.0353, 0.0255,  ..., 0.0000, 0.0587, 0.0000],
        ...,
        [0.0132, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0132, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0132, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(278425.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0542, 0.0000,  ..., 0.0000, 0.0512, 0.0858],
        [0.0000, 0.0398, 0.0000,  ..., 0.0000, 0.0342, 0.0746],
        [0.0073, 0.0199, 0.0000,  ..., 0.0000, 0.0153, 0.0573],
        ...,
        [0.0372, 0.0000, 0.0336,  ..., 0.0000, 0.0000, 0.0281],
        [0.0372, 0.0000, 0.0336,  ..., 0.0000, 0.0000, 0.0281],
        [0.0372, 0.0000, 0.0336,  ..., 0.0000, 0.0000, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2427011., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31166.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(521.3464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3749.7319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15944.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2693.3677, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1256],
        [ 0.1222],
        [ 0.1016],
        ...,
        [-0.3774],
        [-0.3760],
        [-0.3756]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-654013.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1458.2061, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1458.2061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0315,  0.0441,  0.0332,  ..., -0.0558,  0.0683, -0.0597],
        [ 0.0300,  0.0417,  0.0314,  ..., -0.0528,  0.0647, -0.0565],
        [ 0.0219,  0.0289,  0.0214,  ..., -0.0366,  0.0452, -0.0392],
        ...,
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0033, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3570.9473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(771.1534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.7640, device='cuda:0')



h[100].sum tensor(-259.6645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(219.5005, device='cuda:0')



h[200].sum tensor(-121.0382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1240, 0.1728, 0.1303,  ..., 0.0000, 0.2677, 0.0000],
        [0.1108, 0.1522, 0.1143,  ..., 0.0000, 0.2364, 0.0000],
        [0.0753, 0.0964, 0.0710,  ..., 0.0000, 0.1519, 0.0000],
        ...,
        [0.0133, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0133, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(265934.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1054, 0.0000,  ..., 0.0000, 0.1125, 0.1229],
        [0.0000, 0.0910, 0.0000,  ..., 0.0000, 0.0947, 0.1123],
        [0.0000, 0.0671, 0.0000,  ..., 0.0000, 0.0654, 0.0951],
        ...,
        [0.0378, 0.0000, 0.0339,  ..., 0.0000, 0.0000, 0.0281],
        [0.0378, 0.0000, 0.0339,  ..., 0.0000, 0.0000, 0.0281],
        [0.0378, 0.0000, 0.0339,  ..., 0.0000, 0.0000, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2347375.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32396.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(490.8838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3728.9788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13401.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2600.8748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0025],
        [ 0.0240],
        [ 0.0599],
        ...,
        [-0.3830],
        [-0.3816],
        [-0.3812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-707177.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1430.3231, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1430.3231, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0117,  0.0129,  0.0090,  ..., -0.0165,  0.0210, -0.0176],
        [ 0.0070,  0.0054,  0.0033,  ..., -0.0071,  0.0097, -0.0076],
        [ 0.0081,  0.0072,  0.0047,  ..., -0.0094,  0.0124, -0.0100],
        ...,
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3636.0149, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(773.6638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.5257, device='cuda:0')



h[100].sum tensor(-263.3998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.3033, device='cuda:0')



h[200].sum tensor(-119.1178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0286, 0.0229, 0.0141,  ..., 0.0000, 0.0407, 0.0000],
        [0.0372, 0.0363, 0.0245,  ..., 0.0000, 0.0610, 0.0000],
        [0.0238, 0.0159, 0.0104,  ..., 0.0000, 0.0292, 0.0000],
        ...,
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(272289.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0002, 0.0522],
        [0.0020, 0.0083, 0.0000,  ..., 0.0000, 0.0004, 0.0508],
        [0.0107, 0.0034, 0.0003,  ..., 0.0000, 0.0000, 0.0434],
        ...,
        [0.0380, 0.0000, 0.0342,  ..., 0.0000, 0.0000, 0.0282],
        [0.0380, 0.0000, 0.0342,  ..., 0.0000, 0.0000, 0.0282],
        [0.0380, 0.0000, 0.0342,  ..., 0.0000, 0.0000, 0.0282]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2409888.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32186.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.6021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3771.7122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14611.1318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2645.9895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1484],
        [ 0.1284],
        [ 0.0808],
        ...,
        [-0.3873],
        [-0.3859],
        [-0.3854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-701906.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1435.7964, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1435.7964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        ...,
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3715.3435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(775.6891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.7687, device='cuda:0')



h[100].sum tensor(-263.9457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(216.1272, device='cuda:0')



h[200].sum tensor(-119.1795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0187, 0.0080, 0.0052,  ..., 0.0000, 0.0170, 0.0000],
        ...,
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(273316.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0372, 0.0000, 0.0322,  ..., 0.0000, 0.0000, 0.0290],
        [0.0314, 0.0000, 0.0172,  ..., 0.0000, 0.0000, 0.0327],
        [0.0176, 0.0079, 0.0062,  ..., 0.0000, 0.0039, 0.0435],
        ...,
        [0.0377, 0.0000, 0.0343,  ..., 0.0000, 0.0000, 0.0284],
        [0.0377, 0.0000, 0.0343,  ..., 0.0000, 0.0000, 0.0284],
        [0.0377, 0.0000, 0.0343,  ..., 0.0000, 0.0000, 0.0284]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2423092.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32210.3613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.6671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3794.7825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14949.4697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2668.3645, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0830],
        [-0.0149],
        [ 0.0317],
        ...,
        [-0.3897],
        [-0.3884],
        [-0.3880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-712016.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 3000 loss: tensor(553.8116, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1516.6729, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1516.6729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3843.8472, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(782.7679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.3608, device='cuda:0')



h[100].sum tensor(-253.7187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(228.3013, device='cuda:0')



h[200].sum tensor(-125.4969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0192, 0.0087, 0.0058,  ..., 0.0000, 0.0182, 0.0000],
        ...,
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(277340.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0370, 0.0000, 0.0322,  ..., 0.0000, 0.0000, 0.0293],
        [0.0337, 0.0000, 0.0229,  ..., 0.0000, 0.0000, 0.0310],
        [0.0256, 0.0002, 0.0103,  ..., 0.0000, 0.0000, 0.0356],
        ...,
        [0.0376, 0.0000, 0.0344,  ..., 0.0000, 0.0000, 0.0287],
        [0.0376, 0.0000, 0.0344,  ..., 0.0000, 0.0000, 0.0287],
        [0.0376, 0.0000, 0.0344,  ..., 0.0000, 0.0000, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2436319.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31393.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.1063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3834.2488, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14963.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2710.3201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4831],
        [-0.4085],
        [-0.3046],
        ...,
        [-0.3930],
        [-0.3916],
        [-0.3912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-682426.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1423.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1423.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3850.9978, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(779.9894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.2329, device='cuda:0')



h[100].sum tensor(-267.1542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(214.3112, device='cuda:0')



h[200].sum tensor(-118.1136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        ...,
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0135, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(273894.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0376, 0.0000, 0.0347,  ..., 0.0000, 0.0000, 0.0291],
        [0.0359, 0.0000, 0.0306,  ..., 0.0000, 0.0000, 0.0300],
        [0.0320, 0.0000, 0.0216,  ..., 0.0000, 0.0000, 0.0320],
        ...,
        [0.0375, 0.0000, 0.0346,  ..., 0.0000, 0.0000, 0.0289],
        [0.0375, 0.0000, 0.0346,  ..., 0.0000, 0.0000, 0.0289],
        [0.0375, 0.0000, 0.0346,  ..., 0.0000, 0.0000, 0.0289]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2432892., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31575.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(523.0894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3841.3933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14717.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2698.5940, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1953],
        [-0.1092],
        [-0.0223],
        ...,
        [-0.3964],
        [-0.3950],
        [-0.3945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-699769.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1514.6685, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1514.6685, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0210,  0.0274,  0.0203,  ..., -0.0346,  0.0430, -0.0370],
        [ 0.0202,  0.0262,  0.0194,  ..., -0.0332,  0.0413, -0.0355],
        [ 0.0250,  0.0337,  0.0252,  ..., -0.0426,  0.0527, -0.0456],
        ...,
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3965.0161, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(789.2067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.2717, device='cuda:0')



h[100].sum tensor(-255.7301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(227.9996, device='cuda:0')



h[200].sum tensor(-125.0079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0613, 0.0740, 0.0537,  ..., 0.0000, 0.1184, 0.0000],
        [0.0830, 0.1082, 0.0802,  ..., 0.0000, 0.1702, 0.0000],
        [0.0739, 0.0939, 0.0691,  ..., 0.0000, 0.1485, 0.0000],
        ...,
        [0.0136, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0136, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0136, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(283308.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0386, 0.0000,  ..., 0.0000, 0.0303, 0.0737],
        [0.0000, 0.0534, 0.0000,  ..., 0.0000, 0.0468, 0.0852],
        [0.0000, 0.0542, 0.0000,  ..., 0.0000, 0.0473, 0.0864],
        ...,
        [0.0376, 0.0000, 0.0349,  ..., 0.0000, 0.0000, 0.0292],
        [0.0376, 0.0000, 0.0349,  ..., 0.0000, 0.0000, 0.0292],
        [0.0376, 0.0000, 0.0349,  ..., 0.0000, 0.0000, 0.0292]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2501470.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31207.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(555.7644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3894.3604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16172.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2764.9729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1195],
        [ 0.1292],
        [ 0.1381],
        ...,
        [-0.4008],
        [-0.3994],
        [-0.3989]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-688367.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1403.1371, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1403.1371, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0034, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3928.9282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(787.0847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.3182, device='cuda:0')



h[100].sum tensor(-272.0846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(211.2111, device='cuda:0')



h[200].sum tensor(-115.8152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        ...,
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(267196.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0314, 0.0000, 0.0281,  ..., 0.0000, 0.0000, 0.0325],
        [0.0302, 0.0000, 0.0265,  ..., 0.0000, 0.0000, 0.0331],
        [0.0301, 0.0000, 0.0264,  ..., 0.0000, 0.0000, 0.0333],
        ...,
        [0.0380, 0.0000, 0.0352,  ..., 0.0000, 0.0000, 0.0294],
        [0.0380, 0.0000, 0.0352,  ..., 0.0000, 0.0000, 0.0294],
        [0.0380, 0.0000, 0.0352,  ..., 0.0000, 0.0000, 0.0294]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2399771., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32415.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(511.2822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3853.6626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13313.9443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2656.7920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0912],
        [-0.0503],
        [-0.0310],
        ...,
        [-0.4059],
        [-0.4044],
        [-0.4040]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-748898.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1473.1974, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1473.1974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4019.8489, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(796.2074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.4299, device='cuda:0')



h[100].sum tensor(-262.4235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(221.7571, device='cuda:0')



h[200].sum tensor(-121.6181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0138, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        ...,
        [0.0138, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0138, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(278477.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0385, 0.0000, 0.0356,  ..., 0.0000, 0.0000, 0.0298],
        [0.0357, 0.0000, 0.0305,  ..., 0.0000, 0.0000, 0.0311],
        [0.0317, 0.0000, 0.0230,  ..., 0.0000, 0.0000, 0.0330],
        ...,
        [0.0383, 0.0000, 0.0356,  ..., 0.0000, 0.0000, 0.0296],
        [0.0383, 0.0000, 0.0356,  ..., 0.0000, 0.0000, 0.0296],
        [0.0383, 0.0000, 0.0356,  ..., 0.0000, 0.0000, 0.0296]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2467093.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31483.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(558.0514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3912.8455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14622.1494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2734.8113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3198],
        [-0.1982],
        [-0.0644],
        ...,
        [-0.4105],
        [-0.4090],
        [-0.4086]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-700452.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1288.5269, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1288.5269, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3912.7285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(789.1818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.2280, device='cuda:0')



h[100].sum tensor(-289.0362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.9590, device='cuda:0')



h[200].sum tensor(-106.4008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0139, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0139, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0217, 0.0120, 0.0084,  ..., 0.0000, 0.0234, 0.0000],
        ...,
        [0.0139, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0139, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0139, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(261871., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0289, 0.0000, 0.0137,  ..., 0.0000, 0.0000, 0.0358],
        [0.0325, 0.0000, 0.0143,  ..., 0.0000, 0.0000, 0.0338],
        [0.0259, 0.0021, 0.0084,  ..., 0.0000, 0.0000, 0.0384],
        ...,
        [0.0388, 0.0000, 0.0359,  ..., 0.0000, 0.0000, 0.0299],
        [0.0388, 0.0000, 0.0359,  ..., 0.0000, 0.0000, 0.0299],
        [0.0388, 0.0000, 0.0359,  ..., 0.0000, 0.0000, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2402669.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33346.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(502.6409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3863.8599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12851.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2628.1389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0076],
        [-0.0358],
        [-0.0083],
        ...,
        [-0.4151],
        [-0.4136],
        [-0.4131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-782472.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1354.6653, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1354.6653, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4001.6099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(797.0159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.1654, device='cuda:0')



h[100].sum tensor(-280.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(203.9147, device='cuda:0')



h[200].sum tensor(-111.7104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0165, 0.0038, 0.0020,  ..., 0.0000, 0.0109, 0.0000],
        [0.0190, 0.0076, 0.0040,  ..., 0.0000, 0.0170, 0.0000],
        ...,
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(265368.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0282, 0.0000, 0.0205,  ..., 0.0000, 0.0000, 0.0353],
        [0.0211, 0.0000, 0.0106,  ..., 0.0000, 0.0000, 0.0388],
        [0.0164, 0.0000, 0.0047,  ..., 0.0000, 0.0000, 0.0412],
        ...,
        [0.0390, 0.0000, 0.0362,  ..., 0.0000, 0.0000, 0.0302],
        [0.0390, 0.0000, 0.0362,  ..., 0.0000, 0.0000, 0.0302],
        [0.0390, 0.0000, 0.0362,  ..., 0.0000, 0.0000, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2411095.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32714.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(527.2314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3894.5916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12701.5537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2661.4626, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0212],
        [ 0.0728],
        [ 0.0906],
        ...,
        [-0.4190],
        [-0.4175],
        [-0.4170]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-745862.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1467.8224, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1467.8224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4130.0278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(807.5157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.1911, device='cuda:0')



h[100].sum tensor(-264.7530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(220.9480, device='cuda:0')



h[200].sum tensor(-120.7795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0289, 0.0232, 0.0171,  ..., 0.0000, 0.0405, 0.0000],
        ...,
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0140, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(280243.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0352, 0.0000, 0.0256,  ..., 0.0000, 0.0000, 0.0329],
        [0.0339, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0344],
        [0.0227, 0.0047, 0.0026,  ..., 0.0000, 0.0029, 0.0425],
        ...,
        [0.0393, 0.0000, 0.0365,  ..., 0.0000, 0.0000, 0.0305],
        [0.0393, 0.0000, 0.0365,  ..., 0.0000, 0.0000, 0.0305],
        [0.0386, 0.0000, 0.0343,  ..., 0.0000, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2503855.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31919.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(580.8293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3965.5046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14887.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2769.3774, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0257],
        [-0.0057],
        [ 0.0438],
        ...,
        [-0.4208],
        [-0.4082],
        [-0.3680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-704929.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1351.8091, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1351.8091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4133.2148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(803.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.0386, device='cuda:0')



h[100].sum tensor(-281.8598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(203.4848, device='cuda:0')



h[200].sum tensor(-111.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0141, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0141, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0141, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        ...,
        [0.0141, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0141, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0141, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(272107.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0346, 0.0000, 0.0217,  ..., 0.0000, 0.0000, 0.0335],
        [0.0384, 0.0000, 0.0332,  ..., 0.0000, 0.0000, 0.0314],
        [0.0396, 0.0000, 0.0368,  ..., 0.0000, 0.0000, 0.0308],
        ...,
        [0.0394, 0.0000, 0.0367,  ..., 0.0000, 0.0000, 0.0306],
        [0.0394, 0.0000, 0.0367,  ..., 0.0000, 0.0000, 0.0306],
        [0.0394, 0.0000, 0.0367,  ..., 0.0000, 0.0000, 0.0306]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2478925.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32737.9043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(547.8446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3947.9741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14152.6182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2723.0374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1749],
        [-0.3368],
        [-0.4637],
        ...,
        [-0.4261],
        [-0.4246],
        [-0.4241]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-745914., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1449.7013, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1449.7013, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0139,  0.0160,  0.0115,  ..., -0.0203,  0.0259, -0.0217],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0035, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4328.7588, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(813.4555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.3863, device='cuda:0')



h[100].sum tensor(-269.1322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(218.2203, device='cuda:0')



h[200].sum tensor(-118.8436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0517, 0.0581, 0.0414,  ..., 0.0000, 0.0947, 0.0000],
        [0.0226, 0.0130, 0.0092,  ..., 0.0000, 0.0253, 0.0000],
        [0.0245, 0.0160, 0.0115,  ..., 0.0000, 0.0298, 0.0000],
        ...,
        [0.0142, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0142, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0142, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(283375.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0168, 0.0000,  ..., 0.0000, 0.0071, 0.0573],
        [0.0183, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0440],
        [0.0258, 0.0021, 0.0054,  ..., 0.0000, 0.0000, 0.0395],
        ...,
        [0.0395, 0.0000, 0.0366,  ..., 0.0000, 0.0000, 0.0305],
        [0.0395, 0.0000, 0.0366,  ..., 0.0000, 0.0000, 0.0305],
        [0.0395, 0.0000, 0.0366,  ..., 0.0000, 0.0000, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2538664.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31428.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(590.1208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4006.8538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15318.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2801.9275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 9.2032e-02],
        [-2.4416e-04],
        [-1.4449e-01],
        ...,
        [-4.2945e-01],
        [-4.2791e-01],
        [-4.2744e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-690033., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 3300 loss: tensor(538.0370, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1399.3529, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1399.3529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0127,  0.0141,  0.0100,  ..., -0.0178,  0.0230, -0.0191],
        [ 0.0119,  0.0130,  0.0092,  ..., -0.0164,  0.0213, -0.0176],
        [ 0.0168,  0.0206,  0.0151,  ..., -0.0259,  0.0329, -0.0278],
        ...,
        [ 0.0174,  0.0216,  0.0159,  ..., -0.0272,  0.0344, -0.0291],
        [ 0.0103,  0.0104,  0.0072,  ..., -0.0133,  0.0175, -0.0142],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4400.7803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(814.0620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.1502, device='cuda:0')



h[100].sum tensor(-277.6469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(210.6414, device='cuda:0')



h[200].sum tensor(-114.3509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0590, 0.0695, 0.0503,  ..., 0.0000, 0.1120, 0.0000],
        [0.0534, 0.0607, 0.0435,  ..., 0.0000, 0.0987, 0.0000],
        [0.0423, 0.0433, 0.0300,  ..., 0.0000, 0.0723, 0.0000],
        ...,
        [0.0476, 0.0516, 0.0364,  ..., 0.0000, 0.0849, 0.0000],
        [0.0435, 0.0454, 0.0325,  ..., 0.0000, 0.0751, 0.0000],
        [0.0266, 0.0189, 0.0129,  ..., 0.0000, 0.0346, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(273748.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0462, 0.0000,  ..., 0.0000, 0.0349, 0.0798],
        [0.0000, 0.0386, 0.0000,  ..., 0.0000, 0.0268, 0.0748],
        [0.0000, 0.0293, 0.0000,  ..., 0.0000, 0.0170, 0.0685],
        ...,
        [0.0000, 0.0257, 0.0000,  ..., 0.0000, 0.0141, 0.0647],
        [0.0038, 0.0200, 0.0000,  ..., 0.0000, 0.0107, 0.0598],
        [0.0135, 0.0088, 0.0021,  ..., 0.0000, 0.0031, 0.0479]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2480269.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32651.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(547.7246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3984.6768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13635.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2737.6089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1431],
        [ 0.1434],
        [ 0.1405],
        ...,
        [ 0.1065],
        [ 0.0765],
        [-0.0079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-764714.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1724.1261, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1724.1261, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4760.0518, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(836.7043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(76.5745, device='cuda:0')



h[100].sum tensor(-229.8935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(259.5288, device='cuda:0')



h[200].sum tensor(-142.0028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        ...,
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(307948.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0322, 0.0000, 0.0217,  ..., 0.0000, 0.0000, 0.0346],
        [0.0371, 0.0000, 0.0335,  ..., 0.0000, 0.0000, 0.0321],
        [0.0392, 0.0000, 0.0367,  ..., 0.0000, 0.0000, 0.0312],
        ...,
        [0.0394, 0.0000, 0.0369,  ..., 0.0000, 0.0000, 0.0308],
        [0.0394, 0.0000, 0.0369,  ..., 0.0000, 0.0000, 0.0308],
        [0.0394, 0.0000, 0.0369,  ..., 0.0000, 0.0000, 0.0308]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2644382.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31333.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(633.7850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4119.1665, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17925.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2985.6843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0593],
        [-0.1981],
        [-0.3207],
        ...,
        [-0.4364],
        [-0.4349],
        [-0.4344]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-720526.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1434.8516, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1434.8516, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0363,  0.0513,  0.0389,  ..., -0.0641,  0.0796, -0.0687],
        [ 0.0135,  0.0154,  0.0110,  ..., -0.0194,  0.0250, -0.0207],
        [ 0.0147,  0.0174,  0.0126,  ..., -0.0219,  0.0281, -0.0234],
        ...,
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4583.0640, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(818.7661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.7268, device='cuda:0')



h[100].sum tensor(-273.9882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.9850, device='cuda:0')



h[200].sum tensor(-117.0470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1105, 0.1506, 0.1132,  ..., 0.0000, 0.2355, 0.0000],
        [0.0913, 0.1203, 0.0897,  ..., 0.0000, 0.1894, 0.0000],
        [0.0333, 0.0295, 0.0211,  ..., 0.0000, 0.0508, 0.0000],
        ...,
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(276322.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0961, 0.0000,  ..., 0.0000, 0.0910, 0.1110],
        [0.0000, 0.0636, 0.0000,  ..., 0.0000, 0.0562, 0.0895],
        [0.0030, 0.0270, 0.0000,  ..., 0.0000, 0.0178, 0.0648],
        ...,
        [0.0390, 0.0000, 0.0371,  ..., 0.0000, 0.0000, 0.0312],
        [0.0390, 0.0000, 0.0371,  ..., 0.0000, 0.0000, 0.0312],
        [0.0390, 0.0000, 0.0371,  ..., 0.0000, 0.0000, 0.0312]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2481340., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30991.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(543.6221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4033.5598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13127.9287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2801.3701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0586],
        [ 0.0958],
        [ 0.1271],
        ...,
        [-0.4389],
        [-0.4373],
        [-0.4368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-704595.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1492.6321, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1492.6321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0224,  0.0295,  0.0219,  ..., -0.0369,  0.0464, -0.0395],
        [ 0.0245,  0.0327,  0.0244,  ..., -0.0409,  0.0513, -0.0438],
        [ 0.0188,  0.0238,  0.0175,  ..., -0.0298,  0.0377, -0.0319],
        ...,
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4668.5205, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(823.6851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.2930, device='cuda:0')



h[100].sum tensor(-266.9049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(224.6825, device='cuda:0')



h[200].sum tensor(-121.1898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0878, 0.1149, 0.0855,  ..., 0.0000, 0.1812, 0.0000],
        [0.0784, 0.1001, 0.0740,  ..., 0.0000, 0.1587, 0.0000],
        [0.0720, 0.0901, 0.0662,  ..., 0.0000, 0.1434, 0.0000],
        ...,
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0143, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(284228.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0690, 0.0000,  ..., 0.0000, 0.0593, 0.0956],
        [0.0000, 0.0623, 0.0000,  ..., 0.0000, 0.0523, 0.0912],
        [0.0000, 0.0555, 0.0000,  ..., 0.0000, 0.0450, 0.0869],
        ...,
        [0.0391, 0.0000, 0.0374,  ..., 0.0000, 0.0000, 0.0317],
        [0.0391, 0.0000, 0.0374,  ..., 0.0000, 0.0000, 0.0317],
        [0.0391, 0.0000, 0.0374,  ..., 0.0000, 0.0000, 0.0317]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2527536.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30983.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(555.3915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4079.0188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14209.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2874.4104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1308],
        [ 0.1389],
        [ 0.1486],
        ...,
        [-0.4416],
        [-0.4400],
        [-0.4396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-700493.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1481.3411, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1481.3411, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0119,  0.0129,  0.0091,  ..., -0.0162,  0.0212, -0.0174],
        [ 0.0107,  0.0110,  0.0076,  ..., -0.0139,  0.0184, -0.0149],
        ...,
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4700.1479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(826.6466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.7915, device='cuda:0')



h[100].sum tensor(-269.1187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(222.9829, device='cuda:0')



h[200].sum tensor(-120.0548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0227, 0.0129, 0.0091,  ..., 0.0000, 0.0253, 0.0000],
        [0.0283, 0.0215, 0.0148,  ..., 0.0000, 0.0387, 0.0000],
        [0.0576, 0.0673, 0.0485,  ..., 0.0000, 0.1088, 0.0000],
        ...,
        [0.0144, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(287858.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.0026, 0.0024,  ..., 0.0000, 0.0004, 0.0443],
        [0.0113, 0.0080, 0.0000,  ..., 0.0000, 0.0025, 0.0504],
        [0.0011, 0.0205, 0.0000,  ..., 0.0000, 0.0094, 0.0616],
        ...,
        [0.0395, 0.0000, 0.0379,  ..., 0.0000, 0.0000, 0.0319],
        [0.0395, 0.0000, 0.0379,  ..., 0.0000, 0.0000, 0.0319],
        [0.0395, 0.0000, 0.0379,  ..., 0.0000, 0.0000, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2577501.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30795.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(571.6641, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4115.8374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15133.9814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2906.1794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1395],
        [ 0.1203],
        [ 0.0931],
        ...,
        [-0.4471],
        [-0.4455],
        [-0.4450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-686995.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2020.2089, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2020.2089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0079,  0.0065,  0.0042,  ..., -0.0084,  0.0116, -0.0090],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0207,  0.0268,  0.0198,  ..., -0.0335,  0.0423, -0.0359],
        ...,
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5178.9565, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(861.8085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(89.7246, device='cuda:0')



h[100].sum tensor(-189.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(304.0975, device='cuda:0')



h[200].sum tensor(-165.6607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0241, 0.0149, 0.0097,  ..., 0.0000, 0.0286, 0.0000],
        [0.0420, 0.0429, 0.0305,  ..., 0.0000, 0.0714, 0.0000],
        [0.0420, 0.0429, 0.0305,  ..., 0.0000, 0.0714, 0.0000],
        ...,
        [0.0144, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0144, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(351243.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0135, 0.0000,  ..., 0.0000, 0.0025, 0.0582],
        [0.0000, 0.0300, 0.0000,  ..., 0.0000, 0.0189, 0.0687],
        [0.0003, 0.0411, 0.0000,  ..., 0.0000, 0.0305, 0.0758],
        ...,
        [0.0395, 0.0000, 0.0379,  ..., 0.0000, 0.0000, 0.0319],
        [0.0395, 0.0000, 0.0379,  ..., 0.0000, 0.0000, 0.0319],
        [0.0395, 0.0000, 0.0379,  ..., 0.0000, 0.0000, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2942731., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(30522.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(730.4905, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4322.3457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25155.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3321.4944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1298],
        [ 0.1211],
        [ 0.1126],
        ...,
        [-0.4043],
        [-0.4330],
        [-0.4423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-693539.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1290.0200, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1290.0200, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4586.0674, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(820.1970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.2943, device='cuda:0')



h[100].sum tensor(-296.1161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(194.1838, device='cuda:0')



h[200].sum tensor(-104.9111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        ...,
        [0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(268523.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0405, 0.0000, 0.0384,  ..., 0.0000, 0.0000, 0.0320],
        [0.0395, 0.0000, 0.0342,  ..., 0.0000, 0.0000, 0.0327],
        [0.0384, 0.0000, 0.0287,  ..., 0.0000, 0.0000, 0.0335],
        ...,
        [0.0403, 0.0000, 0.0384,  ..., 0.0000, 0.0000, 0.0319],
        [0.0403, 0.0000, 0.0384,  ..., 0.0000, 0.0000, 0.0319],
        [0.0403, 0.0000, 0.0384,  ..., 0.0000, 0.0000, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2491199.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32954.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(508.9916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4064.1340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12421.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2770.6375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5760],
        [-0.5314],
        [-0.4466],
        ...,
        [-0.4543],
        [-0.4527],
        [-0.4522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-785481.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1878.3296, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1878.3296, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0189,  0.0238,  0.0176,  ..., -0.0298,  0.0378, -0.0319],
        [ 0.0113,  0.0119,  0.0083,  ..., -0.0149,  0.0196, -0.0160],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0036, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5156.1069, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(861.6324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.4232, device='cuda:0')



h[100].sum tensor(-211.8329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(282.7407, device='cuda:0')



h[200].sum tensor(-153.1358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0579, 0.0676, 0.0487,  ..., 0.0000, 0.1091, 0.0000],
        [0.0423, 0.0431, 0.0306,  ..., 0.0000, 0.0716, 0.0000],
        [0.0285, 0.0215, 0.0148,  ..., 0.0000, 0.0386, 0.0000],
        ...,
        [0.0146, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0146, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0146, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(330904.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0358, 0.0000,  ..., 0.0000, 0.0225, 0.0721],
        [0.0048, 0.0217, 0.0000,  ..., 0.0000, 0.0115, 0.0617],
        [0.0149, 0.0094, 0.0017,  ..., 0.0000, 0.0032, 0.0497],
        ...,
        [0.0409, 0.0000, 0.0388,  ..., 0.0000, 0.0000, 0.0319],
        [0.0409, 0.0000, 0.0388,  ..., 0.0000, 0.0000, 0.0319],
        [0.0409, 0.0000, 0.0388,  ..., 0.0000, 0.0000, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2807832.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31533.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(686.6948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4297.2676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20493.7051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3181.9695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0509],
        [ 0.0129],
        [-0.0555],
        ...,
        [-0.4594],
        [-0.4578],
        [-0.4573]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-715516.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1404.9718, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1404.9718, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0289,  0.0396,  0.0298,  ..., -0.0493,  0.0618, -0.0528],
        [ 0.0374,  0.0529,  0.0401,  ..., -0.0658,  0.0820, -0.0705],
        [ 0.0279,  0.0380,  0.0286,  ..., -0.0474,  0.0594, -0.0507],
        ...,
        [ 0.0037, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0037, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0037, -0.0002, -0.0011,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4817.7046, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(836.3444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.3997, device='cuda:0')



h[100].sum tensor(-279.5592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(211.4872, device='cuda:0')



h[200].sum tensor(-114.8385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1234, 0.1706, 0.1287,  ..., 0.0000, 0.2657, 0.0000],
        [0.1432, 0.2018, 0.1529,  ..., 0.0000, 0.3132, 0.0000],
        [0.1452, 0.2048, 0.1552,  ..., 0.0000, 0.3178, 0.0000],
        ...,
        [0.0147, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0147, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0147, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(283277.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1217, 0.0000,  ..., 0.0000, 0.1105, 0.1219],
        [0.0000, 0.1484, 0.0000,  ..., 0.0000, 0.1376, 0.1383],
        [0.0000, 0.1484, 0.0000,  ..., 0.0000, 0.1375, 0.1384],
        ...,
        [0.0413, 0.0000, 0.0390,  ..., 0.0000, 0.0000, 0.0319],
        [0.0413, 0.0000, 0.0390,  ..., 0.0000, 0.0000, 0.0319],
        [0.0413, 0.0000, 0.0390,  ..., 0.0000, 0.0000, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2583216., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33679.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(549.0134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4147.8340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14266.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2864.7969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0240],
        [ 0.0106],
        [ 0.0172],
        ...,
        [-0.4638],
        [-0.4621],
        [-0.4616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-815350.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1503.0641, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1503.0641, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0073,  0.0055,  0.0033,  ..., -0.0070,  0.0099, -0.0075],
        [ 0.0081,  0.0068,  0.0044,  ..., -0.0087,  0.0120, -0.0093],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4972.6729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(845.0758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.7563, device='cuda:0')



h[100].sum tensor(-268.6407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(226.2529, device='cuda:0')



h[200].sum tensor(-121.4189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0438, 0.0451, 0.0313,  ..., 0.0000, 0.0750, 0.0000],
        [0.0281, 0.0207, 0.0142,  ..., 0.0000, 0.0374, 0.0000],
        [0.0192, 0.0068, 0.0044,  ..., 0.0000, 0.0161, 0.0000],
        ...,
        [0.0147, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0147, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0199, 0.0079, 0.0052,  ..., 0.0000, 0.0178, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(290331.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0166, 0.0000,  ..., 0.0000, 0.0031, 0.0606],
        [0.0105, 0.0076, 0.0000,  ..., 0.0000, 0.0004, 0.0514],
        [0.0226, 0.0014, 0.0107,  ..., 0.0000, 0.0000, 0.0424],
        ...,
        [0.0406, 0.0000, 0.0372,  ..., 0.0000, 0.0000, 0.0324],
        [0.0351, 0.0000, 0.0215,  ..., 0.0000, 0.0000, 0.0355],
        [0.0222, 0.0025, 0.0093,  ..., 0.0000, 0.0000, 0.0429]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2616317., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33021.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(576.4404, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4190.3877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14883.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2916.5125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1646],
        [ 0.1151],
        [ 0.0084],
        ...,
        [-0.3530],
        [-0.2059],
        [-0.0498]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-790669.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 3600 loss: tensor(489.0368, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1355.8199, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1355.8199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4964.7158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(840.0140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.2167, device='cuda:0')



h[100].sum tensor(-288.3297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(204.0885, device='cuda:0')



h[200].sum tensor(-110.7396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0288, 0.0217, 0.0150,  ..., 0.0000, 0.0390, 0.0000],
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(279111.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0032, 0.0017,  ..., 0.0000, 0.0000, 0.0450],
        [0.0338, 0.0000, 0.0180,  ..., 0.0000, 0.0000, 0.0364],
        [0.0385, 0.0000, 0.0299,  ..., 0.0000, 0.0000, 0.0339],
        ...,
        [0.0411, 0.0000, 0.0393,  ..., 0.0000, 0.0000, 0.0320],
        [0.0411, 0.0000, 0.0393,  ..., 0.0000, 0.0000, 0.0320],
        [0.0411, 0.0000, 0.0393,  ..., 0.0000, 0.0000, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2567831.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32938.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(544.0879, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4163.7349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13422.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2848.0703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2017],
        [-0.3561],
        [-0.4845],
        ...,
        [-0.4699],
        [-0.4682],
        [-0.4677]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-796572.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1321.7577, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.7577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0146,  0.0170,  0.0122,  ..., -0.0212,  0.0275, -0.0227],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0288,  0.0394,  0.0296,  ..., -0.0489,  0.0615, -0.0524],
        ...,
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0002, -0.0010,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4995.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(838.6840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.7039, device='cuda:0')



h[100].sum tensor(-296.0910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.9612, device='cuda:0')



h[200].sum tensor(-106.7177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0574, 0.0667, 0.0489,  ..., 0.0000, 0.1077, 0.0000],
        [0.0690, 0.0848, 0.0620,  ..., 0.0000, 0.1355, 0.0000],
        [0.0665, 0.0809, 0.0600,  ..., 0.0000, 0.1294, 0.0000],
        ...,
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0148, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(274031.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0579, 0.0000,  ..., 0.0000, 0.0422, 0.0839],
        [0.0000, 0.0652, 0.0000,  ..., 0.0000, 0.0493, 0.0884],
        [0.0000, 0.0689, 0.0000,  ..., 0.0000, 0.0533, 0.0899],
        ...,
        [0.0407, 0.0000, 0.0394,  ..., 0.0000, 0.0000, 0.0324],
        [0.0407, 0.0000, 0.0394,  ..., 0.0000, 0.0000, 0.0324],
        [0.0407, 0.0000, 0.0394,  ..., 0.0000, 0.0000, 0.0324]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2535579., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32222.9512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(537.3256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4165.2461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12347.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2837.7048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1265],
        [ 0.1196],
        [ 0.1141],
        ...,
        [-0.4697],
        [-0.4685],
        [-0.4686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-767001.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1487.7236, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1487.7236, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0240,  0.0318,  0.0237,  ..., -0.0395,  0.0500, -0.0423],
        [ 0.0193,  0.0244,  0.0180,  ..., -0.0304,  0.0388, -0.0325],
        [ 0.0150,  0.0177,  0.0128,  ..., -0.0221,  0.0286, -0.0236],
        ...,
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5191.3086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(851.6786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.0750, device='cuda:0')



h[100].sum tensor(-271.9133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(223.9437, device='cuda:0')



h[200].sum tensor(-120.6482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1127, 0.1537, 0.1154,  ..., 0.0000, 0.2403, 0.0000],
        [0.0973, 0.1294, 0.0966,  ..., 0.0000, 0.2034, 0.0000],
        [0.1003, 0.1340, 0.1002,  ..., 0.0000, 0.2104, 0.0000],
        ...,
        [0.0149, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0149, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0149, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(298977.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1114, 0.0000,  ..., 0.0000, 0.0947, 0.1153],
        [0.0000, 0.1052, 0.0000,  ..., 0.0000, 0.0884, 0.1119],
        [0.0000, 0.1011, 0.0000,  ..., 0.0000, 0.0842, 0.1098],
        ...,
        [0.0407, 0.0000, 0.0399,  ..., 0.0000, 0.0000, 0.0329],
        [0.0407, 0.0000, 0.0399,  ..., 0.0000, 0.0000, 0.0329],
        [0.0407, 0.0000, 0.0399,  ..., 0.0000, 0.0000, 0.0329]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2698961., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31532.8340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(609.3726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4270.5654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16687.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3023.3501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1131],
        [ 0.1163],
        [ 0.1227],
        ...,
        [-0.4752],
        [-0.4735],
        [-0.4730]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-736567.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1273.3923, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1273.3923, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0037, -0.0001, -0.0011,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5035.6812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(842.4451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.5558, device='cuda:0')



h[100].sum tensor(-303.7569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(191.6809, device='cuda:0')



h[200].sum tensor(-102.8467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0150, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0150, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0150, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0150, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0150, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0150, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(268327.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0413, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0332],
        [0.0413, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0332],
        [0.0414, 0.0000, 0.0404,  ..., 0.0000, 0.0000, 0.0333],
        ...,
        [0.0412, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0330],
        [0.0412, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0330],
        [0.0412, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0330]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2518237., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32405.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(537.4153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4188.6709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11354.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2820.6514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4443],
        [-0.5358],
        [-0.5985],
        ...,
        [-0.4803],
        [-0.4786],
        [-0.4781]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-773410.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1367.2321, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1367.2321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0080,  0.0066,  0.0042,  ..., -0.0083,  0.0117, -0.0089],
        ...,
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5174.4834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(853.7805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.7236, device='cuda:0')



h[100].sum tensor(-291.8941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.8064, device='cuda:0')



h[200].sum tensor(-109.7982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0151, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0263, 0.0175, 0.0117,  ..., 0.0000, 0.0328, 0.0000],
        [0.0375, 0.0350, 0.0243,  ..., 0.0000, 0.0596, 0.0000],
        ...,
        [0.0151, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0151, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0151, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(280538.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0246, 0.0009, 0.0116,  ..., 0.0000, 0.0000, 0.0427],
        [0.0108, 0.0136, 0.0010,  ..., 0.0000, 0.0050, 0.0548],
        [0.0035, 0.0302, 0.0000,  ..., 0.0000, 0.0163, 0.0678],
        ...,
        [0.0419, 0.0000, 0.0407,  ..., 0.0000, 0.0000, 0.0329],
        [0.0419, 0.0000, 0.0407,  ..., 0.0000, 0.0000, 0.0329],
        [0.0419, 0.0000, 0.0407,  ..., 0.0000, 0.0000, 0.0329]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2601635.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33867.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(565.8821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4241.8818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13436.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2885.3044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0900],
        [ 0.1254],
        [ 0.1396],
        ...,
        [-0.4864],
        [-0.4847],
        [-0.4842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-842865.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1349.3761, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1349.3761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0081,  0.0067,  0.0042,  ..., -0.0084,  0.0118, -0.0090],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5228.4365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(857.8911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.9305, device='cuda:0')



h[100].sum tensor(-294.6740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(203.1185, device='cuda:0')



h[200].sum tensor(-108.5310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0292, 0.0219, 0.0151,  ..., 0.0000, 0.0395, 0.0000],
        [0.0195, 0.0067, 0.0042,  ..., 0.0000, 0.0161, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        ...,
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(279845.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0078, 0.0000,  ..., 0.0000, 0.0004, 0.0525],
        [0.0208, 0.0016, 0.0095,  ..., 0.0000, 0.0000, 0.0444],
        [0.0330, 0.0000, 0.0201,  ..., 0.0000, 0.0000, 0.0380],
        ...,
        [0.0424, 0.0000, 0.0410,  ..., 0.0000, 0.0000, 0.0328],
        [0.0424, 0.0000, 0.0410,  ..., 0.0000, 0.0000, 0.0328],
        [0.0424, 0.0000, 0.0410,  ..., 0.0000, 0.0000, 0.0328]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2600478.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33792.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(575.4969, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4258.0708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12919.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2871.1379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1548],
        [ 0.0693],
        [-0.0623],
        ...,
        [-0.4908],
        [-0.4831],
        [-0.4474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-836271., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1358.7882, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1358.7882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070,  0.0049,  0.0028,  ..., -0.0062,  0.0091, -0.0066],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5328.6621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(861.6141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.3485, device='cuda:0')



h[100].sum tensor(-294.7153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(204.5353, device='cuda:0')



h[200].sum tensor(-108.9275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0088, 0.0050,  ..., 0.0000, 0.0197, 0.0000],
        [0.0210, 0.0088, 0.0050,  ..., 0.0000, 0.0197, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        ...,
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0152, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(282411.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0200, 0.0000, 0.0059,  ..., 0.0000, 0.0000, 0.0434],
        [0.0244, 0.0000, 0.0129,  ..., 0.0000, 0.0000, 0.0413],
        [0.0366, 0.0000, 0.0311,  ..., 0.0000, 0.0000, 0.0358],
        ...,
        [0.0423, 0.0000, 0.0411,  ..., 0.0000, 0.0000, 0.0329],
        [0.0423, 0.0000, 0.0411,  ..., 0.0000, 0.0000, 0.0329],
        [0.0423, 0.0000, 0.0411,  ..., 0.0000, 0.0000, 0.0329]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2622976.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33972.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(575.9241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4275.4717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13447.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2887.6809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0647],
        [-0.1867],
        [-0.3019],
        ...,
        [-0.4956],
        [-0.4939],
        [-0.4933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-864593.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1493.4629, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1493.4629, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0144,  0.0166,  0.0119,  ..., -0.0206,  0.0269, -0.0221],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0136,  0.0152,  0.0109,  ..., -0.0189,  0.0249, -0.0203],
        ...,
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5541.8369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(871.8790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.3299, device='cuda:0')



h[100].sum tensor(-274.5898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(224.8076, device='cuda:0')



h[200].sum tensor(-120.5831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0543, 0.0612, 0.0447,  ..., 0.0000, 0.0997, 0.0000],
        [0.0522, 0.0578, 0.0411,  ..., 0.0000, 0.0946, 0.0000],
        [0.0330, 0.0277, 0.0196,  ..., 0.0000, 0.0484, 0.0000],
        ...,
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(299945.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0502, 0.0000,  ..., 0.0000, 0.0321, 0.0791],
        [0.0000, 0.0386, 0.0000,  ..., 0.0000, 0.0210, 0.0733],
        [0.0021, 0.0195, 0.0000,  ..., 0.0000, 0.0056, 0.0610],
        ...,
        [0.0418, 0.0000, 0.0413,  ..., 0.0000, 0.0000, 0.0332],
        [0.0418, 0.0000, 0.0413,  ..., 0.0000, 0.0000, 0.0332],
        [0.0418, 0.0000, 0.0413,  ..., 0.0000, 0.0000, 0.0332]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2720080., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32840.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(623.5549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4351.6943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15911.8057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3026.3171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1520],
        [ 0.1584],
        [ 0.1628],
        ...,
        [-0.4976],
        [-0.4955],
        [-0.4947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-831898.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1780.2560, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1780.2560, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0213,  0.0275,  0.0204,  ..., -0.0339,  0.0435, -0.0364],
        [ 0.0216,  0.0279,  0.0207,  ..., -0.0344,  0.0441, -0.0369],
        [ 0.0169,  0.0204,  0.0149,  ..., -0.0253,  0.0328, -0.0271],
        ...,
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5901.3916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(891.1910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.0674, device='cuda:0')



h[100].sum tensor(-234.1735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(267.9779, device='cuda:0')



h[200].sum tensor(-143.6178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0411, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0633, 0.0752, 0.0546,  ..., 0.0000, 0.1213, 0.0000],
        [0.0674, 0.0817, 0.0596,  ..., 0.0000, 0.1312, 0.0000],
        [0.0509, 0.0557, 0.0404,  ..., 0.0000, 0.0915, 0.0000],
        ...,
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337467.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0455, 0.0000,  ..., 0.0000, 0.0278, 0.0773],
        [0.0000, 0.0489, 0.0000,  ..., 0.0000, 0.0308, 0.0795],
        [0.0000, 0.0354, 0.0000,  ..., 0.0000, 0.0190, 0.0714],
        ...,
        [0.0413, 0.0000, 0.0414,  ..., 0.0000, 0.0000, 0.0336],
        [0.0413, 0.0000, 0.0414,  ..., 0.0000, 0.0000, 0.0336],
        [0.0413, 0.0000, 0.0414,  ..., 0.0000, 0.0000, 0.0336]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2950149.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31747.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(717.3246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4491.9424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22139.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3294.3464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1506],
        [ 0.1499],
        [ 0.1514],
        ...,
        [-0.5010],
        [-0.4992],
        [-0.4987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-801970.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1272.1678, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1272.1678, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0080,  0.0065,  0.0041,  ..., -0.0082,  0.0116, -0.0087],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0038, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5434.7510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(860.8485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.5014, device='cuda:0')



h[100].sum tensor(-308.9392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(191.4966, device='cuda:0')



h[200].sum tensor(-101.7315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0290, 0.0213, 0.0146,  ..., 0.0000, 0.0389, 0.0000],
        [0.0195, 0.0065, 0.0041,  ..., 0.0000, 0.0162, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        ...,
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0153, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(274094.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0093, 0.0000,  ..., 0.0000, 0.0014, 0.0548],
        [0.0217, 0.0019, 0.0121,  ..., 0.0000, 0.0000, 0.0450],
        [0.0353, 0.0000, 0.0245,  ..., 0.0000, 0.0000, 0.0377],
        ...,
        [0.0412, 0.0000, 0.0417,  ..., 0.0000, 0.0000, 0.0341],
        [0.0412, 0.0000, 0.0417,  ..., 0.0000, 0.0000, 0.0341],
        [0.0412, 0.0000, 0.0417,  ..., 0.0000, 0.0000, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2588326.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31803.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(558.4713, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4306.5068, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11759.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2891.9839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1242],
        [-0.0048],
        [-0.1829],
        ...,
        [-0.5043],
        [-0.5025],
        [-0.5020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-793795.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 3900 loss: tensor(535.8906, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.2212, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.2212, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5615.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(874.9208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.9438, device='cuda:0')



h[100].sum tensor(-287.9099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.3311, device='cuda:0')



h[200].sum tensor(-113.6960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0154, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        ...,
        [0.0154, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0154, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(293821.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0391, 0.0000, 0.0381,  ..., 0.0000, 0.0000, 0.0357],
        [0.0413, 0.0000, 0.0415,  ..., 0.0000, 0.0000, 0.0347],
        [0.0420, 0.0000, 0.0422,  ..., 0.0000, 0.0000, 0.0345],
        ...,
        [0.0418, 0.0000, 0.0421,  ..., 0.0000, 0.0000, 0.0342],
        [0.0418, 0.0000, 0.0421,  ..., 0.0000, 0.0000, 0.0342],
        [0.0418, 0.0000, 0.0421,  ..., 0.0000, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2712855.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31737.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(614.0439, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4389.8418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14671.7451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3018.1213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2257],
        [-0.3765],
        [-0.4991],
        ...,
        [-0.5092],
        [-0.5075],
        [-0.5073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-785200.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1316.2753, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1316.2753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5497.6968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(874.8912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.4604, device='cuda:0')



h[100].sum tensor(-302.8224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.1360, device='cuda:0')



h[200].sum tensor(-105.3389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0155, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0155, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        ...,
        [0.0155, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0155, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0155, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(280748.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0432, 0.0000, 0.0428,  ..., 0.0000, 0.0000, 0.0344],
        [0.0425, 0.0000, 0.0421,  ..., 0.0000, 0.0000, 0.0347],
        [0.0402, 0.0000, 0.0392,  ..., 0.0000, 0.0000, 0.0360],
        ...,
        [0.0430, 0.0000, 0.0427,  ..., 0.0000, 0.0000, 0.0342],
        [0.0430, 0.0000, 0.0427,  ..., 0.0000, 0.0000, 0.0342],
        [0.0430, 0.0000, 0.0427,  ..., 0.0000, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2647458.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33574.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(581.7776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4366.5103, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12293.3037, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2911.2449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5577],
        [-0.4700],
        [-0.3356],
        ...,
        [-0.5201],
        [-0.5183],
        [-0.5178]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-859434.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1451.0240, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1451.0240, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0106,  0.0073,  ..., -0.0131,  0.0178, -0.0140],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0039, -0.0001, -0.0010,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5646.2280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(888.3746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.4451, device='cuda:0')



h[100].sum tensor(-283.8555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(218.4194, device='cuda:0')



h[200].sum tensor(-116.0546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0311, 0.0242, 0.0169,  ..., 0.0000, 0.0432, 0.0000],
        [0.0224, 0.0106, 0.0073,  ..., 0.0000, 0.0223, 0.0000],
        [0.0214, 0.0090, 0.0060,  ..., 0.0000, 0.0199, 0.0000],
        ...,
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(297760.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0121, 0.0154, 0.0000,  ..., 0.0000, 0.0066, 0.0575],
        [0.0198, 0.0029, 0.0000,  ..., 0.0000, 0.0000, 0.0486],
        [0.0178, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0494],
        ...,
        [0.0439, 0.0000, 0.0432,  ..., 0.0000, 0.0000, 0.0343],
        [0.0439, 0.0000, 0.0432,  ..., 0.0000, 0.0000, 0.0343],
        [0.0439, 0.0000, 0.0432,  ..., 0.0000, 0.0000, 0.0343]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2762098.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34469.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(627.8433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4437.4263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14947.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3012.0591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1519],
        [ 0.1613],
        [ 0.1704],
        ...,
        [-0.5272],
        [-0.5242],
        [-0.5169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-890908.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1608.3101, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1608.3101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6610e-02,  3.5697e-02,  2.6734e-02,  ..., -4.3869e-02,
          5.6061e-02, -4.7028e-02],
        [ 2.0773e-02,  2.6490e-02,  1.9590e-02,  ..., -3.2585e-02,
          4.2032e-02, -3.4932e-02],
        [ 8.8469e-03,  7.6804e-03,  4.9947e-03,  ..., -9.5330e-03,
          1.3372e-02, -1.0220e-02],
        ...,
        [ 3.9150e-03, -9.8050e-05, -1.0411e-03,  ...,  0.0000e+00,
          1.5194e-03,  0.0000e+00],
        [ 3.9150e-03, -9.8050e-05, -1.0411e-03,  ...,  0.0000e+00,
          1.5194e-03,  0.0000e+00],
        [ 3.9150e-03, -9.8050e-05, -1.0411e-03,  ...,  0.0000e+00,
          1.5194e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5818.5522, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(898.4822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.4307, device='cuda:0')



h[100].sum tensor(-263.2424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(242.0953, device='cuda:0')



h[200].sum tensor(-127.7274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0821, 0.1044, 0.0771,  ..., 0.0000, 0.1657, 0.0000],
        [0.0701, 0.0855, 0.0625,  ..., 0.0000, 0.1369, 0.0000],
        [0.0565, 0.0640, 0.0458,  ..., 0.0000, 0.1042, 0.0000],
        ...,
        [0.0157, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0157, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0279, 0.0190, 0.0128,  ..., 0.0000, 0.0354, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(306693.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0613, 0.0000,  ..., 0.0000, 0.0408, 0.0869],
        [0.0000, 0.0505, 0.0000,  ..., 0.0000, 0.0312, 0.0808],
        [0.0000, 0.0330, 0.0000,  ..., 0.0000, 0.0155, 0.0716],
        ...,
        [0.0413, 0.0000, 0.0367,  ..., 0.0000, 0.0000, 0.0363],
        [0.0338, 0.0000, 0.0209,  ..., 0.0000, 0.0000, 0.0406],
        [0.0154, 0.0069, 0.0045,  ..., 0.0000, 0.0006, 0.0516]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2781604.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34005.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(656.5095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4489.6313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15288.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3091.8518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1330],
        [ 0.1547],
        [ 0.1786],
        ...,
        [-0.3344],
        [-0.1505],
        [ 0.0177]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-871787., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1492.0873, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1492.0873, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.3448e-03,  8.4902e-03,  5.5965e-03,  ..., -1.0501e-02,
          1.4610e-02, -1.1258e-02],
        [ 3.9101e-03, -8.5068e-05, -1.0565e-03,  ...,  0.0000e+00,
          1.5420e-03,  0.0000e+00],
        [ 3.9101e-03, -8.5068e-05, -1.0565e-03,  ...,  0.0000e+00,
          1.5420e-03,  0.0000e+00],
        ...,
        [ 3.9101e-03, -8.5068e-05, -1.0565e-03,  ...,  0.0000e+00,
          1.5420e-03,  0.0000e+00],
        [ 3.9101e-03, -8.5068e-05, -1.0565e-03,  ...,  0.0000e+00,
          1.5420e-03,  0.0000e+00],
        [ 3.9101e-03, -8.5068e-05, -1.0565e-03,  ...,  0.0000e+00,
          1.5420e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5748.9980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(890.5382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.2688, device='cuda:0')



h[100].sum tensor(-279.7902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(224.6005, device='cuda:0')



h[200].sum tensor(-118.7762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0201, 0.0069, 0.0044,  ..., 0.0000, 0.0168, 0.0000],
        [0.0211, 0.0085, 0.0056,  ..., 0.0000, 0.0192, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(299964.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0280, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0439],
        [0.0325, 0.0000, 0.0199,  ..., 0.0000, 0.0000, 0.0417],
        [0.0395, 0.0000, 0.0325,  ..., 0.0000, 0.0000, 0.0381],
        ...,
        [0.0436, 0.0000, 0.0442,  ..., 0.0000, 0.0000, 0.0355],
        [0.0436, 0.0000, 0.0442,  ..., 0.0000, 0.0000, 0.0355],
        [0.0436, 0.0000, 0.0442,  ..., 0.0000, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2766630.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33537.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(637.0034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4487.4780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14825.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3077.7583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1334],
        [-0.2802],
        [-0.3755],
        ...,
        [-0.5343],
        [-0.5325],
        [-0.5319]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-850925.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1578.5533, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1578.5533, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.9103e-03, -7.5393e-05, -1.0656e-03,  ...,  0.0000e+00,
          1.5704e-03,  0.0000e+00],
        [ 3.9103e-03, -7.5393e-05, -1.0656e-03,  ...,  0.0000e+00,
          1.5704e-03,  0.0000e+00],
        [ 3.9103e-03, -7.5393e-05, -1.0656e-03,  ...,  0.0000e+00,
          1.5704e-03,  0.0000e+00],
        ...,
        [ 3.9103e-03, -7.5393e-05, -1.0656e-03,  ...,  0.0000e+00,
          1.5704e-03,  0.0000e+00],
        [ 3.9103e-03, -7.5393e-05, -1.0656e-03,  ...,  0.0000e+00,
          1.5704e-03,  0.0000e+00],
        [ 3.9103e-03, -7.5393e-05, -1.0656e-03,  ...,  0.0000e+00,
          1.5704e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5947.1479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(896.4164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.1091, device='cuda:0')



h[100].sum tensor(-267.3699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(237.6161, device='cuda:0')



h[200].sum tensor(-126.1084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        ...,
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0156, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(309049.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0360],
        [0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0360],
        [0.0434, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0361],
        ...,
        [0.0369, 0.0000, 0.0327,  ..., 0.0000, 0.0000, 0.0389],
        [0.0409, 0.0000, 0.0402,  ..., 0.0000, 0.0000, 0.0369],
        [0.0432, 0.0000, 0.0445,  ..., 0.0000, 0.0000, 0.0358]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2806789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32682.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(658.3566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4533.5566, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15912.9814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3157.8748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7178],
        [-0.7249],
        [-0.7214],
        ...,
        [-0.3719],
        [-0.4578],
        [-0.5100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-819561., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1319.2532, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1319.2532, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0263e-02,  2.5700e-02,  1.8939e-02,  ..., -3.1519e-02,
          4.0892e-02, -3.3794e-02],
        [ 1.1562e-02,  1.1970e-02,  8.2855e-03,  ..., -1.4730e-02,
          1.9958e-02, -1.5793e-02],
        [ 7.5372e-03,  5.6186e-03,  3.3578e-03,  ..., -6.9644e-03,
          1.0275e-02, -7.4671e-03],
        ...,
        [ 3.9276e-03, -7.7089e-05, -1.0615e-03,  ...,  0.0000e+00,
          1.5905e-03,  0.0000e+00],
        [ 3.9276e-03, -7.7089e-05, -1.0615e-03,  ...,  0.0000e+00,
          1.5905e-03,  0.0000e+00],
        [ 3.9276e-03, -7.7089e-05, -1.0615e-03,  ...,  0.0000e+00,
          1.5905e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5811.5479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(883.5916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.5927, device='cuda:0')



h[100].sum tensor(-305.8557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.5842, device='cuda:0')



h[200].sum tensor(-105.1793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0454, 0.0465, 0.0321,  ..., 0.0000, 0.0778, 0.0000],
        [0.0519, 0.0569, 0.0401,  ..., 0.0000, 0.0935, 0.0000],
        [0.0388, 0.0361, 0.0240,  ..., 0.0000, 0.0619, 0.0000],
        ...,
        [0.0157, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0157, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0157, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(289289.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0338, 0.0000,  ..., 0.0000, 0.0155, 0.0746],
        [0.0000, 0.0326, 0.0000,  ..., 0.0000, 0.0143, 0.0746],
        [0.0000, 0.0213, 0.0000,  ..., 0.0000, 0.0062, 0.0690],
        ...,
        [0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0355],
        [0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0355],
        [0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2736740., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32656.0605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(609.4209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4473.2002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13572.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3017.4465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1921],
        [ 0.2075],
        [ 0.2219],
        ...,
        [-0.5406],
        [-0.5387],
        [-0.5381]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-830570.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1235.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1235.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.9551e-03, -8.4763e-05, -1.0502e-03,  ...,  0.0000e+00,
          1.6024e-03,  0.0000e+00],
        [ 3.9551e-03, -8.4763e-05, -1.0502e-03,  ...,  0.0000e+00,
          1.6024e-03,  0.0000e+00],
        [ 3.9551e-03, -8.4763e-05, -1.0502e-03,  ...,  0.0000e+00,
          1.6024e-03,  0.0000e+00],
        ...,
        [ 3.9551e-03, -8.4763e-05, -1.0502e-03,  ...,  0.0000e+00,
          1.6024e-03,  0.0000e+00],
        [ 3.9551e-03, -8.4763e-05, -1.0502e-03,  ...,  0.0000e+00,
          1.6024e-03,  0.0000e+00],
        [ 3.9551e-03, -8.4763e-05, -1.0502e-03,  ...,  0.0000e+00,
          1.6024e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5877.7666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(883.8071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.8575, device='cuda:0')



h[100].sum tensor(-319.3951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(185.9247, device='cuda:0')



h[200].sum tensor(-98.2088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0158, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0158, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0158, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        ...,
        [0.0158, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0158, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0158, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(278781.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0381, 0.0000, 0.0245,  ..., 0.0000, 0.0000, 0.0387],
        [0.0396, 0.0000, 0.0305,  ..., 0.0000, 0.0000, 0.0376],
        [0.0331, 0.0000, 0.0207,  ..., 0.0000, 0.0000, 0.0411],
        ...,
        [0.0437, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0350],
        [0.0437, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0350],
        [0.0437, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2680458.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33370.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(583.3193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4437.9590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11545.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2920.2002, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1809],
        [-0.1459],
        [-0.0319],
        ...,
        [-0.5460],
        [-0.5441],
        [-0.5435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-879384.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1676.6531, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1676.6531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.9717e-03, -8.5973e-05, -1.0433e-03,  ...,  0.0000e+00,
          1.6254e-03,  0.0000e+00],
        [ 8.2371e-03,  6.6416e-03,  4.1785e-03,  ..., -8.2150e-03,
          1.1889e-02, -8.8088e-03],
        [ 7.5055e-03,  5.4877e-03,  3.2828e-03,  ..., -6.8059e-03,
          1.0129e-02, -7.2980e-03],
        ...,
        [ 3.9717e-03, -8.5973e-05, -1.0433e-03,  ...,  0.0000e+00,
          1.6254e-03,  0.0000e+00],
        [ 3.9717e-03, -8.5973e-05, -1.0433e-03,  ...,  0.0000e+00,
          1.6254e-03,  0.0000e+00],
        [ 3.9717e-03, -8.5973e-05, -1.0433e-03,  ...,  0.0000e+00,
          1.6254e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6525.7505, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(915.1160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.4660, device='cuda:0')



h[100].sum tensor(-257.0976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(252.3828, device='cuda:0')



h[200].sum tensor(-133.4492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0066, 0.0042,  ..., 0.0000, 0.0168, 0.0000],
        [0.0258, 0.0154, 0.0100,  ..., 0.0000, 0.0303, 0.0000],
        [0.0456, 0.0465, 0.0322,  ..., 0.0000, 0.0780, 0.0000],
        ...,
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(323015.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0248, 0.0008, 0.0151,  ..., 0.0000, 0.0000, 0.0449],
        [0.0122, 0.0102, 0.0034,  ..., 0.0000, 0.0015, 0.0545],
        [0.0024, 0.0298, 0.0000,  ..., 0.0000, 0.0132, 0.0690],
        ...,
        [0.0436, 0.0000, 0.0445,  ..., 0.0000, 0.0000, 0.0348],
        [0.0436, 0.0000, 0.0445,  ..., 0.0000, 0.0000, 0.0348],
        [0.0436, 0.0000, 0.0445,  ..., 0.0000, 0.0000, 0.0348]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2891029.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32296.1699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(702.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4590.2437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17072.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3206.4956, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0298],
        [ 0.0542],
        [ 0.0433],
        ...,
        [-0.5486],
        [-0.5469],
        [-0.5465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-854228.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1437.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1437.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.9794e-03, -7.9170e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6472e-03,  0.0000e+00],
        [ 3.9794e-03, -7.9170e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6472e-03,  0.0000e+00],
        [ 3.9794e-03, -7.9170e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6472e-03,  0.0000e+00],
        ...,
        [ 3.9794e-03, -7.9170e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6472e-03,  0.0000e+00],
        [ 3.9794e-03, -7.9170e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6472e-03,  0.0000e+00],
        [ 3.9794e-03, -7.9170e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6472e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6370.4346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(901.4298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.8456, device='cuda:0')



h[100].sum tensor(-292.9235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(216.3876, device='cuda:0')



h[200].sum tensor(-114.0184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        ...,
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0159, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(299155.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0434, 0.0000, 0.0447,  ..., 0.0000, 0.0000, 0.0351],
        [0.0434, 0.0000, 0.0447,  ..., 0.0000, 0.0000, 0.0351],
        [0.0435, 0.0000, 0.0447,  ..., 0.0000, 0.0000, 0.0352],
        ...,
        [0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0349],
        [0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0349],
        [0.0433, 0.0000, 0.0446,  ..., 0.0000, 0.0000, 0.0349]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2766655., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31798.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(638.7837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4524.2920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13427.7314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3059.1765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7369],
        [-0.7461],
        [-0.7477],
        ...,
        [-0.5522],
        [-0.5503],
        [-0.5497]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-831565.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 4200 loss: tensor(546.8540, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1602.0248, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1602.0248, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.9877e-03, -6.9258e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6568e-03,  0.0000e+00],
        [ 3.9877e-03, -6.9258e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6568e-03,  0.0000e+00],
        [ 3.9877e-03, -6.9258e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6568e-03,  0.0000e+00],
        ...,
        [ 3.9877e-03, -6.9258e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6568e-03,  0.0000e+00],
        [ 3.9877e-03, -6.9258e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6568e-03,  0.0000e+00],
        [ 3.9877e-03, -6.9258e-05, -1.0413e-03,  ...,  0.0000e+00,
          1.6568e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6610.2114, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(913.0876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.1515, device='cuda:0')



h[100].sum tensor(-271.1733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(241.1492, device='cuda:0')



h[200].sum tensor(-126.5561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        ...,
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(317004.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0340, 0.0000, 0.0200,  ..., 0.0000, 0.0000, 0.0404],
        [0.0383, 0.0000, 0.0344,  ..., 0.0000, 0.0000, 0.0379],
        [0.0393, 0.0000, 0.0376,  ..., 0.0000, 0.0000, 0.0375],
        ...,
        [0.0432, 0.0000, 0.0449,  ..., 0.0000, 0.0000, 0.0353],
        [0.0432, 0.0000, 0.0449,  ..., 0.0000, 0.0000, 0.0353],
        [0.0432, 0.0000, 0.0449,  ..., 0.0000, 0.0000, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2868167.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(31350.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(680.0367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4596.2715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16041.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3191.3369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0562],
        [-0.0092],
        [-0.0598],
        ...,
        [-0.5302],
        [-0.5492],
        [-0.5529]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-813183.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1273.9204, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1273.9204, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.0050e-03, -6.3305e-05, -1.0373e-03,  ...,  0.0000e+00,
          1.6578e-03,  0.0000e+00],
        [ 4.0050e-03, -6.3305e-05, -1.0373e-03,  ...,  0.0000e+00,
          1.6578e-03,  0.0000e+00],
        [ 4.0050e-03, -6.3305e-05, -1.0373e-03,  ...,  0.0000e+00,
          1.6578e-03,  0.0000e+00],
        ...,
        [ 4.0050e-03, -6.3305e-05, -1.0373e-03,  ...,  0.0000e+00,
          1.6578e-03,  0.0000e+00],
        [ 4.0050e-03, -6.3305e-05, -1.0373e-03,  ...,  0.0000e+00,
          1.6578e-03,  0.0000e+00],
        [ 4.0050e-03, -6.3305e-05, -1.0373e-03,  ...,  0.0000e+00,
          1.6578e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6300.9346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(896.2758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.5793, device='cuda:0')



h[100].sum tensor(-318.5190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(191.7604, device='cuda:0')



h[200].sum tensor(-100.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0194, 0.0052, 0.0031,  ..., 0.0000, 0.0147, 0.0000],
        ...,
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0160, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(288459.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0404, 0.0000, 0.0375,  ..., 0.0000, 0.0000, 0.0374],
        [0.0328, 0.0000, 0.0190,  ..., 0.0000, 0.0000, 0.0415],
        [0.0202, 0.0013, 0.0069,  ..., 0.0000, 0.0000, 0.0482],
        ...,
        [0.0436, 0.0000, 0.0452,  ..., 0.0000, 0.0000, 0.0355],
        [0.0436, 0.0000, 0.0452,  ..., 0.0000, 0.0000, 0.0355],
        [0.0436, 0.0000, 0.0452,  ..., 0.0000, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2750984., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32633.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(595.4856, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4510.5581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12512.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3001.4094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1418],
        [ 0.0050],
        [ 0.1097],
        ...,
        [-0.5614],
        [-0.5595],
        [-0.5589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-859766.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1596.3876, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1596.3876, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2075e-02,  1.2660e-02,  8.8330e-03,  ..., -1.5477e-02,
          2.1056e-02, -1.6600e-02],
        [ 1.0694e-02,  1.0477e-02,  7.1395e-03,  ..., -1.2821e-02,
          1.7727e-02, -1.3751e-02],
        [ 3.4698e-02,  4.8399e-02,  3.6566e-02,  ..., -5.8977e-02,
          7.5580e-02, -6.3253e-02],
        ...,
        [ 4.0253e-03, -5.6596e-05, -1.0347e-03,  ...,  0.0000e+00,
          1.6565e-03,  0.0000e+00],
        [ 4.0253e-03, -5.6596e-05, -1.0347e-03,  ...,  0.0000e+00,
          1.6565e-03,  0.0000e+00],
        [ 4.0253e-03, -5.6596e-05, -1.0347e-03,  ...,  0.0000e+00,
          1.6565e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6694.6787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(920.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.9012, device='cuda:0')



h[100].sum tensor(-274.1843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(240.3006, device='cuda:0')



h[200].sum tensor(-125.6327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0402, 0.0379, 0.0275,  ..., 0.0000, 0.0647, 0.0000],
        [0.0954, 0.1250, 0.0930,  ..., 0.0000, 0.1976, 0.0000],
        [0.0760, 0.0944, 0.0693,  ..., 0.0000, 0.1511, 0.0000],
        ...,
        [0.0161, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0161, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0161, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(315236.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0434, 0.0000,  ..., 0.0000, 0.0245, 0.0753],
        [0.0000, 0.0894, 0.0000,  ..., 0.0000, 0.0622, 0.0996],
        [0.0000, 0.1009, 0.0000,  ..., 0.0000, 0.0718, 0.1062],
        ...,
        [0.0441, 0.0000, 0.0456,  ..., 0.0000, 0.0000, 0.0358],
        [0.0441, 0.0000, 0.0456,  ..., 0.0000, 0.0000, 0.0358],
        [0.0441, 0.0000, 0.0456,  ..., 0.0000, 0.0000, 0.0358]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2872124.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32890.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(667.5719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4614.2026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15618.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3180.1987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1270],
        [ 0.1030],
        [ 0.0785],
        ...,
        [-0.5626],
        [-0.5600],
        [-0.5601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-875634.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1364.1368, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1364.1368, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.4052e-03,  5.2389e-03,  3.0801e-03,  ..., -6.4364e-03,
          9.7259e-03, -6.9034e-03],
        [ 7.4052e-03,  5.2389e-03,  3.0801e-03,  ..., -6.4364e-03,
          9.7259e-03, -6.9034e-03],
        [ 4.0553e-03, -5.3191e-05, -1.0268e-03,  ...,  0.0000e+00,
          1.6518e-03,  0.0000e+00],
        ...,
        [ 4.0553e-03, -5.3191e-05, -1.0268e-03,  ...,  0.0000e+00,
          1.6518e-03,  0.0000e+00],
        [ 4.0553e-03, -5.3191e-05, -1.0268e-03,  ...,  0.0000e+00,
          1.6518e-03,  0.0000e+00],
        [ 4.0553e-03, -5.3191e-05, -1.0268e-03,  ...,  0.0000e+00,
          1.6518e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6491.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(912.3949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.5861, device='cuda:0')



h[100].sum tensor(-306.6551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.3404, device='cuda:0')



h[200].sum tensor(-107.9612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0284, 0.0191, 0.0118,  ..., 0.0000, 0.0359, 0.0000],
        [0.0253, 0.0143, 0.0081,  ..., 0.0000, 0.0286, 0.0000],
        [0.0223, 0.0095, 0.0054,  ..., 0.0000, 0.0213, 0.0000],
        ...,
        [0.0162, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0162, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(302957.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0070, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0552],
        [0.0100, 0.0010, 0.0021,  ..., 0.0000, 0.0000, 0.0534],
        [0.0208, 0.0000, 0.0108,  ..., 0.0000, 0.0000, 0.0482],
        ...,
        [0.0448, 0.0000, 0.0458,  ..., 0.0000, 0.0000, 0.0359],
        [0.0448, 0.0000, 0.0458,  ..., 0.0000, 0.0000, 0.0359],
        [0.0448, 0.0000, 0.0458,  ..., 0.0000, 0.0000, 0.0359]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2868072.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33823.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(638.1594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4587.5098, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14994.9717, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3088.6084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1902],
        [ 0.1423],
        [ 0.0244],
        ...,
        [-0.5746],
        [-0.5726],
        [-0.5720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-903004.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1309.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1309.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.0850e-03, -4.8813e-05, -1.0217e-03,  ...,  0.0000e+00,
          1.6481e-03,  0.0000e+00],
        [ 4.0850e-03, -4.8813e-05, -1.0217e-03,  ...,  0.0000e+00,
          1.6481e-03,  0.0000e+00],
        [ 4.0850e-03, -4.8813e-05, -1.0217e-03,  ...,  0.0000e+00,
          1.6481e-03,  0.0000e+00],
        ...,
        [ 4.0850e-03, -4.8813e-05, -1.0217e-03,  ...,  0.0000e+00,
          1.6481e-03,  0.0000e+00],
        [ 4.0850e-03, -4.8813e-05, -1.0217e-03,  ...,  0.0000e+00,
          1.6481e-03,  0.0000e+00],
        [ 4.0850e-03, -4.8813e-05, -1.0217e-03,  ...,  0.0000e+00,
          1.6481e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6465.2666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(915.0292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.1761, device='cuda:0')



h[100].sum tensor(-315.1164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.1725, device='cuda:0')



h[200].sum tensor(-103.5065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0163, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0163, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0163, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        ...,
        [0.0163, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0163, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0163, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(292456.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0422, 0.0000, 0.0378,  ..., 0.0000, 0.0000, 0.0381],
        [0.0450, 0.0000, 0.0442,  ..., 0.0000, 0.0000, 0.0367],
        [0.0458, 0.0000, 0.0462,  ..., 0.0000, 0.0000, 0.0364],
        ...,
        [0.0456, 0.0000, 0.0461,  ..., 0.0000, 0.0000, 0.0361],
        [0.0456, 0.0000, 0.0461,  ..., 0.0000, 0.0000, 0.0361],
        [0.0456, 0.0000, 0.0461,  ..., 0.0000, 0.0000, 0.0361]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2798069., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34975.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(612.1162, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4565.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12603.9551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3009.2053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6396],
        [-0.6538],
        [-0.6044],
        ...,
        [-0.5815],
        [-0.5795],
        [-0.5789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-948192., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1390.0292, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1390.0292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1025e-03, -3.9005e-05, -1.0248e-03,  ...,  0.0000e+00,
          1.6560e-03,  0.0000e+00],
        [ 4.1025e-03, -3.9005e-05, -1.0248e-03,  ...,  0.0000e+00,
          1.6560e-03,  0.0000e+00],
        [ 8.3734e-03,  6.7079e-03,  4.2113e-03,  ..., -8.1930e-03,
          1.1952e-02, -8.7884e-03],
        ...,
        [ 4.1025e-03, -3.9005e-05, -1.0248e-03,  ...,  0.0000e+00,
          1.6560e-03,  0.0000e+00],
        [ 4.1025e-03, -3.9005e-05, -1.0248e-03,  ...,  0.0000e+00,
          1.6560e-03,  0.0000e+00],
        [ 4.1025e-03, -3.9005e-05, -1.0248e-03,  ...,  0.0000e+00,
          1.6560e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6594.0283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(923.5002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.7361, device='cuda:0')



h[100].sum tensor(-304.6212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.2380, device='cuda:0')



h[200].sum tensor(-109.5622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0164, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0207, 0.0067, 0.0042,  ..., 0.0000, 0.0169, 0.0000],
        [0.0262, 0.0153, 0.0099,  ..., 0.0000, 0.0302, 0.0000],
        ...,
        [0.0164, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0164, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0164, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(300050.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0355, 0.0000, 0.0207,  ..., 0.0000, 0.0000, 0.0421],
        [0.0235, 0.0005, 0.0100,  ..., 0.0000, 0.0000, 0.0485],
        [0.0117, 0.0062, 0.0034,  ..., 0.0000, 0.0000, 0.0562],
        ...,
        [0.0458, 0.0000, 0.0464,  ..., 0.0000, 0.0000, 0.0365],
        [0.0458, 0.0000, 0.0464,  ..., 0.0000, 0.0000, 0.0365],
        [0.0458, 0.0000, 0.0464,  ..., 0.0000, 0.0000, 0.0365]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2834954., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35511.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(627.6982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4601.8965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13529.3818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3065.6072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1256],
        [ 0.1585],
        [ 0.1884],
        ...,
        [-0.5867],
        [-0.5847],
        [-0.5841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-979009., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1471.2657, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1471.2657, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1188e-03, -3.3041e-05, -1.0239e-03,  ...,  0.0000e+00,
          1.6763e-03,  0.0000e+00],
        [ 4.1188e-03, -3.3041e-05, -1.0239e-03,  ...,  0.0000e+00,
          1.6763e-03,  0.0000e+00],
        [ 4.1188e-03, -3.3041e-05, -1.0239e-03,  ...,  0.0000e+00,
          1.6763e-03,  0.0000e+00],
        ...,
        [ 4.1188e-03, -3.3041e-05, -1.0239e-03,  ...,  0.0000e+00,
          1.6763e-03,  0.0000e+00],
        [ 4.1188e-03, -3.3041e-05, -1.0239e-03,  ...,  0.0000e+00,
          1.6763e-03,  0.0000e+00],
        [ 4.1188e-03, -3.3041e-05, -1.0239e-03,  ...,  0.0000e+00,
          1.6763e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6775.1855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(931.2891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.3441, device='cuda:0')



h[100].sum tensor(-295.5162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(221.4663, device='cuda:0')



h[200].sum tensor(-115.0508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0264, 0.0156, 0.0111,  ..., 0.0000, 0.0305, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        ...,
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(304250.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0279, 0.0032, 0.0102,  ..., 0.0000, 0.0000, 0.0476],
        [0.0403, 0.0000, 0.0261,  ..., 0.0000, 0.0000, 0.0403],
        [0.0437, 0.0000, 0.0388,  ..., 0.0000, 0.0000, 0.0383],
        ...,
        [0.0457, 0.0000, 0.0464,  ..., 0.0000, 0.0000, 0.0367],
        [0.0457, 0.0000, 0.0464,  ..., 0.0000, 0.0000, 0.0367],
        [0.0457, 0.0000, 0.0464,  ..., 0.0000, 0.0000, 0.0367]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2842889.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33970.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(654.0311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4634.6553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13263.1104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3102.0781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2038],
        [-0.3125],
        [-0.3291],
        ...,
        [-0.5718],
        [-0.5540],
        [-0.5411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-898732.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1393.2491, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1393.2491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00],
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00],
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00],
        ...,
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00],
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00],
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6832.6758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(929.7911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.8791, device='cuda:0')



h[100].sum tensor(-307.5622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.7227, device='cuda:0')



h[200].sum tensor(-109.0828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        ...,
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(297705.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0455, 0.0000, 0.0463,  ..., 0.0000, 0.0000, 0.0369],
        [0.0455, 0.0000, 0.0463,  ..., 0.0000, 0.0000, 0.0369],
        [0.0428, 0.0000, 0.0386,  ..., 0.0000, 0.0000, 0.0385],
        ...,
        [0.0453, 0.0000, 0.0462,  ..., 0.0000, 0.0000, 0.0367],
        [0.0453, 0.0000, 0.0462,  ..., 0.0000, 0.0000, 0.0367],
        [0.0453, 0.0000, 0.0462,  ..., 0.0000, 0.0000, 0.0367]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2817418.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34199.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(627.9583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4615.5986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12546.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3058.3083, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7752],
        [-0.7275],
        [-0.6258],
        ...,
        [-0.5934],
        [-0.5914],
        [-0.5908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-931344.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1538.0424, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1538.0424, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0599e-02,  1.0179e-02,  6.9052e-03,  ..., -1.2379e-02,
          1.7291e-02, -1.3280e-02],
        [ 1.2679e-02,  1.3465e-02,  9.4555e-03,  ..., -1.6363e-02,
          2.2308e-02, -1.7554e-02],
        [ 1.7109e-02,  2.0462e-02,  1.4886e-02,  ..., -2.4847e-02,
          3.2991e-02, -2.6655e-02],
        ...,
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00],
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00],
        [ 4.1342e-03, -2.9543e-05, -1.0191e-03,  ...,  0.0000e+00,
          1.7041e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7020.3442, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(939.6915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.3099, device='cuda:0')



h[100].sum tensor(-284.9840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(231.5181, device='cuda:0')



h[200].sum tensor(-121.5077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0476, 0.0490, 0.0340,  ..., 0.0000, 0.0818, 0.0000],
        [0.0525, 0.0567, 0.0400,  ..., 0.0000, 0.0936, 0.0000],
        [0.0760, 0.0938, 0.0688,  ..., 0.0000, 0.1502, 0.0000],
        ...,
        [0.0246, 0.0127, 0.0089,  ..., 0.0000, 0.0263, 0.0000],
        [0.0327, 0.0254, 0.0177,  ..., 0.0000, 0.0457, 0.0000],
        [0.0246, 0.0127, 0.0089,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(319397.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0388, 0.0000,  ..., 0.0000, 0.0157, 0.0768],
        [0.0000, 0.0474, 0.0000,  ..., 0.0000, 0.0228, 0.0814],
        [0.0000, 0.0648, 0.0000,  ..., 0.0000, 0.0373, 0.0900],
        ...,
        [0.0201, 0.0038, 0.0015,  ..., 0.0000, 0.0000, 0.0513],
        [0.0129, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0557],
        [0.0216, 0.0038, 0.0027,  ..., 0.0000, 0.0000, 0.0505]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2942679., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34151.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(681.5717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4685.2095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16092.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3201.7539, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1913],
        [0.1887],
        [0.1855],
        ...,
        [0.0380],
        [0.0845],
        [0.0119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-939111.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1481.0806, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].sum tensor(1481.0806, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1497e-03, -2.7465e-05, -1.0135e-03,  ...,  0.0000e+00,
          1.7332e-03,  0.0000e+00],
        [ 4.1497e-03, -2.7465e-05, -1.0135e-03,  ...,  0.0000e+00,
          1.7332e-03,  0.0000e+00],
        [ 4.1497e-03, -2.7465e-05, -1.0135e-03,  ...,  0.0000e+00,
          1.7332e-03,  0.0000e+00],
        ...,
        [ 4.1497e-03, -2.7465e-05, -1.0135e-03,  ...,  0.0000e+00,
          1.7332e-03,  0.0000e+00],
        [ 8.0626e-03,  6.1506e-03,  3.7826e-03,  ..., -7.4859e-03,
          1.1169e-02, -8.0311e-03],
        [ 4.1497e-03, -2.7465e-05, -1.0135e-03,  ...,  0.0000e+00,
          1.7332e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7094.3057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(938.5497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.7800, device='cuda:0')



h[100].sum tensor(-296.4312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(222.9437, device='cuda:0')



h[200].sum tensor(-115.9370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0166, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0166, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0166, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0205, 0.0062, 0.0038,  ..., 0.0000, 0.0164, 0.0000],
        [0.0198, 0.0050, 0.0029,  ..., 0.0000, 0.0146, 0.0000],
        [0.0308, 0.0223, 0.0134,  ..., 0.0000, 0.0412, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(311172.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0451, 0.0000, 0.0460,  ..., 0.0000, 0.0000, 0.0368],
        [0.0451, 0.0000, 0.0460,  ..., 0.0000, 0.0000, 0.0368],
        [0.0452, 0.0000, 0.0461,  ..., 0.0000, 0.0000, 0.0369],
        ...,
        [0.0346, 0.0000, 0.0251,  ..., 0.0000, 0.0000, 0.0417],
        [0.0311, 0.0000, 0.0187,  ..., 0.0000, 0.0000, 0.0434],
        [0.0215, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0480]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2890667.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32529.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(672.9800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4672.4141, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14173.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3151.3379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7409],
        [-0.6845],
        [-0.5944],
        ...,
        [-0.4434],
        [-0.3788],
        [-0.3442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-857065.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 4500 loss: tensor(467.6737, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1494.2887, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1494.2887, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5549e-02,  1.7946e-02,  1.2939e-02,  ..., -2.1758e-02,
          2.9202e-02, -2.3343e-02],
        [ 4.1664e-03, -2.3402e-05, -1.0119e-03,  ...,  0.0000e+00,
          1.7504e-03,  0.0000e+00],
        [ 4.1664e-03, -2.3402e-05, -1.0119e-03,  ...,  0.0000e+00,
          1.7504e-03,  0.0000e+00],
        ...,
        [ 4.1664e-03, -2.3402e-05, -1.0119e-03,  ...,  0.0000e+00,
          1.7504e-03,  0.0000e+00],
        [ 4.1664e-03, -2.3402e-05, -1.0119e-03,  ...,  0.0000e+00,
          1.7504e-03,  0.0000e+00],
        [ 4.1664e-03, -2.3402e-05, -1.0119e-03,  ...,  0.0000e+00,
          1.7504e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7221.5264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(943.5225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.3666, device='cuda:0')



h[100].sum tensor(-293.9109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(224.9319, device='cuda:0')



h[200].sum tensor(-117.7637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0373, 0.0326, 0.0233,  ..., 0.0000, 0.0569, 0.0000],
        [0.0373, 0.0326, 0.0233,  ..., 0.0000, 0.0569, 0.0000],
        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        ...,
        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(315256.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0120, 0.0111, 0.0000,  ..., 0.0000, 0.0003, 0.0573],
        [0.0174, 0.0086, 0.0000,  ..., 0.0000, 0.0003, 0.0540],
        [0.0348, 0.0000, 0.0197,  ..., 0.0000, 0.0000, 0.0434],
        ...,
        [0.0451, 0.0000, 0.0461,  ..., 0.0000, 0.0000, 0.0369],
        [0.0451, 0.0000, 0.0461,  ..., 0.0000, 0.0000, 0.0369],
        [0.0451, 0.0000, 0.0461,  ..., 0.0000, 0.0000, 0.0369]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2916055.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32868.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(680.2682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4696.2593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14781.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3180.3682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0542],
        [ 0.0468],
        [ 0.0322],
        ...,
        [-0.6004],
        [-0.5984],
        [-0.5978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-890395.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1653.5579, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1653.5579, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1753e-03, -1.1622e-05, -1.0175e-03,  ...,  0.0000e+00,
          1.7667e-03,  0.0000e+00],
        [ 4.1753e-03, -1.1622e-05, -1.0175e-03,  ...,  0.0000e+00,
          1.7667e-03,  0.0000e+00],
        [ 1.5781e-02,  1.8311e-02,  1.3207e-02,  ..., -2.2167e-02,
          2.9762e-02, -2.3784e-02],
        ...,
        [ 4.1753e-03, -1.1622e-05, -1.0175e-03,  ...,  0.0000e+00,
          1.7667e-03,  0.0000e+00],
        [ 4.1753e-03, -1.1622e-05, -1.0175e-03,  ...,  0.0000e+00,
          1.7667e-03,  0.0000e+00],
        [ 4.1753e-03, -1.1622e-05, -1.0175e-03,  ...,  0.0000e+00,
          1.7667e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7488.7681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(956.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.4403, device='cuda:0')



h[100].sum tensor(-269.9643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(248.9063, device='cuda:0')



h[200].sum tensor(-131.1855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0378, 0.0333, 0.0238,  ..., 0.0000, 0.0579, 0.0000],
        [0.0378, 0.0333, 0.0238,  ..., 0.0000, 0.0579, 0.0000],
        ...,
        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0167, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(342935.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0358, 0.0000, 0.0206,  ..., 0.0000, 0.0000, 0.0433],
        [0.0173, 0.0088, 0.0000,  ..., 0.0000, 0.0004, 0.0547],
        [0.0117, 0.0111, 0.0000,  ..., 0.0000, 0.0004, 0.0583],
        ...,
        [0.0450, 0.0000, 0.0463,  ..., 0.0000, 0.0000, 0.0374],
        [0.0450, 0.0000, 0.0463,  ..., 0.0000, 0.0000, 0.0374],
        [0.0450, 0.0000, 0.0463,  ..., 0.0000, 0.0000, 0.0374]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3101306.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32193.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(747.8103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4800.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19481.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3376.7251, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3604],
        [-0.2094],
        [-0.1232],
        ...,
        [-0.6040],
        [-0.6020],
        [-0.6014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-855255.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1460.9435, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1460.9435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1896e-02,  2.7972e-02,  2.0682e-02,  ..., -3.3808e-02,
          4.4503e-02, -3.6275e-02],
        [ 2.8297e-02,  3.8080e-02,  2.8527e-02,  ..., -4.6026e-02,
          5.9948e-02, -4.9386e-02],
        [ 2.6503e-02,  3.5247e-02,  2.6328e-02,  ..., -4.2601e-02,
          5.5619e-02, -4.5711e-02],
        ...,
        [ 4.1854e-03,  4.1246e-06, -1.0263e-03,  ...,  0.0000e+00,
          1.7686e-03,  0.0000e+00],
        [ 4.1854e-03,  4.1246e-06, -1.0263e-03,  ...,  0.0000e+00,
          1.7686e-03,  0.0000e+00],
        [ 4.1854e-03,  4.1246e-06, -1.0263e-03,  ...,  0.0000e+00,
          1.7686e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7231.5186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(945.1044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.8856, device='cuda:0')



h[100].sum tensor(-300.2958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(219.9125, device='cuda:0')



h[200].sum tensor(-114.6607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0313e-01, 1.3644e-01, 1.0178e-01,  ..., 0.0000e+00, 2.1553e-01,
         0.0000e+00],
        [1.0931e-01, 1.4619e-01, 1.0935e-01,  ..., 0.0000e+00, 2.3042e-01,
         0.0000e+00],
        [1.2547e-01, 1.7172e-01, 1.2917e-01,  ..., 0.0000e+00, 2.6943e-01,
         0.0000e+00],
        ...,
        [1.6741e-02, 1.6499e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0746e-03,
         0.0000e+00],
        [1.6741e-02, 1.6499e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0746e-03,
         0.0000e+00],
        [1.6741e-02, 1.6499e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0746e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(316590.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1152, 0.0000,  ..., 0.0000, 0.0780, 0.1154],
        [0.0000, 0.1318, 0.0000,  ..., 0.0000, 0.0918, 0.1231],
        [0.0000, 0.1438, 0.0000,  ..., 0.0000, 0.1018, 0.1288],
        ...,
        [0.0453, 0.0000, 0.0468,  ..., 0.0000, 0.0000, 0.0383],
        [0.0453, 0.0000, 0.0468,  ..., 0.0000, 0.0000, 0.0383],
        [0.0453, 0.0000, 0.0468,  ..., 0.0000, 0.0000, 0.0383]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2945689., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(32130.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(687.8524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4740.1465, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14964.0361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3223.3757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1193],
        [ 0.1109],
        [ 0.1095],
        ...,
        [-0.6091],
        [-0.6070],
        [-0.6064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-832623.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1373.7029, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1373.7029, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2073e-03,  6.0983e-06, -1.0286e-03,  ...,  0.0000e+00,
          1.7598e-03,  0.0000e+00],
        [ 9.0109e-03,  7.5919e-03,  4.8589e-03,  ..., -9.1625e-03,
          1.3352e-02, -9.8318e-03],
        [ 4.2073e-03,  6.0983e-06, -1.0286e-03,  ...,  0.0000e+00,
          1.7598e-03,  0.0000e+00],
        ...,
        [ 4.2073e-03,  6.0983e-06, -1.0286e-03,  ...,  0.0000e+00,
          1.7598e-03,  0.0000e+00],
        [ 4.2073e-03,  6.0983e-06, -1.0286e-03,  ...,  0.0000e+00,
          1.7598e-03,  0.0000e+00],
        [ 4.2073e-03,  6.0983e-06, -1.0286e-03,  ...,  0.0000e+00,
          1.7598e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7109.5449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(943.8044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.0110, device='cuda:0')



h[100].sum tensor(-314.0569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(206.7804, device='cuda:0')



h[200].sum tensor(-107.3301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.1633e-02, 7.6102e-03, 4.8589e-03,  ..., 0.0000e+00, 1.8631e-02,
         0.0000e+00],
        [2.0751e-02, 6.2182e-03, 3.7786e-03,  ..., 0.0000e+00, 1.6504e-02,
         0.0000e+00],
        [3.4281e-02, 2.7584e-02, 1.7275e-02,  ..., 0.0000e+00, 4.9153e-02,
         0.0000e+00],
        ...,
        [1.6829e-02, 2.4393e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0393e-03,
         0.0000e+00],
        [1.6829e-02, 2.4393e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0393e-03,
         0.0000e+00],
        [1.6829e-02, 2.4393e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0393e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(310096.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0324, 0.0000, 0.0188,  ..., 0.0000, 0.0000, 0.0460],
        [0.0306, 0.0000, 0.0144,  ..., 0.0000, 0.0000, 0.0469],
        [0.0215, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0518],
        ...,
        [0.0460, 0.0000, 0.0474,  ..., 0.0000, 0.0000, 0.0387],
        [0.0460, 0.0000, 0.0474,  ..., 0.0000, 0.0000, 0.0387],
        [0.0460, 0.0000, 0.0474,  ..., 0.0000, 0.0000, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2943444.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33706.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(664.2791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4736.5781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14581.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3180.7454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0309],
        [ 0.0263],
        [ 0.0282],
        ...,
        [-0.6178],
        [-0.6157],
        [-0.6151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-896857.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1508.3939, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1508.3939, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2355e-03,  4.3054e-06, -1.0271e-03,  ...,  0.0000e+00,
          1.7441e-03,  0.0000e+00],
        [ 4.2355e-03,  4.3054e-06, -1.0271e-03,  ...,  0.0000e+00,
          1.7441e-03,  0.0000e+00],
        [ 4.2355e-03,  4.3054e-06, -1.0271e-03,  ...,  0.0000e+00,
          1.7441e-03,  0.0000e+00],
        ...,
        [ 4.2355e-03,  4.3054e-06, -1.0271e-03,  ...,  0.0000e+00,
          1.7441e-03,  0.0000e+00],
        [ 4.2355e-03,  4.3054e-06, -1.0271e-03,  ...,  0.0000e+00,
          1.7441e-03,  0.0000e+00],
        [ 4.2355e-03,  4.3054e-06, -1.0271e-03,  ...,  0.0000e+00,
          1.7441e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7267.7822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(957.9371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.9931, device='cuda:0')



h[100].sum tensor(-295.3775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(227.0551, device='cuda:0')



h[200].sum tensor(-117.6469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0348, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.6942e-02, 1.7222e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9765e-03,
         0.0000e+00],
        [1.6942e-02, 1.7222e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9765e-03,
         0.0000e+00],
        [1.6942e-02, 1.7222e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9765e-03,
         0.0000e+00],
        ...,
        [1.6942e-02, 1.7222e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9765e-03,
         0.0000e+00],
        [1.6942e-02, 1.7222e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9765e-03,
         0.0000e+00],
        [1.6942e-02, 1.7222e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9765e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(318040.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0473, 0.0000, 0.0482,  ..., 0.0000, 0.0000, 0.0393],
        [0.0473, 0.0000, 0.0482,  ..., 0.0000, 0.0000, 0.0393],
        [0.0473, 0.0000, 0.0482,  ..., 0.0000, 0.0000, 0.0394],
        ...,
        [0.0471, 0.0000, 0.0481,  ..., 0.0000, 0.0000, 0.0391],
        [0.0471, 0.0000, 0.0481,  ..., 0.0000, 0.0000, 0.0391],
        [0.0471, 0.0000, 0.0481,  ..., 0.0000, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2979347.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35617.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(680.8472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4783.7271, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15190.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3229.6157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7158],
        [-0.7744],
        [-0.8187],
        ...,
        [-0.6281],
        [-0.6260],
        [-0.6253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-986715.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1326.8253, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1326.8253, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1910e-02,  2.7874e-02,  2.0602e-02,  ..., -3.3607e-02,
          4.4329e-02, -3.6065e-02],
        [ 3.4838e-02,  4.8287e-02,  3.6445e-02,  ..., -5.8224e-02,
          7.5530e-02, -6.2484e-02],
        [ 1.0102e-02,  9.2295e-03,  6.1324e-03,  ..., -1.1122e-02,
          1.5832e-02, -1.1936e-02],
        ...,
        [ 4.2614e-03,  6.7921e-06, -1.0253e-03,  ...,  0.0000e+00,
          1.7354e-03,  0.0000e+00],
        [ 4.2614e-03,  6.7921e-06, -1.0253e-03,  ...,  0.0000e+00,
          1.7354e-03,  0.0000e+00],
        [ 4.2614e-03,  6.7921e-06, -1.0253e-03,  ...,  0.0000e+00,
          1.7354e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7052.8462, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(951.8619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.9290, device='cuda:0')



h[100].sum tensor(-321.8514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(199.7240, device='cuda:0')



h[200].sum tensor(-103.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.6409e-02, 1.0955e-01, 8.0900e-02,  ..., 0.0000e+00, 1.7434e-01,
         0.0000e+00],
        [7.1056e-02, 8.5308e-02, 6.2085e-02,  ..., 0.0000e+00, 1.3729e-01,
         0.0000e+00],
        [1.0011e-01, 1.3119e-01, 9.7694e-02,  ..., 0.0000e+00, 2.0742e-01,
         0.0000e+00],
        ...,
        [1.7045e-02, 2.7169e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9414e-03,
         0.0000e+00],
        [1.7045e-02, 2.7169e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9414e-03,
         0.0000e+00],
        [1.7045e-02, 2.7169e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9414e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(299831.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0699, 0.0000,  ..., 0.0000, 0.0393, 0.0932],
        [0.0000, 0.0754, 0.0000,  ..., 0.0000, 0.0434, 0.0974],
        [0.0000, 0.0871, 0.0000,  ..., 0.0000, 0.0527, 0.1035],
        ...,
        [0.0480, 0.0000, 0.0487,  ..., 0.0000, 0.0000, 0.0393],
        [0.0480, 0.0000, 0.0487,  ..., 0.0000, 0.0000, 0.0393],
        [0.0480, 0.0000, 0.0487,  ..., 0.0000, 0.0000, 0.0393]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2901546.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36610.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(640.6409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4746.4697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12417.2236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3108.1326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1443],
        [ 0.1537],
        [ 0.1601],
        ...,
        [-0.6362],
        [-0.6341],
        [-0.6335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1021489.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1307.4753, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1307.4753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2793e-03,  1.3343e-05, -1.0224e-03,  ...,  0.0000e+00,
          1.7455e-03,  0.0000e+00],
        [ 4.2793e-03,  1.3343e-05, -1.0224e-03,  ...,  0.0000e+00,
          1.7455e-03,  0.0000e+00],
        [ 9.0954e-03,  7.6177e-03,  4.8793e-03,  ..., -9.1634e-03,
          1.3370e-02, -9.8343e-03],
        ...,
        [ 4.2793e-03,  1.3343e-05, -1.0224e-03,  ...,  0.0000e+00,
          1.7455e-03,  0.0000e+00],
        [ 4.2793e-03,  1.3343e-05, -1.0224e-03,  ...,  0.0000e+00,
          1.7455e-03,  0.0000e+00],
        [ 4.2793e-03,  1.3343e-05, -1.0224e-03,  ...,  0.0000e+00,
          1.7455e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7095.2256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(954.1455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.0696, device='cuda:0')



h[100].sum tensor(-325.9549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(196.8113, device='cuda:0')



h[200].sum tensor(-101.5347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7117e-02, 5.3374e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9820e-03,
         0.0000e+00],
        [2.1933e-02, 7.6577e-03, 4.8793e-03,  ..., 0.0000e+00, 1.8607e-02,
         0.0000e+00],
        [2.7687e-02, 1.6743e-02, 1.0908e-02,  ..., 0.0000e+00, 3.2496e-02,
         0.0000e+00],
        ...,
        [1.7117e-02, 5.3374e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9820e-03,
         0.0000e+00],
        [1.7117e-02, 5.3374e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9820e-03,
         0.0000e+00],
        [1.7117e-02, 5.3374e-05, 0.0000e+00,  ..., 0.0000e+00, 6.9820e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(299506.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0441, 0.0000, 0.0383,  ..., 0.0000, 0.0000, 0.0420],
        [0.0350, 0.0000, 0.0200,  ..., 0.0000, 0.0000, 0.0471],
        [0.0240, 0.0016, 0.0062,  ..., 0.0000, 0.0000, 0.0534],
        ...,
        [0.0483, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0395],
        [0.0483, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0395],
        [0.0483, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2919738., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36620.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(643.4880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4763.2256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12496.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3109.4648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2439],
        [-0.1553],
        [-0.0552],
        ...,
        [-0.6416],
        [-0.6395],
        [-0.6388]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1017197.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1315.9734, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1315.9734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        ...,
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7238.8032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(957.6779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.4470, device='cuda:0')



h[100].sum tensor(-324.2754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.0905, device='cuda:0')



h[200].sum tensor(-103.0223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.2314e-02, 8.2303e-03, 5.2926e-03,  ..., 0.0000e+00, 1.9535e-02,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        ...,
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(306081.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0020, 0.0176,  ..., 0.0000, 0.0000, 0.0493],
        [0.0434, 0.0000, 0.0339,  ..., 0.0000, 0.0000, 0.0425],
        [0.0472, 0.0000, 0.0467,  ..., 0.0000, 0.0000, 0.0404],
        ...,
        [0.0481, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0395],
        [0.0481, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0395],
        [0.0481, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2957019.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35807.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(666.1821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4801.8179, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13303.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3163.1321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2738],
        [-0.4352],
        [-0.5163],
        ...,
        [-0.6436],
        [-0.6415],
        [-0.6411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-981468.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1424.9758, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1424.9758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        ...,
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00],
        [ 4.2903e-03,  2.3828e-05, -1.0212e-03,  ...,  0.0000e+00,
          1.7739e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7372.1084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(964.2137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.2882, device='cuda:0')



h[100].sum tensor(-309.3754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(214.4984, device='cuda:0')



h[200].sum tensor(-111.1377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        ...,
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00],
        [1.7161e-02, 9.5311e-05, 0.0000e+00,  ..., 0.0000e+00, 7.0958e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(318203.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0397],
        [0.0482, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0397],
        [0.0483, 0.0000, 0.0492,  ..., 0.0000, 0.0000, 0.0398],
        ...,
        [0.0481, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0395],
        [0.0481, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0395],
        [0.0481, 0.0000, 0.0491,  ..., 0.0000, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3036095., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35565.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(698.5089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4842.1768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15369.3604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3242.7976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8444],
        [-0.8116],
        [-0.7530],
        ...,
        [-0.6447],
        [-0.6425],
        [-0.6419]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-970369.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1456.9214, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.9214, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2932e-03,  3.8653e-05, -1.0213e-03,  ...,  0.0000e+00,
          1.8159e-03,  0.0000e+00],
        [ 4.2932e-03,  3.8653e-05, -1.0213e-03,  ...,  0.0000e+00,
          1.8159e-03,  0.0000e+00],
        [ 4.2932e-03,  3.8653e-05, -1.0213e-03,  ...,  0.0000e+00,
          1.8159e-03,  0.0000e+00],
        ...,
        [ 4.2932e-03,  3.8653e-05, -1.0213e-03,  ...,  0.0000e+00,
          1.8159e-03,  0.0000e+00],
        [ 4.2932e-03,  3.8653e-05, -1.0213e-03,  ...,  0.0000e+00,
          1.8159e-03,  0.0000e+00],
        [ 4.2932e-03,  3.8653e-05, -1.0213e-03,  ...,  0.0000e+00,
          1.8159e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7571.2217, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(966.7927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.7070, device='cuda:0')



h[100].sum tensor(-306.4802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(219.3071, device='cuda:0')



h[200].sum tensor(-113.4873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        ...,
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0073, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(319029.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0476, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0396],
        [0.0476, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0396],
        [0.0477, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0397],
        ...,
        [0.0475, 0.0000, 0.0489,  ..., 0.0000, 0.0000, 0.0394],
        [0.0475, 0.0000, 0.0489,  ..., 0.0000, 0.0000, 0.0394],
        [0.0475, 0.0000, 0.0489,  ..., 0.0000, 0.0000, 0.0394]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3012187.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34556.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(701.9557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4858.6475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14744.5439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3259.8252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6246],
        [-0.7290],
        [-0.8033],
        ...,
        [-0.6449],
        [-0.6427],
        [-0.6421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-930262.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 4800 loss: tensor(478.9512, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1438.4025, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1438.4025, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7284e-03,  5.4477e-03,  3.1801e-03,  ..., -6.5011e-03,
          1.0119e-02, -6.9781e-03],
        [ 9.1468e-03,  7.6875e-03,  4.9185e-03,  ..., -9.1940e-03,
          1.3546e-02, -9.8685e-03],
        [ 1.6514e-02,  1.9322e-02,  1.3948e-02,  ..., -2.3182e-02,
          3.1345e-02, -2.4882e-02],
        ...,
        [ 4.3041e-03,  4.0335e-05, -1.0166e-03,  ...,  0.0000e+00,
          1.8466e-03,  0.0000e+00],
        [ 4.3041e-03,  4.0335e-05, -1.0166e-03,  ...,  0.0000e+00,
          1.8466e-03,  0.0000e+00],
        [ 4.3041e-03,  4.0335e-05, -1.0166e-03,  ...,  0.0000e+00,
          1.8466e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7697.3521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(967.4312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.8845, device='cuda:0')



h[100].sum tensor(-311.8872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(216.5195, device='cuda:0')



h[200].sum tensor(-111.3903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0445, 0.0433, 0.0294,  ..., 0.0000, 0.0734, 0.0000],
        [0.0396, 0.0355, 0.0244,  ..., 0.0000, 0.0615, 0.0000],
        [0.0450, 0.0441, 0.0310,  ..., 0.0000, 0.0746, 0.0000],
        ...,
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0172, 0.0002, 0.0000,  ..., 0.0000, 0.0074, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(321816.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0170, 0.0000,  ..., 0.0000, 0.0000, 0.0692],
        [0.0000, 0.0238, 0.0000,  ..., 0.0000, 0.0041, 0.0716],
        [0.0000, 0.0347, 0.0000,  ..., 0.0000, 0.0111, 0.0764],
        ...,
        [0.0472, 0.0000, 0.0489,  ..., 0.0000, 0.0000, 0.0391],
        [0.0472, 0.0000, 0.0489,  ..., 0.0000, 0.0000, 0.0391],
        [0.0472, 0.0000, 0.0489,  ..., 0.0000, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3050832.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34008.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(710.4060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4875.7251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15522.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3278.5388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2439],
        [ 0.2327],
        [ 0.2189],
        ...,
        [-0.6471],
        [-0.6449],
        [-0.6443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-905234.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1282.2228, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1282.2228, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7259e-02,  2.0469e-02,  1.4846e-02,  ..., -2.4545e-02,
          3.3129e-02, -2.6348e-02],
        [ 2.0810e-02,  2.6077e-02,  1.9199e-02,  ..., -3.1283e-02,
          4.1711e-02, -3.3580e-02],
        [ 2.3565e-02,  3.0427e-02,  2.2575e-02,  ..., -3.6508e-02,
          4.8367e-02, -3.9189e-02],
        ...,
        [ 4.3209e-03,  3.8457e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8652e-03,  0.0000e+00],
        [ 4.3209e-03,  3.8457e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8652e-03,  0.0000e+00],
        [ 4.3209e-03,  3.8457e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8652e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7616.3018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(960.8237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.9480, device='cuda:0')



h[100].sum tensor(-336.3909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.0101, device='cuda:0')



h[200].sum tensor(-98.8505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.2137e-02, 8.6774e-02, 6.3185e-02,  ..., 0.0000e+00, 1.4001e-01,
         0.0000e+00],
        [8.1443e-02, 1.0147e-01, 7.4591e-02,  ..., 0.0000e+00, 1.6250e-01,
         0.0000e+00],
        [7.9286e-02, 9.8062e-02, 7.1947e-02,  ..., 0.0000e+00, 1.5728e-01,
         0.0000e+00],
        ...,
        [1.7284e-02, 1.5383e-04, 0.0000e+00,  ..., 0.0000e+00, 7.4609e-03,
         0.0000e+00],
        [1.7284e-02, 1.5383e-04, 0.0000e+00,  ..., 0.0000e+00, 7.4609e-03,
         0.0000e+00],
        [1.7284e-02, 1.5383e-04, 0.0000e+00,  ..., 0.0000e+00, 7.4609e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(299944.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0602, 0.0000,  ..., 0.0000, 0.0296, 0.0901],
        [0.0000, 0.0715, 0.0000,  ..., 0.0000, 0.0384, 0.0951],
        [0.0000, 0.0687, 0.0000,  ..., 0.0000, 0.0362, 0.0937],
        ...,
        [0.0473, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0388],
        [0.0473, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0388],
        [0.0473, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2910292.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34773.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(648.6426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4807.1611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11340.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3125.6528, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1818],
        [ 0.1852],
        [ 0.1887],
        ...,
        [-0.6467],
        [-0.6311],
        [-0.5634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-946039.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1277.3702, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1277.3702, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.3372e-03,  4.0326e-05, -1.0093e-03,  ...,  0.0000e+00,
          1.8760e-03,  0.0000e+00],
        [ 4.3372e-03,  4.0326e-05, -1.0093e-03,  ...,  0.0000e+00,
          1.8760e-03,  0.0000e+00],
        [ 4.3372e-03,  4.0326e-05, -1.0093e-03,  ...,  0.0000e+00,
          1.8760e-03,  0.0000e+00],
        ...,
        [ 4.3372e-03,  4.0326e-05, -1.0093e-03,  ...,  0.0000e+00,
          1.8760e-03,  0.0000e+00],
        [ 4.3372e-03,  4.0326e-05, -1.0093e-03,  ...,  0.0000e+00,
          1.8760e-03,  0.0000e+00],
        [ 4.3372e-03,  4.0326e-05, -1.0093e-03,  ...,  0.0000e+00,
          1.8760e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7708.3960, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(964.1605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.7325, device='cuda:0')



h[100].sum tensor(-337.8123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(192.2797, device='cuda:0')



h[200].sum tensor(-98.7735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0173, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0173, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0173, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        ...,
        [0.0173, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0173, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0173, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(304535.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0467, 0.0000, 0.0451,  ..., 0.0000, 0.0000, 0.0396],
        [0.0477, 0.0000, 0.0493,  ..., 0.0000, 0.0000, 0.0390],
        [0.0477, 0.0000, 0.0493,  ..., 0.0000, 0.0000, 0.0391],
        ...,
        [0.0475, 0.0000, 0.0492,  ..., 0.0000, 0.0000, 0.0388],
        [0.0475, 0.0000, 0.0492,  ..., 0.0000, 0.0000, 0.0388],
        [0.0475, 0.0000, 0.0492,  ..., 0.0000, 0.0000, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2951403.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34377.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(670.0117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4835.2988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12065.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3156.9912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7450],
        [-0.8319],
        [-0.8763],
        ...,
        [-0.6568],
        [-0.6546],
        [-0.6540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-931537.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1555.1263, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1555.1263, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2967e-02,  1.3658e-02,  9.5500e-03,  ..., -1.6324e-02,
          2.2712e-02, -1.7525e-02],
        [ 1.4964e-02,  1.6812e-02,  1.1997e-02,  ..., -2.0107e-02,
          2.7539e-02, -2.1585e-02],
        [ 4.3518e-03,  4.9476e-05, -1.0115e-03,  ...,  0.0000e+00,
          1.8812e-03,  0.0000e+00],
        ...,
        [ 4.3518e-03,  4.9476e-05, -1.0115e-03,  ...,  0.0000e+00,
          1.8812e-03,  0.0000e+00],
        [ 4.3518e-03,  4.9476e-05, -1.0115e-03,  ...,  0.0000e+00,
          1.8812e-03,  0.0000e+00],
        [ 4.3518e-03,  4.9476e-05, -1.0115e-03,  ...,  0.0000e+00,
          1.8812e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8138.2939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(984.6318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.0686, device='cuda:0')



h[100].sum tensor(-299.1399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(234.0897, device='cuda:0')



h[200].sum tensor(-120.2975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0671, 0.0786, 0.0568,  ..., 0.0000, 0.1276, 0.0000],
        [0.0380, 0.0327, 0.0232,  ..., 0.0000, 0.0573, 0.0000],
        [0.0280, 0.0170, 0.0120,  ..., 0.0000, 0.0332, 0.0000],
        ...,
        [0.0174, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0174, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0174, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(335808.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0432, 0.0000,  ..., 0.0000, 0.0164, 0.0802],
        [0.0131, 0.0169, 0.0000,  ..., 0.0000, 0.0045, 0.0639],
        [0.0272, 0.0044, 0.0111,  ..., 0.0000, 0.0000, 0.0520],
        ...,
        [0.0479, 0.0000, 0.0497,  ..., 0.0000, 0.0000, 0.0390],
        [0.0479, 0.0000, 0.0497,  ..., 0.0000, 0.0000, 0.0390],
        [0.0479, 0.0000, 0.0497,  ..., 0.0000, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3137917., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34390.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(753.9784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4950.3999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16905.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3369.0178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1869],
        [ 0.1202],
        [-0.0426],
        ...,
        [-0.6623],
        [-0.6599],
        [-0.6590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-937280.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1302.0936, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1302.0936, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6238e-03,  8.3652e-03,  5.4340e-03,  ..., -9.9568e-03,
          1.4603e-02, -1.0690e-02],
        [ 1.7146e-02,  2.0253e-02,  1.4658e-02,  ..., -2.4204e-02,
          3.2799e-02, -2.5985e-02],
        [ 9.6238e-03,  8.3652e-03,  5.4340e-03,  ..., -9.9568e-03,
          1.4603e-02, -1.0690e-02],
        ...,
        [ 4.3668e-03,  5.7680e-05, -1.0121e-03,  ...,  0.0000e+00,
          1.8870e-03,  0.0000e+00],
        [ 4.3668e-03,  5.7680e-05, -1.0121e-03,  ...,  0.0000e+00,
          1.8870e-03,  0.0000e+00],
        [ 4.3668e-03,  5.7680e-05, -1.0121e-03,  ...,  0.0000e+00,
          1.8870e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7874.4814, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(971.8973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.8305, device='cuda:0')



h[100].sum tensor(-336.6290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(196.0012, device='cuda:0')



h[200].sum tensor(-100.7062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0682, 0.0804, 0.0581,  ..., 0.0000, 0.1302, 0.0000],
        [0.0470, 0.0469, 0.0322,  ..., 0.0000, 0.0790, 0.0000],
        [0.0345, 0.0272, 0.0189,  ..., 0.0000, 0.0488, 0.0000],
        ...,
        [0.0175, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0175, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0175, 0.0002, 0.0000,  ..., 0.0000, 0.0075, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(307095.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0219, 0.0000,  ..., 0.0000, 0.0013, 0.0675],
        [0.0074, 0.0130, 0.0000,  ..., 0.0000, 0.0006, 0.0637],
        [0.0196, 0.0074, 0.0025,  ..., 0.0000, 0.0007, 0.0565],
        ...,
        [0.0481, 0.0000, 0.0500,  ..., 0.0000, 0.0000, 0.0393],
        [0.0481, 0.0000, 0.0500,  ..., 0.0000, 0.0000, 0.0393],
        [0.0481, 0.0000, 0.0500,  ..., 0.0000, 0.0000, 0.0393]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2976815.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34806.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(683.7307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4869.4424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12015.1914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3179.4661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1108],
        [ 0.0170],
        [-0.1541],
        ...,
        [-0.6290],
        [-0.6554],
        [-0.6616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-948605.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1600.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1600.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7053e-02,  2.0101e-02,  1.4525e-02,  ..., -2.3984e-02,
          3.2549e-02, -2.5750e-02],
        [ 1.6804e-02,  1.9708e-02,  1.4220e-02,  ..., -2.3513e-02,
          3.1948e-02, -2.5245e-02],
        [ 1.6903e-02,  1.9864e-02,  1.4341e-02,  ..., -2.3700e-02,
          3.2187e-02, -2.5446e-02],
        ...,
        [ 4.3829e-03,  7.3704e-05, -1.0133e-03,  ...,  0.0000e+00,
          1.8922e-03,  0.0000e+00],
        [ 4.3829e-03,  7.3704e-05, -1.0133e-03,  ...,  0.0000e+00,
          1.8922e-03,  0.0000e+00],
        [ 4.3829e-03,  7.3704e-05, -1.0133e-03,  ...,  0.0000e+00,
          1.8922e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8341.7471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(993.9325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.1015, device='cuda:0')



h[100].sum tensor(-295.1604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(240.9797, device='cuda:0')



h[200].sum tensor(-123.7311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0640, 0.0737, 0.0529,  ..., 0.0000, 0.1199, 0.0000],
        [0.0633, 0.0726, 0.0521,  ..., 0.0000, 0.1183, 0.0000],
        [0.0717, 0.0859, 0.0623,  ..., 0.0000, 0.1385, 0.0000],
        ...,
        [0.0175, 0.0003, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0175, 0.0003, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0175, 0.0003, 0.0000,  ..., 0.0000, 0.0076, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(339627.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0805, 0.0000,  ..., 0.0000, 0.0440, 0.0997],
        [0.0000, 0.0804, 0.0000,  ..., 0.0000, 0.0439, 0.0996],
        [0.0000, 0.0852, 0.0000,  ..., 0.0000, 0.0476, 0.1018],
        ...,
        [0.0483, 0.0000, 0.0503,  ..., 0.0000, 0.0000, 0.0397],
        [0.0483, 0.0000, 0.0503,  ..., 0.0000, 0.0000, 0.0397],
        [0.0483, 0.0000, 0.0503,  ..., 0.0000, 0.0000, 0.0397]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3150180., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34215.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(774.2218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4992.7656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16475.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3403.9255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1470],
        [ 0.1533],
        [ 0.1587],
        ...,
        [-0.6729],
        [-0.6707],
        [-0.6700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-919465.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1465.0439, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1465.0439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3108e-02,  1.3847e-02,  9.6655e-03,  ..., -1.6468e-02,
          2.2966e-02, -1.7682e-02],
        [ 4.4026e-03,  8.3964e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8967e-03,  0.0000e+00],
        [ 4.4026e-03,  8.3964e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8967e-03,  0.0000e+00],
        ...,
        [ 4.4026e-03,  8.3964e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8967e-03,  0.0000e+00],
        [ 4.4026e-03,  8.3964e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8967e-03,  0.0000e+00],
        [ 4.4026e-03,  8.3964e-05, -1.0113e-03,  ...,  0.0000e+00,
          1.8967e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8236.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(989.4083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.0677, device='cuda:0')



h[100].sum tensor(-315.9785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(220.5298, device='cuda:0')



h[200].sum tensor(-113.0957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0218, 0.0136,  ..., 0.0000, 0.0405, 0.0000],
        [0.0294, 0.0190, 0.0124,  ..., 0.0000, 0.0361, 0.0000],
        [0.0176, 0.0003, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        ...,
        [0.0176, 0.0003, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0176, 0.0003, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0176, 0.0003, 0.0000,  ..., 0.0000, 0.0076, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(322648.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0624],
        [0.0181, 0.0037, 0.0069,  ..., 0.0000, 0.0000, 0.0565],
        [0.0356, 0.0000, 0.0230,  ..., 0.0000, 0.0000, 0.0471],
        ...,
        [0.0486, 0.0000, 0.0505,  ..., 0.0000, 0.0000, 0.0400],
        [0.0486, 0.0000, 0.0505,  ..., 0.0000, 0.0000, 0.0400],
        [0.0486, 0.0000, 0.0505,  ..., 0.0000, 0.0000, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3066849.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35359.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(722.0803, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4944.6797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14001.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3288.3264, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2238],
        [ 0.1400],
        [-0.0064],
        ...,
        [-0.6579],
        [-0.6587],
        [-0.6625]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-984178.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1407.5692, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1407.5692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4122e-02,  1.5431e-02,  1.0884e-02,  ..., -1.8332e-02,
          2.5375e-02, -1.9684e-02],
        [ 4.4241e-03,  9.8920e-05, -1.0102e-03,  ...,  0.0000e+00,
          1.9005e-03,  0.0000e+00],
        [ 2.2480e-02,  2.8645e-02,  2.1134e-02,  ..., -3.4131e-02,
          4.5606e-02, -3.6649e-02],
        ...,
        [ 4.4241e-03,  9.8920e-05, -1.0102e-03,  ...,  0.0000e+00,
          1.9005e-03,  0.0000e+00],
        [ 4.4241e-03,  9.8920e-05, -1.0102e-03,  ...,  0.0000e+00,
          1.9005e-03,  0.0000e+00],
        [ 4.4241e-03,  9.8920e-05, -1.0102e-03,  ...,  0.0000e+00,
          1.9005e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8224.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(989.9753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.5151, device='cuda:0')



h[100].sum tensor(-325.7975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(211.8782, device='cuda:0')



h[200].sum tensor(-108.2624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0256, 0.0129, 0.0087,  ..., 0.0000, 0.0268, 0.0000],
        [0.0681, 0.0801, 0.0578,  ..., 0.0000, 0.1296, 0.0000],
        [0.0559, 0.0608, 0.0439,  ..., 0.0000, 0.1001, 0.0000],
        ...,
        [0.0177, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0177, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0177, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(315820.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0080, 0.0037,  ..., 0.0000, 0.0000, 0.0563],
        [0.0050, 0.0306, 0.0000,  ..., 0.0000, 0.0085, 0.0730],
        [0.0025, 0.0360, 0.0000,  ..., 0.0000, 0.0105, 0.0765],
        ...,
        [0.0489, 0.0000, 0.0508,  ..., 0.0000, 0.0000, 0.0404],
        [0.0489, 0.0000, 0.0508,  ..., 0.0000, 0.0000, 0.0404],
        [0.0489, 0.0000, 0.0508,  ..., 0.0000, 0.0000, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3039799., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35226.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(713.3064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4941.5815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12792.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3247.1626, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1391],
        [ 0.0232],
        [ 0.1094],
        ...,
        [-0.6865],
        [-0.6840],
        [-0.6828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-973541., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1629.8264, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1629.8264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4555e-03,  9.6512e-05, -9.9995e-04,  ...,  0.0000e+00,
          1.9085e-03,  0.0000e+00],
        [ 4.4555e-03,  9.6512e-05, -9.9995e-04,  ...,  0.0000e+00,
          1.9085e-03,  0.0000e+00],
        [ 4.4555e-03,  9.6512e-05, -9.9995e-04,  ...,  0.0000e+00,
          1.9085e-03,  0.0000e+00],
        ...,
        [ 4.4555e-03,  9.6512e-05, -9.9995e-04,  ...,  0.0000e+00,
          1.9085e-03,  0.0000e+00],
        [ 4.4555e-03,  9.6512e-05, -9.9995e-04,  ...,  0.0000e+00,
          1.9085e-03,  0.0000e+00],
        [ 4.4555e-03,  9.6512e-05, -9.9995e-04,  ...,  0.0000e+00,
          1.9085e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8639.8848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1009.5546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.3863, device='cuda:0')



h[100].sum tensor(-297.2452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(245.3341, device='cuda:0')



h[200].sum tensor(-124.2954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0376, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0178, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0178, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0240, 0.0102, 0.0056,  ..., 0.0000, 0.0226, 0.0000],
        ...,
        [0.0178, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0178, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0178, 0.0004, 0.0000,  ..., 0.0000, 0.0076, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(335613.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0467, 0.0000, 0.0466,  ..., 0.0000, 0.0000, 0.0419],
        [0.0387, 0.0000, 0.0288,  ..., 0.0000, 0.0000, 0.0459],
        [0.0207, 0.0016, 0.0124,  ..., 0.0000, 0.0000, 0.0552],
        ...,
        [0.0493, 0.0000, 0.0508,  ..., 0.0000, 0.0000, 0.0404],
        [0.0493, 0.0000, 0.0508,  ..., 0.0000, 0.0000, 0.0404],
        [0.0493, 0.0000, 0.0508,  ..., 0.0000, 0.0000, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3142487.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36280.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(752.0107, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5009.4385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15414.7744, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3359.6382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3066],
        [-0.0908],
        [ 0.0805],
        ...,
        [-0.6928],
        [-0.6906],
        [-0.6899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1040772.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1336.0100, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.0100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        ...,
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8362.7207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(996.0413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.3369, device='cuda:0')



h[100].sum tensor(-339.1894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(201.1066, device='cuda:0')



h[200].sum tensor(-102.2310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0179, 0.0004, 0.0000,  ..., 0.0000, 0.0077, 0.0000],
        [0.0179, 0.0004, 0.0000,  ..., 0.0000, 0.0077, 0.0000],
        [0.0281, 0.0165, 0.0115,  ..., 0.0000, 0.0324, 0.0000],
        ...,
        [0.0179, 0.0004, 0.0000,  ..., 0.0000, 0.0077, 0.0000],
        [0.0179, 0.0004, 0.0000,  ..., 0.0000, 0.0077, 0.0000],
        [0.0179, 0.0004, 0.0000,  ..., 0.0000, 0.0077, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(317717.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0484, 0.0000, 0.0468,  ..., 0.0000, 0.0000, 0.0415],
        [0.0440, 0.0000, 0.0324,  ..., 0.0000, 0.0000, 0.0439],
        [0.0316, 0.0027, 0.0122,  ..., 0.0000, 0.0000, 0.0512],
        ...,
        [0.0493, 0.0000, 0.0507,  ..., 0.0000, 0.0000, 0.0407],
        [0.0493, 0.0000, 0.0507,  ..., 0.0000, 0.0000, 0.0407],
        [0.0493, 0.0000, 0.0507,  ..., 0.0000, 0.0000, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3091339.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36462.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(703.1857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4961.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13771.6162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3240.5354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5807],
        [-0.3548],
        [-0.1015],
        ...,
        [-0.6973],
        [-0.6951],
        [-0.6944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1050583.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 5100 loss: tensor(528.2152, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1497.1333, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1497.1333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0131,  0.0137,  0.0096,  ..., -0.0162,  0.0228, -0.0174],
        [ 0.0146,  0.0161,  0.0114,  ..., -0.0190,  0.0264, -0.0204],
        [ 0.0131,  0.0137,  0.0096,  ..., -0.0162,  0.0228, -0.0174],
        ...,
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0019,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8677.4902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1008.3572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.4929, device='cuda:0')



h[100].sum tensor(-318.6459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(225.3601, device='cuda:0')



h[200].sum tensor(-113.7301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1099, 0.1458, 0.1087,  ..., 0.0000, 0.2303, 0.0000],
        [0.0753, 0.0912, 0.0663,  ..., 0.0000, 0.1466, 0.0000],
        [0.0490, 0.0496, 0.0351,  ..., 0.0000, 0.0829, 0.0000],
        ...,
        [0.0180, 0.0005, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0180, 0.0005, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0180, 0.0005, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(333048.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1109, 0.0000,  ..., 0.0000, 0.0626, 0.1107],
        [0.0000, 0.0906, 0.0000,  ..., 0.0000, 0.0475, 0.1024],
        [0.0000, 0.0740, 0.0000,  ..., 0.0000, 0.0353, 0.0949],
        ...,
        [0.0490, 0.0000, 0.0506,  ..., 0.0000, 0.0000, 0.0411],
        [0.0490, 0.0000, 0.0506,  ..., 0.0000, 0.0000, 0.0411],
        [0.0490, 0.0000, 0.0506,  ..., 0.0000, 0.0000, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3167530.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34763.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(752.3781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5031.9873, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15375.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3354.3020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1717],
        [ 0.1690],
        [ 0.1693],
        ...,
        [-0.7006],
        [-0.6984],
        [-0.6977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-956862.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1376.5242, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1376.5242, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0087,  0.0067,  0.0041,  ..., -0.0078,  0.0120, -0.0084],
        [ 0.0087,  0.0067,  0.0041,  ..., -0.0078,  0.0120, -0.0084],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0045,  0.0001, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8605.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1005.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.1363, device='cuda:0')



h[100].sum tensor(-334.8424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(207.2051, device='cuda:0')



h[200].sum tensor(-105.5126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0256, 0.0125, 0.0072,  ..., 0.0000, 0.0261, 0.0000],
        [0.0256, 0.0125, 0.0072,  ..., 0.0000, 0.0261, 0.0000],
        [0.0256, 0.0125, 0.0072,  ..., 0.0000, 0.0261, 0.0000],
        ...,
        [0.0180, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0180, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0180, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(324177.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0157, 0.0001, 0.0004,  ..., 0.0000, 0.0000, 0.0580],
        [0.0218, 0.0000, 0.0066,  ..., 0.0000, 0.0000, 0.0550],
        [0.0288, 0.0000, 0.0156,  ..., 0.0000, 0.0000, 0.0517],
        ...,
        [0.0489, 0.0000, 0.0506,  ..., 0.0000, 0.0000, 0.0417],
        [0.0489, 0.0000, 0.0506,  ..., 0.0000, 0.0000, 0.0417],
        [0.0489, 0.0000, 0.0506,  ..., 0.0000, 0.0000, 0.0417]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3135294.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34134.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(737.5906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5024.5547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14213.5771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3306.6736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0861],
        [-0.2721],
        [-0.4703],
        ...,
        [-0.6714],
        [-0.6842],
        [-0.6944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-929110.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1584.7567, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1584.7567, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0045,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0045,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0045,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0045,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0045,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8965.3232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1022.7280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.3846, device='cuda:0')



h[100].sum tensor(-306.6046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(238.5499, device='cuda:0')



h[200].sum tensor(-121.1767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0181, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0181, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        ...,
        [0.0181, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0181, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0181, 0.0006, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350235.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0481, 0.0000, 0.0451,  ..., 0.0000, 0.0000, 0.0430],
        [0.0493, 0.0000, 0.0510,  ..., 0.0000, 0.0000, 0.0422],
        [0.0494, 0.0000, 0.0510,  ..., 0.0000, 0.0000, 0.0423],
        ...,
        [0.0492, 0.0000, 0.0509,  ..., 0.0000, 0.0000, 0.0420],
        [0.0492, 0.0000, 0.0509,  ..., 0.0000, 0.0000, 0.0420],
        [0.0492, 0.0000, 0.0509,  ..., 0.0000, 0.0000, 0.0420]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3303534., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(33919.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(806.7505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5129.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18457.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3481.5696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6726],
        [-0.8130],
        [-0.8919],
        ...,
        [-0.7119],
        [-0.7096],
        [-0.7089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-922731.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1194.7529, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1194.7529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0135,  0.0143,  0.0100,  ..., -0.0168,  0.0236, -0.0181],
        [ 0.0135,  0.0143,  0.0100,  ..., -0.0168,  0.0236, -0.0181],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8450.5225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1003.9460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.0632, device='cuda:0')



h[100].sum tensor(-362.3214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(179.8435, device='cuda:0')



h[200].sum tensor(-91.8077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0345, 0.0263, 0.0179,  ..., 0.0000, 0.0471, 0.0000],
        [0.0345, 0.0263, 0.0179,  ..., 0.0000, 0.0471, 0.0000],
        [0.0410, 0.0366, 0.0239,  ..., 0.0000, 0.0629, 0.0000],
        ...,
        [0.0182, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0182, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0182, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(302905.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0119, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0633],
        [0.0067, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0671],
        [0.0000, 0.0151, 0.0000,  ..., 0.0000, 0.0000, 0.0719],
        ...,
        [0.0499, 0.0000, 0.0514,  ..., 0.0000, 0.0000, 0.0424],
        [0.0499, 0.0000, 0.0514,  ..., 0.0000, 0.0000, 0.0424],
        [0.0499, 0.0000, 0.0514,  ..., 0.0000, 0.0000, 0.0424]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3034384.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36706.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(670.3894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4983.7466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10789.7021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3160.3120, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1895],
        [ 0.2560],
        [ 0.2746],
        ...,
        [-0.7200],
        [-0.7176],
        [-0.7169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1063935.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1442.1395, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1442.1395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8814.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1022.4557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.0505, device='cuda:0')



h[100].sum tensor(-330.5302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(217.0820, device='cuda:0')



h[200].sum tensor(-109.3998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0183, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0183, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0183, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        ...,
        [0.0183, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0183, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0183, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(328217.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0496, 0.0000, 0.0485,  ..., 0.0000, 0.0000, 0.0435],
        [0.0498, 0.0000, 0.0512,  ..., 0.0000, 0.0000, 0.0433],
        [0.0482, 0.0000, 0.0498,  ..., 0.0000, 0.0000, 0.0442],
        ...,
        [0.0504, 0.0000, 0.0518,  ..., 0.0000, 0.0000, 0.0428],
        [0.0504, 0.0000, 0.0518,  ..., 0.0000, 0.0000, 0.0428],
        [0.0504, 0.0000, 0.0518,  ..., 0.0000, 0.0000, 0.0428]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3176286., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35656.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(753.5876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5093.2974, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13976.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3336.2566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6813],
        [-0.6405],
        [-0.5198],
        ...,
        [-0.7279],
        [-0.7256],
        [-0.7248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-989903.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1488.8794, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1488.8794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0019,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0019,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8941.4824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1029.6689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.1264, device='cuda:0')



h[100].sum tensor(-324.2775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(224.1177, device='cuda:0')



h[200].sum tensor(-113.4943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        ...,
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(329988.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0466, 0.0000, 0.0415,  ..., 0.0000, 0.0000, 0.0454],
        [0.0504, 0.0000, 0.0507,  ..., 0.0000, 0.0000, 0.0435],
        [0.0512, 0.0000, 0.0523,  ..., 0.0000, 0.0000, 0.0433],
        ...,
        [0.0510, 0.0000, 0.0522,  ..., 0.0000, 0.0000, 0.0430],
        [0.0510, 0.0000, 0.0522,  ..., 0.0000, 0.0000, 0.0430],
        [0.0510, 0.0000, 0.0522,  ..., 0.0000, 0.0000, 0.0430]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3175257., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36136.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(763.1631, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5117.3232, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13485.4619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3347.7803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3674],
        [-0.5781],
        [-0.7348],
        ...,
        [-0.7358],
        [-0.7334],
        [-0.7327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1009797.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1423.0590, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1423.0590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8948.3496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1027.7352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.2030, device='cuda:0')



h[100].sum tensor(-335.8232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(214.2099, device='cuda:0')



h[200].sum tensor(-108.2590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        ...,
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0184, 0.0007, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(330006.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0457, 0.0000, 0.0369,  ..., 0.0000, 0.0000, 0.0461],
        [0.0457, 0.0000, 0.0367,  ..., 0.0000, 0.0000, 0.0461],
        [0.0443, 0.0000, 0.0323,  ..., 0.0000, 0.0000, 0.0470],
        ...,
        [0.0510, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0431],
        [0.0510, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0431],
        [0.0510, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0431]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3208330., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36207.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(762.9055, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5132.1699, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14319.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3353.4858, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0608],
        [-0.0119],
        [ 0.0519],
        ...,
        [-0.7406],
        [-0.7382],
        [-0.7375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1024352.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1221.1542, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1221.1542, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0197,  0.0240,  0.0175,  ..., -0.0283,  0.0385, -0.0304],
        [ 0.0183,  0.0218,  0.0158,  ..., -0.0256,  0.0351, -0.0275],
        [ 0.0197,  0.0240,  0.0175,  ..., -0.0283,  0.0385, -0.0304],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8805.7383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1019.1304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.2357, device='cuda:0')



h[100].sum tensor(-366.7080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(183.8176, device='cuda:0')



h[200].sum tensor(-92.8083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0445, 0.0418, 0.0299,  ..., 0.0000, 0.0708, 0.0000],
        [0.0844, 0.1049, 0.0769,  ..., 0.0000, 0.1677, 0.0000],
        [0.0846, 0.1053, 0.0772,  ..., 0.0000, 0.1682, 0.0000],
        ...,
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(311899.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0050, 0.0321, 0.0000,  ..., 0.0000, 0.0063, 0.0722],
        [0.0000, 0.0527, 0.0000,  ..., 0.0000, 0.0160, 0.0809],
        [0.0000, 0.0523, 0.0000,  ..., 0.0000, 0.0156, 0.0807],
        ...,
        [0.0471, 0.0000, 0.0424,  ..., 0.0000, 0.0000, 0.0447],
        [0.0501, 0.0000, 0.0501,  ..., 0.0000, 0.0000, 0.0432],
        [0.0510, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0428]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3122262.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37617.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(696.7950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5073.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11806.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3217.2639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2309],
        [ 0.2186],
        [ 0.2026],
        ...,
        [-0.6077],
        [-0.6878],
        [-0.7257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1117067.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1368.4120, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1368.4120, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9012.9346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1027.9689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.7760, device='cuda:0')



h[100].sum tensor(-346.5394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.9840, device='cuda:0')



h[200].sum tensor(-103.5595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        ...,
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0185, 0.0007, 0.0000,  ..., 0.0000, 0.0079, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(323884.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0503, 0.0000, 0.0506,  ..., 0.0000, 0.0000, 0.0434],
        [0.0511, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0430],
        [0.0512, 0.0000, 0.0525,  ..., 0.0000, 0.0000, 0.0431],
        ...,
        [0.0510, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0428],
        [0.0510, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0428],
        [0.0510, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0428]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3178519., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36945.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(735.8552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5117.3989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13293.6709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3299.0137, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6315],
        [-0.7954],
        [-0.9034],
        ...,
        [-0.7451],
        [-0.7427],
        [-0.7420]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1085865.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1642.2017, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1642.2017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0191,  0.0231,  0.0168,  ..., -0.0272,  0.0371, -0.0292],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9573.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1047.4918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(72.9359, device='cuda:0')



h[100].sum tensor(-307.3549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(247.1969, device='cuda:0')



h[200].sum tensor(-125.2460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0008, 0.0000,  ..., 0.0000, 0.0080, 0.0000],
        [0.0438, 0.0407, 0.0290,  ..., 0.0000, 0.0693, 0.0000],
        [0.0479, 0.0472, 0.0330,  ..., 0.0000, 0.0792, 0.0000],
        ...,
        [0.0185, 0.0008, 0.0000,  ..., 0.0000, 0.0080, 0.0000],
        [0.0185, 0.0008, 0.0000,  ..., 0.0000, 0.0080, 0.0000],
        [0.0185, 0.0008, 0.0000,  ..., 0.0000, 0.0080, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350388.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0202, 0.0065, 0.0037,  ..., 0.0000, 0.0000, 0.0592],
        [0.0088, 0.0280, 0.0000,  ..., 0.0000, 0.0062, 0.0712],
        [0.0034, 0.0407, 0.0000,  ..., 0.0000, 0.0098, 0.0782],
        ...,
        [0.0505, 0.0000, 0.0523,  ..., 0.0000, 0.0000, 0.0428],
        [0.0505, 0.0000, 0.0523,  ..., 0.0000, 0.0000, 0.0428],
        [0.0505, 0.0000, 0.0523,  ..., 0.0000, 0.0000, 0.0428]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3287085., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34630.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(818.2634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5230.8564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15858.9150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3489.6965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2344],
        [ 0.2329],
        [ 0.2322],
        ...,
        [-0.7475],
        [-0.7451],
        [-0.7444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-969286.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 5400 loss: tensor(488.6048, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1432.0946, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1432.0946, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097,  0.0082,  0.0052,  ..., -0.0095,  0.0143, -0.0102],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0097,  0.0082,  0.0052,  ..., -0.0095,  0.0143, -0.0102],
        ...,
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0046,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9376.0547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1036.3314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.6043, device='cuda:0')



h[100].sum tensor(-339.8466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.5700, device='cuda:0')



h[200].sum tensor(-108.7105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0227, 0.0074, 0.0041,  ..., 0.0000, 0.0182, 0.0000],
        [0.0370, 0.0300, 0.0187,  ..., 0.0000, 0.0528, 0.0000],
        [0.0227, 0.0074, 0.0041,  ..., 0.0000, 0.0182, 0.0000],
        ...,
        [0.0186, 0.0008, 0.0000,  ..., 0.0000, 0.0081, 0.0000],
        [0.0186, 0.0008, 0.0000,  ..., 0.0000, 0.0081, 0.0000],
        [0.0186, 0.0008, 0.0000,  ..., 0.0000, 0.0081, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(338255.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0343, 0.0000, 0.0196,  ..., 0.0000, 0.0000, 0.0507],
        [0.0214, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0568],
        [0.0269, 0.0000, 0.0082,  ..., 0.0000, 0.0000, 0.0543],
        ...,
        [0.0502, 0.0000, 0.0522,  ..., 0.0000, 0.0000, 0.0429],
        [0.0502, 0.0000, 0.0522,  ..., 0.0000, 0.0000, 0.0429],
        [0.0502, 0.0000, 0.0522,  ..., 0.0000, 0.0000, 0.0429]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3267908.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34500.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(777.4532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5203.4194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14980.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3411.9124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3265],
        [-0.1241],
        [ 0.0394],
        ...,
        [-0.7497],
        [-0.7474],
        [-0.7468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-966289.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1300.8605, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1300.8605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9231.4277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1030.7507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.7758, device='cuda:0')



h[100].sum tensor(-359.1606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(195.8156, device='cuda:0')



h[200].sum tensor(-98.8951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0186, 0.0009, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0186, 0.0009, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0404, 0.0354, 0.0248,  ..., 0.0000, 0.0610, 0.0000],
        ...,
        [0.0186, 0.0009, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0186, 0.0009, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0186, 0.0009, 0.0000,  ..., 0.0000, 0.0082, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(320478.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0234, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0586],
        [0.0219, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0592],
        [0.0100, 0.0209, 0.0000,  ..., 0.0000, 0.0024, 0.0685],
        ...,
        [0.0502, 0.0000, 0.0525,  ..., 0.0000, 0.0000, 0.0434],
        [0.0502, 0.0000, 0.0525,  ..., 0.0000, 0.0000, 0.0434],
        [0.0502, 0.0000, 0.0525,  ..., 0.0000, 0.0000, 0.0434]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3163981., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35246.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(726.5144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5164.2271, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11992.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3306.5999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1957],
        [ 0.2022],
        [ 0.2117],
        ...,
        [-0.7555],
        [-0.7532],
        [-0.7525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1007831.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1431.3396, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1431.3396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0124,  0.0124,  0.0085,  ..., -0.0144,  0.0207, -0.0155],
        [ 0.0098,  0.0083,  0.0053,  ..., -0.0095,  0.0144, -0.0102],
        ...,
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0002, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9435.6895, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1042.1970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.5708, device='cuda:0')



h[100].sum tensor(-341.4930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.4563, device='cuda:0')



h[200].sum tensor(-108.6961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0264, 0.0132, 0.0085,  ..., 0.0000, 0.0268, 0.0000],
        [0.0301, 0.0190, 0.0120,  ..., 0.0000, 0.0358, 0.0000],
        [0.0614, 0.0685, 0.0484,  ..., 0.0000, 0.1118, 0.0000],
        ...,
        [0.0187, 0.0010, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0187, 0.0010, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0187, 0.0010, 0.0000,  ..., 0.0000, 0.0082, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(338583.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0341, 0.0025, 0.0178,  ..., 0.0000, 0.0000, 0.0531],
        [0.0182, 0.0119, 0.0045,  ..., 0.0000, 0.0005, 0.0622],
        [0.0037, 0.0369, 0.0000,  ..., 0.0000, 0.0101, 0.0780],
        ...,
        [0.0507, 0.0000, 0.0531,  ..., 0.0000, 0.0000, 0.0439],
        [0.0507, 0.0000, 0.0531,  ..., 0.0000, 0.0000, 0.0439],
        [0.0507, 0.0000, 0.0531,  ..., 0.0000, 0.0000, 0.0439]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3277706.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35358.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(776.1466, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5247.5796, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14694.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3439.9055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1158],
        [ 0.0729],
        [ 0.1849],
        ...,
        [-0.7635],
        [-0.7611],
        [-0.7604]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1011971.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.7258, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.7258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0248,  0.0320,  0.0237,  ..., -0.0376,  0.0508, -0.0404],
        [ 0.0221,  0.0277,  0.0203,  ..., -0.0324,  0.0442, -0.0349],
        [ 0.0198,  0.0241,  0.0175,  ..., -0.0282,  0.0387, -0.0303],
        ...,
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9414.4131, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1044.8723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.9662, device='cuda:0')



h[100].sum tensor(-346.1010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.4071, device='cuda:0')



h[200].sum tensor(-106.6830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0560, 0.0598, 0.0426,  ..., 0.0000, 0.0984, 0.0000],
        [0.0996, 0.1288, 0.0952,  ..., 0.0000, 0.2042, 0.0000],
        [0.0885, 0.1112, 0.0816,  ..., 0.0000, 0.1773, 0.0000],
        ...,
        [0.0188, 0.0010, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0188, 0.0010, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0188, 0.0010, 0.0000,  ..., 0.0000, 0.0082, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(329806., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0053, 0.0469, 0.0000,  ..., 0.0000, 0.0160, 0.0809],
        [0.0000, 0.0793, 0.0000,  ..., 0.0000, 0.0348, 0.0952],
        [0.0000, 0.0786, 0.0000,  ..., 0.0000, 0.0341, 0.0963],
        ...,
        [0.0513, 0.0000, 0.0537,  ..., 0.0000, 0.0000, 0.0443],
        [0.0513, 0.0000, 0.0537,  ..., 0.0000, 0.0000, 0.0443],
        [0.0513, 0.0000, 0.0537,  ..., 0.0000, 0.0000, 0.0443]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3229297., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35604.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(763.8799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5244.9473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12637.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3389.5510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1842],
        [ 0.2039],
        [ 0.2051],
        ...,
        [-0.7724],
        [-0.7700],
        [-0.7693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1013201.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1485.0706, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.0706, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0094,  0.0061,  ..., -0.0108,  0.0161, -0.0116],
        [ 0.0181,  0.0214,  0.0154,  ..., -0.0250,  0.0345, -0.0269],
        [ 0.0105,  0.0094,  0.0061,  ..., -0.0108,  0.0161, -0.0116],
        ...,
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9576.4336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1054.1765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.9572, device='cuda:0')



h[100].sum tensor(-337.6084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(223.5443, device='cuda:0')



h[200].sum tensor(-111.8398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0543, 0.0571, 0.0396,  ..., 0.0000, 0.0942, 0.0000],
        [0.0509, 0.0517, 0.0354,  ..., 0.0000, 0.0859, 0.0000],
        [0.0550, 0.0581, 0.0404,  ..., 0.0000, 0.0957, 0.0000],
        ...,
        [0.0189, 0.0011, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0189, 0.0011, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0189, 0.0011, 0.0000,  ..., 0.0000, 0.0082, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(344964.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0220, 0.0000,  ..., 0.0000, 0.0008, 0.0743],
        [0.0018, 0.0198, 0.0000,  ..., 0.0000, 0.0000, 0.0719],
        [0.0078, 0.0168, 0.0000,  ..., 0.0000, 0.0008, 0.0695],
        ...,
        [0.0519, 0.0000, 0.0542,  ..., 0.0000, 0.0000, 0.0445],
        [0.0519, 0.0000, 0.0542,  ..., 0.0000, 0.0000, 0.0445],
        [0.0519, 0.0000, 0.0542,  ..., 0.0000, 0.0000, 0.0445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3350785., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35821.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(808.8589, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5314.5986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15485.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3488.3687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2702],
        [ 0.2041],
        [ 0.0751],
        ...,
        [-0.7809],
        [-0.7784],
        [-0.7774]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1029961.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1263.9075, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1263.9075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0082,  0.0058,  0.0033,  ..., -0.0066,  0.0106, -0.0070],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        ...,
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0020,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9308.4834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1044.4783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.1346, device='cuda:0')



h[100].sum tensor(-369.3166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.2531, device='cuda:0')



h[200].sum tensor(-95.6369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0318, 0.0214, 0.0129,  ..., 0.0000, 0.0394, 0.0000],
        [0.0286, 0.0163, 0.0098,  ..., 0.0000, 0.0315, 0.0000],
        [0.0189, 0.0011, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        ...,
        [0.0189, 0.0011, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0189, 0.0011, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0189, 0.0011, 0.0000,  ..., 0.0000, 0.0082, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(320844.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0110, 0.0028, 0.0015,  ..., 0.0000, 0.0000, 0.0646],
        [0.0200, 0.0017, 0.0087,  ..., 0.0000, 0.0000, 0.0604],
        [0.0361, 0.0000, 0.0257,  ..., 0.0000, 0.0000, 0.0528],
        ...,
        [0.0523, 0.0000, 0.0546,  ..., 0.0000, 0.0000, 0.0448],
        [0.0523, 0.0000, 0.0546,  ..., 0.0000, 0.0000, 0.0448],
        [0.0523, 0.0000, 0.0546,  ..., 0.0000, 0.0000, 0.0448]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3230267.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37023.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(744.9041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5252.5454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11841.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3330.1226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2020],
        [ 0.1191],
        [-0.0406],
        ...,
        [-0.7887],
        [-0.7862],
        [-0.7854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1098118.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1390.0537, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1390.0537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0118,  0.0114,  0.0077,  ..., -0.0131,  0.0192, -0.0141],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0118,  0.0114,  0.0077,  ..., -0.0131,  0.0192, -0.0141],
        ...,
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9570.4346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1054.4286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.7372, device='cuda:0')



h[100].sum tensor(-351.3455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.2417, device='cuda:0')



h[200].sum tensor(-105.7026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0247, 0.0103, 0.0061,  ..., 0.0000, 0.0222, 0.0000],
        [0.0446, 0.0417, 0.0275,  ..., 0.0000, 0.0704, 0.0000],
        [0.0247, 0.0103, 0.0061,  ..., 0.0000, 0.0222, 0.0000],
        ...,
        [0.0190, 0.0012, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0190, 0.0012, 0.0000,  ..., 0.0000, 0.0082, 0.0000],
        [0.0190, 0.0012, 0.0000,  ..., 0.0000, 0.0082, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(333210.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0352, 0.0000, 0.0147,  ..., 0.0000, 0.0000, 0.0545],
        [0.0231, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0606],
        [0.0352, 0.0000, 0.0147,  ..., 0.0000, 0.0000, 0.0546],
        ...,
        [0.0524, 0.0000, 0.0551,  ..., 0.0000, 0.0000, 0.0454],
        [0.0524, 0.0000, 0.0551,  ..., 0.0000, 0.0000, 0.0454],
        [0.0524, 0.0000, 0.0551,  ..., 0.0000, 0.0000, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3287303.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37343.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(773.0403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5308.6929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13227.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3430.2278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3770],
        [-0.3401],
        [-0.4142],
        ...,
        [-0.7948],
        [-0.7923],
        [-0.7916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1115779.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1601.1881, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1601.1881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0069,  0.0041,  ..., -0.0077,  0.0121, -0.0083],
        [ 0.0089,  0.0068,  0.0041,  ..., -0.0077,  0.0121, -0.0082],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        ...,
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9955.5430, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1066.6964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.1144, device='cuda:0')



h[100].sum tensor(-324.8934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(241.0232, device='cuda:0')



h[200].sum tensor(-120.1958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0413, 0.0366, 0.0234,  ..., 0.0000, 0.0626, 0.0000],
        [0.0265, 0.0132, 0.0072,  ..., 0.0000, 0.0265, 0.0000],
        [0.0274, 0.0146, 0.0083,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0190, 0.0013, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0190, 0.0013, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0190, 0.0013, 0.0000,  ..., 0.0000, 0.0083, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(358124.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0118, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0662],
        [0.0228, 0.0000, 0.0054,  ..., 0.0000, 0.0000, 0.0604],
        [0.0251, 0.0000, 0.0051,  ..., 0.0000, 0.0000, 0.0594],
        ...,
        [0.0521, 0.0000, 0.0552,  ..., 0.0000, 0.0000, 0.0461],
        [0.0521, 0.0000, 0.0552,  ..., 0.0000, 0.0000, 0.0461],
        [0.0521, 0.0000, 0.0552,  ..., 0.0000, 0.0000, 0.0461]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3447055., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36185.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(841.1057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5404.6763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17300.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3621.1150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1174],
        [-0.2446],
        [-0.3532],
        ...,
        [-0.7974],
        [-0.7949],
        [-0.7941]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1048987.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1106.8348, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1106.8348, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        ...,
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0047,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9244.1455, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1036.9868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(49.1584, device='cuda:0')



h[100].sum tensor(-396.4380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(166.6094, device='cuda:0')



h[200].sum tensor(-83.0551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0190, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0190, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0190, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        ...,
        [0.0190, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0190, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0190, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(306256.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0514, 0.0000, 0.0540,  ..., 0.0000, 0.0000, 0.0471],
        [0.0522, 0.0000, 0.0556,  ..., 0.0000, 0.0000, 0.0468],
        [0.0523, 0.0000, 0.0556,  ..., 0.0000, 0.0000, 0.0469],
        ...,
        [0.0521, 0.0000, 0.0555,  ..., 0.0000, 0.0000, 0.0466],
        [0.0521, 0.0000, 0.0555,  ..., 0.0000, 0.0000, 0.0466],
        [0.0521, 0.0000, 0.0555,  ..., 0.0000, 0.0000, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3177083.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36633.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(710.8691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5251.2041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9460.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3291.9866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7689],
        [-0.9075],
        [-0.9882],
        ...,
        [-0.8015],
        [-0.7989],
        [-0.7982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1065831.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1610.4265, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1610.4265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0135,  0.0142,  0.0097,  ..., -0.0163,  0.0233, -0.0175],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0135,  0.0142,  0.0097,  ..., -0.0163,  0.0233, -0.0175],
        ...,
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10090.8164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1072.0859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.5247, device='cuda:0')



h[100].sum tensor(-326.2527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(242.4139, device='cuda:0')



h[200].sum tensor(-120.7965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0262, 0.0127, 0.0078,  ..., 0.0000, 0.0257, 0.0000],
        [0.0509, 0.0516, 0.0350,  ..., 0.0000, 0.0856, 0.0000],
        [0.0262, 0.0127, 0.0078,  ..., 0.0000, 0.0257, 0.0000],
        ...,
        [0.0191, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0191, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000],
        [0.0191, 0.0014, 0.0000,  ..., 0.0000, 0.0083, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(353847.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0277, 0.0021, 0.0039,  ..., 0.0000, 0.0000, 0.0598],
        [0.0165, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0657],
        [0.0311, 0.0012, 0.0093,  ..., 0.0000, 0.0000, 0.0581],
        ...,
        [0.0526, 0.0000, 0.0561,  ..., 0.0000, 0.0000, 0.0466],
        [0.0526, 0.0000, 0.0561,  ..., 0.0000, 0.0000, 0.0466],
        [0.0526, 0.0000, 0.0561,  ..., 0.0000, 0.0000, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3404262., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36745.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(841.9307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5426.4326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15430.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3605.4568, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2127],
        [ 0.1405],
        [-0.0117],
        ...,
        [-0.8113],
        [-0.8088],
        [-0.8080]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1088195., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 5700 loss: tensor(523.7865, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1704.0009, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1704.0009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0132,  0.0137,  0.0093,  ..., -0.0157,  0.0226, -0.0168],
        [ 0.0082,  0.0057,  0.0031,  ..., -0.0063,  0.0103, -0.0067],
        ...,
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10356.1553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1081.7333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.6806, device='cuda:0')



h[100].sum tensor(-314.7083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(256.4994, device='cuda:0')



h[200].sum tensor(-127.8333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0303, 0.0190, 0.0117,  ..., 0.0000, 0.0355, 0.0000],
        [0.0294, 0.0175, 0.0096,  ..., 0.0000, 0.0333, 0.0000],
        [0.0763, 0.0915, 0.0660,  ..., 0.0000, 0.1470, 0.0000],
        ...,
        [0.0191, 0.0013, 0.0000,  ..., 0.0000, 0.0084, 0.0000],
        [0.0191, 0.0013, 0.0000,  ..., 0.0000, 0.0084, 0.0000],
        [0.0191, 0.0013, 0.0000,  ..., 0.0000, 0.0084, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(373546.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0233, 0.0058, 0.0147,  ..., 0.0000, 0.0000, 0.0616],
        [0.0112, 0.0204, 0.0048,  ..., 0.0000, 0.0045, 0.0724],
        [0.0000, 0.0663, 0.0000,  ..., 0.0000, 0.0259, 0.0925],
        ...,
        [0.0530, 0.0000, 0.0563,  ..., 0.0000, 0.0000, 0.0464],
        [0.0530, 0.0000, 0.0563,  ..., 0.0000, 0.0000, 0.0464],
        [0.0530, 0.0000, 0.0563,  ..., 0.0000, 0.0000, 0.0464]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3554521.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36600.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(905.1052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5506.6191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19261.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3729.3020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1187],
        [ 0.0949],
        [ 0.2053],
        ...,
        [-0.8188],
        [-0.8163],
        [-0.8155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1105383.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1362.3632, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.3632, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0283,  0.0374,  0.0278,  ..., -0.0435,  0.0591, -0.0468],
        [ 0.0189,  0.0226,  0.0163,  ..., -0.0262,  0.0363, -0.0281],
        [ 0.0154,  0.0171,  0.0120,  ..., -0.0197,  0.0279, -0.0212],
        ...,
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9941.6191, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1062.6190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.5073, device='cuda:0')



h[100].sum tensor(-365.7654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.0735, device='cuda:0')



h[200].sum tensor(-102.0169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1149, 0.1525, 0.1133,  ..., 0.0000, 0.2409, 0.0000],
        [0.0848, 0.1049, 0.0764,  ..., 0.0000, 0.1677, 0.0000],
        [0.0690, 0.0799, 0.0580,  ..., 0.0000, 0.1293, 0.0000],
        ...,
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0084, 0.0000],
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0084, 0.0000],
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0084, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(336016.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1471, 0.0000,  ..., 0.0000, 0.0792, 0.1186],
        [0.0000, 0.1145, 0.0000,  ..., 0.0000, 0.0573, 0.1081],
        [0.0000, 0.0856, 0.0000,  ..., 0.0000, 0.0380, 0.0973],
        ...,
        [0.0531, 0.0000, 0.0563,  ..., 0.0000, 0.0000, 0.0463],
        [0.0531, 0.0000, 0.0563,  ..., 0.0000, 0.0000, 0.0463],
        [0.0531, 0.0000, 0.0563,  ..., 0.0000, 0.0000, 0.0463]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3343146.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36624.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(814.0032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5393.3184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12871.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3473.8145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1646],
        [ 0.1864],
        [ 0.2033],
        ...,
        [-0.8237],
        [-0.8211],
        [-0.8203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1089157.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1524.5901, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1524.5901, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        ...,
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0048,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10370.8838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1074.3826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.7124, device='cuda:0')



h[100].sum tensor(-344.0261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(229.4931, device='cuda:0')



h[200].sum tensor(-114.5532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0085, 0.0000],
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0085, 0.0000],
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0085, 0.0000],
        ...,
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0085, 0.0000],
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0085, 0.0000],
        [0.0192, 0.0013, 0.0000,  ..., 0.0000, 0.0085, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(355805.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0528, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0465],
        [0.0528, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0465],
        [0.0529, 0.0000, 0.0563,  ..., 0.0000, 0.0000, 0.0466],
        ...,
        [0.0527, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0463],
        [0.0527, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0463],
        [0.0527, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0463]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3436227.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35305.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(868.5446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5472.2568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15169.4932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3610.3398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9852],
        [-1.0225],
        [-1.0283],
        ...,
        [-0.8223],
        [-0.8211],
        [-0.8207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1020536., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1313.9302, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1313.9302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0093,  0.0075,  0.0045,  ..., -0.0084,  0.0131, -0.0090],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0093,  0.0075,  0.0045,  ..., -0.0084,  0.0131, -0.0090],
        ...,
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10123.3945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1061.7443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.3562, device='cuda:0')



h[100].sum tensor(-376.5149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.7829, device='cuda:0')



h[200].sum tensor(-98.4163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0517, 0.0528, 0.0369,  ..., 0.0000, 0.0876, 0.0000],
        [0.0455, 0.0429, 0.0282,  ..., 0.0000, 0.0724, 0.0000],
        [0.0260, 0.0122, 0.0074,  ..., 0.0000, 0.0252, 0.0000],
        ...,
        [0.0192, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0192, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0192, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(332787.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0515, 0.0000,  ..., 0.0000, 0.0166, 0.0862],
        [0.0033, 0.0304, 0.0000,  ..., 0.0000, 0.0066, 0.0784],
        [0.0172, 0.0091, 0.0067,  ..., 0.0000, 0.0000, 0.0650],
        ...,
        [0.0524, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0466],
        [0.0524, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0466],
        [0.0524, 0.0000, 0.0562,  ..., 0.0000, 0.0000, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3335168., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35798.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(796.0245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5407.1177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12266.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3465.1826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2607],
        [ 0.2548],
        [ 0.2206],
        ...,
        [-0.8244],
        [-0.8231],
        [-0.8225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1052153.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1463.4271, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1463.4271, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0146,  0.0158,  0.0110,  ..., -0.0181,  0.0259, -0.0194],
        [ 0.0088,  0.0067,  0.0039,  ..., -0.0074,  0.0118, -0.0079],
        [ 0.0087,  0.0065,  0.0037,  ..., -0.0072,  0.0116, -0.0077],
        ...,
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10423.2119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1071.7195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.9959, device='cuda:0')



h[100].sum tensor(-357.7815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(220.2864, device='cuda:0')



h[200].sum tensor(-109.0844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0416, 0.0369, 0.0235,  ..., 0.0000, 0.0631, 0.0000],
        [0.0424, 0.0381, 0.0244,  ..., 0.0000, 0.0651, 0.0000],
        [0.0264, 0.0128, 0.0067,  ..., 0.0000, 0.0260, 0.0000],
        ...,
        [0.0192, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        [0.0192, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        [0.0192, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(352903.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0715],
        [0.0114, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0673],
        [0.0274, 0.0000, 0.0163,  ..., 0.0000, 0.0000, 0.0591],
        ...,
        [0.0523, 0.0000, 0.0565,  ..., 0.0000, 0.0000, 0.0471],
        [0.0523, 0.0000, 0.0565,  ..., 0.0000, 0.0000, 0.0471],
        [0.0523, 0.0000, 0.0565,  ..., 0.0000, 0.0000, 0.0471]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3459201., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34857.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(850.4773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5492.6411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15369.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3615.1458, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2600],
        [ 0.1537],
        [-0.0336],
        ...,
        [-0.8323],
        [-0.8298],
        [-0.8291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1001491.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1255.4686, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1255.4686, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        ...,
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0048,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10119.3652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1062.0961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.7598, device='cuda:0')



h[100].sum tensor(-388.1758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(188.9829, device='cuda:0')



h[200].sum tensor(-94.0534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0015, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0193, 0.0015, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0193, 0.0015, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        ...,
        [0.0193, 0.0015, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0193, 0.0015, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0193, 0.0015, 0.0000,  ..., 0.0000, 0.0086, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(331437.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0000, 0.0449,  ..., 0.0000, 0.0000, 0.0508],
        [0.0515, 0.0000, 0.0550,  ..., 0.0000, 0.0000, 0.0483],
        [0.0530, 0.0000, 0.0571,  ..., 0.0000, 0.0000, 0.0477],
        ...,
        [0.0528, 0.0000, 0.0571,  ..., 0.0000, 0.0000, 0.0474],
        [0.0528, 0.0000, 0.0571,  ..., 0.0000, 0.0000, 0.0474],
        [0.0528, 0.0000, 0.0571,  ..., 0.0000, 0.0000, 0.0474]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3353024.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35712.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(793.4865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5440.2427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11818.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3473.3074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2703],
        [-0.5183],
        [-0.7237],
        ...,
        [-0.8419],
        [-0.8393],
        [-0.8385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1036303.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1321.2183, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.2183, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        ...,
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10278.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1072.5045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.6799, device='cuda:0')



h[100].sum tensor(-382.0349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.8800, device='cuda:0')



h[200].sum tensor(-98.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0194, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0194, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        ...,
        [0.0194, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0194, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0194, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(336948.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0541, 0.0000, 0.0578,  ..., 0.0000, 0.0000, 0.0472],
        [0.0534, 0.0000, 0.0573,  ..., 0.0000, 0.0000, 0.0475],
        [0.0518, 0.0000, 0.0561,  ..., 0.0000, 0.0000, 0.0484],
        ...,
        [0.0540, 0.0000, 0.0578,  ..., 0.0000, 0.0000, 0.0470],
        [0.0540, 0.0000, 0.0578,  ..., 0.0000, 0.0000, 0.0470],
        [0.0540, 0.0000, 0.0578,  ..., 0.0000, 0.0000, 0.0470]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3391242.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37221.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(810.5647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5470.5293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12245.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3483.7302, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9398],
        [-0.8276],
        [-0.6702],
        ...,
        [-0.8557],
        [-0.8530],
        [-0.8522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1119924.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1582.4103, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1582.4103, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0049,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0049,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        ...,
        [ 0.0049,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0049,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000],
        [ 0.0049,  0.0003, -0.0010,  ...,  0.0000,  0.0021,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10734.2471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1094.1259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(70.2804, device='cuda:0')



h[100].sum tensor(-348.3160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(238.1967, device='cuda:0')



h[200].sum tensor(-117.2692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0251, 0.0101, 0.0058,  ..., 0.0000, 0.0221, 0.0000],
        [0.0195, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0195, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        ...,
        [0.0195, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0195, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000],
        [0.0195, 0.0014, 0.0000,  ..., 0.0000, 0.0086, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(358817.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0249, 0.0062, 0.0083,  ..., 0.0000, 0.0000, 0.0619],
        [0.0381, 0.0000, 0.0214,  ..., 0.0000, 0.0000, 0.0553],
        [0.0436, 0.0000, 0.0370,  ..., 0.0000, 0.0000, 0.0523],
        ...,
        [0.0548, 0.0000, 0.0583,  ..., 0.0000, 0.0000, 0.0470],
        [0.0548, 0.0000, 0.0583,  ..., 0.0000, 0.0000, 0.0470],
        [0.0548, 0.0000, 0.0583,  ..., 0.0000, 0.0000, 0.0470]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3503010., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38098.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.8359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5551.6499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14913.7373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3614.1680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2335],
        [ 0.1960],
        [ 0.1697],
        ...,
        [-0.8651],
        [-0.8624],
        [-0.8616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1173165.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1285.0543, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1285.0543, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        ...,
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10288.5176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1078.3835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.0738, device='cuda:0')



h[100].sum tensor(-392.0641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.4363, device='cuda:0')



h[200].sum tensor(-95.3563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0297, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0196, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        [0.0196, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        [0.0196, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        ...,
        [0.0196, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        [0.0196, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000],
        [0.0196, 0.0015, 0.0000,  ..., 0.0000, 0.0087, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(330460.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0000, 0.0482,  ..., 0.0000, 0.0000, 0.0515],
        [0.0500, 0.0000, 0.0541,  ..., 0.0000, 0.0000, 0.0495],
        [0.0534, 0.0000, 0.0574,  ..., 0.0000, 0.0000, 0.0482],
        ...,
        [0.0545, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0473],
        [0.0545, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0473],
        [0.0545, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0473]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3359188.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37713.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(794.9310, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5478.8003, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10726.0967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3438.0935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2365],
        [-0.3537],
        [-0.4440],
        ...,
        [-0.8692],
        [-0.8665],
        [-0.8657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1154115.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1170.2285, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1170.2285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        ...,
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10182.4629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1073.3140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(51.9739, device='cuda:0')



h[100].sum tensor(-410.2998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(176.1519, device='cuda:0')



h[200].sum tensor(-86.8365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0196, 0.0016, 0.0000,  ..., 0.0000, 0.0088, 0.0000],
        [0.0196, 0.0016, 0.0000,  ..., 0.0000, 0.0088, 0.0000],
        [0.0196, 0.0016, 0.0000,  ..., 0.0000, 0.0088, 0.0000],
        ...,
        [0.0196, 0.0016, 0.0000,  ..., 0.0000, 0.0088, 0.0000],
        [0.0196, 0.0016, 0.0000,  ..., 0.0000, 0.0088, 0.0000],
        [0.0196, 0.0016, 0.0000,  ..., 0.0000, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(324529.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0495, 0.0000, 0.0532,  ..., 0.0000, 0.0000, 0.0495],
        [0.0482, 0.0000, 0.0519,  ..., 0.0000, 0.0000, 0.0500],
        [0.0456, 0.0000, 0.0495,  ..., 0.0000, 0.0000, 0.0512],
        ...,
        [0.0537, 0.0000, 0.0580,  ..., 0.0000, 0.0000, 0.0475],
        [0.0537, 0.0000, 0.0580,  ..., 0.0000, 0.0000, 0.0475],
        [0.0537, 0.0000, 0.0580,  ..., 0.0000, 0.0000, 0.0475]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3346472.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36792.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(778.2544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5474.5332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10332.7598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3412.4734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2556],
        [-0.2563],
        [-0.2565],
        ...,
        [-0.8672],
        [-0.8646],
        [-0.8640]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1099474.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 6000 loss: tensor(522.8074, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1411.3531, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.3531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0135,  0.0140,  0.0096,  ..., -0.0158,  0.0231, -0.0170],
        ...,
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0049,  0.0004, -0.0010,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10669.1152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1091.0969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.6831, device='cuda:0')



h[100].sum tensor(-377.9579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(212.4478, device='cuda:0')



h[200].sum tensor(-104.6232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0197, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0314, 0.0203, 0.0124,  ..., 0.0000, 0.0375, 0.0000],
        [0.0330, 0.0228, 0.0134,  ..., 0.0000, 0.0413, 0.0000],
        ...,
        [0.0197, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0197, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0197, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(353466.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0426, 0.0000, 0.0338,  ..., 0.0000, 0.0000, 0.0527],
        [0.0221, 0.0035, 0.0126,  ..., 0.0000, 0.0000, 0.0621],
        [0.0105, 0.0062, 0.0023,  ..., 0.0000, 0.0000, 0.0682],
        ...,
        [0.0531, 0.0000, 0.0577,  ..., 0.0000, 0.0000, 0.0477],
        [0.0531, 0.0000, 0.0577,  ..., 0.0000, 0.0000, 0.0477],
        [0.0531, 0.0000, 0.0577,  ..., 0.0000, 0.0000, 0.0477]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3503636.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35753.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(852.3850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5585.0195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14789.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3616.5608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2514],
        [ 0.0312],
        [ 0.2201],
        ...,
        [-0.8656],
        [-0.8630],
        [-0.8622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1051248.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1419.7195, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1419.7195, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0049,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0049,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        ...,
        [ 0.0049,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0049,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0049,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10751.2070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1096.3557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.0547, device='cuda:0')



h[100].sum tensor(-379.8152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.7072, device='cuda:0')



h[200].sum tensor(-104.7201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0198, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0198, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        ...,
        [0.0198, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0198, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0198, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350961.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0470, 0.0000, 0.0493,  ..., 0.0000, 0.0000, 0.0505],
        [0.0408, 0.0000, 0.0375,  ..., 0.0000, 0.0000, 0.0532],
        [0.0323, 0.0000, 0.0216,  ..., 0.0000, 0.0000, 0.0570],
        ...,
        [0.0531, 0.0000, 0.0576,  ..., 0.0000, 0.0000, 0.0476],
        [0.0531, 0.0000, 0.0576,  ..., 0.0000, 0.0000, 0.0476],
        [0.0531, 0.0000, 0.0576,  ..., 0.0000, 0.0000, 0.0476]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3479197.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(34693.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(857.3984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5595.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13601.8506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3596.3726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1309],
        [ 0.0312],
        [ 0.1587],
        ...,
        [-0.8655],
        [-0.8531],
        [-0.8073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-985765.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1259.7955, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1259.7955, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        ...,
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10526.6133, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1094.0288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.9519, device='cuda:0')



h[100].sum tensor(-404.9529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(189.6342, device='cuda:0')



h[200].sum tensor(-92.8100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0199, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0199, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0322, 0.0211, 0.0132,  ..., 0.0000, 0.0389, 0.0000],
        ...,
        [0.0199, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0199, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0199, 0.0018, 0.0000,  ..., 0.0000, 0.0091, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(333912.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0473, 0.0000, 0.0489,  ..., 0.0000, 0.0000, 0.0505],
        [0.0396, 0.0000, 0.0297,  ..., 0.0000, 0.0000, 0.0539],
        [0.0197, 0.0053, 0.0105,  ..., 0.0000, 0.0000, 0.0630],
        ...,
        [0.0537, 0.0000, 0.0579,  ..., 0.0000, 0.0000, 0.0474],
        [0.0537, 0.0000, 0.0579,  ..., 0.0000, 0.0000, 0.0474],
        [0.0537, 0.0000, 0.0579,  ..., 0.0000, 0.0000, 0.0474]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3398362., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35730.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(811.8367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5548.9438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10949.3428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3457.4744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2776],
        [-0.0556],
        [ 0.1378],
        ...,
        [-0.8818],
        [-0.8792],
        [-0.8785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1043934.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1318.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1318.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0200,  0.0241,  0.0174,  ..., -0.0275,  0.0387, -0.0296],
        [ 0.0136,  0.0140,  0.0096,  ..., -0.0157,  0.0231, -0.0169],
        ...,
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10628.3398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1105.2341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.5474, device='cuda:0')



h[100].sum tensor(-398.1863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.4307, device='cuda:0')



h[200].sum tensor(-97.2677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0420, 0.0364, 0.0250,  ..., 0.0000, 0.0624, 0.0000],
        [0.0472, 0.0446, 0.0305,  ..., 0.0000, 0.0750, 0.0000],
        [0.0979, 0.1247, 0.0918,  ..., 0.0000, 0.1985, 0.0000],
        ...,
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(338440.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0322, 0.0046,  ..., 0.0000, 0.0094, 0.0709],
        [0.0063, 0.0651, 0.0000,  ..., 0.0000, 0.0255, 0.0843],
        [0.0000, 0.1207, 0.0000,  ..., 0.0000, 0.0568, 0.1023],
        ...,
        [0.0548, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0475],
        [0.0548, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0475],
        [0.0548, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0475]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3431449., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37014.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(823.1693, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5576.6738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11365.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3467.5911, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1161],
        [ 0.1034],
        [ 0.0709],
        ...,
        [-0.8945],
        [-0.8917],
        [-0.8909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1118491.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1327.1001, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1327.1001, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        ...,
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10651.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1106.0754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.9412, device='cuda:0')



h[100].sum tensor(-396.2678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(199.7654, device='cuda:0')



h[200].sum tensor(-98.2615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        ...,
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0201, 0.0017, 0.0000,  ..., 0.0000, 0.0090, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337934.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0545, 0.0000, 0.0582,  ..., 0.0000, 0.0000, 0.0479],
        [0.0503, 0.0000, 0.0525,  ..., 0.0000, 0.0000, 0.0497],
        [0.0425, 0.0000, 0.0384,  ..., 0.0000, 0.0000, 0.0532],
        ...,
        [0.0548, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0475],
        [0.0548, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0475],
        [0.0548, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0475]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3414634.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37356.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(818.7375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5572.7832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11050.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3463.3132, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6942],
        [-0.4696],
        [-0.2212],
        ...,
        [-0.8945],
        [-0.8917],
        [-0.8909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1141760.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1134.3564, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1134.3564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0022,  0.0000],
        ...,
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0022,  0.0000],
        [ 0.0050,  0.0004, -0.0009,  ...,  0.0000,  0.0022,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10296.0225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1098.7598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(50.3807, device='cuda:0')



h[100].sum tensor(-426.2175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(170.7521, device='cuda:0')



h[200].sum tensor(-83.6961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        ...,
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(320630.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0557, 0.0000, 0.0592,  ..., 0.0000, 0.0000, 0.0482],
        [0.0557, 0.0000, 0.0592,  ..., 0.0000, 0.0000, 0.0482],
        [0.0558, 0.0000, 0.0592,  ..., 0.0000, 0.0000, 0.0483],
        ...,
        [0.0556, 0.0000, 0.0591,  ..., 0.0000, 0.0000, 0.0480],
        [0.0556, 0.0000, 0.0591,  ..., 0.0000, 0.0000, 0.0480],
        [0.0556, 0.0000, 0.0591,  ..., 0.0000, 0.0000, 0.0480]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3363193.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39353.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(763.8110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5524.4355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9361.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3339.5566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9802],
        [-1.0777],
        [-1.1440],
        ...,
        [-0.9047],
        [-0.9020],
        [-0.9012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1249136.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1280.6941, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1280.6941, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        ...,
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10541.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1109.2601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.8801, device='cuda:0')



h[100].sum tensor(-407.0596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(192.7800, device='cuda:0')



h[200].sum tensor(-94.5920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        ...,
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000],
        [0.0202, 0.0018, 0.0000,  ..., 0.0000, 0.0090, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(338334.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0558, 0.0000, 0.0597,  ..., 0.0000, 0.0000, 0.0489],
        [0.0558, 0.0000, 0.0597,  ..., 0.0000, 0.0000, 0.0489],
        [0.0559, 0.0000, 0.0597,  ..., 0.0000, 0.0000, 0.0490],
        ...,
        [0.0557, 0.0000, 0.0596,  ..., 0.0000, 0.0000, 0.0487],
        [0.0557, 0.0000, 0.0596,  ..., 0.0000, 0.0000, 0.0487],
        [0.0557, 0.0000, 0.0596,  ..., 0.0000, 0.0000, 0.0487]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3461437.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38442.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(818.2961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5604.8574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11636.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3476.8127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1614],
        [-1.1727],
        [-1.1736],
        ...,
        [-0.9119],
        [-0.9091],
        [-0.9083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1193146.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1157.0144, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1157.0144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        ...,
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10357.3154, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1101.2661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(51.3871, device='cuda:0')



h[100].sum tensor(-428.0519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.1628, device='cuda:0')



h[200].sum tensor(-84.8511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0019, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0019, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0019, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        ...,
        [0.0202, 0.0019, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0019, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0019, 0.0000,  ..., 0.0000, 0.0091, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(320638.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0557, 0.0000, 0.0599,  ..., 0.0000, 0.0000, 0.0494],
        [0.0557, 0.0000, 0.0599,  ..., 0.0000, 0.0000, 0.0494],
        [0.0557, 0.0000, 0.0599,  ..., 0.0000, 0.0000, 0.0495],
        ...,
        [0.0555, 0.0000, 0.0599,  ..., 0.0000, 0.0000, 0.0492],
        [0.0555, 0.0000, 0.0599,  ..., 0.0000, 0.0000, 0.0492],
        [0.0555, 0.0000, 0.0599,  ..., 0.0000, 0.0000, 0.0492]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3370790., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39101.6758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(763.0205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5552.9492, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9086.4443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3368.9548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8614],
        [-0.9846],
        [-1.0808],
        ...,
        [-0.9168],
        [-0.9140],
        [-0.9132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1229319.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1257.8040, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1257.8040, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0155,  0.0171,  0.0119,  ..., -0.0192,  0.0278, -0.0207],
        [ 0.0050,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0108,  0.0096,  0.0061,  ..., -0.0106,  0.0164, -0.0114],
        ...,
        [ 0.0050,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0050,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10574.7324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1107.4788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.8635, device='cuda:0')



h[100].sum tensor(-415.2680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(189.3344, device='cuda:0')



h[200].sum tensor(-92.5131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0287, 0.0155, 0.0086,  ..., 0.0000, 0.0300, 0.0000],
        [0.0458, 0.0426, 0.0277,  ..., 0.0000, 0.0717, 0.0000],
        [0.0249, 0.0095, 0.0048,  ..., 0.0000, 0.0206, 0.0000],
        ...,
        [0.0202, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(336881.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0160, 0.0012, 0.0017,  ..., 0.0000, 0.0000, 0.0672],
        [0.0151, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0678],
        [0.0355, 0.0000, 0.0206,  ..., 0.0000, 0.0000, 0.0590],
        ...,
        [0.0548, 0.0000, 0.0590,  ..., 0.0000, 0.0000, 0.0501],
        [0.0554, 0.0000, 0.0602,  ..., 0.0000, 0.0000, 0.0499],
        [0.0554, 0.0000, 0.0602,  ..., 0.0000, 0.0000, 0.0499]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3470647.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37161.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(821.6180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5631.0078, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11268.8193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3501.4163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1046],
        [-0.0964],
        [-0.3890],
        ...,
        [-0.7246],
        [-0.8324],
        [-0.8922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1109343.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1307.7239, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1307.7239, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0205,  0.0250,  0.0180,  ..., -0.0284,  0.0401, -0.0306],
        ...,
        [ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10709.3682, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1112.8384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.0806, device='cuda:0')



h[100].sum tensor(-410.1785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(196.8487, device='cuda:0')



h[200].sum tensor(-96.3003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0357, 0.0265, 0.0180,  ..., 0.0000, 0.0469, 0.0000],
        [0.0329, 0.0220, 0.0146,  ..., 0.0000, 0.0400, 0.0000],
        ...,
        [0.0202, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0202, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(340464.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0474, 0.0000, 0.0342,  ..., 0.0000, 0.0000, 0.0542],
        [0.0309, 0.0065, 0.0115,  ..., 0.0000, 0.0000, 0.0621],
        [0.0225, 0.0125, 0.0024,  ..., 0.0000, 0.0014, 0.0671],
        ...,
        [0.0556, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0502],
        [0.0556, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0502],
        [0.0556, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0502]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3480092., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37801.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.7788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5653.2734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11434.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3524.7048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0403],
        [ 0.1215],
        [ 0.2247],
        ...,
        [-0.9291],
        [-0.9262],
        [-0.9254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1167631.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 6300 loss: tensor(447.0256, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1107.0419, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1107.0419, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0132,  0.0133,  0.0090,  ..., -0.0148,  0.0220, -0.0159],
        [ 0.0086,  0.0060,  0.0034,  ..., -0.0064,  0.0108, -0.0069],
        [ 0.0160,  0.0177,  0.0124,  ..., -0.0200,  0.0288, -0.0215],
        ...,
        [ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0010,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10430.8936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1104.6740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(49.1676, device='cuda:0')



h[100].sum tensor(-441.9499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(166.6405, device='cuda:0')



h[200].sum tensor(-81.3931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0338, 0.0232, 0.0136,  ..., 0.0000, 0.0419, 0.0000],
        [0.0662, 0.0746, 0.0526,  ..., 0.0000, 0.1212, 0.0000],
        [0.0786, 0.0942, 0.0678,  ..., 0.0000, 0.1515, 0.0000],
        ...,
        [0.0203, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0203, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0203, 0.0020, 0.0000,  ..., 0.0000, 0.0091, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(323948.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0245, 0.0030,  ..., 0.0000, 0.0046, 0.0756],
        [0.0000, 0.0638, 0.0000,  ..., 0.0000, 0.0214, 0.0898],
        [0.0000, 0.0996, 0.0000,  ..., 0.0000, 0.0441, 0.1004],
        ...,
        [0.0561, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0500],
        [0.0561, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0500],
        [0.0561, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0500]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3425182.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38303.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(788.3337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5607.9746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9281.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3396.4121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2268],
        [ 0.2720],
        [ 0.2716],
        ...,
        [-0.9381],
        [-0.9352],
        [-0.9344]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1195546.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1242.5780, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1242.5780, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0096,  0.0076,  0.0046,  ..., -0.0082,  0.0132, -0.0088],
        [ 0.0087,  0.0062,  0.0035,  ..., -0.0066,  0.0111, -0.0071],
        [ 0.0132,  0.0133,  0.0090,  ..., -0.0148,  0.0220, -0.0159],
        ...,
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10800.6084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1118.1796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.1872, device='cuda:0')



h[100].sum tensor(-425.4749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.0425, device='cuda:0')



h[200].sum tensor(-91.5259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0306, 0.0180, 0.0106,  ..., 0.0000, 0.0341, 0.0000],
        [0.0432, 0.0380, 0.0243,  ..., 0.0000, 0.0649, 0.0000],
        [0.0306, 0.0180, 0.0097,  ..., 0.0000, 0.0341, 0.0000],
        ...,
        [0.0204, 0.0019, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        [0.0204, 0.0019, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        [0.0204, 0.0019, 0.0000,  ..., 0.0000, 0.0092, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337609.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.0019, 0.0116,  ..., 0.0000, 0.0000, 0.0645],
        [0.0083, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0699],
        [0.0144, 0.0013, 0.0060,  ..., 0.0000, 0.0000, 0.0673],
        ...,
        [0.0564, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0495],
        [0.0564, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0495],
        [0.0564, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0495]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3486586., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38014.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(829.9856, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5661.0635, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10535.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3464.8455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0878],
        [ 0.1880],
        [ 0.1534],
        ...,
        [-0.9416],
        [-0.9395],
        [-0.9395]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1197283.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1158.6014, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1158.6014, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        ...,
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10774.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1114.8245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(51.4576, device='cuda:0')



h[100].sum tensor(-442.4071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.4017, device='cuda:0')



h[200].sum tensor(-84.4221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        ...,
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(326925.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0563, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0494],
        [0.0563, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0494],
        [0.0564, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0495],
        ...,
        [0.0562, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0492],
        [0.0562, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0492],
        [0.0562, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0492]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3433147., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37580.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(803.4668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5635.3643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8739.1885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3386.6428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9376],
        [-1.0583],
        [-1.1450],
        ...,
        [-0.9404],
        [-0.9390],
        [-0.9400]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1167795.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1365.8707, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1365.8707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0095,  0.0073,  0.0044,  ..., -0.0079,  0.0129, -0.0086],
        [ 0.0087,  0.0062,  0.0035,  ..., -0.0066,  0.0111, -0.0071],
        ...,
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0023,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11133.6631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1127.4113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.6631, device='cuda:0')



h[100].sum tensor(-413.6731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.6014, device='cuda:0')



h[200].sum tensor(-99.1717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0248, 0.0088, 0.0044,  ..., 0.0000, 0.0199, 0.0000],
        [0.0336, 0.0227, 0.0143,  ..., 0.0000, 0.0413, 0.0000],
        [0.0488, 0.0468, 0.0311,  ..., 0.0000, 0.0786, 0.0000],
        ...,
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0205, 0.0019, 0.0000,  ..., 0.0000, 0.0093, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(346725.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0002, 0.0216,  ..., 0.0000, 0.0000, 0.0597],
        [0.0166, 0.0066, 0.0056,  ..., 0.0000, 0.0000, 0.0667],
        [0.0042, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0729],
        ...,
        [0.0562, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0492],
        [0.0562, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0492],
        [0.0562, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0492]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3529734.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37927.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(848.4114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5696.8076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11693.9756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3517.3386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0450],
        [ 0.1583],
        [ 0.2553],
        ...,
        [-0.9474],
        [-0.9445],
        [-0.9437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1202523., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1423.1918, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1423.1918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0122,  0.0118,  0.0078,  ..., -0.0130,  0.0197, -0.0140],
        [ 0.0250,  0.0319,  0.0235,  ..., -0.0363,  0.0508, -0.0391],
        [ 0.0181,  0.0210,  0.0150,  ..., -0.0237,  0.0340, -0.0255],
        ...,
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11340.5459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1131.8887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.2089, device='cuda:0')



h[100].sum tensor(-407.8790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(214.2298, device='cuda:0')



h[200].sum tensor(-103.4564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0698, 0.0801, 0.0569,  ..., 0.0000, 0.1300, 0.0000],
        [0.0684, 0.0778, 0.0551,  ..., 0.0000, 0.1264, 0.0000],
        [0.0689, 0.0786, 0.0558,  ..., 0.0000, 0.1277, 0.0000],
        ...,
        [0.0205, 0.0020, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0205, 0.0020, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0205, 0.0020, 0.0000,  ..., 0.0000, 0.0094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(356925.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0401, 0.0000,  ..., 0.0000, 0.0090, 0.0784],
        [0.0000, 0.0484, 0.0000,  ..., 0.0000, 0.0112, 0.0821],
        [0.0000, 0.0519, 0.0000,  ..., 0.0000, 0.0133, 0.0847],
        ...,
        [0.0559, 0.0000, 0.0604,  ..., 0.0000, 0.0000, 0.0493],
        [0.0559, 0.0000, 0.0604,  ..., 0.0000, 0.0000, 0.0493],
        [0.0559, 0.0000, 0.0604,  ..., 0.0000, 0.0000, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3586918.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37260.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(872.5837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5742.8467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12963.8311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3596.3696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2667],
        [ 0.2790],
        [ 0.2890],
        ...,
        [-0.9498],
        [-0.9469],
        [-0.9461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1145459.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1245.0311, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1245.0311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11080.8184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1121.4601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.2962, device='cuda:0')



h[100].sum tensor(-434.5510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.4117, device='cuda:0')



h[200].sum tensor(-90.8706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        ...,
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(338224.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0559, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0498],
        [0.0548, 0.0000, 0.0571,  ..., 0.0000, 0.0000, 0.0503],
        [0.0498, 0.0000, 0.0431,  ..., 0.0000, 0.0000, 0.0526],
        ...,
        [0.0558, 0.0000, 0.0604,  ..., 0.0000, 0.0000, 0.0496],
        [0.0558, 0.0000, 0.0604,  ..., 0.0000, 0.0000, 0.0496],
        [0.0558, 0.0000, 0.0604,  ..., 0.0000, 0.0000, 0.0496]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3487143., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36201.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(839.6255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5706.4507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9709.6104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3500.1023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8042],
        [-0.5597],
        [-0.2526],
        ...,
        [-0.9526],
        [-0.9498],
        [-0.9490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1053683.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1421.0306, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.0306, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0051,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11405.1309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1135.1851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.1129, device='cuda:0')



h[100].sum tensor(-413.0874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.9045, device='cuda:0')



h[200].sum tensor(-103.1505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        ...,
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0205, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(353818.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0000, 0.0384,  ..., 0.0000, 0.0000, 0.0544],
        [0.0548, 0.0000, 0.0588,  ..., 0.0000, 0.0000, 0.0507],
        [0.0521, 0.0000, 0.0531,  ..., 0.0000, 0.0000, 0.0519],
        ...,
        [0.0563, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0498],
        [0.0563, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0498],
        [0.0563, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0498]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3576758.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37251.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(872.9846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5768.1406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11990.9229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3601.0452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0256],
        [-0.0987],
        [-0.1171],
        ...,
        [-0.9618],
        [-0.9589],
        [-0.9581]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1117089.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1535.6432, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1535.6432, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11621.9492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1147.0229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.2033, device='cuda:0')



h[100].sum tensor(-398.9020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(231.1569, device='cuda:0')



h[200].sum tensor(-111.5872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0206, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0206, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0206, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        ...,
        [0.0206, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0206, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0206, 0.0021, 0.0000,  ..., 0.0000, 0.0095, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(368211.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0524, 0.0000, 0.0579,  ..., 0.0000, 0.0000, 0.0522],
        [0.0495, 0.0000, 0.0549,  ..., 0.0000, 0.0000, 0.0534],
        [0.0474, 0.0000, 0.0522,  ..., 0.0000, 0.0000, 0.0544],
        ...,
        [0.0566, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0502],
        [0.0571, 0.0000, 0.0616,  ..., 0.0000, 0.0000, 0.0499],
        [0.0571, 0.0000, 0.0616,  ..., 0.0000, 0.0000, 0.0499]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3669205.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37198.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(924.7754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5836.4277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13742.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3692.9521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3891],
        [-0.3253],
        [-0.2698],
        ...,
        [-0.8643],
        [-0.9218],
        [-0.9508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1110063.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1274.6411, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1274.6411, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11170.6133, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1136.5789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.6113, device='cuda:0')



h[100].sum tensor(-438.4589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(191.8689, device='cuda:0')



h[200].sum tensor(-92.6512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0251, 0.0090, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        [0.0207, 0.0021, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0251, 0.0090, 0.0045,  ..., 0.0000, 0.0201, 0.0000],
        ...,
        [0.0207, 0.0021, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0207, 0.0021, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0207, 0.0021, 0.0000,  ..., 0.0000, 0.0094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(343252.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0384, 0.0000, 0.0282,  ..., 0.0000, 0.0000, 0.0583],
        [0.0456, 0.0000, 0.0387,  ..., 0.0000, 0.0000, 0.0554],
        [0.0369, 0.0000, 0.0231,  ..., 0.0000, 0.0000, 0.0591],
        ...,
        [0.0581, 0.0000, 0.0621,  ..., 0.0000, 0.0000, 0.0500],
        [0.0581, 0.0000, 0.0621,  ..., 0.0000, 0.0000, 0.0500],
        [0.0581, 0.0000, 0.0621,  ..., 0.0000, 0.0000, 0.0500]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3562016.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39064.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(858.0115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5764.7349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10364.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3510.0935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1266],
        [ 0.1043],
        [ 0.1555],
        ...,
        [-0.9837],
        [-0.9807],
        [-0.9799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1198868.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1299.0590, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1299.0590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0097,  0.0077,  0.0046,  ..., -0.0082,  0.0134, -0.0089],
        [ 0.0158,  0.0173,  0.0121,  ..., -0.0194,  0.0283, -0.0208],
        ...,
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11230.7236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1141.5532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.6958, device='cuda:0')



h[100].sum tensor(-438.4198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(195.5444, device='cuda:0')



h[200].sum tensor(-93.9238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0253, 0.0092, 0.0046,  ..., 0.0000, 0.0205, 0.0000],
        [0.0383, 0.0298, 0.0196,  ..., 0.0000, 0.0521, 0.0000],
        [0.0530, 0.0531, 0.0358,  ..., 0.0000, 0.0881, 0.0000],
        ...,
        [0.0208, 0.0021, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0208, 0.0021, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0208, 0.0021, 0.0000,  ..., 0.0000, 0.0094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(344126.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0346, 0.0012, 0.0227,  ..., 0.0000, 0.0000, 0.0606],
        [0.0165, 0.0181, 0.0063,  ..., 0.0000, 0.0024, 0.0703],
        [0.0042, 0.0369, 0.0000,  ..., 0.0000, 0.0084, 0.0803],
        ...,
        [0.0586, 0.0000, 0.0625,  ..., 0.0000, 0.0000, 0.0503],
        [0.0586, 0.0000, 0.0625,  ..., 0.0000, 0.0000, 0.0503],
        [0.0586, 0.0000, 0.0625,  ..., 0.0000, 0.0000, 0.0503]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3570822., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41180.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.4285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5774.3408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10763.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3511.1973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1039],
        [ 0.2190],
        [ 0.2717],
        ...,
        [-0.9920],
        [-0.9890],
        [-0.9882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1327920., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 6600 loss: tensor(465.2442, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1470.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1470.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0005, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11649.3525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1155.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.2982, device='cuda:0')



h[100].sum tensor(-415.7567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(221.3109, device='cuda:0')



h[200].sum tensor(-106.8145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        ...,
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0095, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(367320.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0587, 0.0000, 0.0625,  ..., 0.0000, 0.0000, 0.0505],
        [0.0587, 0.0000, 0.0625,  ..., 0.0000, 0.0000, 0.0505],
        [0.0587, 0.0000, 0.0625,  ..., 0.0000, 0.0000, 0.0506],
        ...,
        [0.0585, 0.0000, 0.0624,  ..., 0.0000, 0.0000, 0.0503],
        [0.0585, 0.0000, 0.0624,  ..., 0.0000, 0.0000, 0.0503],
        [0.0585, 0.0000, 0.0624,  ..., 0.0000, 0.0000, 0.0503]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3704017.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39632.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(922.3424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5872.9443, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13955.0576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3676.1108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2693],
        [-1.2715],
        [-1.2598],
        ...,
        [-0.9963],
        [-0.9933],
        [-0.9924]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1228564.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1487.4423, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1487.4423, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11819.5566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1159.5874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.0625, device='cuda:0')



h[100].sum tensor(-415.9120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(223.9013, device='cuda:0')



h[200].sum tensor(-108.2172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        ...,
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(368640.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0584, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0503],
        [0.0584, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0503],
        [0.0585, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0504],
        ...,
        [0.0583, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0501],
        [0.0583, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0501],
        [0.0583, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0501]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3711076.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38313.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(942.9047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5898.7905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13670.9746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3689.6287, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2503],
        [-1.2319],
        [-1.1897],
        ...,
        [-0.9995],
        [-0.9965],
        [-0.9957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1155544.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1266.9144, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1266.9144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11394.8926, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1145.3466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.2681, device='cuda:0')



h[100].sum tensor(-448.4755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.7058, device='cuda:0')



h[200].sum tensor(-91.6535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0260, 0.0103, 0.0054,  ..., 0.0000, 0.0221, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        ...,
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0209, 0.0022, 0.0000,  ..., 0.0000, 0.0096, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(346509.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0000, 0.0332,  ..., 0.0000, 0.0000, 0.0566],
        [0.0536, 0.0000, 0.0534,  ..., 0.0000, 0.0000, 0.0523],
        [0.0576, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0508],
        ...,
        [0.0583, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0501],
        [0.0583, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0501],
        [0.0583, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0501]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3603732., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39769.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(868.7368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5816.0620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10989.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3535.7830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7437],
        [-0.9300],
        [-0.9948],
        ...,
        [-0.9995],
        [-0.9965],
        [-0.9957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1235076.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1650.8274, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1650.8274, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        ...,
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000],
        [ 0.0052,  0.0006, -0.0009,  ...,  0.0000,  0.0024,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12295.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1173.0803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(73.3190, device='cuda:0')



h[100].sum tensor(-395.7105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(248.4953, device='cuda:0')



h[200].sum tensor(-120.1350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0022, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0210, 0.0022, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0210, 0.0022, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        ...,
        [0.0210, 0.0022, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0210, 0.0022, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0210, 0.0022, 0.0000,  ..., 0.0000, 0.0098, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(388505.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0581, 0.0000, 0.0620,  ..., 0.0000, 0.0000, 0.0500],
        [0.0581, 0.0000, 0.0620,  ..., 0.0000, 0.0000, 0.0500],
        [0.0581, 0.0000, 0.0620,  ..., 0.0000, 0.0000, 0.0501],
        ...,
        [0.0579, 0.0000, 0.0620,  ..., 0.0000, 0.0000, 0.0498],
        [0.0579, 0.0000, 0.0620,  ..., 0.0000, 0.0000, 0.0498],
        [0.0579, 0.0000, 0.0620,  ..., 0.0000, 0.0000, 0.0498]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3812663., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37851.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(990.2944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5969.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16341.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3814.0073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1798],
        [-1.2313],
        [-1.2694],
        ...,
        [-0.9981],
        [-0.9943],
        [-0.9930]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1137773.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1279.8850, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1279.8850, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        ...,
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11732.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1152.0634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.8442, device='cuda:0')



h[100].sum tensor(-454.9428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(192.6582, device='cuda:0')



h[200].sum tensor(-91.7549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        ...,
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0099, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(348851.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0472, 0.0000, 0.0419,  ..., 0.0000, 0.0000, 0.0538],
        [0.0517, 0.0000, 0.0518,  ..., 0.0000, 0.0000, 0.0520],
        [0.0522, 0.0000, 0.0533,  ..., 0.0000, 0.0000, 0.0519],
        ...,
        [0.0575, 0.0000, 0.0616,  ..., 0.0000, 0.0000, 0.0495],
        [0.0575, 0.0000, 0.0616,  ..., 0.0000, 0.0000, 0.0495],
        [0.0575, 0.0000, 0.0616,  ..., 0.0000, 0.0000, 0.0495]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3610509.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37410.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(892.2832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5850.6748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10452.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3543.7102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0080],
        [-0.1318],
        [-0.2179],
        ...,
        [-1.0031],
        [-1.0002],
        [-0.9993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1110318.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1419.9579, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1419.9579, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        ...,
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12148.4863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1164.8190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.0653, device='cuda:0')



h[100].sum tensor(-436.3329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.7431, device='cuda:0')



h[200].sum tensor(-102.8128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        ...,
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0211, 0.0023, 0.0000,  ..., 0.0000, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(368328.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0575, 0.0000, 0.0616,  ..., 0.0000, 0.0000, 0.0496],
        [0.0571, 0.0000, 0.0613,  ..., 0.0000, 0.0000, 0.0498],
        [0.0562, 0.0000, 0.0606,  ..., 0.0000, 0.0000, 0.0503],
        ...,
        [0.0574, 0.0000, 0.0615,  ..., 0.0000, 0.0000, 0.0494],
        [0.0574, 0.0000, 0.0615,  ..., 0.0000, 0.0000, 0.0494],
        [0.0574, 0.0000, 0.0615,  ..., 0.0000, 0.0000, 0.0494]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3721917.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37140.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(938.6151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5924.6172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13460.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3671.5759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0998],
        [-0.9835],
        [-0.8115],
        ...,
        [-1.0068],
        [-1.0039],
        [-1.0031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1108276.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1202.7402, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1202.7402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        ...,
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11784.4404, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1153.4734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.4179, device='cuda:0')



h[100].sum tensor(-470.5297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(181.0458, device='cuda:0')



h[200].sum tensor(-86.8574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0277, 0.0127, 0.0071,  ..., 0.0000, 0.0260, 0.0000],
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        ...,
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(346133.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0348, 0.0022, 0.0230,  ..., 0.0000, 0.0000, 0.0594],
        [0.0502, 0.0000, 0.0456,  ..., 0.0000, 0.0000, 0.0531],
        [0.0530, 0.0000, 0.0551,  ..., 0.0000, 0.0000, 0.0521],
        ...,
        [0.0575, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0499],
        [0.0575, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0499],
        [0.0575, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0499]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3607345.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36824.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.9801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5875.3003, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9770.7803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3534.7083, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1007],
        [-0.0480],
        [-0.1237],
        ...,
        [-1.0124],
        [-1.0098],
        [-1.0091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1076800.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1431.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1431.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097,  0.0075,  0.0045,  ..., -0.0079,  0.0132, -0.0086],
        [ 0.0095,  0.0073,  0.0043,  ..., -0.0077,  0.0128, -0.0083],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        ...,
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12218.3730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1167.7273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.5610, device='cuda:0')



h[100].sum tensor(-437.9226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.4231, device='cuda:0')



h[200].sum tensor(-103.3684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0445, 0.0394, 0.0251,  ..., 0.0000, 0.0672, 0.0000],
        [0.0290, 0.0148, 0.0078,  ..., 0.0000, 0.0292, 0.0000],
        [0.0254, 0.0091, 0.0043,  ..., 0.0000, 0.0203, 0.0000],
        ...,
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0024, 0.0000,  ..., 0.0000, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(366029.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0687],
        [0.0266, 0.0000, 0.0169,  ..., 0.0000, 0.0000, 0.0624],
        [0.0385, 0.0000, 0.0313,  ..., 0.0000, 0.0000, 0.0578],
        ...,
        [0.0575, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0499],
        [0.0575, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0499],
        [0.0575, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0499]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3704184., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36649.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(940.6731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5941.3535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12489.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3667.7239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0191],
        [-0.0842],
        [-0.2318],
        ...,
        [-1.0131],
        [-1.0102],
        [-1.0093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1077763., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1344.4049, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1344.4049, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        ...,
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12060.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1165.2722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.7097, device='cuda:0')



h[100].sum tensor(-452.6576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(202.3702, device='cuda:0')



h[200].sum tensor(-97.1212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0212, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        ...,
        [0.0212, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0212, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(364168.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0581, 0.0000, 0.0624,  ..., 0.0000, 0.0000, 0.0507],
        [0.0581, 0.0000, 0.0624,  ..., 0.0000, 0.0000, 0.0507],
        [0.0582, 0.0000, 0.0624,  ..., 0.0000, 0.0000, 0.0508],
        ...,
        [0.0580, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0505],
        [0.0580, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0505],
        [0.0580, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0505]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3726945., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37394.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(934.1416, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5955.0781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12797.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3665.5601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2548],
        [-1.2502],
        [-1.2122],
        ...,
        [-1.0231],
        [-1.0201],
        [-1.0192]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1116592.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1178.3162, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1178.3162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0151,  0.0162,  0.0111,  ..., -0.0178,  0.0265, -0.0192],
        ...,
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11755.0996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1158.9819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(52.3331, device='cuda:0')



h[100].sum tensor(-479.6954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(177.3693, device='cuda:0')



h[200].sum tensor(-84.8497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0356, 0.0253, 0.0158,  ..., 0.0000, 0.0451, 0.0000],
        [0.0391, 0.0307, 0.0201,  ..., 0.0000, 0.0536, 0.0000],
        [0.0569, 0.0589, 0.0410,  ..., 0.0000, 0.0972, 0.0000],
        ...,
        [0.0213, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0213, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0213, 0.0025, 0.0000,  ..., 0.0000, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(341413.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0063, 0.0209, 0.0000,  ..., 0.0000, 0.0034, 0.0744],
        [0.0057, 0.0205, 0.0000,  ..., 0.0000, 0.0021, 0.0738],
        [0.0036, 0.0381, 0.0000,  ..., 0.0000, 0.0103, 0.0791],
        ...,
        [0.0586, 0.0000, 0.0629,  ..., 0.0000, 0.0000, 0.0510],
        [0.0586, 0.0000, 0.0629,  ..., 0.0000, 0.0000, 0.0510],
        [0.0586, 0.0000, 0.0629,  ..., 0.0000, 0.0000, 0.0510]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3608513., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39142.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(867.2468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5895.9531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9312.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3514.0735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3116],
        [ 0.3222],
        [ 0.3246],
        ...,
        [-1.0354],
        [-1.0323],
        [-1.0315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1214648.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 6900 loss: tensor(409.9395, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1209.0081, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1209.0081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        ...,
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0053,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11815.1582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1165.1204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.6963, device='cuda:0')



h[100].sum tensor(-478.9690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(181.9893, device='cuda:0')



h[200].sum tensor(-86.6274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0214, 0.0025, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0214, 0.0025, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0313, 0.0182, 0.0104,  ..., 0.0000, 0.0342, 0.0000],
        ...,
        [0.0214, 0.0025, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0214, 0.0025, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0214, 0.0025, 0.0000,  ..., 0.0000, 0.0099, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(343673.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0509, 0.0000, 0.0521,  ..., 0.0000, 0.0000, 0.0550],
        [0.0422, 0.0000, 0.0385,  ..., 0.0000, 0.0000, 0.0584],
        [0.0244, 0.0005, 0.0155,  ..., 0.0000, 0.0000, 0.0655],
        ...,
        [0.0593, 0.0000, 0.0634,  ..., 0.0000, 0.0000, 0.0514],
        [0.0593, 0.0000, 0.0634,  ..., 0.0000, 0.0000, 0.0514],
        [0.0593, 0.0000, 0.0634,  ..., 0.0000, 0.0000, 0.0514]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3631181., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39359.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(880.9569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5925.9414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9331.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3531.6860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5354],
        [-0.2765],
        [-0.0478],
        ...,
        [-1.0472],
        [-1.0440],
        [-1.0431]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1231195., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1221.5986, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1221.5986, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0152,  0.0163,  0.0113,  ..., -0.0179,  0.0268, -0.0193],
        [ 0.0152,  0.0163,  0.0113,  ..., -0.0179,  0.0268, -0.0193],
        [ 0.0088,  0.0060,  0.0033,  ..., -0.0062,  0.0108, -0.0066],
        ...,
        [ 0.0054,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0006, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11884.1973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1169.4905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.2555, device='cuda:0')



h[100].sum tensor(-480.1249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(183.8845, device='cuda:0')



h[200].sum tensor(-87.5145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0394, 0.0311, 0.0203,  ..., 0.0000, 0.0540, 0.0000],
        [0.0498, 0.0476, 0.0322,  ..., 0.0000, 0.0796, 0.0000],
        [0.0599, 0.0636, 0.0437,  ..., 0.0000, 0.1043, 0.0000],
        ...,
        [0.0215, 0.0026, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0215, 0.0026, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0215, 0.0026, 0.0000,  ..., 0.0000, 0.0099, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(343038.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0270, 0.0000,  ..., 0.0000, 0.0044, 0.0780],
        [0.0000, 0.0410, 0.0000,  ..., 0.0000, 0.0102, 0.0828],
        [0.0000, 0.0550, 0.0000,  ..., 0.0000, 0.0192, 0.0872],
        ...,
        [0.0596, 0.0000, 0.0637,  ..., 0.0000, 0.0000, 0.0517],
        [0.0596, 0.0000, 0.0637,  ..., 0.0000, 0.0000, 0.0517],
        [0.0596, 0.0000, 0.0637,  ..., 0.0000, 0.0000, 0.0517]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3628803., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39796.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(876.0647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5940.2393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8940.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3529.6123, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3395],
        [ 0.3315],
        [ 0.3258],
        ...,
        [-1.0561],
        [-1.0529],
        [-1.0520]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1265800.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1289.7423, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1289.7423, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0227,  0.0281,  0.0204,  ..., -0.0314,  0.0450, -0.0338],
        [ 0.0299,  0.0396,  0.0293,  ..., -0.0444,  0.0627, -0.0479],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12053.8564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1175.4976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.2820, device='cuda:0')



h[100].sum tensor(-473.7899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(194.1420, device='cuda:0')



h[200].sum tensor(-92.2173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1094, 0.1423, 0.1047,  ..., 0.0000, 0.2260, 0.0000],
        [0.0641, 0.0704, 0.0498,  ..., 0.0000, 0.1147, 0.0000],
        [0.0761, 0.0893, 0.0636,  ..., 0.0000, 0.1441, 0.0000],
        ...,
        [0.0215, 0.0027, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0215, 0.0027, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0215, 0.0027, 0.0000,  ..., 0.0000, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(353974.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1305, 0.0000,  ..., 0.0000, 0.0691, 0.1039],
        [0.0028, 0.0772, 0.0000,  ..., 0.0000, 0.0344, 0.0892],
        [0.0065, 0.0481, 0.0000,  ..., 0.0000, 0.0176, 0.0809],
        ...,
        [0.0594, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0521],
        [0.0594, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0521],
        [0.0594, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0521]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3696660.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39025.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(903.9248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5995.9043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10502.6279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3613.4895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2437],
        [ 0.2476],
        [ 0.2144],
        ...,
        [-1.0609],
        [-1.0577],
        [-1.0568]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1228516., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1437.6777, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1437.6777, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0195,  0.0231,  0.0165,  ..., -0.0256,  0.0373, -0.0276],
        [ 0.0213,  0.0259,  0.0187,  ..., -0.0288,  0.0415, -0.0310],
        [ 0.0175,  0.0199,  0.0140,  ..., -0.0219,  0.0322, -0.0236],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12411.3613, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1187.7300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.8523, device='cuda:0')



h[100].sum tensor(-456.2357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(216.4104, device='cuda:0')



h[200].sum tensor(-102.6789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1135, 0.1488, 0.1097,  ..., 0.0000, 0.2361, 0.0000],
        [0.0853, 0.1038, 0.0749,  ..., 0.0000, 0.1666, 0.0000],
        [0.0518, 0.0508, 0.0346,  ..., 0.0000, 0.0844, 0.0000],
        ...,
        [0.0216, 0.0027, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0216, 0.0027, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0216, 0.0027, 0.0000,  ..., 0.0000, 0.0101, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(367557.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1372, 0.0000,  ..., 0.0000, 0.0737, 0.1059],
        [0.0000, 0.0967, 0.0000,  ..., 0.0000, 0.0470, 0.0957],
        [0.0085, 0.0470, 0.0000,  ..., 0.0000, 0.0188, 0.0810],
        ...,
        [0.0594, 0.0000, 0.0639,  ..., 0.0000, 0.0000, 0.0521],
        [0.0594, 0.0000, 0.0639,  ..., 0.0000, 0.0000, 0.0521],
        [0.0594, 0.0000, 0.0639,  ..., 0.0000, 0.0000, 0.0521]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3765500.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38210.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(943.3315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6057.6836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11870.9170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3705.2263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2270],
        [ 0.2372],
        [ 0.2178],
        ...,
        [-1.0669],
        [-1.0638],
        [-1.0628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1184155.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1405.6693, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1405.6693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0090,  0.0064,  0.0036,  ..., -0.0066,  0.0114, -0.0071],
        [ 0.0090,  0.0064,  0.0036,  ..., -0.0066,  0.0114, -0.0071],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0025,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12472.6777, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1188.7688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.4307, device='cuda:0')



h[100].sum tensor(-463.9673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(211.5922, device='cuda:0')



h[200].sum tensor(-100.5447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0282, 0.0132, 0.0063,  ..., 0.0000, 0.0263, 0.0000],
        [0.0282, 0.0132, 0.0063,  ..., 0.0000, 0.0263, 0.0000],
        [0.0282, 0.0132, 0.0063,  ..., 0.0000, 0.0263, 0.0000],
        ...,
        [0.0216, 0.0027, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0216, 0.0027, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0216, 0.0027, 0.0000,  ..., 0.0000, 0.0102, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(370388.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0326, 0.0000, 0.0261,  ..., 0.0000, 0.0000, 0.0627],
        [0.0276, 0.0000, 0.0182,  ..., 0.0000, 0.0000, 0.0646],
        [0.0275, 0.0000, 0.0182,  ..., 0.0000, 0.0000, 0.0648],
        ...,
        [0.0592, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0520],
        [0.0592, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0520],
        [0.0592, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0520]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3796603.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38325.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(940.0779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6074.3374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12713.5244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3719.3284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0323],
        [ 0.0756],
        [ 0.1291],
        ...,
        [-1.0686],
        [-1.0652],
        [-1.0638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1209711., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1085.7510, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1085.7510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11945.3965, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1171.6731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(48.2220, device='cuda:0')



h[100].sum tensor(-513.4927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(163.4357, device='cuda:0')



h[200].sum tensor(-77.4072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        ...,
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337793.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0592, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0521],
        [0.0592, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0521],
        [0.0592, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0522],
        ...,
        [0.0591, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0519],
        [0.0591, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0519],
        [0.0591, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0519]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3636464., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38538.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(854.8032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5979.5684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8059.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3499.2876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3488],
        [-1.3304],
        [-1.2915],
        ...,
        [-1.0759],
        [-1.0728],
        [-1.0719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1223862.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1428.5687, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1428.5687, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0093,  0.0069,  0.0040,  ..., -0.0071,  0.0122, -0.0076],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12613.3428, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1192.7688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.4477, device='cuda:0')



h[100].sum tensor(-465.1921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.0392, device='cuda:0')



h[200].sum tensor(-101.6426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0256, 0.0090, 0.0040,  ..., 0.0000, 0.0199, 0.0000],
        [0.0249, 0.0078, 0.0031,  ..., 0.0000, 0.0181, 0.0000],
        ...,
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(369608.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0434, 0.0000, 0.0433,  ..., 0.0000, 0.0000, 0.0582],
        [0.0397, 0.0000, 0.0368,  ..., 0.0000, 0.0000, 0.0596],
        [0.0350, 0.0000, 0.0285,  ..., 0.0000, 0.0000, 0.0616],
        ...,
        [0.0591, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0519],
        [0.0560, 0.0000, 0.0580,  ..., 0.0000, 0.0000, 0.0531],
        [0.0469, 0.0000, 0.0372,  ..., 0.0000, 0.0000, 0.0567]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3777986., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38693.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(929.0918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6080.0420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12173.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3709.4426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0073],
        [ 0.0485],
        [ 0.1394],
        ...,
        [-0.9570],
        [-0.7424],
        [-0.4079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1242359.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1314.9845, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1314.9845, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12399.1465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1186.0039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.4031, device='cuda:0')



h[100].sum tensor(-480.6811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.9417, device='cuda:0')



h[200].sum tensor(-93.8708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0249, 0.0078, 0.0031,  ..., 0.0000, 0.0181, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0291, 0.0146, 0.0074,  ..., 0.0000, 0.0286, 0.0000],
        ...,
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0102, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(358437.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0353, 0.0000, 0.0277,  ..., 0.0000, 0.0000, 0.0614],
        [0.0378, 0.0000, 0.0315,  ..., 0.0000, 0.0000, 0.0604],
        [0.0263, 0.0000, 0.0139,  ..., 0.0000, 0.0000, 0.0650],
        ...,
        [0.0591, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0519],
        [0.0591, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0519],
        [0.0591, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0519]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3725841.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37890.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(911.4227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6050.0742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10368.6787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3638.5691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1271],
        [ 0.1686],
        [ 0.2198],
        ...,
        [-1.0759],
        [-1.0728],
        [-1.0719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1186187.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1142.8589, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1142.8589, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0130,  0.0127,  0.0084,  ..., -0.0136,  0.0211, -0.0147],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12140.1035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1178.5034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(50.7584, device='cuda:0')



h[100].sum tensor(-509.3017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(172.0320, device='cuda:0')



h[200].sum tensor(-81.2630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0545, 0.0548, 0.0369,  ..., 0.0000, 0.0909, 0.0000],
        [0.0279, 0.0125, 0.0067,  ..., 0.0000, 0.0255, 0.0000],
        [0.0433, 0.0371, 0.0240,  ..., 0.0000, 0.0635, 0.0000],
        ...,
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0217, 0.0028, 0.0000,  ..., 0.0000, 0.0103, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(343412.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0121, 0.0000,  ..., 0.0000, 0.0012, 0.0721],
        [0.0238, 0.0006, 0.0000,  ..., 0.0000, 0.0000, 0.0657],
        [0.0116, 0.0110, 0.0000,  ..., 0.0000, 0.0013, 0.0713],
        ...,
        [0.0589, 0.0000, 0.0637,  ..., 0.0000, 0.0000, 0.0518],
        [0.0589, 0.0000, 0.0637,  ..., 0.0000, 0.0000, 0.0518],
        [0.0589, 0.0000, 0.0637,  ..., 0.0000, 0.0000, 0.0518]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3659336.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38722.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(861.7983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6004.6045, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8669.9521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3530.1956, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1203],
        [ 0.1230],
        [ 0.1824],
        ...,
        [-1.0799],
        [-1.0768],
        [-1.0759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1241287.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1248.3439, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1248.3439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0092,  0.0067,  0.0038,  ..., -0.0068,  0.0119, -0.0073],
        ...,
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0054,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12388.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1187.9724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.4433, device='cuda:0')



h[100].sum tensor(-497.8376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.9104, device='cuda:0')



h[200].sum tensor(-88.5156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0218, 0.0028, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0286, 0.0137, 0.0067,  ..., 0.0000, 0.0272, 0.0000],
        [0.0286, 0.0137, 0.0067,  ..., 0.0000, 0.0272, 0.0000],
        ...,
        [0.0218, 0.0028, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0218, 0.0028, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0218, 0.0028, 0.0000,  ..., 0.0000, 0.0104, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(354179.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0476, 0.0000, 0.0493,  ..., 0.0000, 0.0000, 0.0567],
        [0.0321, 0.0000, 0.0257,  ..., 0.0000, 0.0000, 0.0626],
        [0.0270, 0.0000, 0.0177,  ..., 0.0000, 0.0000, 0.0647],
        ...,
        [0.0590, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0520],
        [0.0590, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0520],
        [0.0590, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0520]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3713965.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37637.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(900.8320, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6057.9678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9547.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3607.3132, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2539],
        [-0.1018],
        [-0.0178],
        ...,
        [-1.0855],
        [-1.0824],
        [-1.0815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1166534.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 7200 loss: tensor(418.6440, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1453.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1453.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0097,  0.0074,  0.0043,  ..., -0.0076,  0.0130, -0.0082],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12768.2744, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1203.2004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.5491, device='cuda:0')



h[100].sum tensor(-471.5728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(218.7721, device='cuda:0')



h[200].sum tensor(-102.8820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0422, 0.0353, 0.0216,  ..., 0.0000, 0.0606, 0.0000],
        [0.0278, 0.0124, 0.0065,  ..., 0.0000, 0.0251, 0.0000],
        [0.0307, 0.0169, 0.0091,  ..., 0.0000, 0.0321, 0.0000],
        ...,
        [0.0218, 0.0029, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0218, 0.0029, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0218, 0.0029, 0.0000,  ..., 0.0000, 0.0104, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(374222.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0123, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0711],
        [0.0234, 0.0007, 0.0111,  ..., 0.0000, 0.0000, 0.0668],
        [0.0230, 0.0000, 0.0058,  ..., 0.0000, 0.0000, 0.0670],
        ...,
        [0.0593, 0.0000, 0.0642,  ..., 0.0000, 0.0000, 0.0526],
        [0.0593, 0.0000, 0.0642,  ..., 0.0000, 0.0000, 0.0526],
        [0.0593, 0.0000, 0.0642,  ..., 0.0000, 0.0000, 0.0526]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3828654., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37710.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(952.2811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6138.8599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12418.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3751.8406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2803],
        [ 0.2886],
        [ 0.2581],
        ...,
        [-1.0934],
        [-1.0902],
        [-1.0893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1171507., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1233.9087, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1233.9087, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12302.6289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1194.3423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.8022, device='cuda:0')



h[100].sum tensor(-504.1931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(185.7375, device='cuda:0')



h[200].sum tensor(-87.7575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0219, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0219, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0219, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        ...,
        [0.0219, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0219, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0219, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(358736.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0000, 0.0373,  ..., 0.0000, 0.0000, 0.0581],
        [0.0548, 0.0000, 0.0566,  ..., 0.0000, 0.0000, 0.0555],
        [0.0564, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0550],
        ...,
        [0.0601, 0.0000, 0.0650,  ..., 0.0000, 0.0000, 0.0531],
        [0.0601, 0.0000, 0.0650,  ..., 0.0000, 0.0000, 0.0531],
        [0.0601, 0.0000, 0.0650,  ..., 0.0000, 0.0000, 0.0531]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3786355., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38902.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(913.1245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6104.9648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10677.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3649.2649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1456],
        [-0.3972],
        [-0.5682],
        ...,
        [-1.1065],
        [-1.1033],
        [-1.1024]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1228473.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1394.6367, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1394.6367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12611.0684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1210.5919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.9407, device='cuda:0')



h[100].sum tensor(-484.2665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.9315, device='cuda:0')



h[200].sum tensor(-99.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0261, 0.0094, 0.0042,  ..., 0.0000, 0.0204, 0.0000],
        ...,
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(371889.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0000, 0.0644,  ..., 0.0000, 0.0000, 0.0538],
        [0.0554, 0.0000, 0.0557,  ..., 0.0000, 0.0000, 0.0557],
        [0.0411, 0.0000, 0.0307,  ..., 0.0000, 0.0000, 0.0614],
        ...,
        [0.0611, 0.0000, 0.0657,  ..., 0.0000, 0.0000, 0.0532],
        [0.0611, 0.0000, 0.0657,  ..., 0.0000, 0.0000, 0.0532],
        [0.0611, 0.0000, 0.0657,  ..., 0.0000, 0.0000, 0.0532]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3839758.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38742.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(969.0068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6173.1372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11224.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3729.7966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6860],
        [-0.4599],
        [-0.1729],
        ...,
        [-1.1194],
        [-1.1161],
        [-1.1151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1211668.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1356.9214, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1356.9214, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0098,  0.0076,  0.0044,  ..., -0.0078,  0.0132, -0.0084],
        [ 0.0118,  0.0106,  0.0068,  ..., -0.0112,  0.0179, -0.0121],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12593.0391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1213.2167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.2656, device='cuda:0')



h[100].sum tensor(-491.9753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(204.2543, device='cuda:0')



h[200].sum tensor(-96.5860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0584, 0.0606, 0.0413,  ..., 0.0000, 0.0998, 0.0000],
        [0.0315, 0.0178, 0.0098,  ..., 0.0000, 0.0335, 0.0000],
        [0.0284, 0.0128, 0.0068,  ..., 0.0000, 0.0257, 0.0000],
        ...,
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(366204.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0247, 0.0000,  ..., 0.0000, 0.0055, 0.0803],
        [0.0185, 0.0080, 0.0112,  ..., 0.0000, 0.0000, 0.0704],
        [0.0325, 0.0009, 0.0210,  ..., 0.0000, 0.0000, 0.0650],
        ...,
        [0.0613, 0.0000, 0.0659,  ..., 0.0000, 0.0000, 0.0533],
        [0.0613, 0.0000, 0.0659,  ..., 0.0000, 0.0000, 0.0533],
        [0.0613, 0.0000, 0.0659,  ..., 0.0000, 0.0000, 0.0533]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3818094.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40966.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(932.1141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6153.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11022.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3678.1765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3728],
        [ 0.3198],
        [ 0.2711],
        ...,
        [-1.1269],
        [-1.1236],
        [-1.1226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1366071.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1244.4573, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1244.4573, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0198,  0.0235,  0.0168,  ..., -0.0258,  0.0379, -0.0278],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0101,  0.0080,  0.0047,  ..., -0.0082,  0.0138, -0.0089],
        ...,
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0007, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12363.7959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1206.1863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.2707, device='cuda:0')



h[100].sum tensor(-508.0957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.3253, device='cuda:0')



h[200].sum tensor(-88.5530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0612, 0.0650, 0.0456,  ..., 0.0000, 0.1066, 0.0000],
        [0.0550, 0.0551, 0.0370,  ..., 0.0000, 0.0913, 0.0000],
        [0.0259, 0.0088, 0.0037,  ..., 0.0000, 0.0195, 0.0000],
        ...,
        [0.0338, 0.0214, 0.0126,  ..., 0.0000, 0.0391, 0.0000],
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000],
        [0.0221, 0.0029, 0.0000,  ..., 0.0000, 0.0103, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(355627.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0569, 0.0000,  ..., 0.0000, 0.0233, 0.0868],
        [0.0077, 0.0294, 0.0000,  ..., 0.0000, 0.0100, 0.0794],
        [0.0253, 0.0046, 0.0155,  ..., 0.0000, 0.0000, 0.0677],
        ...,
        [0.0265, 0.0042, 0.0158,  ..., 0.0000, 0.0000, 0.0668],
        [0.0492, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0580],
        [0.0583, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0545]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3769955.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40711.0820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(908.9308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6120.5361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9493.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3608.4299, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3321],
        [ 0.3131],
        [ 0.2626],
        ...,
        [-0.0736],
        [-0.4477],
        [-0.7952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1337769., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1397.9553, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1397.9553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0099,  0.0078,  0.0046,  ..., -0.0079,  0.0135, -0.0086],
        [ 0.0055,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        ...,
        [ 0.0055,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0055,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12705.4482, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1216.0370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.0881, device='cuda:0')



h[100].sum tensor(-490.1838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(210.4311, device='cuda:0')



h[200].sum tensor(-98.7456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0381, 0.0285, 0.0162,  ..., 0.0000, 0.0499, 0.0000],
        [0.0258, 0.0087, 0.0036,  ..., 0.0000, 0.0193, 0.0000],
        [0.0266, 0.0100, 0.0046,  ..., 0.0000, 0.0213, 0.0000],
        ...,
        [0.0222, 0.0030, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0222, 0.0030, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0222, 0.0030, 0.0000,  ..., 0.0000, 0.0104, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(371039.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0319, 0.0000, 0.0204,  ..., 0.0000, 0.0000, 0.0650],
        [0.0408, 0.0000, 0.0327,  ..., 0.0000, 0.0000, 0.0615],
        [0.0412, 0.0000, 0.0273,  ..., 0.0000, 0.0000, 0.0616],
        ...,
        [0.0607, 0.0000, 0.0658,  ..., 0.0000, 0.0000, 0.0536],
        [0.0607, 0.0000, 0.0658,  ..., 0.0000, 0.0000, 0.0536],
        [0.0607, 0.0000, 0.0658,  ..., 0.0000, 0.0000, 0.0536]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3849164.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38982.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(956.7812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6194.8296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11508.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3732.6550, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0651],
        [ 0.0708],
        [ 0.1088],
        ...,
        [-1.1289],
        [-1.1256],
        [-1.1246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1247851.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1365.4856, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1365.4856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0161,  0.0176,  0.0122,  ..., -0.0191,  0.0288, -0.0206],
        [ 0.0123,  0.0115,  0.0075,  ..., -0.0122,  0.0193, -0.0131],
        [ 0.0154,  0.0165,  0.0113,  ..., -0.0178,  0.0270, -0.0192],
        ...,
        [ 0.0056,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0056,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000],
        [ 0.0056,  0.0008, -0.0009,  ...,  0.0000,  0.0026,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12766.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1216.9176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.6460, device='cuda:0')



h[100].sum tensor(-497.0448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(205.5435, device='cuda:0')



h[200].sum tensor(-96.8770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0554, 0.0559, 0.0375,  ..., 0.0000, 0.0924, 0.0000],
        [0.0604, 0.0639, 0.0437,  ..., 0.0000, 0.1048, 0.0000],
        [0.0600, 0.0632, 0.0432,  ..., 0.0000, 0.1038, 0.0000],
        ...,
        [0.0222, 0.0031, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        [0.0222, 0.0031, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        [0.0222, 0.0031, 0.0000,  ..., 0.0000, 0.0106, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(375064.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0347, 0.0000,  ..., 0.0000, 0.0082, 0.0835],
        [0.0000, 0.0446, 0.0000,  ..., 0.0000, 0.0148, 0.0865],
        [0.0000, 0.0596, 0.0000,  ..., 0.0000, 0.0248, 0.0909],
        ...,
        [0.0600, 0.0000, 0.0654,  ..., 0.0000, 0.0000, 0.0535],
        [0.0600, 0.0000, 0.0654,  ..., 0.0000, 0.0000, 0.0535],
        [0.0600, 0.0000, 0.0654,  ..., 0.0000, 0.0000, 0.0535]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3870858.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36939.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(977.5706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6227.1240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11612.3193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3767.6492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3660],
        [ 0.3648],
        [ 0.3607],
        ...,
        [-1.1282],
        [-1.1250],
        [-1.1241]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1124770.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1276.9055, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1276.9055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12701.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1216.9407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.7118, device='cuda:0')



h[100].sum tensor(-514.0523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(192.2097, device='cuda:0')



h[200].sum tensor(-90.1177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0223, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0223, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0223, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        ...,
        [0.0223, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0223, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0223, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(364780.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0543, 0.0000, 0.0577,  ..., 0.0000, 0.0000, 0.0554],
        [0.0596, 0.0000, 0.0650,  ..., 0.0000, 0.0000, 0.0534],
        [0.0600, 0.0000, 0.0652,  ..., 0.0000, 0.0000, 0.0534],
        ...,
        [0.0598, 0.0000, 0.0652,  ..., 0.0000, 0.0000, 0.0531],
        [0.0598, 0.0000, 0.0652,  ..., 0.0000, 0.0000, 0.0531],
        [0.0598, 0.0000, 0.0652,  ..., 0.0000, 0.0000, 0.0531]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3814735., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37458.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(943.0698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6193.8828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10051.7246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3676.6096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4734],
        [-0.6823],
        [-0.7595],
        ...,
        [-1.1332],
        [-1.1300],
        [-1.1290]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1173929.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1344.4023, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1344.4023, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12917.9570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1226.6381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.7096, device='cuda:0')



h[100].sum tensor(-507.8538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(202.3699, device='cuda:0')



h[200].sum tensor(-94.7151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0224, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0224, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0224, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        ...,
        [0.0224, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0224, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0224, 0.0031, 0.0000,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(370867.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0577, 0.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0541],
        [0.0593, 0.0000, 0.0646,  ..., 0.0000, 0.0000, 0.0534],
        [0.0578, 0.0000, 0.0617,  ..., 0.0000, 0.0000, 0.0541],
        ...,
        [0.0599, 0.0000, 0.0652,  ..., 0.0000, 0.0000, 0.0529],
        [0.0599, 0.0000, 0.0652,  ..., 0.0000, 0.0000, 0.0529],
        [0.0599, 0.0000, 0.0652,  ..., 0.0000, 0.0000, 0.0529]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3844910.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36999.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(965.1315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6227.5840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10404.1572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3703.8430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4542],
        [-0.4237],
        [-0.3087],
        ...,
        [-1.1400],
        [-1.1367],
        [-1.1358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1163298.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1250.0739, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1250.0739, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12745.4287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1226.1162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.5202, device='cuda:0')



h[100].sum tensor(-523.9866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(188.1708, device='cuda:0')



h[200].sum tensor(-88.1054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0256, 0.0080, 0.0029,  ..., 0.0000, 0.0183, 0.0000],
        [0.0344, 0.0221, 0.0131,  ..., 0.0000, 0.0401, 0.0000],
        [0.0372, 0.0265, 0.0165,  ..., 0.0000, 0.0469, 0.0000],
        ...,
        [0.0225, 0.0031, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0225, 0.0031, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0225, 0.0031, 0.0000,  ..., 0.0000, 0.0108, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(364383.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0293, 0.0013, 0.0163,  ..., 0.0000, 0.0000, 0.0648],
        [0.0168, 0.0084, 0.0046,  ..., 0.0000, 0.0000, 0.0700],
        [0.0140, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0711],
        ...,
        [0.0603, 0.0000, 0.0654,  ..., 0.0000, 0.0000, 0.0531],
        [0.0603, 0.0000, 0.0654,  ..., 0.0000, 0.0000, 0.0531],
        [0.0603, 0.0000, 0.0654,  ..., 0.0000, 0.0000, 0.0531]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3838341.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38044.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(942.5657, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6214.7949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9993.0518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3650.8997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2205],
        [ 0.2617],
        [ 0.2643],
        ...,
        [-1.1471],
        [-1.1437],
        [-1.1427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1231724.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 7500 loss: tensor(461.0337, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1611.6145, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1611.6145, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0128,  0.0122,  0.0080,  ..., -0.0129,  0.0204, -0.0139],
        [ 0.0141,  0.0142,  0.0096,  ..., -0.0152,  0.0235, -0.0164],
        ...,
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0056,  0.0008, -0.0008,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13437.1309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1251.5736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.5774, device='cuda:0')



h[100].sum tensor(-475.0629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(242.5927, device='cuda:0')



h[200].sum tensor(-113.3102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0298, 0.0147, 0.0080,  ..., 0.0000, 0.0285, 0.0000],
        [0.0397, 0.0305, 0.0195,  ..., 0.0000, 0.0531, 0.0000],
        [0.0618, 0.0656, 0.0451,  ..., 0.0000, 0.1075, 0.0000],
        ...,
        [0.0226, 0.0032, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0226, 0.0032, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0226, 0.0032, 0.0000,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(398024.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0291, 0.0020, 0.0181,  ..., 0.0000, 0.0000, 0.0658],
        [0.0145, 0.0123, 0.0045,  ..., 0.0000, 0.0009, 0.0725],
        [0.0028, 0.0255, 0.0000,  ..., 0.0000, 0.0038, 0.0795],
        ...,
        [0.0607, 0.0000, 0.0661,  ..., 0.0000, 0.0000, 0.0539],
        [0.0607, 0.0000, 0.0661,  ..., 0.0000, 0.0000, 0.0539],
        [0.0607, 0.0000, 0.0661,  ..., 0.0000, 0.0000, 0.0539]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3997933., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37180.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1047.7073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6355.0869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13543.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3893.7644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1974],
        [ 0.3005],
        [ 0.3502],
        ...,
        [-1.1595],
        [-1.1561],
        [-1.1552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1169938.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1322.5039, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1322.5039, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12784.2930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1236.1968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.7370, device='cuda:0')



h[100].sum tensor(-517.9414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(199.0735, device='cuda:0')



h[200].sum tensor(-92.9745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0226, 0.0033, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0226, 0.0033, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0226, 0.0033, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        ...,
        [0.0226, 0.0033, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0226, 0.0033, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0226, 0.0033, 0.0000,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(370677.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0000, 0.0667,  ..., 0.0000, 0.0000, 0.0549],
        [0.0614, 0.0000, 0.0667,  ..., 0.0000, 0.0000, 0.0549],
        [0.0614, 0.0000, 0.0667,  ..., 0.0000, 0.0000, 0.0550],
        ...,
        [0.0612, 0.0000, 0.0667,  ..., 0.0000, 0.0000, 0.0547],
        [0.0612, 0.0000, 0.0667,  ..., 0.0000, 0.0000, 0.0547],
        [0.0612, 0.0000, 0.0667,  ..., 0.0000, 0.0000, 0.0547]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3891859., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38201.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(980.9487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6285.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10456.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3730.9695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4432],
        [-1.4292],
        [-1.3843],
        ...,
        [-1.1696],
        [-1.1662],
        [-1.1652]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1223234.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1255.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1255.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0161,  0.0175,  0.0120,  ..., -0.0188,  0.0285, -0.0202],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12605.9902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1234.8984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.7733, device='cuda:0')



h[100].sum tensor(-530.0950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(189.0286, device='cuda:0')



h[200].sum tensor(-87.9416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0417, 0.0336, 0.0217,  ..., 0.0000, 0.0576, 0.0000],
        [0.0417, 0.0336, 0.0217,  ..., 0.0000, 0.0576, 0.0000],
        [0.0227, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        ...,
        [0.0227, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0227, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0227, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(362009.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0224, 0.0059, 0.0036,  ..., 0.0000, 0.0000, 0.0702],
        [0.0292, 0.0053, 0.0128,  ..., 0.0000, 0.0000, 0.0677],
        [0.0500, 0.0000, 0.0374,  ..., 0.0000, 0.0000, 0.0601],
        ...,
        [0.0619, 0.0000, 0.0673,  ..., 0.0000, 0.0000, 0.0553],
        [0.0619, 0.0000, 0.0673,  ..., 0.0000, 0.0000, 0.0553],
        [0.0619, 0.0000, 0.0673,  ..., 0.0000, 0.0000, 0.0553]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3852451., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39545.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(953.9558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6268.6958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9188.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3681.7805, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1167],
        [-0.3987],
        [-0.7351],
        ...,
        [-1.1800],
        [-1.1730],
        [-1.1585]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1288109.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1126.2045, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1126.2045, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0009,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12333.1035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1231.8062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(50.0187, device='cuda:0')



h[100].sum tensor(-550.7836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(169.5250, device='cuda:0')



h[200].sum tensor(-78.8653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0293, 0.0137, 0.0071,  ..., 0.0000, 0.0266, 0.0000],
        [0.0228, 0.0034, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        [0.0228, 0.0034, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        ...,
        [0.0228, 0.0034, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        [0.0228, 0.0034, 0.0000,  ..., 0.0000, 0.0106, 0.0000],
        [0.0228, 0.0034, 0.0000,  ..., 0.0000, 0.0106, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(351589.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0483, 0.0000, 0.0390,  ..., 0.0000, 0.0000, 0.0612],
        [0.0555, 0.0000, 0.0551,  ..., 0.0000, 0.0000, 0.0586],
        [0.0523, 0.0000, 0.0509,  ..., 0.0000, 0.0000, 0.0599],
        ...,
        [0.0627, 0.0000, 0.0679,  ..., 0.0000, 0.0000, 0.0556],
        [0.0627, 0.0000, 0.0679,  ..., 0.0000, 0.0000, 0.0556],
        [0.0627, 0.0000, 0.0679,  ..., 0.0000, 0.0000, 0.0556]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3829143., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40775.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(931.9181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6247.7930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8038.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3607.7705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2696],
        [-0.3507],
        [-0.3231],
        ...,
        [-1.1931],
        [-1.1896],
        [-1.1886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1361832.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1211.4189, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1211.4189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12594.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1242.0828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.8034, device='cuda:0')



h[100].sum tensor(-541.5209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(182.3522, device='cuda:0')



h[200].sum tensor(-84.9512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        ...,
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(360202.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0633, 0.0000, 0.0682,  ..., 0.0000, 0.0000, 0.0556],
        [0.0633, 0.0000, 0.0682,  ..., 0.0000, 0.0000, 0.0556],
        [0.0633, 0.0000, 0.0682,  ..., 0.0000, 0.0000, 0.0557],
        ...,
        [0.0632, 0.0000, 0.0682,  ..., 0.0000, 0.0000, 0.0554],
        [0.0632, 0.0000, 0.0682,  ..., 0.0000, 0.0000, 0.0554],
        [0.0632, 0.0000, 0.0682,  ..., 0.0000, 0.0000, 0.0554]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3873388., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41422.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(953.3258, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6281.3281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8990.6973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3648.6921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3149],
        [-1.2737],
        [-1.1599],
        ...,
        [-1.2018],
        [-1.1982],
        [-1.1972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1415126.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1566.8169, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1566.8169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0123,  0.0114,  0.0073,  ..., -0.0118,  0.0190, -0.0127],
        [ 0.0101,  0.0077,  0.0045,  ..., -0.0077,  0.0134, -0.0084],
        [ 0.0095,  0.0069,  0.0039,  ..., -0.0068,  0.0121, -0.0074],
        ...,
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0008, -0.0009,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13500.9121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1265.9216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(69.5878, device='cuda:0')



h[100].sum tensor(-495.5075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(235.8494, device='cuda:0')



h[200].sum tensor(-109.4169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0362, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0401, 0.0307, 0.0187,  ..., 0.0000, 0.0533, 0.0000],
        [0.0428, 0.0351, 0.0220,  ..., 0.0000, 0.0601, 0.0000],
        [0.0635, 0.0681, 0.0468,  ..., 0.0000, 0.1112, 0.0000],
        ...,
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0229, 0.0034, 0.0000,  ..., 0.0000, 0.0108, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(396673.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0029, 0.0183, 0.0000,  ..., 0.0000, 0.0014, 0.0766],
        [0.0029, 0.0192, 0.0000,  ..., 0.0000, 0.0013, 0.0778],
        [0.0000, 0.0321, 0.0000,  ..., 0.0000, 0.0070, 0.0827],
        ...,
        [0.0625, 0.0000, 0.0677,  ..., 0.0000, 0.0000, 0.0549],
        [0.0625, 0.0000, 0.0677,  ..., 0.0000, 0.0000, 0.0549],
        [0.0625, 0.0000, 0.0677,  ..., 0.0000, 0.0000, 0.0549]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4060396., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38920.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1059.7654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6410.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13539.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3889.1052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4121],
        [ 0.4140],
        [ 0.4063],
        ...,
        [-1.2026],
        [-1.1991],
        [-1.1981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1287386.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1307.4423, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1307.4423, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        ...,
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0027,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0027,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13155.7070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1251.0206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.0681, device='cuda:0')



h[100].sum tensor(-536.3693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(196.8063, device='cuda:0')



h[200].sum tensor(-91.2376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0230, 0.0034, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0230, 0.0034, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0230, 0.0034, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        ...,
        [0.0230, 0.0034, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0230, 0.0034, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0230, 0.0034, 0.0000,  ..., 0.0000, 0.0110, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(372696.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0616, 0.0000, 0.0671,  ..., 0.0000, 0.0000, 0.0544],
        [0.0616, 0.0000, 0.0671,  ..., 0.0000, 0.0000, 0.0544],
        [0.0616, 0.0000, 0.0671,  ..., 0.0000, 0.0000, 0.0545],
        ...,
        [0.0615, 0.0000, 0.0671,  ..., 0.0000, 0.0000, 0.0542],
        [0.0615, 0.0000, 0.0671,  ..., 0.0000, 0.0000, 0.0542],
        [0.0615, 0.0000, 0.0671,  ..., 0.0000, 0.0000, 0.0542]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3928212., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37488.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(998.4067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6335.7393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9781.3701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3729.2483, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4798],
        [-1.4798],
        [-1.4659],
        ...,
        [-1.2007],
        [-1.1972],
        [-1.1963]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1210408., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1282.0542, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1282.0542, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0143,  0.0145,  0.0097,  ..., -0.0153,  0.0239, -0.0165],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0097,  0.0071,  0.0040,  ..., -0.0071,  0.0125, -0.0076],
        ...,
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13274.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1250.0358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.9405, device='cuda:0')



h[100].sum tensor(-544.0252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(192.9847, device='cuda:0')



h[200].sum tensor(-89.2779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0448, 0.0383, 0.0245,  ..., 0.0000, 0.0653, 0.0000],
        [0.0421, 0.0340, 0.0203,  ..., 0.0000, 0.0586, 0.0000],
        [0.0327, 0.0190, 0.0096,  ..., 0.0000, 0.0354, 0.0000],
        ...,
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000],
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000],
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(372120.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0029, 0.0232, 0.0000,  ..., 0.0000, 0.0032, 0.0779],
        [0.0015, 0.0159, 0.0000,  ..., 0.0000, 0.0006, 0.0774],
        [0.0066, 0.0051, 0.0001,  ..., 0.0000, 0.0000, 0.0739],
        ...,
        [0.0605, 0.0000, 0.0664,  ..., 0.0000, 0.0000, 0.0539],
        [0.0605, 0.0000, 0.0664,  ..., 0.0000, 0.0000, 0.0539],
        [0.0605, 0.0000, 0.0664,  ..., 0.0000, 0.0000, 0.0539]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3911149.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36967.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(981.6521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6331.9263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9521.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3727.1555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3648],
        [ 0.3647],
        [ 0.3480],
        ...,
        [-1.1976],
        [-1.1943],
        [-1.1935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1187033.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1291.3230, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1291.3230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101,  0.0077,  0.0045,  ..., -0.0077,  0.0135, -0.0083],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        ...,
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0057,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13401.5039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1251.7887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.3522, device='cuda:0')



h[100].sum tensor(-545.3223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(194.3799, device='cuda:0')



h[200].sum tensor(-90.1449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0372, 0.0262, 0.0159,  ..., 0.0000, 0.0465, 0.0000],
        [0.0273, 0.0104, 0.0045,  ..., 0.0000, 0.0219, 0.0000],
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0113, 0.0000],
        ...,
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0113, 0.0000],
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0113, 0.0000],
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0113, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(377749.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0170, 0.0057, 0.0074,  ..., 0.0000, 0.0000, 0.0693],
        [0.0333, 0.0000, 0.0247,  ..., 0.0000, 0.0000, 0.0632],
        [0.0485, 0.0000, 0.0480,  ..., 0.0000, 0.0000, 0.0582],
        ...,
        [0.0601, 0.0000, 0.0663,  ..., 0.0000, 0.0000, 0.0539],
        [0.0601, 0.0000, 0.0663,  ..., 0.0000, 0.0000, 0.0539],
        [0.0601, 0.0000, 0.0663,  ..., 0.0000, 0.0000, 0.0539]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3956212.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35366.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1004.7720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6366.3916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10247.8184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3775.6995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2378],
        [ 0.0471],
        [-0.2120],
        ...,
        [-1.1990],
        [-1.1957],
        [-1.1949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1097367.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1495.4237, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1495.4237, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0207,  0.0247,  0.0177,  ..., -0.0267,  0.0398, -0.0288],
        [ 0.0274,  0.0354,  0.0259,  ..., -0.0387,  0.0564, -0.0417],
        ...,
        [ 0.0058,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0008,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13864.0801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1267.2454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.4170, device='cuda:0')



h[100].sum tensor(-518.9011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(225.1028, device='cuda:0')



h[200].sum tensor(-104.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0486, 0.0443, 0.0300,  ..., 0.0000, 0.0746, 0.0000],
        [0.0777, 0.0906, 0.0651,  ..., 0.0000, 0.1465, 0.0000],
        [0.1228, 0.1626, 0.1201,  ..., 0.0000, 0.2584, 0.0000],
        ...,
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000],
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000],
        [0.0230, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(396775.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0128, 0.0513, 0.0035,  ..., 0.0000, 0.0227, 0.0752],
        [0.0023, 0.1119, 0.0000,  ..., 0.0000, 0.0572, 0.0878],
        [0.0000, 0.1791, 0.0000,  ..., 0.0000, 0.1012, 0.0999],
        ...,
        [0.0605, 0.0000, 0.0669,  ..., 0.0000, 0.0000, 0.0540],
        [0.0605, 0.0000, 0.0669,  ..., 0.0000, 0.0000, 0.0540],
        [0.0573, 0.0000, 0.0615,  ..., 0.0000, 0.0000, 0.0551]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4046790.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(35222.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1057.1514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6451.4546, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12213.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3900.7322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1875],
        [ 0.1786],
        [ 0.1483],
        ...,
        [-1.1761],
        [-1.0538],
        [-0.7959]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1108976.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 7800 loss: tensor(518.0593, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1313.8314, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1313.8314, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0191,  0.0222,  0.0157,  ..., -0.0239,  0.0359, -0.0257],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0144,  0.0146,  0.0098,  ..., -0.0153,  0.0240, -0.0165],
        ...,
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13425.0742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1260.2405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.3519, device='cuda:0')



h[100].sum tensor(-547.2639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.7681, device='cuda:0')



h[200].sum tensor(-91.6138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0489, 0.0445, 0.0302,  ..., 0.0000, 0.0750, 0.0000],
        [0.0482, 0.0434, 0.0284,  ..., 0.0000, 0.0732, 0.0000],
        [0.0499, 0.0462, 0.0306,  ..., 0.0000, 0.0775, 0.0000],
        ...,
        [0.0231, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000],
        [0.0231, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000],
        [0.0231, 0.0035, 0.0000,  ..., 0.0000, 0.0112, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(377422.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0541, 0.0000,  ..., 0.0000, 0.0211, 0.0789],
        [0.0000, 0.0400, 0.0000,  ..., 0.0000, 0.0104, 0.0777],
        [0.0032, 0.0394, 0.0000,  ..., 0.0000, 0.0119, 0.0788],
        ...,
        [0.0616, 0.0000, 0.0678,  ..., 0.0000, 0.0000, 0.0545],
        [0.0616, 0.0000, 0.0678,  ..., 0.0000, 0.0000, 0.0545],
        [0.0616, 0.0000, 0.0678,  ..., 0.0000, 0.0000, 0.0545]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3969578.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36963.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1002.7385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6401.5859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9633.3496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3759.7727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3145],
        [ 0.3217],
        [ 0.3201],
        ...,
        [-1.2274],
        [-1.2238],
        [-1.2228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1204083.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1336.7960, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.7960, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        ...,
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13382.7070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1264.4062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.3718, device='cuda:0')



h[100].sum tensor(-545.6397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(201.2249, device='cuda:0')



h[200].sum tensor(-93.1698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0232, 0.0035, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0232, 0.0035, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0232, 0.0035, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        ...,
        [0.0232, 0.0035, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0232, 0.0035, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0232, 0.0035, 0.0000,  ..., 0.0000, 0.0111, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(381135.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0598, 0.0000, 0.0653,  ..., 0.0000, 0.0000, 0.0566],
        [0.0618, 0.0000, 0.0677,  ..., 0.0000, 0.0000, 0.0559],
        [0.0629, 0.0000, 0.0688,  ..., 0.0000, 0.0000, 0.0557],
        ...,
        [0.0627, 0.0000, 0.0688,  ..., 0.0000, 0.0000, 0.0554],
        [0.0627, 0.0000, 0.0688,  ..., 0.0000, 0.0000, 0.0554],
        [0.0627, 0.0000, 0.0688,  ..., 0.0000, 0.0000, 0.0554]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4014188., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38914.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1005.7649, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6433.5703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10726.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3788.1624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5270],
        [-0.5938],
        [-0.5578],
        ...,
        [-1.2430],
        [-1.2394],
        [-1.2384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1326282., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1195.0143, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1195.0143, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0414,  0.0578,  0.0433,  ..., -0.0637,  0.0911, -0.0687],
        [ 0.0332,  0.0447,  0.0331,  ..., -0.0490,  0.0707, -0.0528],
        [ 0.0254,  0.0322,  0.0234,  ..., -0.0350,  0.0513, -0.0378],
        ...,
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12999.6338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1257.8876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.0748, device='cuda:0')



h[100].sum tensor(-568.3524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(179.8828, device='cuda:0')



h[200].sum tensor(-82.8778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1134, 0.1476, 0.1082,  ..., 0.0000, 0.2347, 0.0000],
        [0.1233, 0.1634, 0.1205,  ..., 0.0000, 0.2592, 0.0000],
        [0.0986, 0.1239, 0.0898,  ..., 0.0000, 0.1979, 0.0000],
        ...,
        [0.0232, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0232, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0232, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(364290.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1178, 0.0000,  ..., 0.0000, 0.0622, 0.0914],
        [0.0000, 0.1271, 0.0000,  ..., 0.0000, 0.0683, 0.0929],
        [0.0000, 0.1056, 0.0000,  ..., 0.0000, 0.0541, 0.0898],
        ...,
        [0.0634, 0.0000, 0.0696,  ..., 0.0000, 0.0000, 0.0561],
        [0.0634, 0.0000, 0.0696,  ..., 0.0000, 0.0000, 0.0561],
        [0.0634, 0.0000, 0.0696,  ..., 0.0000, 0.0000, 0.0561]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3945027., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39125.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(970.5664, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6406.3174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7944.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3687.0991, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2438],
        [ 0.2690],
        [ 0.2925],
        ...,
        [-1.2524],
        [-1.2491],
        [-1.2483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1307237.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1232.4700, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1232.4700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        ...,
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13058.1914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1263.0334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.7383, device='cuda:0')



h[100].sum tensor(-565.3827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(185.5209, device='cuda:0')



h[200].sum tensor(-85.2557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0233, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0233, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0284, 0.0117, 0.0054,  ..., 0.0000, 0.0236, 0.0000],
        ...,
        [0.0233, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0233, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0233, 0.0036, 0.0000,  ..., 0.0000, 0.0110, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(368686.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0632, 0.0000, 0.0686,  ..., 0.0000, 0.0000, 0.0573],
        [0.0590, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0587],
        [0.0483, 0.0000, 0.0428,  ..., 0.0000, 0.0000, 0.0623],
        ...,
        [0.0640, 0.0000, 0.0702,  ..., 0.0000, 0.0000, 0.0568],
        [0.0640, 0.0000, 0.0702,  ..., 0.0000, 0.0000, 0.0568],
        [0.0640, 0.0000, 0.0702,  ..., 0.0000, 0.0000, 0.0568]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3977901.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40744.4023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(969.2107, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6435.6641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8873.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3717.2708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4531],
        [-1.2701],
        [-0.9935],
        ...,
        [-1.2662],
        [-1.2625],
        [-1.2614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1403104.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1183.9561, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1183.9561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        ...,
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0058,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12994.0420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1262.0403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(52.5836, device='cuda:0')



h[100].sum tensor(-574.7891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(178.2182, device='cuda:0')



h[200].sum tensor(-81.7080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0233, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0233, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0233, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        ...,
        [0.0233, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0233, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0233, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(366918.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0641, 0.0000, 0.0703,  ..., 0.0000, 0.0000, 0.0574],
        [0.0637, 0.0000, 0.0701,  ..., 0.0000, 0.0000, 0.0575],
        [0.0627, 0.0000, 0.0694,  ..., 0.0000, 0.0000, 0.0580],
        ...,
        [0.0639, 0.0000, 0.0703,  ..., 0.0000, 0.0000, 0.0572],
        [0.0639, 0.0000, 0.0703,  ..., 0.0000, 0.0000, 0.0572],
        [0.0639, 0.0000, 0.0703,  ..., 0.0000, 0.0000, 0.0572]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3989105.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40214.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(969.1147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6450.3101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8808.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3712.3679, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4240],
        [-1.2944],
        [-1.0984],
        ...,
        [-1.2722],
        [-1.2685],
        [-1.2674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1360652.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1480.5708, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1480.5708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0123,  0.0113,  0.0071,  ..., -0.0115,  0.0188, -0.0125],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        ...,
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13730.0840, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1284.2080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.7573, device='cuda:0')



h[100].sum tensor(-535.6693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(222.8670, device='cuda:0')



h[200].sum tensor(-102.0036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0346, 0.0217, 0.0121,  ..., 0.0000, 0.0390, 0.0000],
        [0.0299, 0.0141, 0.0071,  ..., 0.0000, 0.0272, 0.0000],
        [0.0234, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        ...,
        [0.0234, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0234, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0234, 0.0037, 0.0000,  ..., 0.0000, 0.0111, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(394807.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.0100, 0.0103,  ..., 0.0000, 0.0007, 0.0723],
        [0.0325, 0.0026, 0.0155,  ..., 0.0000, 0.0000, 0.0669],
        [0.0438, 0.0028, 0.0279,  ..., 0.0000, 0.0000, 0.0633],
        ...,
        [0.0641, 0.0000, 0.0704,  ..., 0.0000, 0.0000, 0.0571],
        [0.0641, 0.0000, 0.0704,  ..., 0.0000, 0.0000, 0.0571],
        [0.0641, 0.0000, 0.0704,  ..., 0.0000, 0.0000, 0.0571]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4116801.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39295.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1056.2695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6563.0176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12008.4697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3896.3948, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3400],
        [ 0.3006],
        [ 0.2568],
        ...,
        [-1.2786],
        [-1.2749],
        [-1.2739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1316842.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1396.5673, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1396.5673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0192,  0.0222,  0.0156,  ..., -0.0237,  0.0359, -0.0256],
        [ 0.0193,  0.0224,  0.0158,  ..., -0.0239,  0.0362, -0.0258],
        [ 0.0119,  0.0106,  0.0066,  ..., -0.0107,  0.0178, -0.0116],
        ...,
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13720.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1284.1244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.0264, device='cuda:0')



h[100].sum tensor(-550.6447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(210.2221, device='cuda:0')



h[200].sum tensor(-96.2035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0645, 0.0692, 0.0473,  ..., 0.0000, 0.1130, 0.0000],
        [0.0637, 0.0679, 0.0463,  ..., 0.0000, 0.1110, 0.0000],
        [0.0606, 0.0630, 0.0425,  ..., 0.0000, 0.1033, 0.0000],
        ...,
        [0.0235, 0.0037, 0.0000,  ..., 0.0000, 0.0113, 0.0000],
        [0.0235, 0.0037, 0.0000,  ..., 0.0000, 0.0113, 0.0000],
        [0.0235, 0.0037, 0.0000,  ..., 0.0000, 0.0113, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(386303.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0267, 0.0000,  ..., 0.0000, 0.0040, 0.0765],
        [0.0008, 0.0273, 0.0000,  ..., 0.0000, 0.0038, 0.0768],
        [0.0070, 0.0190, 0.0000,  ..., 0.0000, 0.0031, 0.0748],
        ...,
        [0.0638, 0.0000, 0.0700,  ..., 0.0000, 0.0000, 0.0566],
        [0.0638, 0.0000, 0.0700,  ..., 0.0000, 0.0000, 0.0566],
        [0.0638, 0.0000, 0.0700,  ..., 0.0000, 0.0000, 0.0566]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4075402.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38324.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1046.0667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6546.2930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10254.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3827.3438, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0331],
        [-0.1171],
        ...,
        [-1.2814],
        [-1.2777],
        [-1.2759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1248870.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1148.0599, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1148.0599, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0095,  0.0067,  0.0036,  ..., -0.0064,  0.0117, -0.0069],
        [ 0.0095,  0.0067,  0.0036,  ..., -0.0064,  0.0117, -0.0069],
        ...,
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000],
        [ 0.0059,  0.0009, -0.0009,  ...,  0.0000,  0.0028,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13343.9834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1274.2285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(50.9894, device='cuda:0')



h[100].sum tensor(-588.3516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(172.8149, device='cuda:0')



h[200].sum tensor(-79.3573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0330, 0.0187, 0.0099,  ..., 0.0000, 0.0347, 0.0000],
        [0.0359, 0.0234, 0.0127,  ..., 0.0000, 0.0419, 0.0000],
        [0.0330, 0.0187, 0.0091,  ..., 0.0000, 0.0347, 0.0000],
        ...,
        [0.0236, 0.0037, 0.0000,  ..., 0.0000, 0.0114, 0.0000],
        [0.0236, 0.0037, 0.0000,  ..., 0.0000, 0.0114, 0.0000],
        [0.0236, 0.0037, 0.0000,  ..., 0.0000, 0.0114, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(365211.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0255, 0.0008, 0.0165,  ..., 0.0000, 0.0000, 0.0683],
        [0.0155, 0.0009, 0.0058,  ..., 0.0000, 0.0000, 0.0715],
        [0.0190, 0.0001, 0.0099,  ..., 0.0000, 0.0000, 0.0706],
        ...,
        [0.0637, 0.0000, 0.0696,  ..., 0.0000, 0.0000, 0.0561],
        [0.0637, 0.0000, 0.0696,  ..., 0.0000, 0.0000, 0.0561],
        [0.0637, 0.0000, 0.0696,  ..., 0.0000, 0.0000, 0.0561]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3972020., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40086.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(975.1428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6471.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8174.3311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3666.5581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1378],
        [ 0.2257],
        [ 0.1710],
        ...,
        [-1.2844],
        [-1.2808],
        [-1.2797]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1380300.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1318.4075, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1318.4075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0161,  0.0173,  0.0118,  ..., -0.0182,  0.0283, -0.0196],
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        ...,
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13819.8418, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1287.0444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.5551, device='cuda:0')



h[100].sum tensor(-567.7502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.4569, device='cuda:0')



h[200].sum tensor(-90.5234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0686, 0.0757, 0.0523,  ..., 0.0000, 0.1232, 0.0000],
        [0.0320, 0.0172, 0.0095,  ..., 0.0000, 0.0322, 0.0000],
        [0.0339, 0.0202, 0.0118,  ..., 0.0000, 0.0369, 0.0000],
        ...,
        [0.0237, 0.0038, 0.0000,  ..., 0.0000, 0.0115, 0.0000],
        [0.0237, 0.0038, 0.0000,  ..., 0.0000, 0.0115, 0.0000],
        [0.0237, 0.0038, 0.0000,  ..., 0.0000, 0.0115, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(379984.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0211, 0.0000,  ..., 0.0000, 0.0023, 0.0733],
        [0.0287, 0.0053, 0.0121,  ..., 0.0000, 0.0000, 0.0661],
        [0.0415, 0.0023, 0.0271,  ..., 0.0000, 0.0000, 0.0625],
        ...,
        [0.0633, 0.0000, 0.0692,  ..., 0.0000, 0.0000, 0.0561],
        [0.0633, 0.0000, 0.0692,  ..., 0.0000, 0.0000, 0.0561],
        [0.0633, 0.0000, 0.0692,  ..., 0.0000, 0.0000, 0.0561]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4030077., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39019.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1016.9785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6535.8096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9527.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3769.7725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2880],
        [ 0.0930],
        [-0.2203],
        ...,
        [-1.2854],
        [-1.2818],
        [-1.2808]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1300243.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1223.6938, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1223.6938, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0234,  0.0289,  0.0208,  ..., -0.0310,  0.0463, -0.0335],
        [ 0.0187,  0.0213,  0.0149,  ..., -0.0226,  0.0345, -0.0244],
        [ 0.0139,  0.0137,  0.0090,  ..., -0.0141,  0.0226, -0.0152],
        ...,
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13667.2832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1283.3186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.3485, device='cuda:0')



h[100].sum tensor(-582.9978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(184.1999, device='cuda:0')



h[200].sum tensor(-84.0037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0650, 0.0699, 0.0477,  ..., 0.0000, 0.1141, 0.0000],
        [0.0688, 0.0760, 0.0525,  ..., 0.0000, 0.1236, 0.0000],
        [0.0569, 0.0570, 0.0377,  ..., 0.0000, 0.0940, 0.0000],
        ...,
        [0.0237, 0.0039, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0237, 0.0039, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0237, 0.0039, 0.0000,  ..., 0.0000, 0.0116, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(376011.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0504, 0.0000,  ..., 0.0000, 0.0208, 0.0842],
        [0.0000, 0.0443, 0.0000,  ..., 0.0000, 0.0165, 0.0824],
        [0.0054, 0.0244, 0.0000,  ..., 0.0000, 0.0077, 0.0775],
        ...,
        [0.0630, 0.0000, 0.0691,  ..., 0.0000, 0.0000, 0.0564],
        [0.0630, 0.0000, 0.0691,  ..., 0.0000, 0.0000, 0.0564],
        [0.0630, 0.0000, 0.0691,  ..., 0.0000, 0.0000, 0.0564]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4034331.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38573.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1011.2244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6543.5713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9423.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3756.6736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3859],
        [ 0.3856],
        [ 0.3769],
        ...,
        [-1.2873],
        [-1.2838],
        [-1.2829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1260206.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 8100 loss: tensor(513.5076, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1294.9067, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1294.9067, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0226,  0.0277,  0.0198,  ..., -0.0297,  0.0444, -0.0320],
        [ 0.0215,  0.0259,  0.0184,  ..., -0.0276,  0.0416, -0.0298],
        [ 0.0217,  0.0262,  0.0187,  ..., -0.0280,  0.0421, -0.0302],
        ...,
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0059,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13833.4902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1290.3872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.5113, device='cuda:0')



h[100].sum tensor(-575.4377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(194.9194, device='cuda:0')



h[200].sum tensor(-88.3794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0954, 0.1185, 0.0854,  ..., 0.0000, 0.1896, 0.0000],
        [0.0953, 0.1183, 0.0852,  ..., 0.0000, 0.1893, 0.0000],
        [0.0899, 0.1097, 0.0786,  ..., 0.0000, 0.1760, 0.0000],
        ...,
        [0.0238, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0238, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0238, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(380996.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0975, 0.0000,  ..., 0.0000, 0.0521, 0.0896],
        [0.0000, 0.1006, 0.0000,  ..., 0.0000, 0.0541, 0.0899],
        [0.0000, 0.0934, 0.0000,  ..., 0.0000, 0.0493, 0.0894],
        ...,
        [0.0634, 0.0000, 0.0693,  ..., 0.0000, 0.0000, 0.0569],
        [0.0634, 0.0000, 0.0693,  ..., 0.0000, 0.0000, 0.0569],
        [0.0634, 0.0000, 0.0693,  ..., 0.0000, 0.0000, 0.0569]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4061003., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38569.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1030.3511, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6581.6943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9789.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3798.9631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3027],
        [ 0.2868],
        [ 0.2814],
        ...,
        [-1.2833],
        [-1.2444],
        [-1.1336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1246394.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1250.9175, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1250.9175, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        ...,
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13762.2852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1295.0326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.5576, device='cuda:0')



h[100].sum tensor(-582.6228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(188.2978, device='cuda:0')



h[200].sum tensor(-85.8043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0239, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0239, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0239, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        ...,
        [0.0239, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0239, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0239, 0.0040, 0.0000,  ..., 0.0000, 0.0116, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(380637.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0519, 0.0000, 0.0474,  ..., 0.0000, 0.0000, 0.0611],
        [0.0599, 0.0000, 0.0609,  ..., 0.0000, 0.0000, 0.0586],
        [0.0567, 0.0000, 0.0477,  ..., 0.0000, 0.0000, 0.0594],
        ...,
        [0.0647, 0.0000, 0.0700,  ..., 0.0000, 0.0000, 0.0569],
        [0.0647, 0.0000, 0.0700,  ..., 0.0000, 0.0000, 0.0569],
        [0.0647, 0.0000, 0.0700,  ..., 0.0000, 0.0000, 0.0569]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4088273.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39424.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1041.2080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6596.3979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9534.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3783.1028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2962],
        [-0.4955],
        [-0.4837],
        ...,
        [-1.3117],
        [-1.3080],
        [-1.3070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1268382.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1099.1678, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1099.1678, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0181,  0.0202,  0.0141,  ..., -0.0214,  0.0328, -0.0231],
        [ 0.0116,  0.0099,  0.0061,  ..., -0.0099,  0.0168, -0.0107],
        [ 0.0189,  0.0216,  0.0151,  ..., -0.0228,  0.0349, -0.0246],
        ...,
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13386.4629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1292.4668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(48.8179, device='cuda:0')



h[100].sum tensor(-605.5305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(165.4553, device='cuda:0')



h[200].sum tensor(-75.4092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0457, 0.0385, 0.0234,  ..., 0.0000, 0.0653, 0.0000],
        [0.0663, 0.0714, 0.0490,  ..., 0.0000, 0.1165, 0.0000],
        [0.0548, 0.0532, 0.0348,  ..., 0.0000, 0.0881, 0.0000],
        ...,
        [0.0240, 0.0039, 0.0000,  ..., 0.0000, 0.0115, 0.0000],
        [0.0240, 0.0039, 0.0000,  ..., 0.0000, 0.0115, 0.0000],
        [0.0240, 0.0039, 0.0000,  ..., 0.0000, 0.0115, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(360034.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0237, 0.0000,  ..., 0.0000, 0.0030, 0.0804],
        [0.0000, 0.0282, 0.0000,  ..., 0.0000, 0.0057, 0.0798],
        [0.0036, 0.0164, 0.0000,  ..., 0.0000, 0.0010, 0.0770],
        ...,
        [0.0662, 0.0000, 0.0708,  ..., 0.0000, 0.0000, 0.0572],
        [0.0662, 0.0000, 0.0708,  ..., 0.0000, 0.0000, 0.0572],
        [0.0662, 0.0000, 0.0708,  ..., 0.0000, 0.0000, 0.0572]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3988738.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(42571.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(981.2883, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6534.6802, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6606.6704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3629.9685, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4204],
        [ 0.4091],
        [ 0.3561],
        ...,
        [-1.3288],
        [-1.3250],
        [-1.3239]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1438354.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1288.6707, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1288.6707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0145,  0.0146,  0.0097,  ..., -0.0151,  0.0240, -0.0163],
        ...,
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13766.2852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1306.6599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.2344, device='cuda:0')



h[100].sum tensor(-579.8907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.9807, device='cuda:0')



h[200].sum tensor(-88.3193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0241, 0.0040, 0.0000,  ..., 0.0000, 0.0114, 0.0000],
        [0.0355, 0.0222, 0.0124,  ..., 0.0000, 0.0398, 0.0000],
        [0.0371, 0.0249, 0.0136,  ..., 0.0000, 0.0440, 0.0000],
        ...,
        [0.0241, 0.0040, 0.0000,  ..., 0.0000, 0.0114, 0.0000],
        [0.0241, 0.0040, 0.0000,  ..., 0.0000, 0.0114, 0.0000],
        [0.0241, 0.0040, 0.0000,  ..., 0.0000, 0.0114, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(380372., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0518, 0.0000, 0.0482,  ..., 0.0000, 0.0000, 0.0631],
        [0.0304, 0.0012, 0.0206,  ..., 0.0000, 0.0000, 0.0696],
        [0.0172, 0.0033, 0.0074,  ..., 0.0000, 0.0000, 0.0741],
        ...,
        [0.0669, 0.0000, 0.0714,  ..., 0.0000, 0.0000, 0.0579],
        [0.0669, 0.0000, 0.0714,  ..., 0.0000, 0.0000, 0.0579],
        [0.0669, 0.0000, 0.0714,  ..., 0.0000, 0.0000, 0.0579]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4108320.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(42439.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1042.3940, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6623.2368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9188.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3776.5554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2793],
        [ 0.0420],
        [ 0.2424],
        ...,
        [-1.3405],
        [-1.3367],
        [-1.3356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1415118.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1184.4414, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1184.4414, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0202,  0.0238,  0.0168,  ..., -0.0252,  0.0382, -0.0272],
        [ 0.0158,  0.0166,  0.0112,  ..., -0.0173,  0.0272, -0.0187],
        [ 0.0149,  0.0153,  0.0102,  ..., -0.0158,  0.0251, -0.0171],
        ...,
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13528.0098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1298.3489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(52.6052, device='cuda:0')



h[100].sum tensor(-596.6730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(178.2913, device='cuda:0')



h[200].sum tensor(-80.9209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0533, 0.0509, 0.0328,  ..., 0.0000, 0.0844, 0.0000],
        [0.0617, 0.0642, 0.0431,  ..., 0.0000, 0.1051, 0.0000],
        [0.0491, 0.0442, 0.0275,  ..., 0.0000, 0.0739, 0.0000],
        ...,
        [0.0240, 0.0041, 0.0000,  ..., 0.0000, 0.0115, 0.0000],
        [0.0240, 0.0041, 0.0000,  ..., 0.0000, 0.0115, 0.0000],
        [0.0240, 0.0041, 0.0000,  ..., 0.0000, 0.0115, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(373064.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0232, 0.0000,  ..., 0.0000, 0.0050, 0.0819],
        [0.0000, 0.0291, 0.0000,  ..., 0.0000, 0.0080, 0.0836],
        [0.0049, 0.0162, 0.0000,  ..., 0.0000, 0.0038, 0.0805],
        ...,
        [0.0664, 0.0000, 0.0717,  ..., 0.0000, 0.0000, 0.0588],
        [0.0664, 0.0000, 0.0717,  ..., 0.0000, 0.0000, 0.0588],
        [0.0664, 0.0000, 0.0717,  ..., 0.0000, 0.0000, 0.0588]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4087570.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41852.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1018.3611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6617.4243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8543.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3754.1868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2074],
        [ 0.1835],
        [ 0.0019],
        ...,
        [-1.3446],
        [-1.3408],
        [-1.3397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1375647.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1293.9238, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1293.9238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        ...,
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13827.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1303.5958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.4677, device='cuda:0')



h[100].sum tensor(-582.8634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(194.7714, device='cuda:0')



h[200].sum tensor(-88.4135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0334, 0.0193, 0.0108,  ..., 0.0000, 0.0350, 0.0000],
        [0.0429, 0.0344, 0.0216,  ..., 0.0000, 0.0585, 0.0000],
        [0.0334, 0.0193, 0.0108,  ..., 0.0000, 0.0350, 0.0000],
        ...,
        [0.0240, 0.0042, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0240, 0.0042, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0240, 0.0042, 0.0000,  ..., 0.0000, 0.0116, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(383951.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0391, 0.0011, 0.0203,  ..., 0.0000, 0.0000, 0.0677],
        [0.0285, 0.0022, 0.0058,  ..., 0.0000, 0.0000, 0.0709],
        [0.0393, 0.0011, 0.0204,  ..., 0.0000, 0.0000, 0.0678],
        ...,
        [0.0657, 0.0000, 0.0718,  ..., 0.0000, 0.0000, 0.0596],
        [0.0657, 0.0000, 0.0718,  ..., 0.0000, 0.0000, 0.0596],
        [0.0657, 0.0000, 0.0718,  ..., 0.0000, 0.0000, 0.0596]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4136700.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40547.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1046.1919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6673.7861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9714.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3850.7776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2785],
        [-0.1985],
        [-0.2660],
        ...,
        [-1.3446],
        [-1.3410],
        [-1.3401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1313195.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1208.6224, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1208.6224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0143,  0.0143,  0.0094,  ..., -0.0146,  0.0235, -0.0158],
        [ 0.0098,  0.0071,  0.0038,  ..., -0.0067,  0.0124, -0.0073],
        ...,
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0060,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13735.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1302.8521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.6792, device='cuda:0')



h[100].sum tensor(-599.4812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(181.9312, device='cuda:0')



h[200].sum tensor(-82.0888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0355, 0.0223, 0.0124,  ..., 0.0000, 0.0399, 0.0000],
        [0.0372, 0.0251, 0.0136,  ..., 0.0000, 0.0443, 0.0000],
        [0.0524, 0.0493, 0.0316,  ..., 0.0000, 0.0820, 0.0000],
        ...,
        [0.0241, 0.0041, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0241, 0.0041, 0.0000,  ..., 0.0000, 0.0116, 0.0000],
        [0.0241, 0.0041, 0.0000,  ..., 0.0000, 0.0116, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(375954.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0276, 0.0008, 0.0186,  ..., 0.0000, 0.0000, 0.0711],
        [0.0153, 0.0032, 0.0073,  ..., 0.0000, 0.0000, 0.0754],
        [0.0069, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0783],
        ...,
        [0.0658, 0.0000, 0.0717,  ..., 0.0000, 0.0000, 0.0590],
        [0.0658, 0.0000, 0.0717,  ..., 0.0000, 0.0000, 0.0590],
        [0.0658, 0.0000, 0.0717,  ..., 0.0000, 0.0000, 0.0590]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4110502.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41335.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1011.1194, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6642.0566, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8839.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3769.7925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1718],
        [ 0.3158],
        [ 0.3492],
        ...,
        [-1.3548],
        [-1.3510],
        [-1.3500]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1384067., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1157.8381, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1157.8381, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0105,  0.0082,  0.0047,  ..., -0.0079,  0.0141, -0.0086],
        [ 0.0095,  0.0064,  0.0034,  ..., -0.0060,  0.0114, -0.0065],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13766.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1306.3516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(51.4236, device='cuda:0')



h[100].sum tensor(-610.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.2868, device='cuda:0')



h[200].sum tensor(-78.8574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0287, 0.0111, 0.0047,  ..., 0.0000, 0.0229, 0.0000],
        [0.0341, 0.0197, 0.0105,  ..., 0.0000, 0.0362, 0.0000],
        [0.0500, 0.0452, 0.0285,  ..., 0.0000, 0.0758, 0.0000],
        ...,
        [0.0242, 0.0040, 0.0000,  ..., 0.0000, 0.0117, 0.0000],
        [0.0242, 0.0040, 0.0000,  ..., 0.0000, 0.0117, 0.0000],
        [0.0242, 0.0040, 0.0000,  ..., 0.0000, 0.0117, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(373746.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0282, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0695],
        [0.0197, 0.0016, 0.0039,  ..., 0.0000, 0.0000, 0.0723],
        [0.0097, 0.0056, 0.0005,  ..., 0.0000, 0.0000, 0.0759],
        ...,
        [0.0661, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0582],
        [0.0661, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0582],
        [0.0661, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0582]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4102619., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40903.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1011.8396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6637.6577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7914.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3724.1121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3622],
        [ 0.3762],
        [ 0.3656],
        ...,
        [-1.3633],
        [-1.3594],
        [-1.3584]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1383547.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1083.6244, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1083.6244, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0029,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13701.4707, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1304.4911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(48.1276, device='cuda:0')



h[100].sum tensor(-623.8430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(163.1155, device='cuda:0')



h[200].sum tensor(-73.6195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0243, 0.0039, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0243, 0.0039, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0243, 0.0039, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        ...,
        [0.0243, 0.0039, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0243, 0.0039, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0243, 0.0039, 0.0000,  ..., 0.0000, 0.0118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(363661., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0660, 0.0000, 0.0715,  ..., 0.0000, 0.0000, 0.0582],
        [0.0647, 0.0000, 0.0699,  ..., 0.0000, 0.0000, 0.0586],
        [0.0597, 0.0000, 0.0635,  ..., 0.0000, 0.0000, 0.0603],
        ...,
        [0.0659, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0580],
        [0.0659, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0580],
        [0.0659, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0580]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4043439.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41229.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(975.4966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6605.0713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6239.0361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3645.0730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0652],
        [-0.8013],
        [-0.4916],
        ...,
        [-1.3676],
        [-1.3636],
        [-1.3625]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1425132., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1335.8069, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1335.8069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0100,  0.0074,  0.0041,  ..., -0.0070,  0.0129, -0.0076],
        [ 0.0102,  0.0076,  0.0043,  ..., -0.0073,  0.0132, -0.0079],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14343.8232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1319.1672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.3279, device='cuda:0')



h[100].sum tensor(-589.7365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(201.0760, device='cuda:0')



h[200].sum tensor(-90.7758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0282, 0.0104, 0.0041,  ..., 0.0000, 0.0218, 0.0000],
        [0.0376, 0.0253, 0.0148,  ..., 0.0000, 0.0450, 0.0000],
        [0.0513, 0.0473, 0.0302,  ..., 0.0000, 0.0793, 0.0000],
        ...,
        [0.0243, 0.0040, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0243, 0.0040, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0243, 0.0040, 0.0000,  ..., 0.0000, 0.0118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(387602.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0370, 0.0000, 0.0235,  ..., 0.0000, 0.0000, 0.0672],
        [0.0221, 0.0032, 0.0138,  ..., 0.0000, 0.0000, 0.0719],
        [0.0069, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0768],
        ...,
        [0.0653, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0585],
        [0.0653, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0585],
        [0.0653, 0.0000, 0.0716,  ..., 0.0000, 0.0000, 0.0585]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4150035., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39423.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1044.9436, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6705.0791, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8845.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3827.7529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3318],
        [ 0.3474],
        [ 0.3542],
        ...,
        [-1.3721],
        [-1.3683],
        [-1.3673]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1335480.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 8400 loss: tensor(510.0254, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1443.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1443.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14647.3574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1324.0588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.0922, device='cuda:0')



h[100].sum tensor(-574.7841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(217.2234, device='cuda:0')



h[200].sum tensor(-98.4889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0242, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        ...,
        [0.0242, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(402268., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0592, 0.0000, 0.0622,  ..., 0.0000, 0.0000, 0.0612],
        [0.0638, 0.0000, 0.0703,  ..., 0.0000, 0.0000, 0.0598],
        [0.0599, 0.0000, 0.0592,  ..., 0.0000, 0.0000, 0.0609],
        ...,
        [0.0646, 0.0000, 0.0718,  ..., 0.0000, 0.0000, 0.0593],
        [0.0646, 0.0000, 0.0718,  ..., 0.0000, 0.0000, 0.0593],
        [0.0646, 0.0000, 0.0718,  ..., 0.0000, 0.0000, 0.0593]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4228647.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38184.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1083.3352, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6773.6191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10780.3184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3957.2913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7529],
        [-0.9405],
        [-0.8876],
        ...,
        [-1.3739],
        [-1.3702],
        [-1.3692]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1258989.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1110.7438, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1110.7438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0061,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13860.3506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1303.0222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(49.3320, device='cuda:0')



h[100].sum tensor(-624.1733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(167.1978, device='cuda:0')



h[200].sum tensor(-75.3743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0242, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        ...,
        [0.0242, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0242, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(372621.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0569, 0.0000, 0.0612,  ..., 0.0000, 0.0000, 0.0627],
        [0.0631, 0.0000, 0.0702,  ..., 0.0000, 0.0000, 0.0607],
        [0.0648, 0.0000, 0.0721,  ..., 0.0000, 0.0000, 0.0603],
        ...,
        [0.0646, 0.0000, 0.0721,  ..., 0.0000, 0.0000, 0.0600],
        [0.0646, 0.0000, 0.0721,  ..., 0.0000, 0.0000, 0.0600],
        [0.0646, 0.0000, 0.0721,  ..., 0.0000, 0.0000, 0.0600]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4111582., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37977.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1014.3965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6695.5449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7187.2407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3781.5767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6337],
        [-0.9773],
        [-1.2451],
        ...,
        [-1.3807],
        [-1.3770],
        [-1.3760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1245253.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1271.0343, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1271.0343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14256.8164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1317.1606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.4511, device='cuda:0')



h[100].sum tensor(-603.3224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(191.3259, device='cuda:0')



h[200].sum tensor(-86.1334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0243, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0243, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0300, 0.0133, 0.0053,  ..., 0.0000, 0.0261, 0.0000],
        ...,
        [0.0243, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0243, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0243, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(387143.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0629, 0.0000, 0.0697,  ..., 0.0000, 0.0000, 0.0612],
        [0.0561, 0.0000, 0.0612,  ..., 0.0000, 0.0000, 0.0634],
        [0.0395, 0.0000, 0.0398,  ..., 0.0000, 0.0000, 0.0689],
        ...,
        [0.0655, 0.0000, 0.0728,  ..., 0.0000, 0.0000, 0.0600],
        [0.0655, 0.0000, 0.0728,  ..., 0.0000, 0.0000, 0.0600],
        [0.0655, 0.0000, 0.0728,  ..., 0.0000, 0.0000, 0.0600]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4190218.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39818.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1044.0442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6743.8086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9269.6973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3865.3621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2205],
        [-0.9332],
        [-0.5744],
        ...,
        [-1.3988],
        [-1.3949],
        [-1.3939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1390654.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1156.3571, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1156.3571, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14008.7266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1315.1484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(51.3579, device='cuda:0')



h[100].sum tensor(-621.8212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.0638, device='cuda:0')



h[200].sum tensor(-78.1767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0244, 0.0041, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0244, 0.0041, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0244, 0.0041, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        ...,
        [0.0244, 0.0041, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0244, 0.0041, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0244, 0.0041, 0.0000,  ..., 0.0000, 0.0118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(377055.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0667, 0.0000, 0.0734,  ..., 0.0000, 0.0000, 0.0602],
        [0.0667, 0.0000, 0.0734,  ..., 0.0000, 0.0000, 0.0602],
        [0.0667, 0.0000, 0.0734,  ..., 0.0000, 0.0000, 0.0603],
        ...,
        [0.0665, 0.0000, 0.0734,  ..., 0.0000, 0.0000, 0.0600],
        [0.0665, 0.0000, 0.0734,  ..., 0.0000, 0.0000, 0.0600],
        [0.0665, 0.0000, 0.0734,  ..., 0.0000, 0.0000, 0.0600]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4164888.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40645.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1024.9341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6718.1562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7720.2036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3786.1428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7370],
        [-1.7095],
        [-1.6525],
        ...,
        [-1.4144],
        [-1.4104],
        [-1.4092]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1444429.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1486.9175, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1486.9175, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101,  0.0075,  0.0041,  ..., -0.0071,  0.0130, -0.0077],
        [ 0.0101,  0.0075,  0.0041,  ..., -0.0071,  0.0130, -0.0077],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14866.7168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1339.1289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(66.0392, device='cuda:0')



h[100].sum tensor(-577.1765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(223.8223, device='cuda:0')



h[200].sum tensor(-100.2097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0346, 0.0202, 0.0098,  ..., 0.0000, 0.0370, 0.0000],
        [0.0374, 0.0247, 0.0132,  ..., 0.0000, 0.0439, 0.0000],
        [0.0346, 0.0202, 0.0107,  ..., 0.0000, 0.0370, 0.0000],
        ...,
        [0.0245, 0.0040, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0245, 0.0040, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0245, 0.0040, 0.0000,  ..., 0.0000, 0.0118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(415995.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0221, 0.0000, 0.0112,  ..., 0.0000, 0.0000, 0.0741],
        [0.0192, 0.0000, 0.0089,  ..., 0.0000, 0.0000, 0.0749],
        [0.0281, 0.0000, 0.0188,  ..., 0.0000, 0.0000, 0.0722],
        ...,
        [0.0670, 0.0000, 0.0737,  ..., 0.0000, 0.0000, 0.0600],
        [0.0670, 0.0000, 0.0737,  ..., 0.0000, 0.0000, 0.0600],
        [0.0670, 0.0000, 0.0737,  ..., 0.0000, 0.0000, 0.0600]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4394564., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41153.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1124.4106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6849.3027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13813.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4037.6113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1136],
        [ 0.1943],
        [ 0.1363],
        ...,
        [-1.4243],
        [-1.4203],
        [-1.4191]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1511437.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1275.1537, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1275.1537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14454.0166, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1328.0731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.6340, device='cuda:0')



h[100].sum tensor(-607.6585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(191.9460, device='cuda:0')



h[200].sum tensor(-86.2733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0245, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0245, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0245, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        ...,
        [0.0245, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0245, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0245, 0.0041, 0.0000,  ..., 0.0000, 0.0119, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(388972.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0634, 0.0000, 0.0677,  ..., 0.0000, 0.0000, 0.0615],
        [0.0659, 0.0000, 0.0718,  ..., 0.0000, 0.0000, 0.0608],
        [0.0671, 0.0000, 0.0738,  ..., 0.0000, 0.0000, 0.0606],
        ...,
        [0.0669, 0.0000, 0.0738,  ..., 0.0000, 0.0000, 0.0603],
        [0.0669, 0.0000, 0.0738,  ..., 0.0000, 0.0000, 0.0603],
        [0.0669, 0.0000, 0.0738,  ..., 0.0000, 0.0000, 0.0603]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4231633., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40871.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1058.8179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6772.9785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8997.5420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3868.0193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5202],
        [-0.8224],
        [-1.0838],
        ...,
        [-1.4302],
        [-1.4262],
        [-1.4251]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1469730.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1058.5835, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1058.5835, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0081,  0.0045,  ..., -0.0078,  0.0140, -0.0084],
        [ 0.0097,  0.0067,  0.0035,  ..., -0.0062,  0.0119, -0.0067],
        [ 0.0141,  0.0138,  0.0090,  ..., -0.0140,  0.0229, -0.0151],
        ...,
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0010, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14014.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1315.0839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.0154, device='cuda:0')



h[100].sum tensor(-639.8705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(159.3462, device='cuda:0')



h[200].sum tensor(-71.4523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0377, 0.0254, 0.0145,  ..., 0.0000, 0.0449, 0.0000],
        [0.0533, 0.0504, 0.0320,  ..., 0.0000, 0.0838, 0.0000],
        [0.0377, 0.0254, 0.0136,  ..., 0.0000, 0.0449, 0.0000],
        ...,
        [0.0245, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0245, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0245, 0.0042, 0.0000,  ..., 0.0000, 0.0119, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(368789.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0235, 0.0029, 0.0151,  ..., 0.0000, 0.0000, 0.0733],
        [0.0068, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0781],
        [0.0173, 0.0022, 0.0079,  ..., 0.0000, 0.0000, 0.0756],
        ...,
        [0.0666, 0.0000, 0.0737,  ..., 0.0000, 0.0000, 0.0607],
        [0.0666, 0.0000, 0.0737,  ..., 0.0000, 0.0000, 0.0607],
        [0.0666, 0.0000, 0.0737,  ..., 0.0000, 0.0000, 0.0607]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4137601.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41290.6289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1000.0752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6714.9170, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6807.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3750.9226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2599],
        [ 0.3250],
        [ 0.2058],
        ...,
        [-1.4334],
        [-1.4294],
        [-1.4283]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1497297.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.0522, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.0522, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0011, -0.0010,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0010,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0010,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0061,  0.0011, -0.0010,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0010,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0061,  0.0011, -0.0010,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14985.8066, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1338.8090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(62.9363, device='cuda:0')



h[100].sum tensor(-590.6544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.3057, device='cuda:0')



h[200].sum tensor(-95.3975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0246, 0.0043, 0.0000,  ..., 0.0000, 0.0120, 0.0000],
        [0.0246, 0.0043, 0.0000,  ..., 0.0000, 0.0120, 0.0000],
        [0.0246, 0.0043, 0.0000,  ..., 0.0000, 0.0120, 0.0000],
        ...,
        [0.0246, 0.0043, 0.0000,  ..., 0.0000, 0.0120, 0.0000],
        [0.0246, 0.0043, 0.0000,  ..., 0.0000, 0.0120, 0.0000],
        [0.0246, 0.0043, 0.0000,  ..., 0.0000, 0.0120, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(405481.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0629, 0.0000, 0.0713,  ..., 0.0000, 0.0000, 0.0623],
        [0.0628, 0.0000, 0.0712,  ..., 0.0000, 0.0000, 0.0623],
        [0.0626, 0.0000, 0.0711,  ..., 0.0000, 0.0000, 0.0625],
        ...,
        [0.0663, 0.0000, 0.0735,  ..., 0.0000, 0.0000, 0.0608],
        [0.0663, 0.0000, 0.0735,  ..., 0.0000, 0.0000, 0.0608],
        [0.0663, 0.0000, 0.0735,  ..., 0.0000, 0.0000, 0.0608]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4315416., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38841.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1117.9822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6852.1357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10733.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4010.2136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1973],
        [-1.0900],
        [-0.9301],
        ...,
        [-1.4363],
        [-1.4324],
        [-1.4313]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1324557.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1199.8131, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1199.8131, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14623.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1331.6516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.2879, device='cuda:0')



h[100].sum tensor(-623.3442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(180.6052, device='cuda:0')



h[200].sum tensor(-80.9636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0104, 0.0039,  ..., 0.0000, 0.0217, 0.0000],
        [0.0247, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0247, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        ...,
        [0.0247, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0247, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0247, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(385333.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0443, 0.0000, 0.0405,  ..., 0.0000, 0.0000, 0.0670],
        [0.0574, 0.0000, 0.0621,  ..., 0.0000, 0.0000, 0.0633],
        [0.0620, 0.0000, 0.0695,  ..., 0.0000, 0.0000, 0.0621],
        ...,
        [0.0664, 0.0000, 0.0733,  ..., 0.0000, 0.0000, 0.0602],
        [0.0664, 0.0000, 0.0733,  ..., 0.0000, 0.0000, 0.0602],
        [0.0664, 0.0000, 0.0733,  ..., 0.0000, 0.0000, 0.0602]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4221771., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39351.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1064.0479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6789.4517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8045.2158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3853.7202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1814],
        [-0.5111],
        [-0.7861],
        ...,
        [-1.4421],
        [-1.4382],
        [-1.4371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1367274.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1429.4496, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1429.4496, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0148,  0.0149,  0.0098,  ..., -0.0151,  0.0246, -0.0163],
        [ 0.0160,  0.0167,  0.0112,  ..., -0.0171,  0.0274, -0.0185],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15303.8242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1350.2495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.4869, device='cuda:0')



h[100].sum tensor(-594.2125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(215.1718, device='cuda:0')



h[200].sum tensor(-95.7607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0749, 0.0846, 0.0587,  ..., 0.0000, 0.1374, 0.0000],
        [0.0536, 0.0505, 0.0331,  ..., 0.0000, 0.0842, 0.0000],
        [0.0690, 0.0751, 0.0513,  ..., 0.0000, 0.1226, 0.0000],
        ...,
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(407993.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0774, 0.0000,  ..., 0.0000, 0.0369, 0.0838],
        [0.0003, 0.0534, 0.0000,  ..., 0.0000, 0.0210, 0.0813],
        [0.0000, 0.0481, 0.0000,  ..., 0.0000, 0.0173, 0.0813],
        ...,
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0598],
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0598],
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0598]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4325945.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38312.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1132.3130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6875.7568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10161.0928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3992.3845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2789],
        [ 0.3111],
        [ 0.3261],
        ...,
        [-1.4473],
        [-1.4435],
        [-1.4425]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1305594.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1387.5514, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1387.5514, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0184,  0.0206,  0.0142,  ..., -0.0214,  0.0335, -0.0231],
        [ 0.0117,  0.0098,  0.0059,  ..., -0.0096,  0.0167, -0.0104],
        [ 0.0106,  0.0081,  0.0046,  ..., -0.0077,  0.0141, -0.0084],
        ...,
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15218.1377, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1348.0768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.6260, device='cuda:0')



h[100].sum tensor(-599.2262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(208.8650, device='cuda:0')



h[200].sum tensor(-93.3911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0402, 0.0289, 0.0164,  ..., 0.0000, 0.0507, 0.0000],
        [0.0530, 0.0495, 0.0315,  ..., 0.0000, 0.0828, 0.0000],
        [0.0483, 0.0419, 0.0255,  ..., 0.0000, 0.0708, 0.0000],
        ...,
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0121, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(407121.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0112, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0744],
        [0.0105, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0743],
        [0.0189, 0.0017, 0.0029,  ..., 0.0000, 0.0000, 0.0726],
        ...,
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0598],
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0598],
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0598]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4342760., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38874.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1124.7339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6869.9482, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10881.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3984.9617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3670],
        [ 0.2958],
        [ 0.1509],
        ...,
        [-1.4471],
        [-1.4430],
        [-1.4418]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1354850.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1385.7742, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1385.7742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        ...,
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15330.4551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1351.8195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(61.5471, device='cuda:0')



h[100].sum tensor(-601.3338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(208.5975, device='cuda:0')



h[200].sum tensor(-93.2157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        ...,
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0248, 0.0042, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(405978.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0627, 0.0000, 0.0651,  ..., 0.0000, 0.0000, 0.0608],
        [0.0666, 0.0000, 0.0730,  ..., 0.0000, 0.0000, 0.0599],
        [0.0666, 0.0000, 0.0730,  ..., 0.0000, 0.0000, 0.0600],
        ...,
        [0.0665, 0.0000, 0.0730,  ..., 0.0000, 0.0000, 0.0597],
        [0.0665, 0.0000, 0.0730,  ..., 0.0000, 0.0000, 0.0597],
        [0.0665, 0.0000, 0.0730,  ..., 0.0000, 0.0000, 0.0597]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4320135., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39011.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1114.9280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6874.2837, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10218.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3970.3792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2749],
        [-1.3656],
        [-1.2863],
        ...,
        [-1.4527],
        [-1.4487],
        [-1.4476]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1351196.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1359.7651, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1359.7651, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0103,  0.0076,  0.0041,  ..., -0.0071,  0.0132, -0.0077],
        [ 0.0103,  0.0076,  0.0041,  ..., -0.0071,  0.0132, -0.0077],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        ...,
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15296.4160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1351.8113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.3919, device='cuda:0')



h[100].sum tensor(-607.3623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(204.6824, device='cuda:0')



h[200].sum tensor(-90.8404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0350, 0.0206, 0.0099,  ..., 0.0000, 0.0376, 0.0000],
        [0.0378, 0.0251, 0.0134,  ..., 0.0000, 0.0446, 0.0000],
        [0.0350, 0.0206, 0.0108,  ..., 0.0000, 0.0376, 0.0000],
        ...,
        [0.0249, 0.0043, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0249, 0.0043, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0249, 0.0043, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(403648.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0210, 0.0000, 0.0106,  ..., 0.0000, 0.0000, 0.0730],
        [0.0180, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0737],
        [0.0282, 0.0000, 0.0204,  ..., 0.0000, 0.0000, 0.0710],
        ...,
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0601],
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0601],
        [0.0665, 0.0000, 0.0732,  ..., 0.0000, 0.0000, 0.0601]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4328763.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38711.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1111.7686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6885.8589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9987.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3967.5591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1119],
        [ 0.1673],
        [ 0.0302],
        ...,
        [-1.4592],
        [-1.4553],
        [-1.4542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1324481., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1421.2061, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.2061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0108,  0.0085,  0.0048,  ..., -0.0081,  0.0146, -0.0087],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        ...,
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15485.4932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1358.1345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.1207, device='cuda:0')



h[100].sum tensor(-598.8071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(213.9309, device='cuda:0')



h[200].sum tensor(-95.2371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0327, 0.0169, 0.0079,  ..., 0.0000, 0.0317, 0.0000],
        [0.0295, 0.0118, 0.0048,  ..., 0.0000, 0.0237, 0.0000],
        [0.0249, 0.0044, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        ...,
        [0.0249, 0.0044, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0249, 0.0044, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0249, 0.0044, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(408957.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0369, 0.0000, 0.0252,  ..., 0.0000, 0.0000, 0.0687],
        [0.0494, 0.0000, 0.0451,  ..., 0.0000, 0.0000, 0.0654],
        [0.0562, 0.0000, 0.0581,  ..., 0.0000, 0.0000, 0.0639],
        ...,
        [0.0667, 0.0000, 0.0735,  ..., 0.0000, 0.0000, 0.0606],
        [0.0667, 0.0000, 0.0735,  ..., 0.0000, 0.0000, 0.0606],
        [0.0667, 0.0000, 0.0735,  ..., 0.0000, 0.0000, 0.0606]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4343185., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38606.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1125.1982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6921.5620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9959.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4016.2942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4903],
        [-0.5440],
        [-0.4575],
        ...,
        [-1.4674],
        [-1.4635],
        [-1.4624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1304812.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1269.1302, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1269.1302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0274,  0.0350,  0.0254,  ..., -0.0369,  0.0558, -0.0399],
        [ 0.0213,  0.0253,  0.0179,  ..., -0.0264,  0.0408, -0.0285],
        [ 0.0143,  0.0140,  0.0091,  ..., -0.0140,  0.0231, -0.0151],
        ...,
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0062,  0.0011, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15091.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1351.8113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.3665, device='cuda:0')



h[100].sum tensor(-622.7667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(191.0393, device='cuda:0')



h[200].sum tensor(-84.2751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0810, 0.0943, 0.0660,  ..., 0.0000, 0.1521, 0.0000],
        [0.0844, 0.0997, 0.0703,  ..., 0.0000, 0.1606, 0.0000],
        [0.0929, 0.1135, 0.0810,  ..., 0.0000, 0.1821, 0.0000],
        ...,
        [0.0250, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0250, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0250, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(392465.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0831, 0.0000,  ..., 0.0000, 0.0406, 0.0820],
        [0.0000, 0.0764, 0.0000,  ..., 0.0000, 0.0361, 0.0810],
        [0.0000, 0.0790, 0.0000,  ..., 0.0000, 0.0376, 0.0810],
        ...,
        [0.0672, 0.0000, 0.0740,  ..., 0.0000, 0.0000, 0.0610],
        [0.0672, 0.0000, 0.0740,  ..., 0.0000, 0.0000, 0.0610],
        [0.0672, 0.0000, 0.0740,  ..., 0.0000, 0.0000, 0.0610]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4281300.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40000.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1075.8716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6880.3584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8277.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3907.6287, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3892],
        [ 0.4003],
        [ 0.4126],
        ...,
        [-1.4797],
        [-1.4757],
        [-1.4746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1398235.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1176.1317, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1176.1317, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14912.1299, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1351.4143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(52.2361, device='cuda:0')



h[100].sum tensor(-636.7924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(177.0405, device='cuda:0')



h[200].sum tensor(-78.1958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0300, 0.0124, 0.0053,  ..., 0.0000, 0.0246, 0.0000],
        [0.0251, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0251, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        ...,
        [0.0251, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0251, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0251, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(382578.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0446, 0.0000, 0.0366,  ..., 0.0000, 0.0000, 0.0674],
        [0.0584, 0.0000, 0.0615,  ..., 0.0000, 0.0000, 0.0640],
        [0.0633, 0.0000, 0.0700,  ..., 0.0000, 0.0000, 0.0629],
        ...,
        [0.0679, 0.0000, 0.0743,  ..., 0.0000, 0.0000, 0.0611],
        [0.0679, 0.0000, 0.0743,  ..., 0.0000, 0.0000, 0.0611],
        [0.0679, 0.0000, 0.0743,  ..., 0.0000, 0.0000, 0.0611]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4241318., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41370.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1044.0061, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6855.8628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6894.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3831.3237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1551],
        [-0.5346],
        [-0.8782],
        ...,
        [-1.4921],
        [-1.4880],
        [-1.4869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1480412.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1124.3624, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1124.3624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0150,  0.0150,  0.0099,  ..., -0.0151,  0.0247, -0.0164],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14834.5586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1352.8247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(49.9369, device='cuda:0')



h[100].sum tensor(-644.6786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(169.2477, device='cuda:0')



h[200].sum tensor(-74.9294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0409, 0.0298, 0.0178,  ..., 0.0000, 0.0515, 0.0000],
        [0.0409, 0.0298, 0.0178,  ..., 0.0000, 0.0515, 0.0000],
        [0.0252, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        ...,
        [0.0252, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0252, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0252, 0.0045, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(377957.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0225, 0.0060, 0.0083,  ..., 0.0000, 0.0000, 0.0713],
        [0.0292, 0.0040, 0.0127,  ..., 0.0000, 0.0000, 0.0698],
        [0.0502, 0.0000, 0.0365,  ..., 0.0000, 0.0000, 0.0655],
        ...,
        [0.0685, 0.0000, 0.0747,  ..., 0.0000, 0.0000, 0.0612],
        [0.0685, 0.0000, 0.0747,  ..., 0.0000, 0.0000, 0.0612],
        [0.0685, 0.0000, 0.0747,  ..., 0.0000, 0.0000, 0.0612]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4229483.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41944.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1036.0691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6853.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6129.0088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3796.6743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3658],
        [ 0.3000],
        [ 0.1978],
        ...,
        [-1.5038],
        [-1.4996],
        [-1.4985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1518879.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1522.8733, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1522.8733, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094,  0.0061,  0.0030,  ..., -0.0054,  0.0108, -0.0059],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        ...,
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000],
        [ 0.0063,  0.0011, -0.0009,  ...,  0.0000,  0.0030,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15830.1934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1377.4791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(67.6361, device='cuda:0')



h[100].sum tensor(-590.9233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(229.2347, device='cuda:0')



h[200].sum tensor(-100.4221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0309, 0.0137, 0.0053,  ..., 0.0000, 0.0263, 0.0000],
        [0.0309, 0.0137, 0.0053,  ..., 0.0000, 0.0263, 0.0000],
        [0.0252, 0.0046, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        ...,
        [0.0252, 0.0046, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0252, 0.0046, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0252, 0.0046, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(425632.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0353, 0.0000, 0.0335,  ..., 0.0000, 0.0000, 0.0711],
        [0.0400, 0.0000, 0.0391,  ..., 0.0000, 0.0000, 0.0698],
        [0.0552, 0.0000, 0.0571,  ..., 0.0000, 0.0000, 0.0656],
        ...,
        [0.0684, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0617],
        [0.0684, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0617],
        [0.0684, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0617]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4517241., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40781.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1163.3623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7026.5825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13223.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4126.0674, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0567],
        [-0.1577],
        [-0.3134],
        ...,
        [-1.5108],
        [-1.5067],
        [-1.5055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1457001., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1284.1445, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1284.1445, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        ...,
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15365.4092, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1364.1736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.0334, device='cuda:0')



h[100].sum tensor(-623.6243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.2994, device='cuda:0')



h[200].sum tensor(-85.5753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0297, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0252, 0.0047, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0252, 0.0047, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0252, 0.0047, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        ...,
        [0.0252, 0.0047, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0252, 0.0047, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0252, 0.0047, 0.0000,  ..., 0.0000, 0.0123, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(396178.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0555, 0.0000, 0.0618,  ..., 0.0000, 0.0000, 0.0659],
        [0.0605, 0.0000, 0.0677,  ..., 0.0000, 0.0000, 0.0645],
        [0.0644, 0.0000, 0.0722,  ..., 0.0000, 0.0000, 0.0635],
        ...,
        [0.0680, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0620],
        [0.0680, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0620],
        [0.0680, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0620]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4332256., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40378.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1083.9709, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6942.7324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8073.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3940.1497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8885],
        [-1.0487],
        [-1.2201],
        ...,
        [-1.5147],
        [-1.5106],
        [-1.5094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1419524.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1328.8671, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1328.8671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0102,  0.0073,  0.0039,  ..., -0.0067,  0.0127, -0.0073],
        [ 0.0105,  0.0078,  0.0043,  ..., -0.0073,  0.0135, -0.0079],
        [ 0.0143,  0.0140,  0.0091,  ..., -0.0140,  0.0232, -0.0151],
        ...,
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15579.6787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1370.7068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.0196, device='cuda:0')



h[100].sum tensor(-620.8033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(200.0314, device='cuda:0')



h[200].sum tensor(-87.7779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0386, 0.0261, 0.0148,  ..., 0.0000, 0.0457, 0.0000],
        [0.0521, 0.0477, 0.0299,  ..., 0.0000, 0.0795, 0.0000],
        [0.0386, 0.0261, 0.0140,  ..., 0.0000, 0.0457, 0.0000],
        ...,
        [0.0253, 0.0046, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0253, 0.0046, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0253, 0.0046, 0.0000,  ..., 0.0000, 0.0123, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(401230.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0226, 0.0041, 0.0135,  ..., 0.0000, 0.0000, 0.0724],
        [0.0080, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0756],
        [0.0162, 0.0051, 0.0088,  ..., 0.0000, 0.0000, 0.0741],
        ...,
        [0.0683, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616],
        [0.0683, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616],
        [0.0683, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4364230., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40485.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1098.4897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6964.0835, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8616.9424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3957.1580, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2888],
        [ 0.3817],
        [ 0.3889],
        ...,
        [-1.5228],
        [-1.5186],
        [-1.5175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1442096.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1449.6060, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1449.6060, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0109,  0.0085,  0.0048,  ..., -0.0079,  0.0145, -0.0086],
        ...,
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15884.3574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1378.1862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.3821, device='cuda:0')



h[100].sum tensor(-603.5355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(218.2059, device='cuda:0')



h[200].sum tensor(-95.8491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0253, 0.0046, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0298, 0.0119, 0.0048,  ..., 0.0000, 0.0237, 0.0000],
        [0.0290, 0.0106, 0.0037,  ..., 0.0000, 0.0216, 0.0000],
        ...,
        [0.0253, 0.0046, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0253, 0.0046, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0253, 0.0046, 0.0000,  ..., 0.0000, 0.0123, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(414721.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0623, 0.0000, 0.0671,  ..., 0.0000, 0.0000, 0.0635],
        [0.0483, 0.0000, 0.0487,  ..., 0.0000, 0.0000, 0.0672],
        [0.0396, 0.0000, 0.0365,  ..., 0.0000, 0.0000, 0.0696],
        ...,
        [0.0683, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616],
        [0.0683, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616],
        [0.0683, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4434898., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39773.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1140.2659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7012.6279, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10179.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4049.2217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6574],
        [-0.2624],
        [ 0.0461],
        ...,
        [-1.5201],
        [-1.5148],
        [-1.5128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1389350.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1248.6444, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1248.6444, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0103,  0.0075,  0.0041,  ..., -0.0069,  0.0131, -0.0075],
        ...,
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15510.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1369.3888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.4567, device='cuda:0')



h[100].sum tensor(-633.8654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.9556, device='cuda:0')



h[200].sum tensor(-82.4929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0254, 0.0046, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0293, 0.0110, 0.0041,  ..., 0.0000, 0.0223, 0.0000],
        [0.0387, 0.0259, 0.0148,  ..., 0.0000, 0.0456, 0.0000],
        ...,
        [0.0254, 0.0046, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0254, 0.0046, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0254, 0.0046, 0.0000,  ..., 0.0000, 0.0124, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(394907.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0528, 0.0000, 0.0437,  ..., 0.0000, 0.0000, 0.0646],
        [0.0389, 0.0000, 0.0251,  ..., 0.0000, 0.0000, 0.0680],
        [0.0224, 0.0045, 0.0120,  ..., 0.0000, 0.0000, 0.0719],
        ...,
        [0.0684, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0613],
        [0.0684, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0613],
        [0.0684, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0613]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4335007.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39934.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1090.4199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6953.7383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7129.2319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3907.1062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1682],
        [ 0.2633],
        [ 0.3571],
        ...,
        [-1.5299],
        [-1.5257],
        [-1.5246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1403567.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1369.3831, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1369.3831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0237,  0.0291,  0.0208,  ..., -0.0303,  0.0466, -0.0327],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0167,  0.0178,  0.0121,  ..., -0.0181,  0.0291, -0.0195],
        ...,
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15928.9043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1378.1002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.8191, device='cuda:0')



h[100].sum tensor(-617.7313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(206.1301, device='cuda:0')



h[200].sum tensor(-90.6333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0700, 0.0763, 0.0530,  ..., 0.0000, 0.1242, 0.0000],
        [0.0758, 0.0856, 0.0593,  ..., 0.0000, 0.1387, 0.0000],
        [0.0339, 0.0183, 0.0097,  ..., 0.0000, 0.0336, 0.0000],
        ...,
        [0.0254, 0.0046, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0254, 0.0046, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0254, 0.0046, 0.0000,  ..., 0.0000, 0.0124, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(405617.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0040, 0.0675, 0.0000,  ..., 0.0000, 0.0283, 0.0728],
        [0.0069, 0.0411, 0.0000,  ..., 0.0000, 0.0132, 0.0718],
        [0.0295, 0.0077, 0.0104,  ..., 0.0000, 0.0000, 0.0677],
        ...,
        [0.0682, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0614],
        [0.0682, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0614],
        [0.0682, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0614]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4383930.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39852.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1112.4307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6992.6836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8540.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3978.2598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3208],
        [ 0.2703],
        [ 0.1587],
        ...,
        [-1.5337],
        [-1.5296],
        [-1.5286]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1414392., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1284.0317, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1284.0317, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        ...,
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0063,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15822.9590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1373.0842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.0283, device='cuda:0')



h[100].sum tensor(-631.5100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.2824, device='cuda:0')



h[200].sum tensor(-84.7759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0297, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        ...,
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(401872.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0635, 0.0000, 0.0709,  ..., 0.0000, 0.0000, 0.0631],
        [0.0660, 0.0000, 0.0733,  ..., 0.0000, 0.0000, 0.0624],
        [0.0676, 0.0000, 0.0745,  ..., 0.0000, 0.0000, 0.0620],
        ...,
        [0.0678, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616],
        [0.0678, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616],
        [0.0678, 0.0000, 0.0748,  ..., 0.0000, 0.0000, 0.0616]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4380209., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40159.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1090.6689, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6981.1646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8729.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3954.6492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8514],
        [-1.1262],
        [-1.3727],
        ...,
        [-1.5382],
        [-1.5341],
        [-1.5330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1442970., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1441.6382, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1441.6382, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16322.1807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1383.7916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(64.0282, device='cuda:0')



h[100].sum tensor(-610.7020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(217.0065, device='cuda:0')



h[200].sum tensor(-94.9780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        ...,
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0254, 0.0047, 0.0000,  ..., 0.0000, 0.0125, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(416861.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0501, 0.0000, 0.0476,  ..., 0.0000, 0.0000, 0.0660],
        [0.0533, 0.0000, 0.0509,  ..., 0.0000, 0.0000, 0.0651],
        [0.0591, 0.0000, 0.0590,  ..., 0.0000, 0.0000, 0.0638],
        ...,
        [0.0679, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0617],
        [0.0679, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0617],
        [0.0679, 0.0000, 0.0749,  ..., 0.0000, 0.0000, 0.0617]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4450134., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38964.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1142.1641, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7044.3564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9852.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4060.1472, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0138],
        [-0.1816],
        [-0.3869],
        ...,
        [-1.4417],
        [-1.5125],
        [-1.5327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1362785.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1160.8677, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1160.8677, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0148,  0.0147,  0.0096,  ..., -0.0146,  0.0242, -0.0158],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0203,  0.0236,  0.0165,  ..., -0.0242,  0.0381, -0.0262],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15717.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1368.8522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(51.5582, device='cuda:0')



h[100].sum tensor(-652.0059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(174.7428, device='cuda:0')



h[200].sum tensor(-76.4088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0369, 0.0230, 0.0115,  ..., 0.0000, 0.0411, 0.0000],
        [0.0569, 0.0551, 0.0356,  ..., 0.0000, 0.0913, 0.0000],
        [0.0671, 0.0716, 0.0493,  ..., 0.0000, 0.1170, 0.0000],
        ...,
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(387518.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0737],
        [0.0003, 0.0400, 0.0000,  ..., 0.0000, 0.0134, 0.0750],
        [0.0018, 0.0773, 0.0000,  ..., 0.0000, 0.0344, 0.0739],
        ...,
        [0.0682, 0.0000, 0.0751,  ..., 0.0000, 0.0000, 0.0616],
        [0.0682, 0.0000, 0.0751,  ..., 0.0000, 0.0000, 0.0616],
        [0.0682, 0.0000, 0.0751,  ..., 0.0000, 0.0000, 0.0616]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4311821., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40418.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1055.9175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6945.4956, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6334.8569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3857.4607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4276],
        [ 0.4206],
        [ 0.3946],
        ...,
        [-1.5534],
        [-1.5492],
        [-1.5481]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1448827.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1282.5935, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1282.5935, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0031,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16122.3223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1377.7604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.9645, device='cuda:0')



h[100].sum tensor(-635.5976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.0659, device='cuda:0')



h[200].sum tensor(-84.5286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        ...,
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(403587.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0685, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0620],
        [0.0685, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0620],
        [0.0686, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0621],
        ...,
        [0.0684, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0618],
        [0.0684, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0618],
        [0.0684, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0618]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4401241., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40600.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1096.3833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7001.6367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8533.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3965.0100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7305],
        [-1.8039],
        [-1.8540],
        ...,
        [-1.5616],
        [-1.5574],
        [-1.5563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1461824.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1201.1306, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1201.1306, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0099,  0.0068,  0.0035,  ..., -0.0061,  0.0119, -0.0066],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15981.7949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1374.6461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.3464, device='cuda:0')



h[100].sum tensor(-649.2974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(180.8035, device='cuda:0')



h[200].sum tensor(-78.7184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0319, 0.0149, 0.0061,  ..., 0.0000, 0.0286, 0.0000],
        [0.0319, 0.0149, 0.0061,  ..., 0.0000, 0.0286, 0.0000],
        ...,
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0255, 0.0047, 0.0000,  ..., 0.0000, 0.0126, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(394521., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0521, 0.0000, 0.0553,  ..., 0.0000, 0.0000, 0.0664],
        [0.0411, 0.0000, 0.0427,  ..., 0.0000, 0.0000, 0.0692],
        [0.0367, 0.0000, 0.0356,  ..., 0.0000, 0.0000, 0.0703],
        ...,
        [0.0688, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0618],
        [0.0688, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0618],
        [0.0688, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0618]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4366597.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40961.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1074.5564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6976.8418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7239.1089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3902.8772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2324],
        [-0.3262],
        [-0.3872],
        ...,
        [-1.5707],
        [-1.5665],
        [-1.5653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1481310.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1303.2213, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1303.2213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0110,  0.0086,  0.0048,  ..., -0.0080,  0.0147, -0.0086],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16345.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1383.8983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.8806, device='cuda:0')



h[100].sum tensor(-635.1761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(196.1710, device='cuda:0')



h[200].sum tensor(-85.7982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0256, 0.0046, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0302, 0.0120, 0.0048,  ..., 0.0000, 0.0242, 0.0000],
        [0.0293, 0.0107, 0.0038,  ..., 0.0000, 0.0221, 0.0000],
        ...,
        [0.0256, 0.0046, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0256, 0.0046, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0299, 0.0115, 0.0045,  ..., 0.0000, 0.0234, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(407571.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0635, 0.0000, 0.0679,  ..., 0.0000, 0.0000, 0.0634],
        [0.0554, 0.0000, 0.0554,  ..., 0.0000, 0.0000, 0.0652],
        [0.0483, 0.0000, 0.0458,  ..., 0.0000, 0.0000, 0.0670],
        ...,
        [0.0682, 0.0000, 0.0745,  ..., 0.0000, 0.0000, 0.0620],
        [0.0611, 0.0000, 0.0615,  ..., 0.0000, 0.0000, 0.0634],
        [0.0444, 0.0000, 0.0337,  ..., 0.0000, 0.0000, 0.0666]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4439303.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40574.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1119.0806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7028.8242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8580.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3989.8933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8079],
        [-0.6278],
        [-0.3465],
        ...,
        [-1.3317],
        [-0.9590],
        [-0.4392]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1454251.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1217.5273, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1217.5273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0213,  0.0251,  0.0177,  ..., -0.0259,  0.0406, -0.0280],
        [ 0.0114,  0.0092,  0.0053,  ..., -0.0087,  0.0157, -0.0094],
        [ 0.0169,  0.0181,  0.0123,  ..., -0.0183,  0.0296, -0.0198],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16193.5713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1381.0876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.0747, device='cuda:0')



h[100].sum tensor(-648.6678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(183.2717, device='cuda:0')



h[200].sum tensor(-80.0854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0672, 0.0716, 0.0484,  ..., 0.0000, 0.1171, 0.0000],
        [0.0724, 0.0798, 0.0548,  ..., 0.0000, 0.1300, 0.0000],
        [0.0392, 0.0265, 0.0152,  ..., 0.0000, 0.0468, 0.0000],
        ...,
        [0.0256, 0.0046, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0256, 0.0046, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0256, 0.0046, 0.0000,  ..., 0.0000, 0.0126, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(398184.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0357, 0.0000,  ..., 0.0000, 0.0063, 0.0712],
        [0.0086, 0.0276, 0.0000,  ..., 0.0000, 0.0048, 0.0702],
        [0.0307, 0.0081, 0.0159,  ..., 0.0000, 0.0000, 0.0678],
        ...,
        [0.0693, 0.0000, 0.0760,  ..., 0.0000, 0.0000, 0.0620],
        [0.0693, 0.0000, 0.0760,  ..., 0.0000, 0.0000, 0.0620],
        [0.0693, 0.0000, 0.0760,  ..., 0.0000, 0.0000, 0.0620]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4395268., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40844.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1097.6748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7005.5488, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7219.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3927.6584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4278],
        [ 0.2884],
        [ 0.0466],
        ...,
        [-1.5884],
        [-1.5842],
        [-1.5830]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1475740.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 9300 loss: tensor(1410.1771, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1353.2345, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1353.2345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16645.3027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1390.9822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.1019, device='cuda:0')



h[100].sum tensor(-632.2579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(203.6993, device='cuda:0')



h[200].sum tensor(-88.4702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0490, 0.0422, 0.0264,  ..., 0.0000, 0.0713, 0.0000],
        [0.0289, 0.0098, 0.0031,  ..., 0.0000, 0.0208, 0.0000],
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        ...,
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(419216.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0102, 0.0390, 0.0046,  ..., 0.0000, 0.0138, 0.0715],
        [0.0208, 0.0088, 0.0043,  ..., 0.0000, 0.0000, 0.0695],
        [0.0266, 0.0040, 0.0047,  ..., 0.0000, 0.0000, 0.0687],
        ...,
        [0.0689, 0.0000, 0.0758,  ..., 0.0000, 0.0000, 0.0619],
        [0.0689, 0.0000, 0.0758,  ..., 0.0000, 0.0000, 0.0619],
        [0.0689, 0.0000, 0.0758,  ..., 0.0000, 0.0000, 0.0619]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4544607., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39652.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1152.7593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7080.9219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10790.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4066.0537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3759],
        [ 0.3873],
        [ 0.3892],
        ...,
        [-1.5916],
        [-1.5874],
        [-1.5862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1415312., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1426.0577, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1426.0577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0103,  0.0074,  0.0039,  ..., -0.0067,  0.0129, -0.0073],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16851.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1395.8661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(63.3362, device='cuda:0')



h[100].sum tensor(-620.9551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(214.6613, device='cuda:0')



h[200].sum tensor(-93.6891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0389, 0.0260, 0.0148,  ..., 0.0000, 0.0460, 0.0000],
        [0.0295, 0.0109, 0.0039,  ..., 0.0000, 0.0225, 0.0000],
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        ...,
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0257, 0.0047, 0.0000,  ..., 0.0000, 0.0127, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(424422.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0232, 0.0048, 0.0140,  ..., 0.0000, 0.0000, 0.0711],
        [0.0415, 0.0000, 0.0330,  ..., 0.0000, 0.0000, 0.0679],
        [0.0568, 0.0000, 0.0584,  ..., 0.0000, 0.0000, 0.0651],
        ...,
        [0.0689, 0.0000, 0.0758,  ..., 0.0000, 0.0000, 0.0619],
        [0.0689, 0.0000, 0.0758,  ..., 0.0000, 0.0000, 0.0619],
        [0.0689, 0.0000, 0.0758,  ..., 0.0000, 0.0000, 0.0619]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4540871., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39650.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1165.2021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7097.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10799.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4100.2271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2120],
        [-0.0729],
        [-0.4249],
        ...,
        [-1.5904],
        [-1.5873],
        [-1.5862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1415017.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1339.0776, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1339.0776, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16699.0977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1390.5114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.4731, device='cuda:0')



h[100].sum tensor(-635.9133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(201.5683, device='cuda:0')



h[200].sum tensor(-87.3942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        ...,
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(410801.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0000, 0.0719,  ..., 0.0000, 0.0000, 0.0628],
        [0.0606, 0.0000, 0.0623,  ..., 0.0000, 0.0000, 0.0638],
        [0.0542, 0.0000, 0.0507,  ..., 0.0000, 0.0000, 0.0651],
        ...,
        [0.0682, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0622],
        [0.0682, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0622],
        [0.0682, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0622]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4453946.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40698.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1099.3699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7045.1709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9554.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4008.3083, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4776],
        [-0.2501],
        [-0.0104],
        ...,
        [-1.5920],
        [-1.5878],
        [-1.5866]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1517348.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1472.8097, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1472.8097, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0210,  0.0246,  0.0173,  ..., -0.0252,  0.0398, -0.0272],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17171.6621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1400.6965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.4126, device='cuda:0')



h[100].sum tensor(-616.9718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(221.6987, device='cuda:0')



h[200].sum tensor(-96.7923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0786, 0.0899, 0.0625,  ..., 0.0000, 0.1457, 0.0000],
        [0.0462, 0.0378, 0.0238,  ..., 0.0000, 0.0643, 0.0000],
        [0.0575, 0.0559, 0.0370,  ..., 0.0000, 0.0927, 0.0000],
        ...,
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0257, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(427424.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0050, 0.0280, 0.0000,  ..., 0.0000, 0.0032, 0.0693],
        [0.0113, 0.0166, 0.0000,  ..., 0.0000, 0.0002, 0.0695],
        [0.0091, 0.0215, 0.0012,  ..., 0.0000, 0.0030, 0.0705],
        ...,
        [0.0674, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0623],
        [0.0674, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0623],
        [0.0674, 0.0000, 0.0753,  ..., 0.0000, 0.0000, 0.0623]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4534250., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37698.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1160.5005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7116.8438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10628.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4130.0547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4192],
        [ 0.3961],
        [ 0.2896],
        ...,
        [-1.5817],
        [-1.5751],
        [-1.5712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1319092.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1266.6885, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1266.6885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0064,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16610.1914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1392.2224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.2581, device='cuda:0')



h[100].sum tensor(-648.8362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.6718, device='cuda:0')



h[200].sum tensor(-82.8266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0473, 0.0394, 0.0251,  ..., 0.0000, 0.0669, 0.0000],
        [0.0258, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0390, 0.0261, 0.0148,  ..., 0.0000, 0.0461, 0.0000],
        ...,
        [0.0258, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0258, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0258, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(405662.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0177, 0.0323, 0.0064,  ..., 0.0000, 0.0125, 0.0681],
        [0.0333, 0.0009, 0.0133,  ..., 0.0000, 0.0000, 0.0673],
        [0.0232, 0.0031, 0.0123,  ..., 0.0000, 0.0000, 0.0703],
        ...,
        [0.0678, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0623],
        [0.0678, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0623],
        [0.0678, 0.0000, 0.0756,  ..., 0.0000, 0.0000, 0.0623]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4431346., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37179.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1123.0156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7066.4907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6949.0488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3979.9285, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4005],
        [ 0.4239],
        [ 0.4421],
        ...,
        [-1.6030],
        [-1.5988],
        [-1.5977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1288223.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1314.8080, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1314.8080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16688.1660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1401.0111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.3952, device='cuda:0')



h[100].sum tensor(-644.3087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.9151, device='cuda:0')



h[200].sum tensor(-85.5634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0259, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0259, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        ...,
        [0.0259, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0259, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0259, 0.0048, 0.0000,  ..., 0.0000, 0.0129, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(411983.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0624],
        [0.0689, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0624],
        [0.0689, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0625],
        ...,
        [0.0688, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0622],
        [0.0688, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0622],
        [0.0688, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0622]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4481576., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38166.6680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1149.5659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7099.1021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7799.5532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4010.4192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9203],
        [-1.9405],
        [-1.9433],
        ...,
        [-1.6183],
        [-1.6135],
        [-1.6118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1365800.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1699.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1699.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0151,  0.0150,  0.0099,  ..., -0.0148,  0.0248, -0.0161],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0113,  0.0089,  0.0051,  ..., -0.0082,  0.0152, -0.0089],
        ...,
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17570.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1426.8807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(75.4847, device='cuda:0')



h[100].sum tensor(-592.2227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(255.8353, device='cuda:0')



h[200].sum tensor(-109.8287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0547, 0.0510, 0.0333,  ..., 0.0000, 0.0850, 0.0000],
        [0.0619, 0.0626, 0.0414,  ..., 0.0000, 0.1030, 0.0000],
        [0.0490, 0.0419, 0.0262,  ..., 0.0000, 0.0707, 0.0000],
        ...,
        [0.0260, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0260, 0.0048, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0385, 0.0250, 0.0148,  ..., 0.0000, 0.0444, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(448875.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3243e-06, 4.9459e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6713e-02,
         7.3971e-02],
        [0.0000e+00, 4.0297e-02, 0.0000e+00,  ..., 0.0000e+00, 1.1142e-02,
         7.5260e-02],
        [1.0361e-03, 2.3679e-02, 0.0000e+00,  ..., 0.0000e+00, 2.1436e-03,
         7.5584e-02],
        ...,
        [6.7853e-02, 0.0000e+00, 7.3056e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.2881e-02],
        [6.2516e-02, 0.0000e+00, 6.0514e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.3399e-02],
        [4.7572e-02, 1.9223e-03, 3.3870e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.4744e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4687265., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38710.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1242.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7232.6040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12867.9189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4260.5249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3968],
        [ 0.4197],
        [ 0.4117],
        ...,
        [-1.4648],
        [-1.2208],
        [-0.8751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1417053.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1315.5201, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1315.5201, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16465.8379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1403.8759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.4269, device='cuda:0')



h[100].sum tensor(-645.5788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.0223, device='cuda:0')



h[200].sum tensor(-85.3723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0260, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0260, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0260, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        ...,
        [0.0260, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0260, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0260, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(414055.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0697, 0.0000, 0.0773,  ..., 0.0000, 0.0000, 0.0639],
        [0.0697, 0.0000, 0.0773,  ..., 0.0000, 0.0000, 0.0639],
        [0.0697, 0.0000, 0.0773,  ..., 0.0000, 0.0000, 0.0640],
        ...,
        [0.0696, 0.0000, 0.0774,  ..., 0.0000, 0.0000, 0.0637],
        [0.0696, 0.0000, 0.0774,  ..., 0.0000, 0.0000, 0.0637],
        [0.0696, 0.0000, 0.0774,  ..., 0.0000, 0.0000, 0.0637]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4532098.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40224.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1137.9561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7128.4575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9138.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4049.5249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9848],
        [-1.9721],
        [-1.9370],
        ...,
        [-1.6412],
        [-1.6376],
        [-1.6368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1506972., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1131.8799, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1131.8799, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0100,  0.0069,  0.0035,  ..., -0.0060,  0.0120, -0.0065],
        [ 0.0100,  0.0069,  0.0035,  ..., -0.0060,  0.0120, -0.0065],
        [ 0.0065,  0.0013, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        ...,
        [ 0.0065,  0.0013, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0013, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0013, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15904.8525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1394.3823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(50.2708, device='cuda:0')



h[100].sum tensor(-672.2020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(170.3793, device='cuda:0')



h[200].sum tensor(-73.4816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0323, 0.0152, 0.0061,  ..., 0.0000, 0.0287, 0.0000],
        [0.0323, 0.0152, 0.0061,  ..., 0.0000, 0.0287, 0.0000],
        [0.0323, 0.0152, 0.0061,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0260, 0.0050, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0260, 0.0050, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0260, 0.0050, 0.0000,  ..., 0.0000, 0.0128, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(393191.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0386, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0722],
        [0.0409, 0.0000, 0.0432,  ..., 0.0000, 0.0000, 0.0717],
        [0.0469, 0.0000, 0.0503,  ..., 0.0000, 0.0000, 0.0704],
        ...,
        [0.0700, 0.0000, 0.0779,  ..., 0.0000, 0.0000, 0.0643],
        [0.0700, 0.0000, 0.0779,  ..., 0.0000, 0.0000, 0.0643],
        [0.0700, 0.0000, 0.0779,  ..., 0.0000, 0.0000, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4429641., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40954.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1083.1401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7072.5298, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6255.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3923.7566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4672],
        [-0.5247],
        [-0.6140],
        ...,
        [-1.6545],
        [-1.6500],
        [-1.6487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1537453.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1337.7782, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1337.7782, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0191,  0.0215,  0.0148,  ..., -0.0217,  0.0348, -0.0235],
        [ 0.0123,  0.0106,  0.0064,  ..., -0.0100,  0.0178, -0.0108],
        [ 0.0139,  0.0132,  0.0084,  ..., -0.0128,  0.0218, -0.0138],
        ...,
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0065,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16429.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1410.6918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.4154, device='cuda:0')



h[100].sum tensor(-644.8802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(201.3727, device='cuda:0')



h[200].sum tensor(-86.6503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0559, 0.0531, 0.0337,  ..., 0.0000, 0.0878, 0.0000],
        [0.0563, 0.0537, 0.0342,  ..., 0.0000, 0.0888, 0.0000],
        [0.0427, 0.0317, 0.0180,  ..., 0.0000, 0.0545, 0.0000],
        ...,
        [0.0261, 0.0050, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0261, 0.0050, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0261, 0.0050, 0.0000,  ..., 0.0000, 0.0128, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(411652.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0400, 0.0000,  ..., 0.0000, 0.0124, 0.0757],
        [0.0026, 0.0238, 0.0000,  ..., 0.0000, 0.0050, 0.0756],
        [0.0140, 0.0069, 0.0049,  ..., 0.0000, 0.0000, 0.0741],
        ...,
        [0.0706, 0.0000, 0.0783,  ..., 0.0000, 0.0000, 0.0644],
        [0.0706, 0.0000, 0.0783,  ..., 0.0000, 0.0000, 0.0644],
        [0.0706, 0.0000, 0.0783,  ..., 0.0000, 0.0000, 0.0644]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4509502., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40740.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1144.0554, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7146.5615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7588., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4045.3486, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4851],
        [ 0.4956],
        [ 0.4867],
        ...,
        [-1.6662],
        [-1.6617],
        [-1.6605]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1523297.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 9600 loss: tensor(459.6658, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2678],
        [0.4895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1342.1108, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2678],
        [0.4895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1342.1108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0080,  0.0044,  ..., -0.0072,  0.0137, -0.0078],
        [ 0.0181,  0.0198,  0.0135,  ..., -0.0198,  0.0321, -0.0214],
        [ 0.0146,  0.0142,  0.0092,  ..., -0.0138,  0.0234, -0.0150],
        ...,
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16524.3887, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1417.6323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.6078, device='cuda:0')



h[100].sum tensor(-647.0198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(202.0249, device='cuda:0')



h[200].sum tensor(-86.8100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0523, 0.0470, 0.0300,  ..., 0.0000, 0.0785, 0.0000],
        [0.0603, 0.0598, 0.0391,  ..., 0.0000, 0.0984, 0.0000],
        [0.0727, 0.0798, 0.0546,  ..., 0.0000, 0.1297, 0.0000],
        ...,
        [0.0262, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0262, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0262, 0.0049, 0.0000,  ..., 0.0000, 0.0128, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(415205.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0054, 0.0206, 0.0000,  ..., 0.0000, 0.0031, 0.0738],
        [0.0013, 0.0297, 0.0000,  ..., 0.0000, 0.0054, 0.0750],
        [0.0000, 0.0362, 0.0000,  ..., 0.0000, 0.0079, 0.0743],
        ...,
        [0.0714, 0.0000, 0.0785,  ..., 0.0000, 0.0000, 0.0637],
        [0.0714, 0.0000, 0.0785,  ..., 0.0000, 0.0000, 0.0637],
        [0.0714, 0.0000, 0.0785,  ..., 0.0000, 0.0000, 0.0637]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4554585., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(42487.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1148.0037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7153.2295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8859.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4040.1372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5224],
        [ 0.5184],
        [ 0.4957],
        ...,
        [-1.6784],
        [-1.6738],
        [-1.6725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1659235.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1219.7507, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1219.7507, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0117,  0.0095,  0.0055,  ..., -0.0088,  0.0161, -0.0095],
        [ 0.0145,  0.0139,  0.0090,  ..., -0.0136,  0.0231, -0.0147],
        [ 0.0151,  0.0150,  0.0098,  ..., -0.0147,  0.0248, -0.0160],
        ...,
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0032,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16327.6455, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1413.0540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.1734, device='cuda:0')



h[100].sum tensor(-667.4287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(183.6063, device='cuda:0')



h[200].sum tensor(-78.5562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0383, 0.0243, 0.0134,  ..., 0.0000, 0.0433, 0.0000],
        [0.0464, 0.0373, 0.0226,  ..., 0.0000, 0.0636, 0.0000],
        [0.0661, 0.0691, 0.0464,  ..., 0.0000, 0.1132, 0.0000],
        ...,
        [0.0263, 0.0049, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0263, 0.0049, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0263, 0.0049, 0.0000,  ..., 0.0000, 0.0129, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(401687.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0291, 0.0021, 0.0125,  ..., 0.0000, 0.0000, 0.0699],
        [0.0127, 0.0137, 0.0004,  ..., 0.0000, 0.0017, 0.0714],
        [0.0033, 0.0332, 0.0000,  ..., 0.0000, 0.0082, 0.0724],
        ...,
        [0.0710, 0.0000, 0.0781,  ..., 0.0000, 0.0000, 0.0632],
        [0.0710, 0.0000, 0.0781,  ..., 0.0000, 0.0000, 0.0632],
        [0.0710, 0.0000, 0.0781,  ..., 0.0000, 0.0000, 0.0632]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4471959., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(41449.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1118.4084, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7111.6514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6371.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3938.6716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0333],
        [ 0.3075],
        [ 0.4401],
        ...,
        [-1.6794],
        [-1.6748],
        [-1.6736]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1587346.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.0000],
        [0.5942],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1055.3108, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.0000],
        [0.5942],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1055.3108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0257,  0.0322,  0.0231,  ..., -0.0330,  0.0516, -0.0357],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        ...,
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16020.1523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1401.6697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.8700, device='cuda:0')



h[100].sum tensor(-691.7133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(158.8536, device='cuda:0')



h[200].sum tensor(-68.1495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0756, 0.0845, 0.0582,  ..., 0.0000, 0.1372, 0.0000],
        [0.0419, 0.0302, 0.0179,  ..., 0.0000, 0.0525, 0.0000],
        [0.0785, 0.0892, 0.0619,  ..., 0.0000, 0.1446, 0.0000],
        ...,
        [0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(386753.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0273, 0.0000,  ..., 0.0000, 0.0045, 0.0711],
        [0.0123, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0693],
        [0.0016, 0.0335, 0.0000,  ..., 0.0000, 0.0077, 0.0693],
        ...,
        [0.0696, 0.0000, 0.0775,  ..., 0.0000, 0.0000, 0.0633],
        [0.0696, 0.0000, 0.0775,  ..., 0.0000, 0.0000, 0.0633],
        [0.0696, 0.0000, 0.0775,  ..., 0.0000, 0.0000, 0.0633]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4387569., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40551.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1060.6809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7058.9346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4777.1206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3848.4019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5224],
        [ 0.5186],
        [ 0.5119],
        ...,
        [-1.6731],
        [-1.6687],
        [-1.6675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1538625., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1536.1394, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1536.1394, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        ...,
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0012, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17298.7773, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1430.8599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(68.2253, device='cuda:0')



h[100].sum tensor(-624.0922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(231.2316, device='cuda:0')



h[200].sum tensor(-98.9537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0301, 0.0113, 0.0040,  ..., 0.0000, 0.0229, 0.0000],
        ...,
        [0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0262, 0.0050, 0.0000,  ..., 0.0000, 0.0131, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(439925.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0688, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0637],
        [0.0637, 0.0000, 0.0683,  ..., 0.0000, 0.0000, 0.0647],
        [0.0500, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0676],
        ...,
        [0.0696, 0.0000, 0.0775,  ..., 0.0000, 0.0000, 0.0633],
        [0.0696, 0.0000, 0.0775,  ..., 0.0000, 0.0000, 0.0633],
        [0.0696, 0.0000, 0.0775,  ..., 0.0000, 0.0000, 0.0633]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4666970., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39683.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1204.7598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7236.0117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11928.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4202.7681, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4029],
        [-1.0526],
        [-0.6157],
        ...,
        [-1.6698],
        [-1.6606],
        [-1.6322]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1497056.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1254.0023, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1254.0023, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0155,  0.0157,  0.0103,  ..., -0.0154,  0.0259, -0.0167],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0160,  0.0165,  0.0110,  ..., -0.0163,  0.0272, -0.0176],
        ...,
        [ 0.0107,  0.0080,  0.0043,  ..., -0.0072,  0.0138, -0.0078],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16673.1582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1413.6689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.6946, device='cuda:0')



h[100].sum tensor(-664.6940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(188.7621, device='cuda:0')



h[200].sum tensor(-81.0918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0469, 0.0385, 0.0242,  ..., 0.0000, 0.0654, 0.0000],
        [0.0447, 0.0348, 0.0213,  ..., 0.0000, 0.0597, 0.0000],
        [0.0486, 0.0412, 0.0263,  ..., 0.0000, 0.0696, 0.0000],
        ...,
        [0.0296, 0.0106, 0.0034,  ..., 0.0000, 0.0218, 0.0000],
        [0.0304, 0.0118, 0.0043,  ..., 0.0000, 0.0237, 0.0000],
        [0.0262, 0.0051, 0.0000,  ..., 0.0000, 0.0132, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(413031.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0227, 0.0000,  ..., 0.0000, 0.0049, 0.0696],
        [0.0067, 0.0169, 0.0000,  ..., 0.0000, 0.0013, 0.0692],
        [0.0094, 0.0280, 0.0000,  ..., 0.0000, 0.0090, 0.0695],
        ...,
        [0.0507, 0.0000, 0.0533,  ..., 0.0000, 0.0000, 0.0673],
        [0.0551, 0.0000, 0.0586,  ..., 0.0000, 0.0000, 0.0663],
        [0.0643, 0.0000, 0.0712,  ..., 0.0000, 0.0000, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4525660., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(37421.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1138.6799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7158.0225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7673.7329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4036.8850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4820],
        [ 0.4822],
        [ 0.4746],
        ...,
        [-1.2774],
        [-1.3918],
        [-1.5190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1321909.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1317.4576, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1317.4576, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        ...,
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16883.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1420.0623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(58.5129, device='cuda:0')



h[100].sum tensor(-657.1964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(198.3139, device='cuda:0')



h[200].sum tensor(-84.9559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0051, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0263, 0.0051, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0263, 0.0051, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        ...,
        [0.0263, 0.0051, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0263, 0.0051, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0263, 0.0051, 0.0000,  ..., 0.0000, 0.0133, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(418043.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0686, 0.0000, 0.0770,  ..., 0.0000, 0.0000, 0.0637],
        [0.0686, 0.0000, 0.0770,  ..., 0.0000, 0.0000, 0.0637],
        [0.0683, 0.0000, 0.0767,  ..., 0.0000, 0.0000, 0.0639],
        ...,
        [0.0685, 0.0000, 0.0771,  ..., 0.0000, 0.0000, 0.0635],
        [0.0685, 0.0000, 0.0771,  ..., 0.0000, 0.0000, 0.0635],
        [0.0685, 0.0000, 0.0771,  ..., 0.0000, 0.0000, 0.0635]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4549268., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(36932.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1158.5449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7184.4551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7824.1362, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4070.9431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7024],
        [-1.5967],
        [-1.3742],
        ...,
        [-1.6746],
        [-1.6703],
        [-1.6692]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1293657., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1202.6851, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1202.6851, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0245,  0.0301,  0.0216,  ..., -0.0308,  0.0484, -0.0333],
        ...,
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16576.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1419.1249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.4155, device='cuda:0')



h[100].sum tensor(-675.0960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(181.0375, device='cuda:0')



h[200].sum tensor(-77.3268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0399, 0.0269, 0.0152,  ..., 0.0000, 0.0474, 0.0000],
        [0.0553, 0.0516, 0.0344,  ..., 0.0000, 0.0860, 0.0000],
        [0.0581, 0.0562, 0.0371,  ..., 0.0000, 0.0932, 0.0000],
        ...,
        [0.0264, 0.0050, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0264, 0.0050, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0264, 0.0050, 0.0000,  ..., 0.0000, 0.0133, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(404613.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0199, 0.0000,  ..., 0.0000, 0.0000, 0.0708],
        [0.0039, 0.0357, 0.0000,  ..., 0.0000, 0.0098, 0.0699],
        [0.0016, 0.0442, 0.0000,  ..., 0.0000, 0.0139, 0.0704],
        ...,
        [0.0695, 0.0000, 0.0776,  ..., 0.0000, 0.0000, 0.0634],
        [0.0695, 0.0000, 0.0776,  ..., 0.0000, 0.0000, 0.0634],
        [0.0695, 0.0000, 0.0776,  ..., 0.0000, 0.0000, 0.0634]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4484793., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(38874.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1123.3611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7144.6318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6223.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3964.1099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3926],
        [ 0.3775],
        [ 0.3644],
        ...,
        [-1.6900],
        [-1.6856],
        [-1.6844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1432784., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1283.0941, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1283.0941, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0112,  0.0086,  0.0049,  ..., -0.0079,  0.0149, -0.0085],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        ...,
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16783.6445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1423.8182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(56.9867, device='cuda:0')



h[100].sum tensor(-664.2227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(193.1413, device='cuda:0')



h[200].sum tensor(-82.2617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0360, 0.0205, 0.0103,  ..., 0.0000, 0.0375, 0.0000],
        [0.0310, 0.0124, 0.0049,  ..., 0.0000, 0.0248, 0.0000],
        [0.0264, 0.0050, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        ...,
        [0.0264, 0.0050, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0264, 0.0050, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0264, 0.0050, 0.0000,  ..., 0.0000, 0.0133, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(407924.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0298, 0.0010, 0.0220,  ..., 0.0000, 0.0000, 0.0705],
        [0.0491, 0.0000, 0.0447,  ..., 0.0000, 0.0000, 0.0671],
        [0.0639, 0.0000, 0.0684,  ..., 0.0000, 0.0000, 0.0647],
        ...,
        [0.0695, 0.0000, 0.0776,  ..., 0.0000, 0.0000, 0.0634],
        [0.0695, 0.0000, 0.0776,  ..., 0.0000, 0.0000, 0.0634],
        [0.0695, 0.0000, 0.0776,  ..., 0.0000, 0.0000, 0.0634]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4480745., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39357.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1124.7039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7150.8101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6493.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3984.1492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1061],
        [-0.5892],
        [-1.0908],
        ...,
        [-1.5321],
        [-1.6438],
        [-1.6735]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1463907.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1336.6963, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.6963, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        ...,
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16893.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1432.7250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(59.3674, device='cuda:0')



h[100].sum tensor(-657.8451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(201.2099, device='cuda:0')



h[200].sum tensor(-85.4505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0459, 0.0363, 0.0226,  ..., 0.0000, 0.0622, 0.0000],
        [0.0396, 0.0261, 0.0146,  ..., 0.0000, 0.0462, 0.0000],
        [0.0305, 0.0114, 0.0041,  ..., 0.0000, 0.0233, 0.0000],
        ...,
        [0.0265, 0.0050, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0265, 0.0050, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0265, 0.0050, 0.0000,  ..., 0.0000, 0.0132, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(423249.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0120, 0.0142, 0.0012,  ..., 0.0000, 0.0002, 0.0711],
        [0.0212, 0.0063, 0.0093,  ..., 0.0000, 0.0000, 0.0705],
        [0.0410, 0.0000, 0.0294,  ..., 0.0000, 0.0000, 0.0685],
        ...,
        [0.0705, 0.0000, 0.0782,  ..., 0.0000, 0.0000, 0.0635],
        [0.0705, 0.0000, 0.0782,  ..., 0.0000, 0.0000, 0.0635],
        [0.0705, 0.0000, 0.0782,  ..., 0.0000, 0.0000, 0.0635]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4628369., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(39375.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1188.2087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7219.9805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9197.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4081.9885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3594],
        [ 0.1838],
        [-0.1171],
        ...,
        [-1.7064],
        [-1.7018],
        [-1.7005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1468142.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1152.5856, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1152.5856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        ...,
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16343.4785, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1423.7313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(51.1904, device='cuda:0')



h[100].sum tensor(-683.4747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(173.4961, device='cuda:0')



h[200].sum tensor(-73.8395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0051, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0266, 0.0051, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0266, 0.0051, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        ...,
        [0.0266, 0.0051, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0266, 0.0051, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0266, 0.0051, 0.0000,  ..., 0.0000, 0.0132, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(401596.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0618, 0.0000, 0.0673,  ..., 0.0000, 0.0000, 0.0666],
        [0.0672, 0.0000, 0.0754,  ..., 0.0000, 0.0000, 0.0655],
        [0.0697, 0.0000, 0.0777,  ..., 0.0000, 0.0000, 0.0650],
        ...,
        [0.0711, 0.0000, 0.0789,  ..., 0.0000, 0.0000, 0.0642],
        [0.0711, 0.0000, 0.0789,  ..., 0.0000, 0.0000, 0.0642],
        [0.0711, 0.0000, 0.0789,  ..., 0.0000, 0.0000, 0.0642]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4506550., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40417.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1135.6765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7162.4302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5834.0337, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(3951.1140, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6666],
        [-1.0202],
        [-1.2963],
        ...,
        [-1.7196],
        [-1.7150],
        [-1.7137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1515080.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 9900 loss: tensor(402.0144, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3469],
        [0.2426],
        [0.3691],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1239.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3469],
        [0.2426],
        [0.3691],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1239.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0218,  0.0258,  0.0182,  ..., -0.0261,  0.0417, -0.0282],
        [ 0.0259,  0.0323,  0.0232,  ..., -0.0329,  0.0517, -0.0356],
        [ 0.0187,  0.0208,  0.0142,  ..., -0.0207,  0.0337, -0.0224],
        ...,
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000],
        [ 0.0066,  0.0013, -0.0009,  ...,  0.0000,  0.0033,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16537.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1429.0463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(55.0468, device='cuda:0')



h[100].sum tensor(-670.8312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(186.5666, device='cuda:0')



h[200].sum tensor(-79.4718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0879, 0.1041, 0.0732,  ..., 0.0000, 0.1678, 0.0000],
        [0.0885, 0.1051, 0.0740,  ..., 0.0000, 0.1694, 0.0000],
        [0.0923, 0.1112, 0.0787,  ..., 0.0000, 0.1789, 0.0000],
        ...,
        [0.0265, 0.0052, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0265, 0.0052, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0265, 0.0052, 0.0000,  ..., 0.0000, 0.0132, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(412474.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1000, 0.0000,  ..., 0.0000, 0.0543, 0.0717],
        [0.0000, 0.1115, 0.0000,  ..., 0.0000, 0.0618, 0.0703],
        [0.0000, 0.1147, 0.0000,  ..., 0.0000, 0.0637, 0.0699],
        ...,
        [0.0711, 0.0000, 0.0793,  ..., 0.0000, 0.0000, 0.0650],
        [0.0711, 0.0000, 0.0793,  ..., 0.0000, 0.0000, 0.0650],
        [0.0711, 0.0000, 0.0793,  ..., 0.0000, 0.0000, 0.0650]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4573414., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(40183.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1170.6572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7212.4385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7341.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(4048.1628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3117],
        [ 0.2972],
        [ 0.2919],
        ...,
        [-1.7281],
        [-1.7235],
        [-1.7222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1488111.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 80, in <module>
    outi = net(batcheddglgraph, featbatch).reshape(BatchSize, 6796)#.to('cpu')#.type(torch.LongTensor)  # Perform a single forward pass.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/ModelBha.py", line 52, in forward
    h2 = self.conv2(g, h1)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 440, in forward
    degs = graph.in_degrees().float().clamp(min=1)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/heterograph.py", line 3496, in in_degrees
    deg = self._graph.in_degrees(etid, v_tensor)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/heterograph_index.py", line 577, in in_degrees
    return F.from_dgl_nd(_CAPI_DGLHeteroInDegrees(
  File "dgl/_ffi/_cython/./function.pxi", line 287, in dgl._ffi._cy3.core.FunctionBase.__call__
  File "dgl/_ffi/_cython/./function.pxi", line 222, in dgl._ffi._cy3.core.FuncCall
  File "dgl/_ffi/_cython/./function.pxi", line 211, in dgl._ffi._cy3.core.FuncCall3
  File "dgl/_ffi/_cython/./base.pxi", line 155, in dgl._ffi._cy3.core.CALL
dgl._ffi.base.DGLError: [22:12:44] /opt/dgl/src/runtime/cuda/cuda_device_api.cc:97: Check failed: e == cudaSuccess || e == cudaErrorCudartUnloading: CUDA: out of memory
Stack trace:
  [bt] (0) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x4f) [0x2b53410543bf]
  [bt] (1) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dgl::runtime::CUDADeviceAPI::AllocDataSpace(DLContext, unsigned long, unsigned long, DLDataType)+0x108) [0x2b53419cbeb8]
  [bt] (2) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dgl::runtime::NDArray::Empty(std::vector<long, std::allocator<long> >, DLDataType, DLContext)+0x351) [0x2b5341856651]
  [bt] (3) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dgl::runtime::NDArray dgl::aten::impl::CSRGetRowNNZ<(DLDeviceType)2, long>(dgl::aten::CSRMatrix, dgl::runtime::NDArray)+0x8a) [0x2b5341e1742a]
  [bt] (4) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dgl::aten::CSRGetRowNNZ(dgl::aten::CSRMatrix, dgl::runtime::NDArray)+0x460) [0x2b5341042230]
  [bt] (5) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dgl::UnitGraph::CSR::OutDegrees(unsigned long, dgl::runtime::NDArray) const+0xb4) [0x2b53419a87d4]
  [bt] (6) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dgl::UnitGraph::InDegrees(unsigned long, dgl::runtime::NDArray) const+0xc7) [0x2b53419a3107]
  [bt] (7) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(dgl::HeteroGraph::InDegrees(unsigned long, dgl::runtime::NDArray) const+0x46) [0x2b53418a5726]
  [bt] (8) /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/libdgl.so(+0xe0ccc9) [0x2b53418adcc9]



real	0m52.806s
user	0m29.172s
sys	0m21.122s
