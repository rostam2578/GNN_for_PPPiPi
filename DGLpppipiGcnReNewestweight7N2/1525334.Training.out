0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 14:24:14 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   35C    P0    45W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b463f18bfa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	1m12.104s
user	0m3.010s
sys	0m1.206s
[14:25:26] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.3034],
        [ 1.4263],
        [ 0.0726],
        ...,
        [-0.4864],
        [ 0.5464],
        [-0.1232]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-121.0332, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.1523,  0.0674,  0.1407, -0.0439,  0.1014,  0.0782,  0.0626, -0.0787,
         -0.0918, -0.0959, -0.0823, -0.0114,  0.0780,  0.0079, -0.0791, -0.0533,
         -0.1221,  0.0753,  0.0589,  0.0291, -0.1255, -0.1295, -0.0439,  0.0146,
         -0.1067, -0.0987,  0.0216, -0.1430,  0.0844,  0.0423,  0.0359, -0.1210,
         -0.0539, -0.0577, -0.0814,  0.0469, -0.0768,  0.1235, -0.1074, -0.0337,
         -0.0135, -0.0148, -0.1026,  0.0729,  0.1444,  0.1225,  0.0800, -0.0338,
          0.0583,  0.0035,  0.0282,  0.0081,  0.0638,  0.1300,  0.1138,  0.0497,
          0.0701, -0.1146,  0.0021, -0.1381,  0.1268, -0.1485, -0.1321, -0.1495,
         -0.1054,  0.1315,  0.0993, -0.1329, -0.0372,  0.0310,  0.1254,  0.0439,
         -0.1144, -0.1098, -0.1293,  0.0545, -0.0756, -0.1251,  0.0934, -0.1043,
         -0.0472, -0.0372, -0.0593, -0.0266, -0.0241, -0.0937, -0.1203,  0.0436,
          0.1168, -0.1175,  0.1328, -0.0118,  0.1098, -0.0167,  0.0686, -0.1169,
          0.0711,  0.0179, -0.0300,  0.1158, -0.0095, -0.1409, -0.1419,  0.0378,
         -0.0147, -0.1098,  0.0283,  0.0081,  0.1055, -0.0158, -0.0309, -0.0630,
          0.0853, -0.0685,  0.0666,  0.1356,  0.0204,  0.0570, -0.0459,  0.1017,
          0.0560,  0.1394, -0.0625, -0.1043, -0.1232,  0.0755,  0.0421,  0.1206,
          0.1395,  0.1283,  0.0084, -0.1440,  0.0019, -0.1435, -0.0830, -0.0395,
         -0.0792,  0.1372,  0.1198,  0.1272,  0.0438,  0.0585,  0.0224,  0.0125,
          0.0215,  0.0635, -0.0846, -0.0996, -0.0671,  0.1388, -0.1285, -0.0738,
          0.1258, -0.1465,  0.1217, -0.1504, -0.1465,  0.1474, -0.0945,  0.1172,
          0.0038, -0.1050,  0.0949, -0.0556, -0.1153,  0.0179, -0.0560,  0.1491,
         -0.0729,  0.1483,  0.0078,  0.0203, -0.1359, -0.0636, -0.0198, -0.0844,
          0.0025,  0.1130, -0.0781, -0.0824, -0.0625,  0.0044, -0.1520, -0.1498,
         -0.0368, -0.1033, -0.0377, -0.1119, -0.1421,  0.1197,  0.1168,  0.1109,
          0.0511, -0.0876,  0.1469,  0.0911, -0.1323, -0.1155, -0.0148,  0.0621,
          0.1058,  0.0172, -0.1339, -0.0695,  0.0281,  0.0014,  0.0440,  0.1274,
          0.0531, -0.0464, -0.0492,  0.0863,  0.1478, -0.1152,  0.0413,  0.1207,
         -0.0981,  0.0037,  0.0849, -0.1306, -0.0086, -0.0149,  0.0464, -0.0027,
         -0.1290, -0.0367, -0.0555,  0.0820,  0.0976, -0.1120,  0.0810,  0.1215,
         -0.1026,  0.0923, -0.1042, -0.1193,  0.0198,  0.0210,  0.1086, -0.0509,
         -0.1348,  0.0471, -0.0042, -0.0064, -0.0418,  0.1180,  0.0924, -0.0705,
          0.1469,  0.1305,  0.1178,  0.0727, -0.1226, -0.0967, -0.0771, -0.0715]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1523,  0.0674,  0.1407, -0.0439,  0.1014,  0.0782,  0.0626, -0.0787,
         -0.0918, -0.0959, -0.0823, -0.0114,  0.0780,  0.0079, -0.0791, -0.0533,
         -0.1221,  0.0753,  0.0589,  0.0291, -0.1255, -0.1295, -0.0439,  0.0146,
         -0.1067, -0.0987,  0.0216, -0.1430,  0.0844,  0.0423,  0.0359, -0.1210,
         -0.0539, -0.0577, -0.0814,  0.0469, -0.0768,  0.1235, -0.1074, -0.0337,
         -0.0135, -0.0148, -0.1026,  0.0729,  0.1444,  0.1225,  0.0800, -0.0338,
          0.0583,  0.0035,  0.0282,  0.0081,  0.0638,  0.1300,  0.1138,  0.0497,
          0.0701, -0.1146,  0.0021, -0.1381,  0.1268, -0.1485, -0.1321, -0.1495,
         -0.1054,  0.1315,  0.0993, -0.1329, -0.0372,  0.0310,  0.1254,  0.0439,
         -0.1144, -0.1098, -0.1293,  0.0545, -0.0756, -0.1251,  0.0934, -0.1043,
         -0.0472, -0.0372, -0.0593, -0.0266, -0.0241, -0.0937, -0.1203,  0.0436,
          0.1168, -0.1175,  0.1328, -0.0118,  0.1098, -0.0167,  0.0686, -0.1169,
          0.0711,  0.0179, -0.0300,  0.1158, -0.0095, -0.1409, -0.1419,  0.0378,
         -0.0147, -0.1098,  0.0283,  0.0081,  0.1055, -0.0158, -0.0309, -0.0630,
          0.0853, -0.0685,  0.0666,  0.1356,  0.0204,  0.0570, -0.0459,  0.1017,
          0.0560,  0.1394, -0.0625, -0.1043, -0.1232,  0.0755,  0.0421,  0.1206,
          0.1395,  0.1283,  0.0084, -0.1440,  0.0019, -0.1435, -0.0830, -0.0395,
         -0.0792,  0.1372,  0.1198,  0.1272,  0.0438,  0.0585,  0.0224,  0.0125,
          0.0215,  0.0635, -0.0846, -0.0996, -0.0671,  0.1388, -0.1285, -0.0738,
          0.1258, -0.1465,  0.1217, -0.1504, -0.1465,  0.1474, -0.0945,  0.1172,
          0.0038, -0.1050,  0.0949, -0.0556, -0.1153,  0.0179, -0.0560,  0.1491,
         -0.0729,  0.1483,  0.0078,  0.0203, -0.1359, -0.0636, -0.0198, -0.0844,
          0.0025,  0.1130, -0.0781, -0.0824, -0.0625,  0.0044, -0.1520, -0.1498,
         -0.0368, -0.1033, -0.0377, -0.1119, -0.1421,  0.1197,  0.1168,  0.1109,
          0.0511, -0.0876,  0.1469,  0.0911, -0.1323, -0.1155, -0.0148,  0.0621,
          0.1058,  0.0172, -0.1339, -0.0695,  0.0281,  0.0014,  0.0440,  0.1274,
          0.0531, -0.0464, -0.0492,  0.0863,  0.1478, -0.1152,  0.0413,  0.1207,
         -0.0981,  0.0037,  0.0849, -0.1306, -0.0086, -0.0149,  0.0464, -0.0027,
         -0.1290, -0.0367, -0.0555,  0.0820,  0.0976, -0.1120,  0.0810,  0.1215,
         -0.1026,  0.0923, -0.1042, -0.1193,  0.0198,  0.0210,  0.1086, -0.0509,
         -0.1348,  0.0471, -0.0042, -0.0064, -0.0418,  0.1180,  0.0924, -0.0705,
          0.1469,  0.1305,  0.1178,  0.0727, -0.1226, -0.0967, -0.0771, -0.0715]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0254, -0.0949, -0.0448,  ..., -0.0770,  0.0856, -0.1019],
        [-0.1007, -0.0493, -0.0826,  ...,  0.0732, -0.1096, -0.0488],
        [-0.0505,  0.0098,  0.0947,  ..., -0.0542, -0.0466, -0.0291],
        ...,
        [ 0.0424, -0.0637, -0.1238,  ...,  0.0525,  0.0010, -0.0209],
        [-0.0822,  0.0142, -0.0244,  ..., -0.0107,  0.0066, -0.0740],
        [ 0.0361, -0.0247, -0.0303,  ...,  0.0518,  0.0763,  0.0152]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0254, -0.0949, -0.0448,  ..., -0.0770,  0.0856, -0.1019],
        [-0.1007, -0.0493, -0.0826,  ...,  0.0732, -0.1096, -0.0488],
        [-0.0505,  0.0098,  0.0947,  ..., -0.0542, -0.0466, -0.0291],
        ...,
        [ 0.0424, -0.0637, -0.1238,  ...,  0.0525,  0.0010, -0.0209],
        [-0.0822,  0.0142, -0.0244,  ..., -0.0107,  0.0066, -0.0740],
        [ 0.0361, -0.0247, -0.0303,  ...,  0.0518,  0.0763,  0.0152]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0824, -0.0525,  0.1308,  ...,  0.0702, -0.0407, -0.1121],
        [-0.0453,  0.1440,  0.0887,  ...,  0.1638, -0.1597, -0.1650],
        [ 0.0837,  0.0867,  0.0542,  ...,  0.1414, -0.1017, -0.1329],
        ...,
        [ 0.0355, -0.0105,  0.1252,  ..., -0.0016, -0.1744, -0.1204],
        [ 0.0444,  0.1254, -0.0186,  ...,  0.0112, -0.1323,  0.0192],
        [-0.0186,  0.0694, -0.0679,  ..., -0.0510, -0.1000,  0.1560]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0824, -0.0525,  0.1308,  ...,  0.0702, -0.0407, -0.1121],
        [-0.0453,  0.1440,  0.0887,  ...,  0.1638, -0.1597, -0.1650],
        [ 0.0837,  0.0867,  0.0542,  ...,  0.1414, -0.1017, -0.1329],
        ...,
        [ 0.0355, -0.0105,  0.1252,  ..., -0.0016, -0.1744, -0.1204],
        [ 0.0444,  0.1254, -0.0186,  ...,  0.0112, -0.1323,  0.0192],
        [-0.0186,  0.0694, -0.0679,  ..., -0.0510, -0.1000,  0.1560]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0932, -0.1110,  0.2164,  ..., -0.1553, -0.1907,  0.1802],
        [-0.1204,  0.1588, -0.2081,  ...,  0.0215,  0.1576,  0.0480],
        [ 0.1562,  0.1966, -0.1644,  ..., -0.1151,  0.0244,  0.1411],
        ...,
        [-0.2214, -0.2027,  0.1780,  ..., -0.2136,  0.1972,  0.0106],
        [ 0.2270, -0.2355, -0.0363,  ..., -0.0745,  0.0938, -0.2159],
        [-0.1788, -0.2193, -0.1006,  ..., -0.1923, -0.1561,  0.1961]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0932, -0.1110,  0.2164,  ..., -0.1553, -0.1907,  0.1802],
        [-0.1204,  0.1588, -0.2081,  ...,  0.0215,  0.1576,  0.0480],
        [ 0.1562,  0.1966, -0.1644,  ..., -0.1151,  0.0244,  0.1411],
        ...,
        [-0.2214, -0.2027,  0.1780,  ..., -0.2136,  0.1972,  0.0106],
        [ 0.2270, -0.2355, -0.0363,  ..., -0.0745,  0.0938, -0.2159],
        [-0.1788, -0.2193, -0.1006,  ..., -0.1923, -0.1561,  0.1961]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.0402],
        [ 0.2711],
        [ 0.1283],
        [-0.2546],
        [ 0.1626],
        [ 0.2602],
        [-0.0451],
        [-0.2342],
        [ 0.3570],
        [-0.2783],
        [-0.3403],
        [-0.2022],
        [ 0.1832],
        [ 0.2709],
        [-0.2622],
        [-0.2405],
        [-0.3574],
        [ 0.3637],
        [-0.1028],
        [-0.1899],
        [-0.0034],
        [ 0.0256],
        [ 0.3711],
        [-0.3309],
        [ 0.1876],
        [-0.3042],
        [-0.1822],
        [ 0.0283],
        [ 0.1454],
        [-0.2249],
        [ 0.0302],
        [ 0.2431]], device='cuda:0') 
 Parameter containing:
tensor([[-0.0402],
        [ 0.2711],
        [ 0.1283],
        [-0.2546],
        [ 0.1626],
        [ 0.2602],
        [-0.0451],
        [-0.2342],
        [ 0.3570],
        [-0.2783],
        [-0.3403],
        [-0.2022],
        [ 0.1832],
        [ 0.2709],
        [-0.2622],
        [-0.2405],
        [-0.3574],
        [ 0.3637],
        [-0.1028],
        [-0.1899],
        [-0.0034],
        [ 0.0256],
        [ 0.3711],
        [-0.3309],
        [ 0.1876],
        [-0.3042],
        [-0.1822],
        [ 0.0283],
        [ 0.1454],
        [-0.2249],
        [ 0.0302],
        [ 0.2431]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0648, -0.1047, -0.1073,  0.0058,  0.1184,  0.1193, -0.1485, -0.0700,
          0.1294,  0.0709,  0.0649, -0.0934, -0.0778,  0.1485,  0.1433, -0.1298,
          0.1035,  0.0115,  0.1485, -0.0704, -0.0116, -0.0440, -0.0089, -0.0586,
         -0.0752, -0.0123, -0.1149, -0.1337,  0.0766, -0.1249, -0.0918,  0.0843,
          0.0662, -0.1022, -0.1490,  0.0316, -0.0259,  0.0650,  0.1431,  0.0181,
         -0.0895,  0.0958,  0.1221, -0.1414, -0.0023, -0.1028,  0.0476, -0.1181,
         -0.0784, -0.0928, -0.0390, -0.0049,  0.0553, -0.0026,  0.1170, -0.0931,
         -0.0422,  0.0402, -0.0636,  0.0576,  0.1218, -0.0217,  0.0559, -0.0904,
         -0.0109, -0.0612, -0.0138,  0.1312,  0.1043,  0.1382, -0.1277, -0.0059,
          0.0607, -0.0632, -0.0547, -0.1507,  0.0028, -0.0672,  0.1226, -0.0030,
          0.0779, -0.0798, -0.1206, -0.1288, -0.0846, -0.0780, -0.1144,  0.0074,
         -0.0212,  0.0045,  0.0949,  0.1372, -0.0937,  0.0708,  0.0007,  0.0171,
         -0.1489, -0.1441,  0.0346, -0.0929,  0.1299,  0.1401, -0.0730, -0.1488,
         -0.1291, -0.0547, -0.0351,  0.0255,  0.1458, -0.1341, -0.1376, -0.0258,
          0.1024, -0.0221,  0.1393,  0.0819,  0.0503,  0.1305, -0.0606,  0.0986,
         -0.0425,  0.1246, -0.1400, -0.0731,  0.0756,  0.0059, -0.1186, -0.1264,
          0.1107,  0.0413, -0.0648, -0.1385, -0.0509, -0.0177, -0.0917,  0.1333,
          0.0375, -0.0484,  0.0273,  0.0714, -0.1104, -0.1203, -0.1013,  0.1332,
          0.0885,  0.1363,  0.0742, -0.0911, -0.1354,  0.1140,  0.0869, -0.0746,
         -0.0903, -0.0084, -0.0947, -0.0317,  0.0612,  0.0504,  0.0064, -0.0150,
         -0.0320, -0.0703,  0.0287, -0.1341, -0.0837, -0.0696,  0.1301,  0.0701,
         -0.0760,  0.0281,  0.0232,  0.1054,  0.1378, -0.0929, -0.0401, -0.0661,
         -0.1467,  0.0599,  0.0984,  0.0218, -0.0964, -0.1201,  0.0814, -0.1306,
          0.0839,  0.0445,  0.0046,  0.0783, -0.0417, -0.0794, -0.0475,  0.0280,
         -0.0934,  0.0322,  0.1146, -0.1064, -0.0894, -0.1294,  0.0848,  0.0788,
          0.0833,  0.0251,  0.0954, -0.1471, -0.0896, -0.1140,  0.1427, -0.0701,
         -0.1396,  0.1052,  0.1205, -0.1107, -0.1142, -0.0351,  0.0006,  0.0464,
         -0.0666, -0.0975,  0.0112, -0.0241,  0.1466, -0.0477,  0.0218,  0.1526,
         -0.1182,  0.0196,  0.0184,  0.1422,  0.1089, -0.1370,  0.1013,  0.0280,
         -0.0557, -0.0898,  0.0135,  0.0794,  0.0444,  0.1212, -0.1398, -0.0260,
         -0.0246, -0.1267,  0.0911, -0.0646,  0.1104, -0.0740,  0.1279,  0.0507,
         -0.1059, -0.0149, -0.0632, -0.0400,  0.1149, -0.0850, -0.1440, -0.1499]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0648, -0.1047, -0.1073,  0.0058,  0.1184,  0.1193, -0.1485, -0.0700,
          0.1294,  0.0709,  0.0649, -0.0934, -0.0778,  0.1485,  0.1433, -0.1298,
          0.1035,  0.0115,  0.1485, -0.0704, -0.0116, -0.0440, -0.0089, -0.0586,
         -0.0752, -0.0123, -0.1149, -0.1337,  0.0766, -0.1249, -0.0918,  0.0843,
          0.0662, -0.1022, -0.1490,  0.0316, -0.0259,  0.0650,  0.1431,  0.0181,
         -0.0895,  0.0958,  0.1221, -0.1414, -0.0023, -0.1028,  0.0476, -0.1181,
         -0.0784, -0.0928, -0.0390, -0.0049,  0.0553, -0.0026,  0.1170, -0.0931,
         -0.0422,  0.0402, -0.0636,  0.0576,  0.1218, -0.0217,  0.0559, -0.0904,
         -0.0109, -0.0612, -0.0138,  0.1312,  0.1043,  0.1382, -0.1277, -0.0059,
          0.0607, -0.0632, -0.0547, -0.1507,  0.0028, -0.0672,  0.1226, -0.0030,
          0.0779, -0.0798, -0.1206, -0.1288, -0.0846, -0.0780, -0.1144,  0.0074,
         -0.0212,  0.0045,  0.0949,  0.1372, -0.0937,  0.0708,  0.0007,  0.0171,
         -0.1489, -0.1441,  0.0346, -0.0929,  0.1299,  0.1401, -0.0730, -0.1488,
         -0.1291, -0.0547, -0.0351,  0.0255,  0.1458, -0.1341, -0.1376, -0.0258,
          0.1024, -0.0221,  0.1393,  0.0819,  0.0503,  0.1305, -0.0606,  0.0986,
         -0.0425,  0.1246, -0.1400, -0.0731,  0.0756,  0.0059, -0.1186, -0.1264,
          0.1107,  0.0413, -0.0648, -0.1385, -0.0509, -0.0177, -0.0917,  0.1333,
          0.0375, -0.0484,  0.0273,  0.0714, -0.1104, -0.1203, -0.1013,  0.1332,
          0.0885,  0.1363,  0.0742, -0.0911, -0.1354,  0.1140,  0.0869, -0.0746,
         -0.0903, -0.0084, -0.0947, -0.0317,  0.0612,  0.0504,  0.0064, -0.0150,
         -0.0320, -0.0703,  0.0287, -0.1341, -0.0837, -0.0696,  0.1301,  0.0701,
         -0.0760,  0.0281,  0.0232,  0.1054,  0.1378, -0.0929, -0.0401, -0.0661,
         -0.1467,  0.0599,  0.0984,  0.0218, -0.0964, -0.1201,  0.0814, -0.1306,
          0.0839,  0.0445,  0.0046,  0.0783, -0.0417, -0.0794, -0.0475,  0.0280,
         -0.0934,  0.0322,  0.1146, -0.1064, -0.0894, -0.1294,  0.0848,  0.0788,
          0.0833,  0.0251,  0.0954, -0.1471, -0.0896, -0.1140,  0.1427, -0.0701,
         -0.1396,  0.1052,  0.1205, -0.1107, -0.1142, -0.0351,  0.0006,  0.0464,
         -0.0666, -0.0975,  0.0112, -0.0241,  0.1466, -0.0477,  0.0218,  0.1526,
         -0.1182,  0.0196,  0.0184,  0.1422,  0.1089, -0.1370,  0.1013,  0.0280,
         -0.0557, -0.0898,  0.0135,  0.0794,  0.0444,  0.1212, -0.1398, -0.0260,
         -0.0246, -0.1267,  0.0911, -0.0646,  0.1104, -0.0740,  0.1279,  0.0507,
         -0.1059, -0.0149, -0.0632, -0.0400,  0.1149, -0.0850, -0.1440, -0.1499]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0727,  0.0020,  0.0746,  ...,  0.0777, -0.0527, -0.0013],
        [ 0.1191,  0.0210,  0.0318,  ...,  0.0079, -0.0208,  0.0756],
        [ 0.1036, -0.0781,  0.0120,  ..., -0.0549,  0.0522,  0.0035],
        ...,
        [ 0.1171, -0.1213, -0.0758,  ...,  0.0864,  0.0129, -0.0826],
        [-0.0038,  0.0543, -0.0692,  ..., -0.0983, -0.0639, -0.0661],
        [ 0.1161,  0.0285,  0.0628,  ..., -0.1037,  0.0393, -0.0169]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0727,  0.0020,  0.0746,  ...,  0.0777, -0.0527, -0.0013],
        [ 0.1191,  0.0210,  0.0318,  ...,  0.0079, -0.0208,  0.0756],
        [ 0.1036, -0.0781,  0.0120,  ..., -0.0549,  0.0522,  0.0035],
        ...,
        [ 0.1171, -0.1213, -0.0758,  ...,  0.0864,  0.0129, -0.0826],
        [-0.0038,  0.0543, -0.0692,  ..., -0.0983, -0.0639, -0.0661],
        [ 0.1161,  0.0285,  0.0628,  ..., -0.1037,  0.0393, -0.0169]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.1440, -0.1305,  0.1634,  ..., -0.0067, -0.1736, -0.0996],
        [ 0.0115, -0.1034, -0.0637,  ..., -0.0913,  0.1604, -0.0963],
        [-0.0023, -0.0636, -0.0248,  ..., -0.0356,  0.0880,  0.0972],
        ...,
        [ 0.1639, -0.1360,  0.0617,  ...,  0.1582, -0.1733, -0.0608],
        [-0.1736,  0.0876, -0.0211,  ..., -0.1726, -0.1042, -0.0905],
        [-0.1715,  0.0916, -0.1174,  ...,  0.0582,  0.0695,  0.0034]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1440, -0.1305,  0.1634,  ..., -0.0067, -0.1736, -0.0996],
        [ 0.0115, -0.1034, -0.0637,  ..., -0.0913,  0.1604, -0.0963],
        [-0.0023, -0.0636, -0.0248,  ..., -0.0356,  0.0880,  0.0972],
        ...,
        [ 0.1639, -0.1360,  0.0617,  ...,  0.1582, -0.1733, -0.0608],
        [-0.1736,  0.0876, -0.0211,  ..., -0.1726, -0.1042, -0.0905],
        [-0.1715,  0.0916, -0.1174,  ...,  0.0582,  0.0695,  0.0034]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0049,  0.2202, -0.0738,  ...,  0.1219, -0.2136,  0.2142],
        [ 0.2204, -0.1522, -0.1771,  ..., -0.0562,  0.0764,  0.0542],
        [-0.2488, -0.2253, -0.1389,  ...,  0.1232, -0.2481, -0.1160],
        ...,
        [ 0.1257, -0.0775,  0.0660,  ..., -0.0573,  0.1940, -0.0072],
        [-0.0910,  0.2309,  0.0461,  ...,  0.1090,  0.0886, -0.0702],
        [-0.2021, -0.0886,  0.0569,  ..., -0.1103, -0.0837,  0.0364]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0049,  0.2202, -0.0738,  ...,  0.1219, -0.2136,  0.2142],
        [ 0.2204, -0.1522, -0.1771,  ..., -0.0562,  0.0764,  0.0542],
        [-0.2488, -0.2253, -0.1389,  ...,  0.1232, -0.2481, -0.1160],
        ...,
        [ 0.1257, -0.0775,  0.0660,  ..., -0.0573,  0.1940, -0.0072],
        [-0.0910,  0.2309,  0.0461,  ...,  0.1090,  0.0886, -0.0702],
        [-0.2021, -0.0886,  0.0569,  ..., -0.1103, -0.0837,  0.0364]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.1569],
        [ 0.3089],
        [ 0.1435],
        [ 0.2594],
        [-0.3429],
        [ 0.1321],
        [-0.0826],
        [-0.1240],
        [ 0.1254],
        [ 0.1233],
        [ 0.3397],
        [-0.3163],
        [-0.1165],
        [-0.2160],
        [-0.0573],
        [ 0.0671],
        [ 0.1425],
        [ 0.3680],
        [ 0.3789],
        [ 0.1196],
        [ 0.3503],
        [-0.0740],
        [-0.3098],
        [ 0.1027],
        [ 0.3696],
        [-0.1464],
        [ 0.3894],
        [-0.2656],
        [-0.2840],
        [ 0.3471],
        [ 0.1971],
        [-0.2676]], device='cuda:0') 
 Parameter containing:
tensor([[-0.1569],
        [ 0.3089],
        [ 0.1435],
        [ 0.2594],
        [-0.3429],
        [ 0.1321],
        [-0.0826],
        [-0.1240],
        [ 0.1254],
        [ 0.1233],
        [ 0.3397],
        [-0.3163],
        [-0.1165],
        [-0.2160],
        [-0.0573],
        [ 0.0671],
        [ 0.1425],
        [ 0.3680],
        [ 0.3789],
        [ 0.1196],
        [ 0.3503],
        [-0.0740],
        [-0.3098],
        [ 0.1027],
        [ 0.3696],
        [-0.1464],
        [ 0.3894],
        [-0.2656],
        [-0.2840],
        [ 0.3471],
        [ 0.1971],
        [-0.2676]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(26.8953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.9305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(1.9813, device='cuda:0')



h[100].sum tensor(-3.3168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.4041, device='cuda:0')



h[200].sum tensor(0.2937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.3015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3086.5056, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(15409., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-37.2403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-13.8607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2.6341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0734],
        [0.0899],
        [0.1296],
        ...,
        [0.0207],
        [0.0208],
        [0.0164]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(1852.1108, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0734],
        [0.0899],
        [0.1296],
        ...,
        [0.0207],
        [0.0208],
        [0.0164]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-111.5310, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.0649, device='cuda:0')



h[100].sum tensor(-2.6134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.5923, device='cuda:0')



h[200].sum tensor(12.0246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9271, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(14629.7871, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0104, 0.0108, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0022, 0.0023, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(68241.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(842.3357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(59.2622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3383.6421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(238.0549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.2527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0969],
        [-0.0594],
        [-0.0364],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-5690.9668, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0734],
        [0.0899],
        [0.1296],
        ...,
        [0.0207],
        [0.0208],
        [0.0164]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 3998 
BatchSize 5 
EpochNum 1 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0821, -0.0476,  0.0502,  0.0247, -0.0700,  0.1176, -0.0805, -0.0507,
          0.1065, -0.0815,  0.0657, -0.0338, -0.1485, -0.0171, -0.0873, -0.1323,
          0.1492, -0.1168, -0.0561, -0.1359,  0.1047,  0.0151,  0.0084, -0.1431,
          0.0869,  0.1068,  0.0422, -0.1335,  0.1462, -0.0791, -0.1101, -0.0545,
          0.0343,  0.0730,  0.0167, -0.0977, -0.0463, -0.0439,  0.0892,  0.0454,
         -0.0389, -0.0085,  0.1522, -0.0051, -0.1388, -0.1038,  0.1312, -0.0272,
          0.1137,  0.0273,  0.0210,  0.0903, -0.0408, -0.0721,  0.0843,  0.0820,
         -0.1459, -0.0829, -0.0666,  0.0650,  0.1190, -0.0248, -0.1060, -0.0315,
         -0.0027, -0.0306,  0.1282,  0.0709,  0.1258, -0.0119,  0.1347,  0.1207,
         -0.1009, -0.1150,  0.1216, -0.1033,  0.0250,  0.0870,  0.0698, -0.0821,
         -0.0005, -0.0717, -0.1288,  0.1342, -0.1179, -0.0083, -0.0015,  0.0854,
         -0.1121, -0.0020,  0.0781,  0.1299,  0.0817, -0.1273,  0.0858, -0.1412,
         -0.0168, -0.0033, -0.0911,  0.0040, -0.0929,  0.0778,  0.0882,  0.0866,
          0.1152,  0.1035, -0.1017,  0.1099,  0.0933,  0.1374, -0.1456,  0.1328,
          0.0390,  0.1430,  0.0090,  0.1218,  0.0324,  0.1051,  0.1204, -0.0568,
          0.1215,  0.0376,  0.0454,  0.0700, -0.0571,  0.0612,  0.0287,  0.1406,
          0.0888,  0.0330,  0.0166,  0.0778,  0.0663, -0.1330, -0.0215,  0.0203,
          0.1013, -0.0826,  0.1049, -0.1152,  0.0796, -0.0470, -0.0310,  0.1186,
          0.0471,  0.0872,  0.0787,  0.1141, -0.0059,  0.1383,  0.0354, -0.1095,
          0.0548, -0.1425, -0.1416,  0.0546,  0.0002,  0.0443,  0.0747, -0.0309,
         -0.0694, -0.0165, -0.0073, -0.0545,  0.0628,  0.0395,  0.0433,  0.0852,
         -0.0633,  0.0522, -0.1318, -0.1333, -0.0154, -0.0631, -0.0128, -0.0171,
          0.0235, -0.0060,  0.0500, -0.0653, -0.0256,  0.0308,  0.0775,  0.0257,
         -0.1183,  0.1278, -0.1190,  0.0488, -0.0188,  0.1003, -0.0802, -0.0559,
          0.0287,  0.0963,  0.0794,  0.0861,  0.0365, -0.0260, -0.0661, -0.1382,
          0.0759,  0.0471,  0.1218,  0.1179, -0.0116, -0.1452, -0.0279,  0.0378,
         -0.0008,  0.1029, -0.0039, -0.0914, -0.0493,  0.0061, -0.1419,  0.0075,
          0.1024,  0.0315, -0.0922,  0.0892, -0.0725,  0.0087,  0.1042,  0.0240,
         -0.0979,  0.1421, -0.0585,  0.1131,  0.1105, -0.0612, -0.1335,  0.0195,
         -0.0144, -0.0994, -0.0354,  0.1495,  0.0691,  0.0933, -0.0554,  0.0374,
          0.0959,  0.0454,  0.0488,  0.1095,  0.0166,  0.0972,  0.0031, -0.1218,
          0.1284,  0.0729,  0.0361,  0.1072,  0.0654,  0.0409, -0.0533,  0.1519]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0995,  0.0264, -0.0397,  ...,  0.1128,  0.0216,  0.0091],
        [-0.1091,  0.0525,  0.1042,  ...,  0.0996, -0.0415, -0.1101],
        [-0.0843,  0.1213, -0.0894,  ..., -0.0484,  0.1249,  0.0984],
        ...,
        [-0.0614, -0.1182, -0.0058,  ..., -0.0918,  0.0747,  0.0738],
        [-0.1083, -0.0594,  0.0907,  ...,  0.0801, -0.1148,  0.0092],
        [-0.0092, -0.0973,  0.0536,  ...,  0.0389, -0.0361, -0.0843]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0885,  0.1054,  0.1002,  ...,  0.0350,  0.0279,  0.0325],
        [-0.0862,  0.0970,  0.1319,  ..., -0.0665,  0.0307,  0.0857],
        [-0.0459, -0.0074, -0.0593,  ...,  0.0870,  0.1160,  0.0991],
        ...,
        [ 0.0566,  0.1038, -0.0842,  ..., -0.1548, -0.0780, -0.1284],
        [ 0.0791,  0.0692, -0.1460,  ..., -0.1193,  0.0425,  0.0293],
        [ 0.1007, -0.0101,  0.1179,  ...,  0.1504, -0.1595, -0.1220]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2201,  0.1616, -0.2142,  ..., -0.1628,  0.2377,  0.2476],
        [-0.2314,  0.0482,  0.1046,  ...,  0.1257,  0.0903,  0.1359],
        [-0.1419, -0.1377, -0.1530,  ..., -0.2486, -0.2162,  0.0368],
        ...,
        [-0.0115,  0.2231, -0.1782,  ..., -0.2388, -0.0526,  0.0772],
        [ 0.1225, -0.1718, -0.0393,  ..., -0.1189, -0.0389, -0.2437],
        [ 0.1280, -0.1158,  0.0749,  ...,  0.2173,  0.0878, -0.1790]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0883],
        [ 0.2505],
        [ 0.2812],
        [ 0.1243],
        [-0.0209],
        [ 0.2281],
        [ 0.0281],
        [-0.4118],
        [ 0.1889],
        [-0.2162],
        [ 0.2570],
        [ 0.1811],
        [-0.0291],
        [ 0.4011],
        [ 0.3500],
        [-0.0542],
        [-0.1770],
        [-0.1337],
        [ 0.2150],
        [ 0.0335],
        [ 0.1644],
        [-0.2947],
        [-0.0047],
        [-0.3372],
        [-0.3616],
        [-0.1652],
        [-0.3014],
        [ 0.2264],
        [ 0.0343],
        [ 0.1988],
        [-0.4221],
        [ 0.0159]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0821, -0.0476,  0.0502,  0.0247, -0.0700,  0.1176, -0.0805, -0.0507,
          0.1065, -0.0815,  0.0657, -0.0338, -0.1485, -0.0171, -0.0873, -0.1323,
          0.1492, -0.1168, -0.0561, -0.1359,  0.1047,  0.0151,  0.0084, -0.1431,
          0.0869,  0.1068,  0.0422, -0.1335,  0.1462, -0.0791, -0.1101, -0.0545,
          0.0343,  0.0730,  0.0167, -0.0977, -0.0463, -0.0439,  0.0892,  0.0454,
         -0.0389, -0.0085,  0.1522, -0.0051, -0.1388, -0.1038,  0.1312, -0.0272,
          0.1137,  0.0273,  0.0210,  0.0903, -0.0408, -0.0721,  0.0843,  0.0820,
         -0.1459, -0.0829, -0.0666,  0.0650,  0.1190, -0.0248, -0.1060, -0.0315,
         -0.0027, -0.0306,  0.1282,  0.0709,  0.1258, -0.0119,  0.1347,  0.1207,
         -0.1009, -0.1150,  0.1216, -0.1033,  0.0250,  0.0870,  0.0698, -0.0821,
         -0.0005, -0.0717, -0.1288,  0.1342, -0.1179, -0.0083, -0.0015,  0.0854,
         -0.1121, -0.0020,  0.0781,  0.1299,  0.0817, -0.1273,  0.0858, -0.1412,
         -0.0168, -0.0033, -0.0911,  0.0040, -0.0929,  0.0778,  0.0882,  0.0866,
          0.1152,  0.1035, -0.1017,  0.1099,  0.0933,  0.1374, -0.1456,  0.1328,
          0.0390,  0.1430,  0.0090,  0.1218,  0.0324,  0.1051,  0.1204, -0.0568,
          0.1215,  0.0376,  0.0454,  0.0700, -0.0571,  0.0612,  0.0287,  0.1406,
          0.0888,  0.0330,  0.0166,  0.0778,  0.0663, -0.1330, -0.0215,  0.0203,
          0.1013, -0.0826,  0.1049, -0.1152,  0.0796, -0.0470, -0.0310,  0.1186,
          0.0471,  0.0872,  0.0787,  0.1141, -0.0059,  0.1383,  0.0354, -0.1095,
          0.0548, -0.1425, -0.1416,  0.0546,  0.0002,  0.0443,  0.0747, -0.0309,
         -0.0694, -0.0165, -0.0073, -0.0545,  0.0628,  0.0395,  0.0433,  0.0852,
         -0.0633,  0.0522, -0.1318, -0.1333, -0.0154, -0.0631, -0.0128, -0.0171,
          0.0235, -0.0060,  0.0500, -0.0653, -0.0256,  0.0308,  0.0775,  0.0257,
         -0.1183,  0.1278, -0.1190,  0.0488, -0.0188,  0.1003, -0.0802, -0.0559,
          0.0287,  0.0963,  0.0794,  0.0861,  0.0365, -0.0260, -0.0661, -0.1382,
          0.0759,  0.0471,  0.1218,  0.1179, -0.0116, -0.1452, -0.0279,  0.0378,
         -0.0008,  0.1029, -0.0039, -0.0914, -0.0493,  0.0061, -0.1419,  0.0075,
          0.1024,  0.0315, -0.0922,  0.0892, -0.0725,  0.0087,  0.1042,  0.0240,
         -0.0979,  0.1421, -0.0585,  0.1131,  0.1105, -0.0612, -0.1335,  0.0195,
         -0.0144, -0.0994, -0.0354,  0.1495,  0.0691,  0.0933, -0.0554,  0.0374,
          0.0959,  0.0454,  0.0488,  0.1095,  0.0166,  0.0972,  0.0031, -0.1218,
          0.1284,  0.0729,  0.0361,  0.1072,  0.0654,  0.0409, -0.0533,  0.1519]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0995,  0.0264, -0.0397,  ...,  0.1128,  0.0216,  0.0091],
        [-0.1091,  0.0525,  0.1042,  ...,  0.0996, -0.0415, -0.1101],
        [-0.0843,  0.1213, -0.0894,  ..., -0.0484,  0.1249,  0.0984],
        ...,
        [-0.0614, -0.1182, -0.0058,  ..., -0.0918,  0.0747,  0.0738],
        [-0.1083, -0.0594,  0.0907,  ...,  0.0801, -0.1148,  0.0092],
        [-0.0092, -0.0973,  0.0536,  ...,  0.0389, -0.0361, -0.0843]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0885,  0.1054,  0.1002,  ...,  0.0350,  0.0279,  0.0325],
        [-0.0862,  0.0970,  0.1319,  ..., -0.0665,  0.0307,  0.0857],
        [-0.0459, -0.0074, -0.0593,  ...,  0.0870,  0.1160,  0.0991],
        ...,
        [ 0.0566,  0.1038, -0.0842,  ..., -0.1548, -0.0780, -0.1284],
        [ 0.0791,  0.0692, -0.1460,  ..., -0.1193,  0.0425,  0.0293],
        [ 0.1007, -0.0101,  0.1179,  ...,  0.1504, -0.1595, -0.1220]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2201,  0.1616, -0.2142,  ..., -0.1628,  0.2377,  0.2476],
        [-0.2314,  0.0482,  0.1046,  ...,  0.1257,  0.0903,  0.1359],
        [-0.1419, -0.1377, -0.1530,  ..., -0.2486, -0.2162,  0.0368],
        ...,
        [-0.0115,  0.2231, -0.1782,  ..., -0.2388, -0.0526,  0.0772],
        [ 0.1225, -0.1718, -0.0393,  ..., -0.1189, -0.0389, -0.2437],
        [ 0.1280, -0.1158,  0.0749,  ...,  0.2173,  0.0878, -0.1790]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0883],
        [ 0.2505],
        [ 0.2812],
        [ 0.1243],
        [-0.0209],
        [ 0.2281],
        [ 0.0281],
        [-0.4118],
        [ 0.1889],
        [-0.2162],
        [ 0.2570],
        [ 0.1811],
        [-0.0291],
        [ 0.4011],
        [ 0.3500],
        [-0.0542],
        [-0.1770],
        [-0.1337],
        [ 0.2150],
        [ 0.0335],
        [ 0.1644],
        [-0.2947],
        [-0.0047],
        [-0.3372],
        [-0.3616],
        [-0.1652],
        [-0.3014],
        [ 0.2264],
        [ 0.0343],
        [ 0.1988],
        [-0.4221],
        [ 0.0159]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0120, -0.0070,  0.0074,  ...,  0.0060, -0.0078,  0.0223],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(877.8719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0222, device='cuda:0')



h[100].sum tensor(-27.1756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8206, device='cuda:0')



h[200].sum tensor(22.2015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0455, 0.0000, 0.0278,  ..., 0.0227, 0.0000, 0.0842],
        [0.0374, 0.0000, 0.0229,  ..., 0.0187, 0.0000, 0.0693],
        [0.0088, 0.0000, 0.0054,  ..., 0.0044, 0.0000, 0.0162],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33316.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0964,  ..., 0.0848, 0.0000, 0.2502],
        [0.0000, 0.0000, 0.0826,  ..., 0.0727, 0.0000, 0.2144],
        [0.0000, 0.0000, 0.0663,  ..., 0.0584, 0.0000, 0.1722],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(148241., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.0562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(221.0289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(154.4858, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.2108e-01],
        [-6.7204e-01],
        [-7.4158e-01],
        ...,
        [-7.4614e-07],
        [-9.8032e-08],
        [ 0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-11575.2695, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(73.8589, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365909.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0000e-04,  0.0000e+00, -1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00, -1.0000e-04],
        [-1.0000e-04,  0.0000e+00, -1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00, -1.0000e-04],
        [-1.0000e-04,  0.0000e+00, -1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00, -1.0000e-04],
        ...,
        [-1.0000e-04,  0.0000e+00, -1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00, -1.0000e-04],
        [-1.0000e-04,  0.0000e+00, -1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00, -1.0000e-04],
        [-1.0000e-04,  0.0000e+00, -1.0000e-04,  ...,  1.0000e-04,
          0.0000e+00, -1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(809.3226, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0204, device='cuda:0')



h[100].sum tensor(-25.0395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7549, device='cuda:0')



h[200].sum tensor(17.0534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33884.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0013,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0006, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158873., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(147.5202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(228.1596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(168.3360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0181],
        [0.0213],
        [0.0246],
        ...,
        [0.0066],
        [0.0066],
        [0.0066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(6431.2725, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365909.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365886.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002,  0.0000, -0.0002,  ...,  0.0002,  0.0000, -0.0002],
        [-0.0002,  0.0000, -0.0002,  ...,  0.0002,  0.0000, -0.0002],
        [-0.0002,  0.0000, -0.0002,  ...,  0.0002,  0.0000, -0.0002],
        ...,
        [-0.0002,  0.0000, -0.0002,  ...,  0.0002,  0.0000, -0.0002],
        [-0.0002,  0.0000, -0.0002,  ...,  0.0002,  0.0000, -0.0002],
        [-0.0002,  0.0000, -0.0002,  ...,  0.0002,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(502.1160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.2730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0145, device='cuda:0')



h[100].sum tensor(-17.7817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5365, device='cuda:0')



h[200].sum tensor(7.8817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7086, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0398, 0.0000, 0.0241,  ..., 0.0210, 0.0000, 0.0743],
        [0.0264, 0.0000, 0.0160,  ..., 0.0142, 0.0000, 0.0493],
        [0.0218, 0.0000, 0.0132,  ..., 0.0119, 0.0000, 0.0407],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26955., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 9.2476e-02,  ..., 7.6086e-02, 0.0000e+00,
         2.0213e-01],
        [0.0000e+00, 0.0000e+00, 7.4905e-02,  ..., 6.1021e-02, 0.0000e+00,
         1.6207e-01],
        [0.0000e+00, 0.0000e+00, 5.7867e-02,  ..., 4.6350e-02, 0.0000e+00,
         1.2305e-01],
        ...,
        [1.4465e-03, 0.0000e+00, 3.0281e-03,  ..., 7.4170e-05, 0.0000e+00,
         1.8593e-04],
        [1.4465e-03, 0.0000e+00, 3.0281e-03,  ..., 7.4170e-05, 0.0000e+00,
         1.8593e-04],
        [1.4465e-03, 0.0000e+00, 3.0281e-03,  ..., 7.4170e-05, 0.0000e+00,
         1.8593e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(135024.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(254.9325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(143.9382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(180.4378, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(129.8390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0829],
        [0.0827],
        [0.0809],
        ...,
        [0.0027],
        [0.0027],
        [0.0027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(10041.4492, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365886.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365870.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042, -0.0026,  0.0025,  ...,  0.0025, -0.0029,  0.0080],
        [ 0.0106, -0.0063,  0.0063,  ...,  0.0057, -0.0070,  0.0198],
        [ 0.0077, -0.0046,  0.0046,  ...,  0.0042, -0.0051,  0.0144],
        ...,
        [-0.0002,  0.0000, -0.0002,  ...,  0.0003,  0.0000, -0.0002],
        [-0.0002,  0.0000, -0.0002,  ...,  0.0003,  0.0000, -0.0002],
        [-0.0002,  0.0000, -0.0002,  ...,  0.0003,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(481.2654, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.1473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-19.5030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5898, device='cuda:0')



h[200].sum tensor(7.3018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6731, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0376, 0.0000, 0.0225,  ..., 0.0205, 0.0000, 0.0705],
        [0.0240, 0.0000, 0.0142,  ..., 0.0136, 0.0000, 0.0452],
        [0.0202, 0.0000, 0.0120,  ..., 0.0116, 0.0000, 0.0380],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27982.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0669,  ..., 0.0520, 0.0000, 0.1352],
        [0.0000, 0.0000, 0.0606,  ..., 0.0464, 0.0000, 0.1206],
        [0.0000, 0.0000, 0.0527,  ..., 0.0398, 0.0000, 0.1038],
        ...,
        [0.0022, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0006],
        [0.0022, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0006],
        [0.0022, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(138136.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(424.3963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(158.5023, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(190.2414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(131.9374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1441],
        [ 0.1406],
        [ 0.1320],
        ...,
        [-0.0077],
        [-0.0076],
        [-0.0076]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(5300.2832, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365870.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365859.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        ...,
        [-0.0003,  0.0000, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0003,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0003,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(369.2186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.4984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-18.0755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5491, device='cuda:0')



h[200].sum tensor(4.3861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26092.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0004],
        [0.0014, 0.0000, 0.0044,  ..., 0.0005, 0.0000, 0.0018],
        [0.0009, 0.0000, 0.0074,  ..., 0.0026, 0.0000, 0.0077],
        ...,
        [0.0018, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0003],
        [0.0018, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0003],
        [0.0018, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(130029.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(408.7200, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(151.0549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(182.2531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(124.9652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0257],
        [ 0.0375],
        [ 0.0595],
        ...,
        [-0.0132],
        [-0.0163],
        [-0.0166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-2068.7021, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365859.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365854.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0073, -0.0044,  0.0043,  ...,  0.0042, -0.0049,  0.0137],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        ...,
        [-0.0003,  0.0000, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0004,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0003,  ...,  0.0004,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(592.8992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.3005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0213, device='cuda:0')



h[100].sum tensor(-26.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7869, device='cuda:0')



h[200].sum tensor(9.4215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0277, 0.0000, 0.0166,  ..., 0.0159, 0.0000, 0.0518],
        [0.0073, 0.0000, 0.0043,  ..., 0.0055, 0.0000, 0.0137],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38702.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 6.1194e-02,  ..., 4.8935e-02, 0.0000e+00,
         1.2039e-01],
        [0.0000e+00, 0.0000e+00, 2.9510e-02,  ..., 2.1812e-02, 0.0000e+00,
         5.3839e-02],
        [1.0427e-03, 0.0000e+00, 1.0574e-02,  ..., 5.9881e-03, 0.0000e+00,
         1.4857e-02],
        ...,
        [2.0415e-03, 0.0000e+00, 3.2640e-03,  ..., 1.4410e-06, 0.0000e+00,
         0.0000e+00],
        [2.0414e-03, 0.0000e+00, 3.2639e-03,  ..., 1.4587e-06, 0.0000e+00,
         0.0000e+00],
        [2.0414e-03, 0.0000e+00, 3.2639e-03,  ..., 1.4605e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199410.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(425.7993, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.5250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(266.7691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(186.5666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1210],
        [ 0.0825],
        [ 0.0383],
        ...,
        [-0.0293],
        [-0.0291],
        [-0.0291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-2163.5203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365854.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365852.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0003],
        ...,
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0003],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(465.2889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.9930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-23.3273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6945, device='cuda:0')



h[200].sum tensor(4.8693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34880.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0000, 0.0129,  ..., 0.0080, 0.0000, 0.0199],
        [0.0018, 0.0000, 0.0048,  ..., 0.0015, 0.0000, 0.0039],
        [0.0026, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0026, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0026, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180116.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(605.8762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(180.9118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(243.2529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(170.5236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0418],
        [ 0.0067],
        [-0.0207],
        ...,
        [-0.0387],
        [-0.0385],
        [-0.0384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-16380.1240, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365852.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365853.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0137, -0.0080,  0.0081,  ...,  0.0075, -0.0090,  0.0255],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        ...,
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(281.2836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(5.5592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0149, device='cuda:0')



h[100].sum tensor(-18.2339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5513, device='cuda:0')



h[200].sum tensor(-0.3326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9780, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0245, 0.0000, 0.0145,  ..., 0.0146, 0.0000, 0.0456],
        [0.0137, 0.0000, 0.0081,  ..., 0.0090, 0.0000, 0.0254],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28518.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 8.0630e-02,  ..., 6.7868e-02, 0.0000e+00,
         1.6880e-01],
        [0.0000e+00, 0.0000e+00, 4.1419e-02,  ..., 3.3624e-02, 0.0000e+00,
         8.3761e-02],
        [2.4436e-05, 0.0000e+00, 1.5450e-02,  ..., 1.0640e-02, 0.0000e+00,
         2.6749e-02],
        ...,
        [3.6219e-03, 0.0000e+00, 2.0333e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.6218e-03, 0.0000e+00, 2.0332e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [3.6218e-03, 0.0000e+00, 2.0332e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156030.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(789.6864, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.2838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(198.7355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(139.9898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0495],
        [ 0.0410],
        [ 0.0427],
        ...,
        [-0.0467],
        [-0.0464],
        [-0.0464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-22092.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365853.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [1.0000],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365857.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        ...,
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(164.5345, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.8527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0123, device='cuda:0')



h[100].sum tensor(-14.8564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4550, device='cuda:0')



h[200].sum tensor(-3.2437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0000, 0.0112,  ..., 0.0120, 0.0000, 0.0358],
        [0.0096, 0.0000, 0.0056,  ..., 0.0070, 0.0000, 0.0179],
        [0.0169, 0.0000, 0.0101,  ..., 0.0107, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24607.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0388,  ..., 0.0299, 0.0000, 0.0765],
        [0.0000, 0.0000, 0.0349,  ..., 0.0269, 0.0000, 0.0688],
        [0.0000, 0.0000, 0.0415,  ..., 0.0331, 0.0000, 0.0842],
        ...,
        [0.0048, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0048, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0048, 0.0000, 0.0019,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(143394.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1118.0071, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(142.4543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(167.7365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(119.5361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0188],
        [ 0.0220],
        [ 0.0228],
        ...,
        [-0.0545],
        [-0.0543],
        [-0.0542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-28192.6211, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [1.0000],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365857.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [1.0000],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365857.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [ 0.0052, -0.0032,  0.0030,  ...,  0.0033, -0.0036,  0.0099],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        ...,
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004],
        [-0.0003,  0.0000, -0.0004,  ...,  0.0005,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(117.9637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.6161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0110, device='cuda:0')



h[100].sum tensor(-13.4681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4085, device='cuda:0')



h[200].sum tensor(-4.3827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0000, 0.0030,  ..., 0.0049, 0.0000, 0.0099],
        [0.0075, 0.0000, 0.0044,  ..., 0.0060, 0.0000, 0.0141],
        [0.0256, 0.0000, 0.0146,  ..., 0.0155, 0.0000, 0.0480],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23339.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.3012e-05, 0.0000e+00, 1.8048e-02,  ..., 1.1828e-02, 0.0000e+00,
         3.0774e-02],
        [0.0000e+00, 0.0000e+00, 2.5820e-02,  ..., 1.7811e-02, 0.0000e+00,
         4.6411e-02],
        [0.0000e+00, 0.0000e+00, 3.8863e-02,  ..., 2.7928e-02, 0.0000e+00,
         7.2698e-02],
        ...,
        [4.8035e-03, 0.0000e+00, 1.8571e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.8032e-03, 0.0000e+00, 1.8570e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [4.8033e-03, 0.0000e+00, 1.8570e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(139546.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1096.1969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(139.0301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(159.2679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(113.7233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0347],
        [ 0.0569],
        [ 0.0736],
        ...,
        [-0.0545],
        [-0.0543],
        [-0.0544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-27309.6211, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [1.0000],
        ...,
        [1.0002],
        [1.0002],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365857.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [1.0000],
        ...,
        [1.0002],
        [1.0002],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365864.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0000, -0.0005,  ...,  0.0005,  0.0000, -0.0005],
        [ 0.0180, -0.0105,  0.0107,  ...,  0.0097, -0.0118,  0.0335],
        [ 0.0183, -0.0106,  0.0109,  ...,  0.0099, -0.0119,  0.0339],
        ...,
        [-0.0004,  0.0000, -0.0005,  ...,  0.0005,  0.0000, -0.0005],
        [-0.0004,  0.0000, -0.0005,  ...,  0.0005,  0.0000, -0.0005],
        [-0.0004,  0.0000, -0.0005,  ...,  0.0005,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(321.2526, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.4700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-19.4392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5886, device='cuda:0')



h[200].sum tensor(1.0969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0235, 0.0000, 0.0137,  ..., 0.0142, 0.0000, 0.0437],
        [0.0326, 0.0000, 0.0190,  ..., 0.0189, 0.0000, 0.0607],
        [0.0690, 0.0000, 0.0410,  ..., 0.0374, 0.0000, 0.1282],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30325.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 5.6764e-02,  ..., 4.4018e-02, 0.0000e+00,
         1.1690e-01],
        [0.0000e+00, 0.0000e+00, 8.6663e-02,  ..., 6.9645e-02, 0.0000e+00,
         1.8374e-01],
        [0.0000e+00, 0.0000e+00, 1.2927e-01,  ..., 1.0639e-01, 0.0000e+00,
         2.7950e-01],
        ...,
        [6.0163e-03, 0.0000e+00, 1.8949e-03,  ..., 6.7599e-05, 0.0000e+00,
         0.0000e+00],
        [6.0160e-03, 0.0000e+00, 1.8949e-03,  ..., 6.7639e-05, 0.0000e+00,
         0.0000e+00],
        [6.0160e-03, 0.0000e+00, 1.8949e-03,  ..., 6.7636e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170226.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1347.5779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(161.3766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(199.2984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(144.6278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0720],
        [ 0.0612],
        [ 0.0534],
        ...,
        [-0.0625],
        [-0.0622],
        [-0.0621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26107.5195, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [1.0000],
        ...,
        [1.0002],
        [1.0002],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365864.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(513.8430, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0001],
        ...,
        [1.0003],
        [1.0003],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365872.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061, -0.0037,  0.0034,  ...,  0.0038, -0.0041,  0.0115],
        [ 0.0075, -0.0045,  0.0042,  ...,  0.0045, -0.0050,  0.0140],
        [ 0.0125, -0.0074,  0.0073,  ...,  0.0070, -0.0083,  0.0234],
        ...,
        [-0.0004,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0005],
        [-0.0004,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0005],
        [-0.0004,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(597.0847, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.8562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0224, device='cuda:0')



h[100].sum tensor(-27.4205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8272, device='cuda:0')



h[200].sum tensor(8.6572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.9708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0322, 0.0000, 0.0184,  ..., 0.0190, 0.0000, 0.0604],
        [0.0348, 0.0000, 0.0200,  ..., 0.0203, 0.0000, 0.0652],
        [0.0236, 0.0000, 0.0131,  ..., 0.0147, 0.0000, 0.0445],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39227.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 5.0524e-02,  ..., 3.5980e-02, 0.0000e+00,
         1.0207e-01],
        [0.0000e+00, 0.0000e+00, 6.3872e-02,  ..., 4.6419e-02, 0.0000e+00,
         1.3116e-01],
        [0.0000e+00, 0.0000e+00, 7.7632e-02,  ..., 5.7237e-02, 0.0000e+00,
         1.6124e-01],
        ...,
        [7.0611e-03, 0.0000e+00, 1.7941e-03,  ..., 4.7656e-05, 0.0000e+00,
         0.0000e+00],
        [7.0607e-03, 0.0000e+00, 1.7940e-03,  ..., 4.7702e-05, 0.0000e+00,
         0.0000e+00],
        [7.0608e-03, 0.0000e+00, 1.7940e-03,  ..., 4.7700e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(205041.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1540.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(188.2395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(250.6505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(188.5125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0682],
        [ 0.0701],
        [ 0.0686],
        ...,
        [-0.0645],
        [-0.0642],
        [-0.0641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-22371.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0001],
        ...,
        [1.0003],
        [1.0003],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365872.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0001],
        ...,
        [1.0004],
        [1.0004],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365882., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0006],
        ...,
        [-0.0005,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0006,  ...,  0.0005,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(956.5281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0311, device='cuda:0')



h[100].sum tensor(-37.7281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1523, device='cuda:0')



h[200].sum tensor(17.6658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.8540, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0086, 0.0000, 0.0049,  ..., 0.0066, 0.0000, 0.0161],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51540.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.6311e-03, 0.0000e+00, 2.6745e-03,  ..., 8.5150e-04, 0.0000e+00,
         2.1004e-03],
        [5.2582e-03, 0.0000e+00, 6.9528e-03,  ..., 3.7136e-03, 0.0000e+00,
         1.1010e-02],
        [1.9102e-03, 0.0000e+00, 1.9029e-02,  ..., 1.2125e-02, 0.0000e+00,
         3.6352e-02],
        ...,
        [8.1864e-03, 0.0000e+00, 1.6205e-03,  ..., 8.2982e-05, 0.0000e+00,
         0.0000e+00],
        [8.1858e-03, 0.0000e+00, 1.6205e-03,  ..., 8.3028e-05, 0.0000e+00,
         0.0000e+00],
        [8.1859e-03, 0.0000e+00, 1.6205e-03,  ..., 8.3029e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(264920.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1750.6639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(323.7447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.9950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(245.0731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0239],
        [ 0.0145],
        [ 0.0594],
        ...,
        [-0.0644],
        [-0.0641],
        [-0.0640]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17154.8945, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0001],
        ...,
        [1.0004],
        [1.0004],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365882., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0002],
        ...,
        [1.0004],
        [1.0004],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365890.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0180, -0.0105,  0.0106,  ...,  0.0098, -0.0117,  0.0335],
        [ 0.0264, -0.0152,  0.0157,  ...,  0.0140, -0.0171,  0.0490],
        [ 0.0188, -0.0109,  0.0111,  ...,  0.0101, -0.0122,  0.0348],
        ...,
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(619.4177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.6137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0228, device='cuda:0')



h[100].sum tensor(-27.5325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8428, device='cuda:0')



h[200].sum tensor(9.6183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.2532, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0810, 0.0000, 0.0479,  ..., 0.0435, 0.0000, 0.1504],
        [0.0949, 0.0000, 0.0563,  ..., 0.0504, 0.0000, 0.1760],
        [0.0783, 0.0000, 0.0462,  ..., 0.0422, 0.0000, 0.1455],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41407.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.3583e-01,  ..., 1.0787e-01, 0.0000e+00,
         3.0669e-01],
        [0.0000e+00, 0.0000e+00, 1.6451e-01,  ..., 1.3185e-01, 0.0000e+00,
         3.7340e-01],
        [0.0000e+00, 0.0000e+00, 1.5963e-01,  ..., 1.2693e-01, 0.0000e+00,
         3.6092e-01],
        ...,
        [9.0618e-03, 0.0000e+00, 1.6012e-03,  ..., 1.3044e-06, 0.0000e+00,
         0.0000e+00],
        [9.0612e-03, 0.0000e+00, 1.6011e-03,  ..., 1.3623e-06, 0.0000e+00,
         0.0000e+00],
        [9.0612e-03, 0.0000e+00, 1.6012e-03,  ..., 1.3612e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(227826.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1999.3589, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(192.4350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(253.9415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.3716, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(199.5665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0150],
        [ 0.0016],
        [-0.0066],
        ...,
        [-0.0681],
        [-0.0678],
        [-0.0677]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-21234.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0002],
        ...,
        [1.0004],
        [1.0004],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365890.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0003],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365899.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        ...,
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(596.3865, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0216, device='cuda:0')



h[100].sum tensor(-26.5194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7993, device='cuda:0')



h[200].sum tensor(8.7028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4658, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41393.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0000, 0.0017,  ..., 0.0000, 0.0003, 0.0000],
        [0.0100, 0.0000, 0.0017,  ..., 0.0000, 0.0003, 0.0000],
        [0.0100, 0.0000, 0.0017,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0099, 0.0000, 0.0017,  ..., 0.0000, 0.0003, 0.0000],
        [0.0099, 0.0000, 0.0017,  ..., 0.0000, 0.0003, 0.0000],
        [0.0099, 0.0000, 0.0017,  ..., 0.0000, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(238574.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2194.6340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.7571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(252.0316, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(200.8240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0887],
        [-0.0821],
        [-0.0656],
        ...,
        [-0.0544],
        [-0.0670],
        [-0.0712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-24405.1758, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [1.0003],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365899.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9999],
        [1.0003],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0076, -0.0046,  0.0043,  ...,  0.0046, -0.0051,  0.0143],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [ 0.0134, -0.0078,  0.0077,  ...,  0.0074, -0.0087,  0.0249],
        ...,
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(511.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0196, device='cuda:0')



h[100].sum tensor(-23.7584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7253, device='cuda:0')



h[200].sum tensor(6.5820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0000, 0.0034,  ..., 0.0054, 0.0000, 0.0116],
        [0.0333, 0.0000, 0.0187,  ..., 0.0197, 0.0000, 0.0624],
        [0.0182, 0.0000, 0.0099,  ..., 0.0119, 0.0000, 0.0344],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38914.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0242,  ..., 0.0134, 0.0000, 0.0472],
        [0.0000, 0.0000, 0.0475,  ..., 0.0285, 0.0000, 0.0974],
        [0.0000, 0.0000, 0.0494,  ..., 0.0283, 0.0000, 0.1000],
        ...,
        [0.0106, 0.0000, 0.0018,  ..., 0.0000, 0.0013, 0.0000],
        [0.0106, 0.0000, 0.0018,  ..., 0.0000, 0.0013, 0.0000],
        [0.0106, 0.0000, 0.0018,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230346.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2295.1165, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.7108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(233.8101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(189.1112, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0366],
        [ 0.0696],
        [ 0.0921],
        ...,
        [-0.0814],
        [-0.0810],
        [-0.0809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-24220.3086, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9999],
        [1.0003],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9999],
        [1.0004],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365918.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0067, -0.0041,  0.0037,  ...,  0.0041, -0.0045,  0.0126],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0007],
        [ 0.0067, -0.0041,  0.0037,  ...,  0.0041, -0.0045,  0.0126],
        ...,
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0007],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0007],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0005,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(331.1350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.7578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-18.2724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5592, device='cuda:0')



h[200].sum tensor(2.1489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1199, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0000, 0.0029,  ..., 0.0051, 0.0000, 0.0102],
        [0.0242, 0.0000, 0.0132,  ..., 0.0152, 0.0000, 0.0456],
        [0.0054, 0.0000, 0.0029,  ..., 0.0051, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31236.0430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0000, 0.0171,  ..., 0.0079, 0.0000, 0.0311],
        [0.0000, 0.0000, 0.0280,  ..., 0.0146, 0.0000, 0.0545],
        [0.0037, 0.0000, 0.0171,  ..., 0.0079, 0.0000, 0.0312],
        ...,
        [0.0110, 0.0000, 0.0018,  ..., 0.0000, 0.0025, 0.0000],
        [0.0110, 0.0000, 0.0018,  ..., 0.0000, 0.0025, 0.0000],
        [0.0110, 0.0000, 0.0018,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199149.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2525.8806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(172.0037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(186.7791, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(155.0492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0828],
        [-0.0726],
        [-0.0725],
        ...,
        [-0.0909],
        [-0.0904],
        [-0.0903]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-40828.2383, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9999],
        [1.0004],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365918.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9999],
        [1.0004],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365928.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0151, -0.0088,  0.0088,  ...,  0.0084, -0.0098,  0.0281],
        [ 0.0140, -0.0081,  0.0081,  ...,  0.0078, -0.0091,  0.0259],
        [ 0.0218, -0.0125,  0.0129,  ...,  0.0117, -0.0140,  0.0403],
        ...,
        [-0.0005,  0.0000, -0.0007,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0006,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(464.7780, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.3192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-21.8513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6720, device='cuda:0')



h[200].sum tensor(5.4962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0603, 0.0000, 0.0351,  ..., 0.0333, 0.0000, 0.1119],
        [0.0788, 0.0000, 0.0464,  ..., 0.0425, 0.0000, 0.1459],
        [0.0955, 0.0000, 0.0566,  ..., 0.0508, 0.0000, 0.1766],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34768.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1269,  ..., 0.0925, 0.0000, 0.2894],
        [0.0000, 0.0000, 0.1505,  ..., 0.1128, 0.0000, 0.3471],
        [0.0000, 0.0000, 0.1774,  ..., 0.1360, 0.0000, 0.4133],
        ...,
        [0.0113, 0.0000, 0.0018,  ..., 0.0000, 0.0037, 0.0000],
        [0.0113, 0.0000, 0.0018,  ..., 0.0000, 0.0037, 0.0000],
        [0.0113, 0.0000, 0.0018,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(213880.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2560.9595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(192.1765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(209.4359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(169.5320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1309],
        [-0.1544],
        [-0.1749],
        ...,
        [-0.0994],
        [-0.0989],
        [-0.0987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44139.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9999],
        [1.0004],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365928.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9999],
        [1.0005],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365939.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0031,  0.0026,  ...,  0.0033, -0.0035,  0.0095],
        [ 0.0046, -0.0029,  0.0024,  ...,  0.0032, -0.0032,  0.0088],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0006,  0.0000, -0.0007],
        ...,
        [-0.0005,  0.0000, -0.0007,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0005,  0.0000, -0.0007,  ...,  0.0006,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(912.8724, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.0453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0284, device='cuda:0')



h[100].sum tensor(-34.4281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0494, device='cuda:0')



h[200].sum tensor(16.4437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0405, 0.0000, 0.0230,  ..., 0.0236, 0.0000, 0.0757],
        [0.0087, 0.0000, 0.0044,  ..., 0.0072, 0.0000, 0.0165],
        [0.0195, 0.0000, 0.0106,  ..., 0.0129, 0.0000, 0.0367],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53426.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0743,  ..., 0.0473, 0.0000, 0.1635],
        [0.0000, 0.0000, 0.0460,  ..., 0.0253, 0.0000, 0.0963],
        [0.0000, 0.0000, 0.0506,  ..., 0.0281, 0.0000, 0.1063],
        ...,
        [0.0118, 0.0000, 0.0018,  ..., 0.0000, 0.0048, 0.0000],
        [0.0118, 0.0000, 0.0018,  ..., 0.0000, 0.0048, 0.0000],
        [0.0118, 0.0000, 0.0018,  ..., 0.0000, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(311107.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2420.0376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.6289, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(328.5107, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(251.9735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0020],
        [ 0.0229],
        [ 0.0371],
        ...,
        [-0.0935],
        [-0.0928],
        [-0.0947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-40852.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9999],
        [1.0005],
        ...,
        [1.0005],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365939.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0005],
        ...,
        [1.0006],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365951.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0000, -0.0008,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0006,  0.0000, -0.0007],
        ...,
        [-0.0006,  0.0000, -0.0008,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0006,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0006,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(622.3035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.4690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0213, device='cuda:0')



h[100].sum tensor(-25.8912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7900, device='cuda:0')



h[200].sum tensor(10.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41228.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0125, 0.0000, 0.0017,  ..., 0.0000, 0.0060, 0.0000],
        [0.0125, 0.0000, 0.0017,  ..., 0.0000, 0.0060, 0.0000],
        [0.0125, 0.0000, 0.0017,  ..., 0.0000, 0.0060, 0.0000],
        ...,
        [0.0124, 0.0000, 0.0017,  ..., 0.0000, 0.0059, 0.0000],
        [0.0124, 0.0000, 0.0017,  ..., 0.0000, 0.0059, 0.0000],
        [0.0124, 0.0000, 0.0017,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254173.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2685.7876, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.7301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.0183, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(191.1810, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1612],
        [-0.1629],
        [-0.1610],
        ...,
        [-0.1084],
        [-0.1053],
        [-0.0994]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-36160.3008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0005],
        ...,
        [1.0006],
        [1.0005],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365951.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0006],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365962.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063, -0.0038,  0.0034,  ...,  0.0041, -0.0043,  0.0120],
        [ 0.0066, -0.0040,  0.0036,  ...,  0.0043, -0.0045,  0.0126],
        [ 0.0063, -0.0038,  0.0034,  ...,  0.0041, -0.0043,  0.0120],
        ...,
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(787.6116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.5348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0252, device='cuda:0')



h[100].sum tensor(-30.2971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9324, device='cuda:0')



h[200].sum tensor(14.7883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.8736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0295, 0.0000, 0.0162,  ..., 0.0185, 0.0000, 0.0558],
        [0.0286, 0.0000, 0.0156,  ..., 0.0180, 0.0000, 0.0541],
        [0.0117, 0.0000, 0.0062,  ..., 0.0090, 0.0000, 0.0222],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48502.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0451,  ..., 0.0211, 0.0000, 0.0947],
        [0.0000, 0.0000, 0.0457,  ..., 0.0211, 0.0000, 0.0958],
        [0.0013, 0.0000, 0.0363,  ..., 0.0150, 0.0000, 0.0743],
        ...,
        [0.0128, 0.0000, 0.0016,  ..., 0.0000, 0.0070, 0.0000],
        [0.0128, 0.0000, 0.0016,  ..., 0.0000, 0.0070, 0.0000],
        [0.0128, 0.0000, 0.0016,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(292544.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2787.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(244.6825, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(292.1758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(222.2988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0749],
        [ 0.0915],
        [ 0.1022],
        ...,
        [-0.1085],
        [-0.1073],
        [-0.1077]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39141.4766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0006],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365962.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(573.5645, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0007],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365973.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046, -0.0029,  0.0023,  ...,  0.0033, -0.0032,  0.0089],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007],
        ...,
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0006,  0.0000, -0.0008,  ...,  0.0007,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(463.6915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0175, device='cuda:0')



h[100].sum tensor(-20.8926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6482, device='cuda:0')



h[200].sum tensor(8.3777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0000, 0.0105,  ..., 0.0128, 0.0000, 0.0355],
        [0.0046, 0.0000, 0.0023,  ..., 0.0054, 0.0000, 0.0089],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35354.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0000, 0.0514,  ..., 0.0284, 0.0000, 0.1140],
        [0.0049, 0.0000, 0.0270,  ..., 0.0124, 0.0004, 0.0574],
        [0.0094, 0.0000, 0.0097,  ..., 0.0032, 0.0040, 0.0183],
        ...,
        [0.0131, 0.0000, 0.0015,  ..., 0.0000, 0.0080, 0.0000],
        [0.0131, 0.0000, 0.0015,  ..., 0.0000, 0.0080, 0.0000],
        [0.0131, 0.0000, 0.0015,  ..., 0.0000, 0.0080, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(229990.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3013.3740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.8174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(204.5784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.6454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(159.9843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0424],
        [ 0.0228],
        [-0.0121],
        ...,
        [-0.1145],
        [-0.1139],
        [-0.1138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-41853.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0007],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365973.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0007],
        [1.0006],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365985.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000, -0.0009,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0007,  0.0000, -0.0007],
        ...,
        [-0.0007,  0.0000, -0.0009,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0007,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0007,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(367.5997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0150, device='cuda:0')



h[100].sum tensor(-17.9453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5533, device='cuda:0')



h[200].sum tensor(7.2040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0119, 0.0000, 0.0068,  ..., 0.0092, 0.0000, 0.0223],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31337.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0128, 0.0000, 0.0042,  ..., 0.0002, 0.0047, 0.0062],
        [0.0099, 0.0000, 0.0093,  ..., 0.0030, 0.0038, 0.0184],
        [0.0053, 0.0000, 0.0253,  ..., 0.0124, 0.0000, 0.0563],
        ...,
        [0.0136, 0.0000, 0.0015,  ..., 0.0000, 0.0090, 0.0004],
        [0.0136, 0.0000, 0.0015,  ..., 0.0000, 0.0090, 0.0004],
        [0.0136, 0.0000, 0.0015,  ..., 0.0000, 0.0090, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(211267.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3166.1086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(174.2775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(178.2012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(139.3021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0214],
        [ 0.0061],
        [-0.0052],
        ...,
        [-0.1148],
        [-0.1142],
        [-0.1140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-47553.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0007],
        [1.0006],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365985.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365997.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007],
        ...,
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(668.6288, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0217, device='cuda:0')



h[100].sum tensor(-26.2315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8042, device='cuda:0')



h[200].sum tensor(15.1549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43907.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0000, 0.0135,  ..., 0.0049, 0.0037, 0.0287],
        [0.0128, 0.0000, 0.0046,  ..., 0.0007, 0.0065, 0.0084],
        [0.0141, 0.0000, 0.0021,  ..., 0.0000, 0.0086, 0.0026],
        ...,
        [0.0142, 0.0000, 0.0014,  ..., 0.0000, 0.0102, 0.0009],
        [0.0142, 0.0000, 0.0014,  ..., 0.0000, 0.0102, 0.0009],
        [0.0142, 0.0000, 0.0014,  ..., 0.0000, 0.0102, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(275342.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3143.4363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.1880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(252.3419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.6141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(189.3584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0115],
        [-0.0191],
        [-0.0043],
        ...,
        [-0.1153],
        [-0.1147],
        [-0.1145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-29553.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0006],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365997.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366008.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0038, -0.0025,  0.0018,  ...,  0.0030, -0.0028,  0.0075],
        [ 0.0050, -0.0031,  0.0026,  ...,  0.0037, -0.0035,  0.0098],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007],
        ...,
        [ 0.0067, -0.0041,  0.0036,  ...,  0.0045, -0.0046,  0.0128],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0008,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(566.0219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0195, device='cuda:0')



h[100].sum tensor(-23.3625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7222, device='cuda:0')



h[200].sum tensor(14.3725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0396, 0.0000, 0.0222,  ..., 0.0244, 0.0000, 0.0751],
        [0.0156, 0.0000, 0.0086,  ..., 0.0117, 0.0000, 0.0299],
        [0.0050, 0.0000, 0.0026,  ..., 0.0061, 0.0000, 0.0098],
        ...,
        [0.0512, 0.0000, 0.0297,  ..., 0.0298, 0.0000, 0.0958],
        [0.0451, 0.0000, 0.0260,  ..., 0.0268, 0.0000, 0.0846],
        [0.0325, 0.0000, 0.0188,  ..., 0.0201, 0.0000, 0.0608]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38460.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0679,  ..., 0.0362, 0.0000, 0.1541],
        [0.0016, 0.0000, 0.0458,  ..., 0.0218, 0.0000, 0.1027],
        [0.0060, 0.0000, 0.0241,  ..., 0.0088, 0.0011, 0.0524],
        ...,
        [0.0000, 0.0000, 0.1485,  ..., 0.1049, 0.0000, 0.3582],
        [0.0000, 0.0000, 0.1201,  ..., 0.0821, 0.0000, 0.2875],
        [0.0000, 0.0000, 0.0844,  ..., 0.0549, 0.0000, 0.2001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(249989.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3113.2407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(207.8137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(220.2487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2.1244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(167.8214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1143],
        [ 0.0960],
        [ 0.0606],
        ...,
        [-0.1412],
        [-0.0909],
        [-0.0494]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46876.6523, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366008.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0009],
        [1.0008],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366019.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0080, -0.0048,  0.0044,  ...,  0.0052, -0.0054,  0.0153],
        [ 0.0080, -0.0048,  0.0044,  ...,  0.0052, -0.0054,  0.0153],
        ...,
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(449.1062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0168, device='cuda:0')



h[100].sum tensor(-20.1459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6220, device='cuda:0')



h[200].sum tensor(13.1718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0144, 0.0000, 0.0078,  ..., 0.0113, 0.0000, 0.0276],
        [0.0207, 0.0000, 0.0112,  ..., 0.0148, 0.0000, 0.0398],
        [0.0311, 0.0000, 0.0175,  ..., 0.0200, 0.0000, 0.0589],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35334.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6418e-04, 0.0000e+00, 5.2371e-02,  ..., 2.7847e-02, 0.0000e+00,
         1.1990e-01],
        [0.0000e+00, 0.0000e+00, 7.3731e-02,  ..., 4.3368e-02, 0.0000e+00,
         1.7152e-01],
        [0.0000e+00, 0.0000e+00, 9.9431e-02,  ..., 6.3646e-02, 0.0000e+00,
         2.3521e-01],
        ...,
        [1.3419e-02, 0.0000e+00, 8.6715e-04,  ..., 0.0000e+00, 1.2578e-02,
         2.0060e-04],
        [1.3418e-02, 0.0000e+00, 8.6713e-04,  ..., 0.0000e+00, 1.2577e-02,
         2.0055e-04],
        [1.3418e-02, 0.0000e+00, 8.6714e-04,  ..., 0.0000e+00, 1.2577e-02,
         2.0055e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(241171.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3042.6636, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(201.3982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.9503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.3141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(155.3792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0609],
        [-0.1018],
        [-0.1420],
        ...,
        [-0.1334],
        [-0.1328],
        [-0.1326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-60558.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0009],
        [1.0008],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366019.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366030.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0047, -0.0030,  0.0024,  ...,  0.0036, -0.0034,  0.0093],
        ...,
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(479.7651, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-20.8893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6531, device='cuda:0')



h[200].sum tensor(15.5753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0024,  ..., 0.0064, 0.0000, 0.0093],
        [0.0110, 0.0000, 0.0058,  ..., 0.0098, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37998.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0000, 0.0076,  ..., 0.0004, 0.0055, 0.0146],
        [0.0059, 0.0000, 0.0218,  ..., 0.0064, 0.0017, 0.0468],
        [0.0020, 0.0000, 0.0407,  ..., 0.0164, 0.0000, 0.0902],
        ...,
        [0.0131, 0.0000, 0.0007,  ..., 0.0000, 0.0136, 0.0000],
        [0.0131, 0.0000, 0.0007,  ..., 0.0000, 0.0136, 0.0000],
        [0.0131, 0.0000, 0.0007,  ..., 0.0000, 0.0136, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259455.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2843.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(223.2577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(217.4035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1966, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(165.2336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0234],
        [ 0.0476],
        [ 0.0622],
        ...,
        [-0.1407],
        [-0.1400],
        [-0.1398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-54095.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366030.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366041.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0144, -0.0083,  0.0084,  ...,  0.0085, -0.0094,  0.0271],
        [ 0.0150, -0.0086,  0.0087,  ...,  0.0088, -0.0097,  0.0281],
        [ 0.0125, -0.0073,  0.0072,  ...,  0.0075, -0.0082,  0.0236],
        ...,
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(381.1447, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0150, device='cuda:0')



h[100].sum tensor(-17.9814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5564, device='cuda:0')



h[200].sum tensor(15.0497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0699, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0542, 0.0000, 0.0313,  ..., 0.0322, 0.0000, 0.1020],
        [0.0586, 0.0000, 0.0339,  ..., 0.0343, 0.0000, 0.1100],
        [0.0730, 0.0000, 0.0427,  ..., 0.0415, 0.0000, 0.1365],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32769.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1021,  ..., 0.0628, 0.0000, 0.2412],
        [0.0000, 0.0000, 0.1111,  ..., 0.0695, 0.0000, 0.2630],
        [0.0000, 0.0000, 0.1206,  ..., 0.0768, 0.0000, 0.2865],
        ...,
        [0.0130, 0.0000, 0.0006,  ..., 0.0000, 0.0144, 0.0000],
        [0.0130, 0.0000, 0.0006,  ..., 0.0000, 0.0144, 0.0000],
        [0.0130, 0.0000, 0.0006,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234886.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2896.4641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.7793, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(186.2317, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(142.4483, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0452],
        [ 0.0461],
        [ 0.0487],
        ...,
        [-0.1463],
        [-0.1456],
        [-0.1454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65737.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0008],
        [1.0007],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366041.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366052.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0051, -0.0032,  0.0027,  ...,  0.0039, -0.0036,  0.0100],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(490.2453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.0106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-20.7327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6420, device='cuda:0')



h[200].sum tensor(19.4843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0224, 0.0000, 0.0119,  ..., 0.0164, 0.0000, 0.0437],
        [0.0040, 0.0000, 0.0020,  ..., 0.0062, 0.0000, 0.0080],
        [0.0051, 0.0000, 0.0027,  ..., 0.0068, 0.0000, 0.0100],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36846.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 4.4437e-02,  ..., 1.6696e-02, 0.0000e+00,
         9.8018e-02],
        [3.6049e-03, 0.0000e+00, 2.7183e-02,  ..., 7.6924e-03, 0.0000e+00,
         5.9220e-02],
        [6.9496e-03, 0.0000e+00, 1.6474e-02,  ..., 3.2190e-03, 1.8944e-03,
         3.4987e-02],
        ...,
        [1.2984e-02, 0.0000e+00, 5.3188e-04,  ..., 0.0000e+00, 1.4811e-02,
         1.0042e-05],
        [1.2983e-02, 0.0000e+00, 5.3187e-04,  ..., 0.0000e+00, 1.4810e-02,
         1.0028e-05],
        [1.2983e-02, 0.0000e+00, 5.3188e-04,  ..., 0.0000e+00, 1.4810e-02,
         1.0027e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254703.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2809.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(208.0372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.2483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(161.1684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0732],
        [ 0.0636],
        [ 0.0476],
        ...,
        [-0.1498],
        [-0.1491],
        [-0.1488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-63511.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366052.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366064., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0007,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(640.5120, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0206, device='cuda:0')



h[100].sum tensor(-24.5510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7612, device='cuda:0')



h[200].sum tensor(24.5057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7765, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0087, 0.0000, 0.0045,  ..., 0.0090, 0.0000, 0.0174],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0066, 0.0000, 0.0036,  ..., 0.0075, 0.0000, 0.0128]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41850.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0124, 0.0000, 0.0030,  ..., 0.0000, 0.0103, 0.0053],
        [0.0074, 0.0000, 0.0135,  ..., 0.0035, 0.0062, 0.0294],
        [0.0033, 0.0000, 0.0442,  ..., 0.0209, 0.0000, 0.1019],
        ...,
        [0.0128, 0.0000, 0.0014,  ..., 0.0000, 0.0132, 0.0021],
        [0.0086, 0.0000, 0.0097,  ..., 0.0023, 0.0075, 0.0213],
        [0.0041, 0.0000, 0.0277,  ..., 0.0104, 0.0016, 0.0625]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(277276., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2819.4709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(251.2691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(235.3873, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.7236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(182.2289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0829],
        [-0.0953],
        [-0.1376],
        ...,
        [-0.1025],
        [-0.0580],
        [-0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-54964.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0006],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366064., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366075.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(365.1668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0140, device='cuda:0')



h[100].sum tensor(-16.7413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5181, device='cuda:0')



h[200].sum tensor(19.8616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31676.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0133, 0.0000, 0.0015,  ..., 0.0000, 0.0137, 0.0021],
        [0.0137, 0.0000, 0.0008,  ..., 0.0000, 0.0152, 0.0005],
        [0.0137, 0.0000, 0.0008,  ..., 0.0000, 0.0153, 0.0005],
        ...,
        [0.0136, 0.0000, 0.0008,  ..., 0.0000, 0.0152, 0.0005],
        [0.0136, 0.0000, 0.0008,  ..., 0.0000, 0.0152, 0.0005],
        [0.0136, 0.0000, 0.0008,  ..., 0.0000, 0.0152, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231745.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2962.9663, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(211.0860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(170.7050, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.2138, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(136.4418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0966],
        [-0.1464],
        [-0.1857],
        ...,
        [-0.1490],
        [-0.1483],
        [-0.1481]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-64988.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366075.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(547.5582, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366086.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        ...,
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(530.5815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-20.9171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6516, device='cuda:0')



h[200].sum tensor(24.4272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37720.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0143, 0.0000, 0.0018,  ..., 0.0000, 0.0142, 0.0026],
        [0.0143, 0.0000, 0.0012,  ..., 0.0000, 0.0151, 0.0015],
        [0.0143, 0.0000, 0.0009,  ..., 0.0000, 0.0156, 0.0009],
        ...,
        [0.0142, 0.0000, 0.0009,  ..., 0.0000, 0.0155, 0.0009],
        [0.0142, 0.0000, 0.0009,  ..., 0.0000, 0.0155, 0.0009],
        [0.0142, 0.0000, 0.0009,  ..., 0.0000, 0.0155, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259186.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3003.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(239.1938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.7880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2.9683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(157.8020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0268],
        [-0.0624],
        [-0.0977],
        ...,
        [-0.1457],
        [-0.1448],
        [-0.1445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-51592.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366086.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366099.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [ 0.0056, -0.0035,  0.0030,  ...,  0.0041, -0.0039,  0.0111],
        ...,
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.5914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0196, device='cuda:0')



h[100].sum tensor(-23.8333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7245, device='cuda:0')



h[200].sum tensor(27.0200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0097, 0.0000, 0.0051,  ..., 0.0095, 0.0000, 0.0195],
        [0.0258, 0.0000, 0.0146,  ..., 0.0179, 0.0000, 0.0500],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42654.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0000, 0.0165,  ..., 0.0032, 0.0041, 0.0359],
        [0.0042, 0.0000, 0.0404,  ..., 0.0150, 0.0000, 0.0918],
        [0.0000, 0.0000, 0.0732,  ..., 0.0374, 0.0000, 0.1710],
        ...,
        [0.0150, 0.0000, 0.0009,  ..., 0.0000, 0.0161, 0.0013],
        [0.0150, 0.0000, 0.0009,  ..., 0.0000, 0.0161, 0.0013],
        [0.0150, 0.0000, 0.0009,  ..., 0.0000, 0.0161, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287476.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3175.2009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.4382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(233.9925, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2.2503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(177.2400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0641],
        [ 0.0952],
        [ 0.1102],
        ...,
        [-0.1350],
        [-0.1423],
        [-0.1442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-51160.4258, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0007],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366099.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0006],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366111.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        ...,
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(386.3152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0139, device='cuda:0')



h[100].sum tensor(-16.6975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5154, device='cuda:0')



h[200].sum tensor(20.8811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32553.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0154, 0.0000, 0.0010,  ..., 0.0000, 0.0166, 0.0014],
        [0.0154, 0.0000, 0.0010,  ..., 0.0000, 0.0166, 0.0014],
        [0.0155, 0.0000, 0.0010,  ..., 0.0000, 0.0167, 0.0014],
        ...,
        [0.0154, 0.0000, 0.0010,  ..., 0.0000, 0.0166, 0.0014],
        [0.0154, 0.0000, 0.0010,  ..., 0.0000, 0.0166, 0.0014],
        [0.0154, 0.0000, 0.0010,  ..., 0.0000, 0.0166, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239440.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3339.2876, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.8844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(167.5623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2.3546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(128.8868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2034],
        [-0.2167],
        [-0.2272],
        ...,
        [-0.1484],
        [-0.1476],
        [-0.1473]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52173.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0006],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366111.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0006],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366122.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0227, -0.0127,  0.0135,  ...,  0.0127, -0.0143,  0.0427],
        [ 0.0243, -0.0136,  0.0145,  ...,  0.0135, -0.0153,  0.0456],
        [ 0.0192, -0.0108,  0.0114,  ...,  0.0110, -0.0122,  0.0363],
        ...,
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0009,  ...,  0.0010,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(883.9170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0247, device='cuda:0')



h[100].sum tensor(-29.8902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9156, device='cuda:0')



h[200].sum tensor(32.1449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.5710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0947, 0.0000, 0.0563,  ..., 0.0528, 0.0000, 0.1779],
        [0.0977, 0.0000, 0.0582,  ..., 0.0542, 0.0000, 0.1834],
        [0.1074, 0.0000, 0.0641,  ..., 0.0590, 0.0000, 0.2012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52181.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1845,  ..., 0.1266, 0.0000, 0.4526],
        [0.0000, 0.0000, 0.2002,  ..., 0.1398, 0.0000, 0.4929],
        [0.0000, 0.0000, 0.2101,  ..., 0.1484, 0.0000, 0.5186],
        ...,
        [0.0155, 0.0000, 0.0010,  ..., 0.0000, 0.0172, 0.0012],
        [0.0155, 0.0000, 0.0010,  ..., 0.0000, 0.0172, 0.0012],
        [0.0155, 0.0000, 0.0010,  ..., 0.0000, 0.0172, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342342.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3111.3088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.8147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(298.3105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(220.5687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0055],
        [-0.0251],
        [-0.0284],
        ...,
        [-0.1550],
        [-0.1543],
        [-0.1541]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56615.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0006],
        [1.0005],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366122.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366133.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054, -0.0034,  0.0030,  ...,  0.0041, -0.0038,  0.0109],
        [-0.0008,  0.0000, -0.0008,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0008,  ...,  0.0010,  0.0000, -0.0006],
        ...,
        [-0.0008,  0.0000, -0.0008,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0008,  ...,  0.0010,  0.0000, -0.0006],
        [-0.0008,  0.0000, -0.0008,  ...,  0.0010,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(405.1038, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-17.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5310, device='cuda:0')



h[200].sum tensor(21.7322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6092, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0023,  ..., 0.0067, 0.0000, 0.0088],
        [0.0054, 0.0000, 0.0030,  ..., 0.0072, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33345.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0097, 0.0000, 0.0146,  ..., 0.0001, 0.0004, 0.0301],
        [0.0109, 0.0000, 0.0115,  ..., 0.0009, 0.0040, 0.0236],
        [0.0140, 0.0000, 0.0044,  ..., 0.0000, 0.0108, 0.0078],
        ...,
        [0.0154, 0.0000, 0.0011,  ..., 0.0000, 0.0177, 0.0007],
        [0.0154, 0.0000, 0.0011,  ..., 0.0000, 0.0177, 0.0007],
        [0.0154, 0.0000, 0.0011,  ..., 0.0000, 0.0177, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(247109.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3247.6335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(231.4549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(179.3134, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(137.1398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1325],
        [-0.1369],
        [-0.1348],
        ...,
        [-0.1656],
        [-0.1648],
        [-0.1646]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62511.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0008],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366133.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366145.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105, -0.0061,  0.0061,  ...,  0.0067, -0.0069,  0.0203],
        [ 0.0122, -0.0070,  0.0071,  ...,  0.0075, -0.0079,  0.0233],
        [ 0.0356, -0.0196,  0.0214,  ...,  0.0192, -0.0221,  0.0663],
        ...,
        [-0.0008,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0008,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0008,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(742.9618, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0214, device='cuda:0')



h[100].sum tensor(-25.8118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7917, device='cuda:0')



h[200].sum tensor(29.2832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0853, 0.0000, 0.0509,  ..., 0.0483, 0.0000, 0.1608],
        [0.0972, 0.0000, 0.0582,  ..., 0.0543, 0.0000, 0.1826],
        [0.0578, 0.0000, 0.0342,  ..., 0.0347, 0.0000, 0.1103],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47189.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.7019e-01,  ..., 1.1311e-01, 0.0000e+00,
         4.1262e-01],
        [0.0000e+00, 0.0000e+00, 1.8179e-01,  ..., 1.2271e-01, 0.0000e+00,
         4.4221e-01],
        [0.0000e+00, 0.0000e+00, 1.5227e-01,  ..., 9.8672e-02, 0.0000e+00,
         3.6738e-01],
        ...,
        [1.5489e-02, 0.0000e+00, 1.2379e-03,  ..., 0.0000e+00, 1.8272e-02,
         3.5472e-04],
        [1.5488e-02, 0.0000e+00, 1.2379e-03,  ..., 0.0000e+00, 1.8271e-02,
         3.5470e-04],
        [1.5488e-02, 0.0000e+00, 1.2379e-03,  ..., 0.0000e+00, 1.8271e-02,
         3.5470e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(318180.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3124.0562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(285.2096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.5406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1228, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(202.2618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0363],
        [-0.0444],
        [-0.0349],
        ...,
        [-0.1735],
        [-0.1727],
        [-0.1725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66622.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366145.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366157.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        ...,
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(699.1766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0208, device='cuda:0')



h[100].sum tensor(-24.5152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7707, device='cuda:0')



h[200].sum tensor(27.9405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9475, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0137, 0.0000, 0.0079,  ..., 0.0120, 0.0000, 0.0273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44447.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0135, 0.0000, 0.0056,  ..., 0.0000, 0.0098, 0.0100],
        [0.0116, 0.0000, 0.0104,  ..., 0.0002, 0.0088, 0.0202],
        [0.0050, 0.0000, 0.0293,  ..., 0.0060, 0.0000, 0.0627],
        ...,
        [0.0157, 0.0000, 0.0013,  ..., 0.0000, 0.0188, 0.0002],
        [0.0157, 0.0000, 0.0013,  ..., 0.0000, 0.0188, 0.0002],
        [0.0157, 0.0000, 0.0013,  ..., 0.0000, 0.0188, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(303479.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3080.0693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.5085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(255.6001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(189.1089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0446],
        [-0.0061],
        [ 0.0377],
        ...,
        [-0.1796],
        [-0.1787],
        [-0.1785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-64056.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366157.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366157.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0034,  0.0031,  ...,  0.0043, -0.0039,  0.0112],
        [ 0.0033, -0.0022,  0.0017,  ...,  0.0031, -0.0025,  0.0070],
        [ 0.0097, -0.0056,  0.0056,  ...,  0.0063, -0.0064,  0.0188],
        ...,
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(411.0240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-16.9272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5283, device='cuda:0')



h[200].sum tensor(21.4679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0110, 0.0000, 0.0062,  ..., 0.0106, 0.0000, 0.0222],
        [0.0315, 0.0000, 0.0182,  ..., 0.0217, 0.0000, 0.0619],
        [0.0185, 0.0000, 0.0105,  ..., 0.0148, 0.0000, 0.0370],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34206.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0000, 0.0377,  ..., 0.0106, 0.0000, 0.0820],
        [0.0000, 0.0000, 0.0550,  ..., 0.0204, 0.0000, 0.1219],
        [0.0005, 0.0000, 0.0501,  ..., 0.0170, 0.0000, 0.1102],
        ...,
        [0.0157, 0.0000, 0.0013,  ..., 0.0000, 0.0188, 0.0002],
        [0.0157, 0.0000, 0.0013,  ..., 0.0000, 0.0188, 0.0002],
        [0.0157, 0.0000, 0.0013,  ..., 0.0000, 0.0188, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255753.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3245.1213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.4082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(194.1487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(145.4706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0637],
        [ 0.0747],
        [ 0.0707],
        ...,
        [-0.1672],
        [-0.1584],
        [-0.1515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78407.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0000],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366157.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0001],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366169.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        ...,
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1043.4775, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.3382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0279, device='cuda:0')



h[100].sum tensor(-33.3804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0321, device='cuda:0')



h[200].sum tensor(35.1487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.6782, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0025, 0.0000, 0.0013,  ..., 0.0060, 0.0000, 0.0057]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53972.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0159, 0.0000, 0.0014,  ..., 0.0000, 0.0192, 0.0002],
        [0.0159, 0.0000, 0.0014,  ..., 0.0000, 0.0192, 0.0002],
        [0.0153, 0.0000, 0.0025,  ..., 0.0000, 0.0169, 0.0026],
        ...,
        [0.0152, 0.0000, 0.0035,  ..., 0.0000, 0.0153, 0.0044],
        [0.0142, 0.0000, 0.0065,  ..., 0.0000, 0.0096, 0.0107],
        [0.0127, 0.0000, 0.0115,  ..., 0.0000, 0.0040, 0.0209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339401.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3172.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.0375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(320.8917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(235.1033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2297],
        [-0.1916],
        [-0.1337],
        ...,
        [-0.1371],
        [-0.0993],
        [-0.0648]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71127.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0001],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366169.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0001],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366181.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        ...,
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(625.0481, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0191, device='cuda:0')



h[100].sum tensor(-22.2869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7080, device='cuda:0')



h[200].sum tensor(25.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8135, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0000, 0.0068,  ..., 0.0106, 0.0000, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40917.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0000, 0.0320,  ..., 0.0102, 0.0000, 0.0713],
        [0.0073, 0.0000, 0.0178,  ..., 0.0036, 0.0030, 0.0382],
        [0.0111, 0.0000, 0.0110,  ..., 0.0004, 0.0032, 0.0218],
        ...,
        [0.0161, 0.0000, 0.0014,  ..., 0.0000, 0.0196, 0.0002],
        [0.0161, 0.0000, 0.0014,  ..., 0.0000, 0.0196, 0.0002],
        [0.0161, 0.0000, 0.0014,  ..., 0.0000, 0.0196, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(285299.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3225.0786, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(270.8705, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(234.4763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(174.7272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0455],
        [ 0.0378],
        [ 0.0337],
        ...,
        [-0.1847],
        [-0.1839],
        [-0.1836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71294.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0001],
        [1.0009],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366181.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(508.8870, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0001],
        [1.0010],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366193.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0023,  0.0019,  ...,  0.0032, -0.0026,  0.0074],
        [ 0.0034, -0.0023,  0.0019,  ...,  0.0032, -0.0026,  0.0074],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        ...,
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(549.2642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0171, device='cuda:0')



h[100].sum tensor(-20.1146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6319, device='cuda:0')



h[200].sum tensor(22.8537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0000, 0.0033,  ..., 0.0083, 0.0000, 0.0134],
        [0.0061, 0.0000, 0.0033,  ..., 0.0083, 0.0000, 0.0134],
        [0.0061, 0.0000, 0.0033,  ..., 0.0083, 0.0000, 0.0134],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38210.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.5178e-03, 0.0000e+00, 2.6031e-02,  ..., 3.8936e-05, 0.0000e+00,
         5.0943e-02],
        [9.8046e-03, 0.0000e+00, 2.0385e-02,  ..., 5.0902e-05, 2.8618e-04,
         3.8977e-02],
        [1.1521e-02, 0.0000e+00, 1.5527e-02,  ..., 0.0000e+00, 3.2583e-03,
         2.8996e-02],
        ...,
        [1.6347e-02, 0.0000e+00, 1.5537e-03,  ..., 0.0000e+00, 1.9802e-02,
         2.9638e-04],
        [1.6346e-02, 0.0000e+00, 1.5536e-03,  ..., 0.0000e+00, 1.9801e-02,
         2.9636e-04],
        [1.6346e-02, 0.0000e+00, 1.5536e-03,  ..., 0.0000e+00, 1.9801e-02,
         2.9637e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(270921.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3287.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.5365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(216.2235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.6613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0592],
        [ 0.0077],
        [-0.0614],
        ...,
        [-0.1857],
        [-0.1849],
        [-0.1846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66729.7578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0001],
        [1.0010],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366193.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0002],
        [1.0010],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366205.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [ 0.0186, -0.0104,  0.0112,  ...,  0.0108, -0.0117,  0.0354],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        ...,
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(856.2724, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0233, device='cuda:0')



h[100].sum tensor(-27.9384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8625, device='cuda:0')



h[200].sum tensor(28.8264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0289, 0.0000, 0.0172,  ..., 0.0196, 0.0000, 0.0554],
        [0.0364, 0.0000, 0.0218,  ..., 0.0234, 0.0000, 0.0693],
        [0.0785, 0.0000, 0.0471,  ..., 0.0453, 0.0000, 0.1490],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49495.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0000, 0.0575,  ..., 0.0282, 0.0000, 0.1324],
        [0.0000, 0.0000, 0.0759,  ..., 0.0412, 0.0000, 0.1765],
        [0.0000, 0.0000, 0.0923,  ..., 0.0536, 0.0000, 0.2167],
        ...,
        [0.0167, 0.0000, 0.0016,  ..., 0.0000, 0.0201, 0.0005],
        [0.0167, 0.0000, 0.0016,  ..., 0.0000, 0.0201, 0.0005],
        [0.0167, 0.0000, 0.0016,  ..., 0.0000, 0.0201, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326205.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3251.4009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(305.6837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(286.7246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(213.1700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0159],
        [ 0.0221],
        [ 0.0143],
        ...,
        [-0.1855],
        [-0.1847],
        [-0.1844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61509.6758, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0002],
        [1.0010],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366205.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0002],
        [1.0011],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366217.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0310, -0.0169,  0.0187,  ...,  0.0169, -0.0191,  0.0581],
        [ 0.0293, -0.0160,  0.0177,  ...,  0.0161, -0.0181,  0.0549],
        [ 0.0296, -0.0162,  0.0179,  ...,  0.0163, -0.0183,  0.0556],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0005],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(700.3226, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0200, device='cuda:0')



h[100].sum tensor(-23.5731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7395, device='cuda:0')



h[200].sum tensor(24.1358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3837, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1168, 0.0000, 0.0705,  ..., 0.0642, 0.0000, 0.2194],
        [0.1276, 0.0000, 0.0770,  ..., 0.0696, 0.0000, 0.2391],
        [0.1250, 0.0000, 0.0755,  ..., 0.0683, 0.0000, 0.2345],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43462.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 2.3552e-01,  ..., 1.6302e-01, 0.0000e+00,
         5.6858e-01],
        [0.0000e+00, 0.0000e+00, 2.4301e-01,  ..., 1.6927e-01, 0.0000e+00,
         5.8755e-01],
        [0.0000e+00, 0.0000e+00, 2.3096e-01,  ..., 1.5900e-01, 0.0000e+00,
         5.5686e-01],
        ...,
        [1.7035e-02, 0.0000e+00, 1.7905e-03,  ..., 0.0000e+00, 2.0382e-02,
         5.1338e-04],
        [1.7033e-02, 0.0000e+00, 1.7904e-03,  ..., 0.0000e+00, 2.0381e-02,
         5.1336e-04],
        [1.7034e-02, 0.0000e+00, 1.7904e-03,  ..., 0.0000e+00, 2.0381e-02,
         5.1336e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(298389.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3317.9463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.9637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(247.1349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(183.3706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0156],
        [-0.0133],
        [ 0.0016],
        ...,
        [-0.1868],
        [-0.1860],
        [-0.1857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58522.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0002],
        [1.0011],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366217.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0002],
        [1.0011],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366229.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0004],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(842.4248, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0230, device='cuda:0')



h[100].sum tensor(-26.7911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8517, device='cuda:0')



h[200].sum tensor(26.2911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.4131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47470.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0135, 0.0000, 0.0109,  ..., 0.0005, 0.0091, 0.0196],
        [0.0170, 0.0000, 0.0042,  ..., 0.0000, 0.0168, 0.0047],
        [0.0174, 0.0000, 0.0031,  ..., 0.0000, 0.0189, 0.0025],
        ...,
        [0.0173, 0.0000, 0.0021,  ..., 0.0000, 0.0205, 0.0005],
        [0.0173, 0.0000, 0.0021,  ..., 0.0000, 0.0205, 0.0005],
        [0.0173, 0.0000, 0.0021,  ..., 0.0000, 0.0205, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(317868.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3382.6218, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.6040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(278.9195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(203.2845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0192],
        [-0.0273],
        [-0.0878],
        ...,
        [-0.1865],
        [-0.1856],
        [-0.1854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-68273.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0002],
        [1.0011],
        ...,
        [1.0006],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366229.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0012],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366241., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [ 0.0077, -0.0045,  0.0045,  ...,  0.0053, -0.0051,  0.0153],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(524.9216, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-18.2712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5760, device='cuda:0')



h[200].sum tensor(17.7209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0140, 0.0000, 0.0084,  ..., 0.0116, 0.0000, 0.0270],
        [0.0077, 0.0000, 0.0045,  ..., 0.0084, 0.0000, 0.0153],
        [0.0234, 0.0000, 0.0139,  ..., 0.0167, 0.0000, 0.0454],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36866.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0477,  ..., 0.0179, 0.0000, 0.1045],
        [0.0000, 0.0000, 0.0452,  ..., 0.0158, 0.0000, 0.0981],
        [0.0000, 0.0000, 0.0615,  ..., 0.0273, 0.0000, 0.1371],
        ...,
        [0.0176, 0.0000, 0.0022,  ..., 0.0000, 0.0207, 0.0004],
        [0.0176, 0.0000, 0.0022,  ..., 0.0000, 0.0207, 0.0004],
        [0.0176, 0.0000, 0.0022,  ..., 0.0000, 0.0207, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(272743.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3538.0220, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(264.2247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(210.1828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(153.0106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0348],
        [ 0.0318],
        [ 0.0271],
        ...,
        [-0.1870],
        [-0.1862],
        [-0.1859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-68464.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0012],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366241., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0013],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366253.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097, -0.0056,  0.0058,  ...,  0.0063, -0.0063,  0.0191],
        [ 0.0043, -0.0027,  0.0024,  ...,  0.0036, -0.0031,  0.0090],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(956.3491, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0245, device='cuda:0')



h[100].sum tensor(-28.9883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9062, device='cuda:0')



h[200].sum tensor(25.3972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.3996, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0418, 0.0000, 0.0249,  ..., 0.0266, 0.0000, 0.0816],
        [0.0254, 0.0000, 0.0152,  ..., 0.0176, 0.0000, 0.0490],
        [0.0043, 0.0000, 0.0024,  ..., 0.0067, 0.0000, 0.0090],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50423., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0908,  ..., 0.0435, 0.0000, 0.2023],
        [0.0000, 0.0000, 0.0679,  ..., 0.0292, 0.0000, 0.1495],
        [0.0004, 0.0000, 0.0441,  ..., 0.0131, 0.0000, 0.0937],
        ...,
        [0.0179, 0.0000, 0.0023,  ..., 0.0000, 0.0211, 0.0003],
        [0.0179, 0.0000, 0.0023,  ..., 0.0000, 0.0211, 0.0003],
        [0.0179, 0.0000, 0.0023,  ..., 0.0000, 0.0211, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(328954.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3425.0181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.0168, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(295.8585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(212.1768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0710],
        [-0.0313],
        [ 0.0026],
        ...,
        [-0.1886],
        [-0.1877],
        [-0.1875]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-60042.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0013],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366253.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0013],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366265.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(554.1218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-18.5651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5846, device='cuda:0')



h[200].sum tensor(14.5681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36467.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0076, 0.0000, 0.0330,  ..., 0.0134, 0.0050, 0.0716],
        [0.0132, 0.0000, 0.0109,  ..., 0.0015, 0.0104, 0.0197],
        [0.0177, 0.0000, 0.0044,  ..., 0.0000, 0.0177, 0.0047],
        ...,
        [0.0182, 0.0000, 0.0023,  ..., 0.0000, 0.0216, 0.0003],
        [0.0182, 0.0000, 0.0023,  ..., 0.0000, 0.0216, 0.0003],
        [0.0182, 0.0000, 0.0023,  ..., 0.0000, 0.0216, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266242.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3714.9272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(266.1984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(207.4728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(149.0428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0131],
        [ 0.0202],
        [ 0.0301],
        ...,
        [-0.1898],
        [-0.1892],
        [-0.1891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66680.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0013],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366265.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0014],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366278.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [ 0.0051, -0.0032,  0.0030,  ...,  0.0040, -0.0036,  0.0107],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(685.4771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-21.8341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6947, device='cuda:0')



h[200].sum tensor(15.4541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5730, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0000, 0.0064,  ..., 0.0099, 0.0000, 0.0209],
        [0.0051, 0.0000, 0.0030,  ..., 0.0071, 0.0000, 0.0107],
        [0.0115, 0.0000, 0.0067,  ..., 0.0107, 0.0000, 0.0236],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40554.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0067, 0.0000, 0.0277,  ..., 0.0040, 0.0000, 0.0561],
        [0.0062, 0.0000, 0.0293,  ..., 0.0028, 0.0000, 0.0583],
        [0.0035, 0.0000, 0.0417,  ..., 0.0081, 0.0000, 0.0848],
        ...,
        [0.0185, 0.0000, 0.0022,  ..., 0.0000, 0.0223, 0.0004],
        [0.0185, 0.0000, 0.0022,  ..., 0.0000, 0.0223, 0.0004],
        [0.0185, 0.0000, 0.0022,  ..., 0.0000, 0.0223, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(285682.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3657.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.2393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(235.0964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(168.0517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0579],
        [ 0.0646],
        [ 0.0711],
        ...,
        [-0.1946],
        [-0.1937],
        [-0.1935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69719., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0003],
        [1.0014],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366278.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0014],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366291.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0010,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(592.9914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-19.5011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6184, device='cuda:0')



h[200].sum tensor(11.4527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0082, 0.0000, 0.0048,  ..., 0.0087, 0.0000, 0.0163],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37758.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8096e-02, 0.0000e+00, 4.8293e-03,  ..., 0.0000e+00, 1.7932e-02,
         6.0725e-03],
        [1.5729e-02, 0.0000e+00, 8.4223e-03,  ..., 4.4046e-05, 1.2680e-02,
         1.4194e-02],
        [9.8859e-03, 0.0000e+00, 2.1707e-02,  ..., 4.0499e-03, 4.6403e-03,
         4.3599e-02],
        ...,
        [1.9002e-02, 0.0000e+00, 2.0371e-03,  ..., 0.0000e+00, 2.3142e-02,
         4.3767e-04],
        [1.9001e-02, 0.0000e+00, 2.0370e-03,  ..., 0.0000e+00, 2.3141e-02,
         4.3767e-04],
        [1.9001e-02, 0.0000e+00, 2.0370e-03,  ..., 0.0000e+00, 2.3141e-02,
         4.3767e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276349.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3821.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.2711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.3947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(154.7507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0347],
        [-0.0646],
        [-0.0766],
        ...,
        [-0.1987],
        [-0.1978],
        [-0.1976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66141.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0014],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366291.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0015],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366305.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [ 0.0112, -0.0063,  0.0067,  ...,  0.0070, -0.0071,  0.0219],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(568.4333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0164, device='cuda:0')



h[100].sum tensor(-18.9901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6054, device='cuda:0')



h[200].sum tensor(9.2374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9555, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0000, 0.0091,  ..., 0.0127, 0.0000, 0.0310],
        [0.0090, 0.0000, 0.0053,  ..., 0.0091, 0.0000, 0.0178],
        [0.0531, 0.0000, 0.0317,  ..., 0.0323, 0.0000, 0.1026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38786.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0000, 0.0396,  ..., 0.0088, 0.0000, 0.0818],
        [0.0033, 0.0000, 0.0393,  ..., 0.0107, 0.0000, 0.0830],
        [0.0000, 0.0000, 0.0676,  ..., 0.0294, 0.0000, 0.1483],
        ...,
        [0.0195, 0.0000, 0.0018,  ..., 0.0000, 0.0241, 0.0004],
        [0.0195, 0.0000, 0.0018,  ..., 0.0000, 0.0241, 0.0004],
        [0.0195, 0.0000, 0.0018,  ..., 0.0000, 0.0241, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(289763.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3880.6531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(276.1586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.2946, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(159.4009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0774],
        [ 0.0580],
        [ 0.0315],
        ...,
        [-0.2046],
        [-0.2037],
        [-0.2034]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66310.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0015],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366305.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(569.0380, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0016],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366318.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [ 0.0065, -0.0039,  0.0038,  ...,  0.0047, -0.0044,  0.0133],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1647.7833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.6050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0389, device='cuda:0')



h[100].sum tensor(-46.0978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.4413, device='cuda:0')



h[200].sum tensor(30.9750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0097, 0.0000, 0.0055,  ..., 0.0100, 0.0000, 0.0205],
        [0.0241, 0.0000, 0.0141,  ..., 0.0175, 0.0000, 0.0481],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75648.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0138, 0.0000, 0.0157,  ..., 0.0013, 0.0088, 0.0294],
        [0.0070, 0.0000, 0.0373,  ..., 0.0089, 0.0014, 0.0759],
        [0.0012, 0.0000, 0.0655,  ..., 0.0229, 0.0000, 0.1385],
        ...,
        [0.0200, 0.0000, 0.0015,  ..., 0.0000, 0.0251, 0.0003],
        [0.0200, 0.0000, 0.0015,  ..., 0.0000, 0.0251, 0.0003],
        [0.0200, 0.0000, 0.0015,  ..., 0.0000, 0.0251, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463993.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3659.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(416.1378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(458.9219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(326.9069, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0655],
        [ 0.0677],
        [ 0.0684],
        ...,
        [-0.2106],
        [-0.2096],
        [-0.2093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59019.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0004],
        [1.0016],
        ...,
        [1.0007],
        [1.0005],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366318.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0004],
        [1.0016],
        ...,
        [1.0007],
        [1.0006],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366331.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(762.4478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0204, device='cuda:0')



h[100].sum tensor(-23.9716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7547, device='cuda:0')



h[200].sum tensor(11.1007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47925.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8073e-02, 0.0000e+00, 8.0326e-03,  ..., 0.0000e+00, 1.3873e-02,
         1.3107e-02],
        [1.8710e-02, 0.0000e+00, 6.2310e-03,  ..., 0.0000e+00, 1.7099e-02,
         9.5895e-03],
        [1.9956e-02, 0.0000e+00, 2.8802e-03,  ..., 0.0000e+00, 2.3157e-02,
         3.0606e-03],
        ...,
        [2.0457e-02, 0.0000e+00, 1.3381e-03,  ..., 0.0000e+00, 2.5854e-02,
         8.3563e-05],
        [2.0456e-02, 0.0000e+00, 1.3381e-03,  ..., 0.0000e+00, 2.5853e-02,
         8.3574e-05],
        [2.0456e-02, 0.0000e+00, 1.3381e-03,  ..., 0.0000e+00, 2.5853e-02,
         8.3575e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347724.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3917.9473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.2723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(287.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(203.0550, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0024],
        [-0.0259],
        [-0.0562],
        ...,
        [-0.2175],
        [-0.2165],
        [-0.2162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86066.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0004],
        [1.0016],
        ...,
        [1.0007],
        [1.0006],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366331.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0005],
        [1.0017],
        ...,
        [1.0007],
        [1.0006],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366344.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0141, -0.0078,  0.0084,  ...,  0.0085, -0.0088,  0.0271],
        [ 0.0066, -0.0039,  0.0039,  ...,  0.0048, -0.0044,  0.0135],
        [ 0.0066, -0.0039,  0.0039,  ...,  0.0048, -0.0044,  0.0135],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(688.2598, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0190, device='cuda:0')



h[100].sum tensor(-22.1021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7035, device='cuda:0')



h[200].sum tensor(9.4289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0359, 0.0000, 0.0213,  ..., 0.0240, 0.0000, 0.0713],
        [0.0373, 0.0000, 0.0221,  ..., 0.0247, 0.0000, 0.0737],
        [0.0119, 0.0000, 0.0069,  ..., 0.0112, 0.0000, 0.0244],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43433.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0637,  ..., 0.0237, 0.0000, 0.1358],
        [0.0000, 0.0000, 0.0574,  ..., 0.0201, 0.0000, 0.1219],
        [0.0060, 0.0000, 0.0364,  ..., 0.0080, 0.0000, 0.0746],
        ...,
        [0.0208, 0.0000, 0.0011,  ..., 0.0000, 0.0267, 0.0000],
        [0.0208, 0.0000, 0.0011,  ..., 0.0000, 0.0267, 0.0000],
        [0.0208, 0.0000, 0.0011,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(316817.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4047.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.5146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(257.1756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(179.6586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1046],
        [ 0.1016],
        [ 0.0943],
        ...,
        [-0.2248],
        [-0.2238],
        [-0.2237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77413.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0005],
        [1.0017],
        ...,
        [1.0007],
        [1.0006],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366344.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0005],
        [1.0017],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366356.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0145, -0.0079,  0.0087,  ...,  0.0087, -0.0090,  0.0279],
        [ 0.0098, -0.0055,  0.0058,  ...,  0.0064, -0.0062,  0.0192],
        [ 0.0141, -0.0078,  0.0085,  ...,  0.0086, -0.0088,  0.0273],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(663.2943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-21.3636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6857, device='cuda:0')



h[200].sum tensor(8.8850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4088, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0345, 0.0000, 0.0205,  ..., 0.0234, 0.0000, 0.0688],
        [0.0392, 0.0000, 0.0233,  ..., 0.0257, 0.0000, 0.0772],
        [0.0240, 0.0000, 0.0142,  ..., 0.0178, 0.0000, 0.0481],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41657.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0861,  ..., 0.0356, 0.0000, 0.1849],
        [0.0000, 0.0000, 0.0836,  ..., 0.0350, 0.0000, 0.1803],
        [0.0000, 0.0000, 0.0700,  ..., 0.0265, 0.0000, 0.1496],
        ...,
        [0.0210, 0.0000, 0.0010,  ..., 0.0000, 0.0273, 0.0000],
        [0.0210, 0.0000, 0.0010,  ..., 0.0000, 0.0273, 0.0000],
        [0.0210, 0.0000, 0.0010,  ..., 0.0000, 0.0273, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(306991.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4114.7842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(297.4382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.5062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(170.6352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0892],
        [ 0.0942],
        [ 0.0995],
        ...,
        [-0.2305],
        [-0.2295],
        [-0.2291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82138.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0005],
        [1.0017],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366356.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0005],
        [1.0018],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366369.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(566.2023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-18.6246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5885, device='cuda:0')



h[200].sum tensor(6.9352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38119.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0211, 0.0000, 0.0009,  ..., 0.0000, 0.0273, 0.0000],
        [0.0211, 0.0000, 0.0009,  ..., 0.0000, 0.0273, 0.0000],
        [0.0212, 0.0000, 0.0009,  ..., 0.0000, 0.0274, 0.0000],
        ...,
        [0.0212, 0.0000, 0.0009,  ..., 0.0000, 0.0274, 0.0000],
        [0.0212, 0.0000, 0.0009,  ..., 0.0000, 0.0274, 0.0000],
        [0.0212, 0.0000, 0.0009,  ..., 0.0000, 0.0274, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(294239.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4172.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(282.1841, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.3255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(155.9582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2788],
        [-0.2684],
        [-0.2434],
        ...,
        [-0.2320],
        [-0.2309],
        [-0.2306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90291.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0005],
        [1.0018],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366369.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0019],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366382.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(746.4722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0195, device='cuda:0')



h[100].sum tensor(-22.5641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7233, device='cuda:0')



h[200].sum tensor(11.0294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0898, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41526.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0211, 0.0000, 0.0009,  ..., 0.0000, 0.0271, 0.0001],
        [0.0211, 0.0000, 0.0009,  ..., 0.0000, 0.0271, 0.0001],
        [0.0183, 0.0000, 0.0057,  ..., 0.0007, 0.0194, 0.0108],
        ...,
        [0.0211, 0.0000, 0.0009,  ..., 0.0000, 0.0271, 0.0001],
        [0.0211, 0.0000, 0.0009,  ..., 0.0000, 0.0271, 0.0001],
        [0.0211, 0.0000, 0.0009,  ..., 0.0000, 0.0271, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(301136.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4185.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.2947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(239.5357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(172.6332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0798],
        [-0.1117],
        [-0.1118],
        ...,
        [-0.2306],
        [-0.2296],
        [-0.2292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88703.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0019],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366382.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0019],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366395.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1103.4956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0264, device='cuda:0')



h[100].sum tensor(-30.7612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9783, device='cuda:0')



h[200].sum tensor(19.0728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.7057, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54776.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0000, 0.0136,  ..., 0.0000, 0.0050, 0.0256],
        [0.0177, 0.0000, 0.0094,  ..., 0.0000, 0.0111, 0.0171],
        [0.0188, 0.0000, 0.0064,  ..., 0.0000, 0.0165, 0.0113],
        ...,
        [0.0209, 0.0000, 0.0010,  ..., 0.0000, 0.0268, 0.0005],
        [0.0209, 0.0000, 0.0010,  ..., 0.0000, 0.0268, 0.0005],
        [0.0209, 0.0000, 0.0010,  ..., 0.0000, 0.0268, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(365356.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3852.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.9679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.7122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(230.8396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0864],
        [ 0.0581],
        [ 0.0240],
        ...,
        [-0.2267],
        [-0.2257],
        [-0.2253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66338.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0019],
        ...,
        [1.0008],
        [1.0006],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366395.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0020],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366408.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0198, -0.0106,  0.0120,  ...,  0.0114, -0.0120,  0.0378],
        [ 0.0049, -0.0030,  0.0029,  ...,  0.0040, -0.0034,  0.0104],
        [ 0.0172, -0.0093,  0.0104,  ...,  0.0101, -0.0105,  0.0330],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0011,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1468.6552, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(5.5345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0335, device='cuda:0')



h[100].sum tensor(-39.1919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.2415, device='cuda:0')



h[200].sum tensor(27.7835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.4683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0280, 0.0000, 0.0168,  ..., 0.0199, 0.0000, 0.0559],
        [0.0721, 0.0000, 0.0436,  ..., 0.0422, 0.0000, 0.1384],
        [0.0446, 0.0000, 0.0268,  ..., 0.0285, 0.0000, 0.0878],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69831.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0887,  ..., 0.0419, 0.0000, 0.1940],
        [0.0000, 0.0000, 0.1318,  ..., 0.0722, 0.0000, 0.2943],
        [0.0000, 0.0000, 0.1362,  ..., 0.0745, 0.0000, 0.3036],
        ...,
        [0.0204, 0.0000, 0.0010,  ..., 0.0000, 0.0265, 0.0006],
        [0.0184, 0.0000, 0.0044,  ..., 0.0000, 0.0190, 0.0080],
        [0.0122, 0.0000, 0.0144,  ..., 0.0022, 0.0080, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456546.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3624.4546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(412.0750, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(409.4207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(301.3421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1099],
        [ 0.1099],
        [ 0.1077],
        ...,
        [-0.1547],
        [-0.0907],
        [-0.0303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65482.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0006],
        [1.0020],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366408.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0020],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366420.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0012,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1044.1710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0245, device='cuda:0')



h[100].sum tensor(-28.4937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9056, device='cuda:0')



h[200].sum tensor(19.7305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.3887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51550.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0197, 0.0000, 0.0026,  ..., 0.0000, 0.0237, 0.0033],
        [0.0199, 0.0000, 0.0013,  ..., 0.0000, 0.0259, 0.0010],
        [0.0199, 0.0000, 0.0010,  ..., 0.0000, 0.0264, 0.0005],
        ...,
        [0.0199, 0.0000, 0.0010,  ..., 0.0000, 0.0264, 0.0005],
        [0.0199, 0.0000, 0.0010,  ..., 0.0000, 0.0264, 0.0005],
        [0.0199, 0.0000, 0.0010,  ..., 0.0000, 0.0264, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355777.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3593.5515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.4617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(289.1711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(217.2186, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1201],
        [-0.1960],
        [-0.2574],
        ...,
        [-0.2313],
        [-0.2303],
        [-0.2300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66713.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0020],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366420.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0021],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366433.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(710.4286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0175, device='cuda:0')



h[100].sum tensor(-20.1359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6468, device='cuda:0')



h[200].sum tensor(13.7415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7057, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41765.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0194, 0.0000, 0.0012,  ..., 0.0000, 0.0262, 0.0004],
        [0.0194, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004],
        [0.0195, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004],
        ...,
        [0.0195, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004],
        [0.0195, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004],
        [0.0195, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312459.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3677.4595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.9846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(224.7400, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(172.1276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3289],
        [-0.3149],
        [-0.2882],
        ...,
        [-0.2355],
        [-0.2344],
        [-0.2341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-75986.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0021],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366433.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0021],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366433.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(882.8632, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0210, device='cuda:0')



h[100].sum tensor(-24.2923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7776, device='cuda:0')



h[200].sum tensor(17.3997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0727, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0049, 0.0000, 0.0029,  ..., 0.0076, 0.0000, 0.0105],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45407.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0191, 0.0000, 0.0021,  ..., 0.0000, 0.0245, 0.0022],
        [0.0164, 0.0000, 0.0080,  ..., 0.0000, 0.0131, 0.0142],
        [0.0108, 0.0000, 0.0205,  ..., 0.0021, 0.0049, 0.0396],
        ...,
        [0.0195, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004],
        [0.0195, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004],
        [0.0195, 0.0000, 0.0012,  ..., 0.0000, 0.0263, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322644.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3624.6963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(330.9024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(249.7704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(189.4122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3277e-02],
        [-2.6271e-05],
        [ 5.0487e-02],
        ...,
        [-2.3545e-01],
        [-2.3441e-01],
        [-2.3409e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81212.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0021],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366433.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0022],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366445.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0129, -0.0071,  0.0078,  ...,  0.0080, -0.0080,  0.0252],
        [ 0.0174, -0.0093,  0.0106,  ...,  0.0102, -0.0106,  0.0334],
        [ 0.0081, -0.0046,  0.0049,  ...,  0.0057, -0.0052,  0.0164],
        ...,
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(524.6206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.8060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0134, device='cuda:0')



h[100].sum tensor(-15.5012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4955, device='cuda:0')



h[200].sum tensor(10.8139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0789, 0.0000, 0.0481,  ..., 0.0457, 0.0000, 0.1511],
        [0.0653, 0.0000, 0.0398,  ..., 0.0390, 0.0000, 0.1261],
        [0.0548, 0.0000, 0.0334,  ..., 0.0333, 0.0000, 0.1052],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35707.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.9022e-01,  ..., 1.1779e-01, 0.0000e+00,
         4.3351e-01],
        [0.0000e+00, 0.0000e+00, 1.7278e-01,  ..., 1.0500e-01, 0.0000e+00,
         3.9237e-01],
        [0.0000e+00, 0.0000e+00, 1.3660e-01,  ..., 8.0222e-02, 0.0000e+00,
         3.0896e-01],
        ...,
        [1.9165e-02, 0.0000e+00, 1.3476e-03,  ..., 0.0000e+00, 2.6615e-02,
         3.1033e-04],
        [1.9165e-02, 0.0000e+00, 1.3476e-03,  ..., 0.0000e+00, 2.6615e-02,
         3.1037e-04],
        [1.9165e-02, 0.0000e+00, 1.3476e-03,  ..., 0.0000e+00, 2.6614e-02,
         3.1036e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287870.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3640.2495, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.7206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(185.2735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(143.8498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0773],
        [-0.0739],
        [-0.0633],
        ...,
        [-0.2399],
        [-0.2389],
        [-0.2386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77581.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0022],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366445.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0022],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366458.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0023,  0.0021,  ...,  0.0034, -0.0026,  0.0080],
        [ 0.0099, -0.0055,  0.0060,  ...,  0.0066, -0.0063,  0.0198],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1246.4209, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.7007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0287, device='cuda:0')



h[100].sum tensor(-32.7219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0629, device='cuda:0')



h[200].sum tensor(26.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0351, 0.0000, 0.0213,  ..., 0.0241, 0.0000, 0.0707],
        [0.0142, 0.0000, 0.0086,  ..., 0.0133, 0.0000, 0.0308],
        [0.0126, 0.0000, 0.0076,  ..., 0.0120, 0.0000, 0.0263],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54251.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0590,  ..., 0.0191, 0.0000, 0.1219],
        [0.0016, 0.0000, 0.0459,  ..., 0.0109, 0.0000, 0.0928],
        [0.0059, 0.0000, 0.0344,  ..., 0.0072, 0.0019, 0.0693],
        ...,
        [0.0192, 0.0000, 0.0014,  ..., 0.0000, 0.0272, 0.0003],
        [0.0192, 0.0000, 0.0014,  ..., 0.0000, 0.0272, 0.0003],
        [0.0192, 0.0000, 0.0014,  ..., 0.0000, 0.0272, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359743.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3480.2515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(365.9056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(310.4268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(230.9014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0464],
        [ 0.0475],
        [ 0.0385],
        ...,
        [-0.2428],
        [-0.2354],
        [-0.2237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99037.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0022],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366458.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0022],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366458.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(626.3328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-17.8902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5730, device='cuda:0')



h[200].sum tensor(13.6461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3707, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37431.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.0000, 0.0034,  ..., 0.0000, 0.0235, 0.0041],
        [0.0190, 0.0000, 0.0017,  ..., 0.0000, 0.0267, 0.0007],
        [0.0192, 0.0000, 0.0014,  ..., 0.0000, 0.0272, 0.0003],
        ...,
        [0.0192, 0.0000, 0.0014,  ..., 0.0000, 0.0272, 0.0003],
        [0.0192, 0.0000, 0.0014,  ..., 0.0000, 0.0272, 0.0003],
        [0.0192, 0.0000, 0.0014,  ..., 0.0000, 0.0272, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(292883.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3597.5581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.6703, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(197.0960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(151.8754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1028],
        [-0.1370],
        [-0.1624],
        ...,
        [-0.2454],
        [-0.2443],
        [-0.2440]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87325.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0007],
        [1.0022],
        ...,
        [1.0009],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366458.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0023],
        ...,
        [1.0010],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366472.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0006,  ...,  0.0012,  0.0000, -0.0001],
        [ 0.0257, -0.0136,  0.0157,  ...,  0.0145, -0.0154,  0.0489],
        [ 0.0116, -0.0064,  0.0071,  ...,  0.0075, -0.0073,  0.0230],
        ...,
        [ 0.0087, -0.0049,  0.0053,  ...,  0.0060, -0.0055,  0.0175],
        [ 0.0058, -0.0034,  0.0035,  ...,  0.0046, -0.0039,  0.0122],
        [ 0.0087, -0.0049,  0.0053,  ...,  0.0060, -0.0055,  0.0175]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(795.5599, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0192, device='cuda:0')



h[100].sum tensor(-21.9271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7109, device='cuda:0')



h[200].sum tensor(17.4468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0857, 0.0000, 0.0523,  ..., 0.0493, 0.0000, 0.1637],
        [0.0316, 0.0000, 0.0192,  ..., 0.0220, 0.0000, 0.0627],
        [0.0865, 0.0000, 0.0528,  ..., 0.0497, 0.0000, 0.1653],
        ...,
        [0.0128, 0.0000, 0.0077,  ..., 0.0122, 0.0000, 0.0267],
        [0.0368, 0.0000, 0.0224,  ..., 0.0251, 0.0000, 0.0741],
        [0.0287, 0.0000, 0.0174,  ..., 0.0211, 0.0000, 0.0592]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40986.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1495,  ..., 0.0898, 0.0000, 0.3382],
        [0.0000, 0.0000, 0.1156,  ..., 0.0643, 0.0000, 0.2576],
        [0.0000, 0.0000, 0.1541,  ..., 0.0926, 0.0000, 0.3485],
        ...,
        [0.0047, 0.0000, 0.0337,  ..., 0.0076, 0.0000, 0.0684],
        [0.0000, 0.0000, 0.0534,  ..., 0.0190, 0.0000, 0.1117],
        [0.0000, 0.0000, 0.0585,  ..., 0.0216, 0.0000, 0.1227]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(304581.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3592.6157, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.1826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.3538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(168.5382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0451],
        [-0.0367],
        [-0.0484],
        ...,
        [-0.0429],
        [-0.0064],
        [ 0.0023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96597.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0023],
        ...,
        [1.0010],
        [1.0007],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366472.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0023],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366485.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000, -0.0006,  ...,  0.0013,  0.0000, -0.0001],
        [-0.0010,  0.0000, -0.0006,  ...,  0.0013,  0.0000, -0.0001],
        [-0.0010,  0.0000, -0.0006,  ...,  0.0013,  0.0000, -0.0001],
        ...,
        [-0.0010,  0.0000, -0.0006,  ...,  0.0013,  0.0000, -0.0001],
        [-0.0010,  0.0000, -0.0006,  ...,  0.0013,  0.0000, -0.0001],
        [-0.0010,  0.0000, -0.0006,  ...,  0.0013,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(628.4723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-18.0142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5786, device='cuda:0')



h[200].sum tensor(13.6887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38281.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0000, 0.0189,  ..., 0.0037, 0.0086, 0.0369],
        [0.0167, 0.0000, 0.0080,  ..., 0.0000, 0.0158, 0.0131],
        [0.0191, 0.0000, 0.0046,  ..., 0.0000, 0.0237, 0.0057],
        ...,
        [0.0197, 0.0000, 0.0018,  ..., 0.0000, 0.0288, 0.0002],
        [0.0197, 0.0000, 0.0018,  ..., 0.0000, 0.0288, 0.0002],
        [0.0197, 0.0000, 0.0018,  ..., 0.0000, 0.0288, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(301499.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3683.8628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.6342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(203.8365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(154.0231, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.8526e-03],
        [ 1.8261e-04],
        [ 9.0337e-03],
        ...,
        [-2.5414e-01],
        [-2.5299e-01],
        [-2.5263e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96561.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0023],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366485.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0024],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366499.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.7473e-04,  0.0000e+00, -6.7629e-04,  ...,  1.2840e-03,
          0.0000e+00, -7.3761e-05],
        [-9.7473e-04,  0.0000e+00, -6.7629e-04,  ...,  1.2840e-03,
          0.0000e+00, -7.3761e-05],
        [-9.7473e-04,  0.0000e+00, -6.7629e-04,  ...,  1.2840e-03,
          0.0000e+00, -7.3761e-05],
        ...,
        [-9.7473e-04,  0.0000e+00, -6.7629e-04,  ...,  1.2840e-03,
          0.0000e+00, -7.3761e-05],
        [-9.7473e-04,  0.0000e+00, -6.7629e-04,  ...,  1.2840e-03,
          0.0000e+00, -7.3761e-05],
        [-9.7473e-04,  0.0000e+00, -6.7629e-04,  ...,  1.2840e-03,
          0.0000e+00, -7.3761e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(743.8334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0181, device='cuda:0')



h[100].sum tensor(-20.8072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6694, device='cuda:0')



h[200].sum tensor(15.6572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42552.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0141, 0.0000, 0.0110,  ..., 0.0019, 0.0148, 0.0206],
        [0.0185, 0.0000, 0.0044,  ..., 0.0000, 0.0239, 0.0057],
        [0.0183, 0.0000, 0.0053,  ..., 0.0000, 0.0226, 0.0071],
        ...,
        [0.0202, 0.0000, 0.0019,  ..., 0.0000, 0.0297, 0.0002],
        [0.0202, 0.0000, 0.0019,  ..., 0.0000, 0.0297, 0.0002],
        [0.0202, 0.0000, 0.0019,  ..., 0.0000, 0.0297, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(318741.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3697.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.1601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(227.9643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(170.7236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1130],
        [-0.1074],
        [-0.0700],
        ...,
        [-0.2557],
        [-0.2549],
        [-0.2548]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79023.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0008],
        [1.0024],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366499.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0009],
        [1.0024],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366512.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.8904e-04,  0.0000e+00, -6.9486e-04,  ...,  1.3172e-03,
          0.0000e+00, -4.6824e-05],
        [-9.8904e-04,  0.0000e+00, -6.9486e-04,  ...,  1.3172e-03,
          0.0000e+00, -4.6824e-05],
        [-9.8904e-04,  0.0000e+00, -6.9486e-04,  ...,  1.3172e-03,
          0.0000e+00, -4.6824e-05],
        ...,
        [-9.8904e-04,  0.0000e+00, -6.9486e-04,  ...,  1.3172e-03,
          0.0000e+00, -4.6824e-05],
        [-9.8904e-04,  0.0000e+00, -6.9486e-04,  ...,  1.3172e-03,
          0.0000e+00, -4.6824e-05],
        [-9.8904e-04,  0.0000e+00, -6.9486e-04,  ...,  1.3172e-03,
          0.0000e+00, -4.6824e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(465.9517, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0125, device='cuda:0')



h[100].sum tensor(-14.3790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4644, device='cuda:0')



h[200].sum tensor(9.8887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4039, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0028, 0.0000, 0.0016,  ..., 0.0072, 0.0000, 0.0070],
        [0.0057, 0.0000, 0.0033,  ..., 0.0091, 0.0000, 0.0139],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32694.1621, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0159, 0.0000, 0.0135,  ..., 0.0000, 0.0101, 0.0226],
        [0.0126, 0.0000, 0.0214,  ..., 0.0007, 0.0046, 0.0384],
        [0.0095, 0.0000, 0.0283,  ..., 0.0024, 0.0017, 0.0524],
        ...,
        [0.0205, 0.0000, 0.0020,  ..., 0.0000, 0.0307, 0.0001],
        [0.0205, 0.0000, 0.0020,  ..., 0.0000, 0.0307, 0.0001],
        [0.0205, 0.0000, 0.0020,  ..., 0.0000, 0.0307, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276960.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3893.1731, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.2680, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(170.4522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(127.1492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0550],
        [ 0.0813],
        [ 0.0960],
        ...,
        [-0.2606],
        [-0.2594],
        [-0.2590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105517.6172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0009],
        [1.0024],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366512.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0009],
        [1.0025],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366526.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4689e-03, -5.2785e-03,  5.6999e-03,  ...,  6.5434e-03,
         -5.9953e-03,  1.9226e-02],
        [ 3.3215e-03, -2.1800e-03,  1.9370e-03,  ...,  3.4959e-03,
         -2.4760e-03,  7.9290e-03],
        [ 5.1438e-03, -3.0985e-03,  3.0525e-03,  ...,  4.3993e-03,
         -3.5193e-03,  1.1278e-02],
        ...,
        [-1.0036e-03,  0.0000e+00, -7.1043e-04,  ...,  1.3518e-03,
          0.0000e+00, -1.9207e-05],
        [-1.0036e-03,  0.0000e+00, -7.1043e-04,  ...,  1.3518e-03,
          0.0000e+00, -1.9207e-05],
        [-1.0036e-03,  0.0000e+00, -7.1043e-04,  ...,  1.3518e-03,
          0.0000e+00, -1.9207e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(740.3298, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-20.9669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6765, device='cuda:0')



h[200].sum tensor(15.5738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0000, 0.0085,  ..., 0.0140, 0.0000, 0.0318],
        [0.0351, 0.0000, 0.0211,  ..., 0.0248, 0.0000, 0.0718],
        [0.0153, 0.0000, 0.0092,  ..., 0.0140, 0.0000, 0.0318],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41219.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4224e-03, 0.0000e+00, 4.7603e-02,  ..., 1.3234e-02, 0.0000e+00,
         9.4181e-02],
        [0.0000e+00, 0.0000e+00, 6.0907e-02,  ..., 2.1666e-02, 0.0000e+00,
         1.2355e-01],
        [1.3031e-03, 0.0000e+00, 4.9559e-02,  ..., 1.6231e-02, 0.0000e+00,
         1.0009e-01],
        ...,
        [2.0822e-02, 0.0000e+00, 1.9616e-03,  ..., 0.0000e+00, 3.1746e-02,
         6.4641e-05],
        [2.0823e-02, 0.0000e+00, 1.9616e-03,  ..., 0.0000e+00, 3.1747e-02,
         6.4661e-05],
        [2.0822e-02, 0.0000e+00, 1.9616e-03,  ..., 0.0000e+00, 3.1747e-02,
         6.4664e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(315254.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3831.8757, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.3291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(226.9606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(165.0536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1355],
        [ 0.1408],
        [ 0.1380],
        ...,
        [-0.2637],
        [-0.2627],
        [-0.2626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108954.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0009],
        [1.0025],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366526.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0026],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366539.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0122e-03,  0.0000e+00, -7.0962e-04,  ...,  1.3801e-03,
          0.0000e+00,  1.4526e-06],
        [-1.0122e-03,  0.0000e+00, -7.0962e-04,  ...,  1.3801e-03,
          0.0000e+00,  1.4526e-06],
        [-1.0122e-03,  0.0000e+00, -7.0962e-04,  ...,  1.3801e-03,
          0.0000e+00,  1.4526e-06],
        ...,
        [-1.0122e-03,  0.0000e+00, -7.0962e-04,  ...,  1.3801e-03,
          0.0000e+00,  1.4526e-06],
        [-1.0122e-03,  0.0000e+00, -7.0962e-04,  ...,  1.3801e-03,
          0.0000e+00,  1.4526e-06],
        [-1.0122e-03,  0.0000e+00, -7.0962e-04,  ...,  1.3801e-03,
          0.0000e+00,  1.4526e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(689.6515, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-19.8260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6423, device='cuda:0')



h[200].sum tensor(15.1202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5197e-03, 0.0000e+00,
         5.8098e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5227e-03, 0.0000e+00,
         5.8129e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5268e-03, 0.0000e+00,
         5.8173e-06],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5547e-03, 0.0000e+00,
         5.8466e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5550e-03, 0.0000e+00,
         5.8469e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5549e-03, 0.0000e+00,
         5.8468e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41974.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7444e-02, 0.0000e+00, 1.0633e-02,  ..., 1.2372e-04, 1.6173e-02,
         1.6822e-02],
        [2.0047e-02, 0.0000e+00, 4.3482e-03,  ..., 0.0000e+00, 2.7984e-02,
         4.4434e-03],
        [2.0965e-02, 0.0000e+00, 2.0325e-03,  ..., 0.0000e+00, 3.2451e-02,
         4.3384e-05],
        ...,
        [2.0987e-02, 0.0000e+00, 2.0296e-03,  ..., 0.0000e+00, 3.2497e-02,
         3.9334e-05],
        [2.0988e-02, 0.0000e+00, 2.0296e-03,  ..., 0.0000e+00, 3.2499e-02,
         3.9354e-05],
        [2.0988e-02, 0.0000e+00, 2.0296e-03,  ..., 0.0000e+00, 3.2498e-02,
         3.9358e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(323624.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3844.3901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.6724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(226.7767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(165.3408, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0063],
        [-0.0712],
        [-0.1548],
        ...,
        [-0.2694],
        [-0.2682],
        [-0.2679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84315.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0026],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366539.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(1012.6768, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0026],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366552.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4466e-02, -7.7699e-03,  8.7925e-03,  ...,  9.0855e-03,
         -8.8296e-03,  2.8471e-02],
        [ 1.9643e-02, -1.0368e-02,  1.1963e-02,  ...,  1.1652e-02,
         -1.1782e-02,  3.7986e-02],
        [ 2.5609e-02, -1.3361e-02,  1.5616e-02,  ...,  1.4610e-02,
         -1.5184e-02,  4.8951e-02],
        ...,
        [-1.0175e-03,  0.0000e+00, -6.8980e-04,  ...,  1.4089e-03,
          0.0000e+00,  1.2123e-05],
        [-1.0175e-03,  0.0000e+00, -6.8980e-04,  ...,  1.4089e-03,
          0.0000e+00,  1.2123e-05],
        [-1.0175e-03,  0.0000e+00, -6.8980e-04,  ...,  1.4089e-03,
          0.0000e+00,  1.2123e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1031.0990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0241, device='cuda:0')



h[100].sum tensor(-27.6817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8917, device='cuda:0')



h[200].sum tensor(23.1644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.5424e-02, 0.0000e+00, 3.9798e-02,  ..., 4.0088e-02, 0.0000e+00,
         1.2777e-01],
        [7.1998e-02, 0.0000e+00, 4.3824e-02,  ..., 4.3352e-02, 0.0000e+00,
         1.3986e-01],
        [8.6432e-02, 0.0000e+00, 5.2663e-02,  ..., 5.0513e-02, 0.0000e+00,
         1.6640e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6712e-03, 0.0000e+00,
         4.8800e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6716e-03, 0.0000e+00,
         4.8803e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6714e-03, 0.0000e+00,
         4.8802e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50510.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.2093e-01,  ..., 7.0215e-02, 0.0000e+00,
         2.6758e-01],
        [0.0000e+00, 0.0000e+00, 1.4339e-01,  ..., 8.6818e-02, 0.0000e+00,
         3.2044e-01],
        [0.0000e+00, 0.0000e+00, 1.6348e-01,  ..., 1.0093e-01, 0.0000e+00,
         3.6707e-01],
        ...,
        [2.1023e-02, 0.0000e+00, 2.1342e-03,  ..., 0.0000e+00, 3.3034e-02,
         5.9713e-05],
        [2.1024e-02, 0.0000e+00, 2.1343e-03,  ..., 0.0000e+00, 3.3035e-02,
         5.9733e-05],
        [2.1023e-02, 0.0000e+00, 2.1343e-03,  ..., 0.0000e+00, 3.3035e-02,
         5.9734e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355214.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3757.1538, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(366.6839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(282.7764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(204.3121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0651],
        [ 0.0843],
        [ 0.1038],
        ...,
        [-0.2748],
        [-0.2735],
        [-0.2732]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88919.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0010],
        [1.0026],
        ...,
        [1.0010],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366552.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0027],
        ...,
        [1.0011],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366564.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0188e-03,  0.0000e+00, -6.6175e-04,  ...,  1.4344e-03,
          0.0000e+00,  1.2963e-05],
        [-1.0188e-03,  0.0000e+00, -6.6175e-04,  ...,  1.4344e-03,
          0.0000e+00,  1.2963e-05],
        [-1.0188e-03,  0.0000e+00, -6.6175e-04,  ...,  1.4344e-03,
          0.0000e+00,  1.2963e-05],
        ...,
        [-1.0188e-03,  0.0000e+00, -6.6175e-04,  ...,  1.4344e-03,
          0.0000e+00,  1.2963e-05],
        [-1.0188e-03,  0.0000e+00, -6.6175e-04,  ...,  1.4344e-03,
          0.0000e+00,  1.2963e-05],
        [-1.0188e-03,  0.0000e+00, -6.6175e-04,  ...,  1.4344e-03,
          0.0000e+00,  1.2963e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(776.4269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0189, device='cuda:0')



h[100].sum tensor(-21.5241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6987, device='cuda:0')



h[200].sum tensor(19.2994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6443, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1356e-02, 0.0000e+00, 6.8828e-03,  ..., 1.2377e-02, 0.0000e+00,
         2.4667e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7404e-03, 0.0000e+00,
         5.1875e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7449e-03, 0.0000e+00,
         5.1915e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7748e-03, 0.0000e+00,
         5.2185e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7752e-03, 0.0000e+00,
         5.2189e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7750e-03, 0.0000e+00,
         5.2187e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43759.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.2741e-03, 0.0000e+00, 3.4559e-02,  ..., 6.8730e-03, 1.2399e-03,
         6.5954e-02],
        [1.4367e-02, 0.0000e+00, 1.6982e-02,  ..., 1.4973e-03, 1.1812e-02,
         2.9661e-02],
        [1.8435e-02, 0.0000e+00, 8.4120e-03,  ..., 1.8416e-04, 2.1081e-02,
         1.2285e-02],
        ...,
        [2.0965e-02, 0.0000e+00, 2.2443e-03,  ..., 0.0000e+00, 3.3302e-02,
         8.9756e-05],
        [2.0967e-02, 0.0000e+00, 2.2444e-03,  ..., 0.0000e+00, 3.3304e-02,
         8.9776e-05],
        [2.0966e-02, 0.0000e+00, 2.2444e-03,  ..., 0.0000e+00, 3.3304e-02,
         8.9776e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(332440.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3787.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.7458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(237.9004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(174.5495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0778],
        [ 0.0255],
        [-0.0462],
        ...,
        [-0.2803],
        [-0.2791],
        [-0.2786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93814.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0027],
        ...,
        [1.0011],
        [1.0008],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366564.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0027],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366577.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0219e-03,  0.0000e+00, -6.4654e-04,  ...,  1.4526e-03,
          0.0000e+00,  2.1078e-05],
        [ 7.5951e-03, -4.3043e-03,  4.6353e-03,  ...,  5.7239e-03,
         -4.8939e-03,  1.5857e-02],
        [ 5.2095e-03, -3.1126e-03,  3.1730e-03,  ...,  4.5414e-03,
         -3.5390e-03,  1.1473e-02],
        ...,
        [-1.0219e-03,  0.0000e+00, -6.4654e-04,  ...,  1.4526e-03,
          0.0000e+00,  2.1078e-05],
        [-1.0219e-03,  0.0000e+00, -6.4654e-04,  ...,  1.4526e-03,
          0.0000e+00,  2.1078e-05],
        [-1.0219e-03,  0.0000e+00, -6.4654e-04,  ...,  1.4526e-03,
          0.0000e+00,  2.1078e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(788.9971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0189, device='cuda:0')



h[100].sum tensor(-21.5430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7006, device='cuda:0')



h[200].sum tensor(21.1607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8355e-02, 0.0000e+00, 1.1210e-02,  ..., 1.5921e-02, 0.0000e+00,
         3.7572e-02],
        [1.6321e-02, 0.0000e+00, 9.9639e-03,  ..., 1.4917e-02, 0.0000e+00,
         3.3837e-02],
        [3.8588e-02, 0.0000e+00, 2.3572e-02,  ..., 2.6974e-02, 0.0000e+00,
         7.8521e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8488e-03, 0.0000e+00,
         8.4869e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8492e-03, 0.0000e+00,
         8.4875e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8491e-03, 0.0000e+00,
         8.4873e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45241.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0000, 0.0440,  ..., 0.0168, 0.0000, 0.0905],
        [0.0008, 0.0000, 0.0477,  ..., 0.0178, 0.0000, 0.0974],
        [0.0000, 0.0000, 0.0674,  ..., 0.0291, 0.0000, 0.1403],
        ...,
        [0.0208, 0.0000, 0.0024,  ..., 0.0000, 0.0332, 0.0002],
        [0.0208, 0.0000, 0.0024,  ..., 0.0000, 0.0332, 0.0002],
        [0.0208, 0.0000, 0.0024,  ..., 0.0000, 0.0332, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343464.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3726.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.8706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.1570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(182.9306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0316],
        [ 0.0365],
        [ 0.0438],
        ...,
        [-0.2841],
        [-0.2828],
        [-0.2824]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97636.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0027],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366577.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0028],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366590.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0291e-03,  0.0000e+00, -6.4831e-04,  ...,  1.4539e-03,
          0.0000e+00,  5.0816e-05],
        [ 3.0461e-03, -2.0310e-03,  1.8501e-03,  ...,  3.4734e-03,
         -2.3098e-03,  7.5402e-03],
        [ 3.0461e-03, -2.0310e-03,  1.8501e-03,  ...,  3.4734e-03,
         -2.3098e-03,  7.5402e-03],
        ...,
        [-1.0291e-03,  0.0000e+00, -6.4831e-04,  ...,  1.4539e-03,
          0.0000e+00,  5.0816e-05],
        [-1.0291e-03,  0.0000e+00, -6.4831e-04,  ...,  1.4539e-03,
          0.0000e+00,  5.0816e-05],
        [-1.0291e-03,  0.0000e+00, -6.4831e-04,  ...,  1.4539e-03,
          0.0000e+00,  5.0816e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(849.6210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0200, device='cuda:0')



h[100].sum tensor(-22.5539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7402, device='cuda:0')



h[200].sum tensor(24.0828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0032,  ..., 0.0095, 0.0000, 0.0138],
        [0.0053, 0.0000, 0.0032,  ..., 0.0095, 0.0000, 0.0138],
        [0.0054, 0.0000, 0.0032,  ..., 0.0095, 0.0000, 0.0138],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44473.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6217e-02, 0.0000e+00, 1.6281e-02,  ..., 1.1402e-05, 9.9665e-03,
         2.6835e-02],
        [1.2642e-02, 0.0000e+00, 2.3615e-02,  ..., 2.2011e-03, 3.5839e-03,
         4.2023e-02],
        [6.9326e-03, 0.0000e+00, 3.8825e-02,  ..., 1.0363e-02, 0.0000e+00,
         7.6229e-02],
        ...,
        [2.0590e-02, 0.0000e+00, 2.6152e-03,  ..., 0.0000e+00, 3.2811e-02,
         6.2324e-04],
        [2.0591e-02, 0.0000e+00, 2.6154e-03,  ..., 0.0000e+00, 3.2813e-02,
         6.2331e-04],
        [2.0591e-02, 0.0000e+00, 2.6154e-03,  ..., 0.0000e+00, 3.2812e-02,
         6.2327e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334032.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3657.3674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.5616, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(237.1845, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(178.9073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1143],
        [-0.0406],
        [ 0.0042],
        ...,
        [-0.2851],
        [-0.2838],
        [-0.2834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93722.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0011],
        [1.0028],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366590.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0029],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366604.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        ...,
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.5182, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0153, device='cuda:0')



h[100].sum tensor(-17.4949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5674, device='cuda:0')



h[200].sum tensor(21.4858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37494.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0203, 0.0000, 0.0030,  ..., 0.0000, 0.0320, 0.0010],
        [0.0204, 0.0000, 0.0030,  ..., 0.0000, 0.0320, 0.0010],
        [0.0204, 0.0000, 0.0030,  ..., 0.0000, 0.0321, 0.0010],
        ...,
        [0.0205, 0.0000, 0.0030,  ..., 0.0000, 0.0322, 0.0010],
        [0.0205, 0.0000, 0.0030,  ..., 0.0000, 0.0322, 0.0010],
        [0.0205, 0.0000, 0.0030,  ..., 0.0000, 0.0322, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(303219.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3690.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.7525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(194.1953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(146.5124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4075],
        [-0.3995],
        [-0.3833],
        ...,
        [-0.2835],
        [-0.2823],
        [-0.2819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116601.1953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0029],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366604.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0029],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366604.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        ...,
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05],
        [-1.0413e-03,  0.0000e+00, -6.6866e-04,  ...,  1.4454e-03,
          0.0000e+00,  9.5467e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(788.9382, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-20.7393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6771, device='cuda:0')



h[200].sum tensor(24.3908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0000, 0.0089,  ..., 0.0140, 0.0000, 0.0310],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46038.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0000, 0.0400,  ..., 0.0109, 0.0009, 0.0785],
        [0.0141, 0.0000, 0.0177,  ..., 0.0022, 0.0130, 0.0307],
        [0.0189, 0.0000, 0.0085,  ..., 0.0000, 0.0220, 0.0114],
        ...,
        [0.0205, 0.0000, 0.0030,  ..., 0.0000, 0.0322, 0.0010],
        [0.0205, 0.0000, 0.0030,  ..., 0.0000, 0.0322, 0.0010],
        [0.0205, 0.0000, 0.0030,  ..., 0.0000, 0.0322, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359417.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3636.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.3031, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(247.2347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(185.2002, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0279],
        [-0.0354],
        [-0.1168],
        ...,
        [-0.2833],
        [-0.2820],
        [-0.2815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100393.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0012],
        [1.0029],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366604.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0012],
        [1.0029],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366618.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(703.4197, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-18.4730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6034, device='cuda:0')



h[200].sum tensor(23.3307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0070, 0.0000, 0.0042,  ..., 0.0097, 0.0000, 0.0153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41265.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0108, 0.0000, 0.0235,  ..., 0.0065, 0.0093, 0.0440],
        [0.0176, 0.0000, 0.0088,  ..., 0.0005, 0.0203, 0.0122],
        [0.0199, 0.0000, 0.0045,  ..., 0.0000, 0.0297, 0.0033],
        ...,
        [0.0205, 0.0000, 0.0035,  ..., 0.0000, 0.0319, 0.0012],
        [0.0205, 0.0000, 0.0035,  ..., 0.0000, 0.0319, 0.0012],
        [0.0205, 0.0000, 0.0035,  ..., 0.0000, 0.0319, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(326675.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3673.2410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(342.1712, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(211.5125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(157.8766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0669],
        [-0.1556],
        [-0.2372],
        ...,
        [-0.2797],
        [-0.2785],
        [-0.2782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86934.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0012],
        [1.0029],
        ...,
        [1.0011],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366618.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0030],
        ...,
        [1.0012],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366631.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(616.1946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0144, device='cuda:0')



h[100].sum tensor(-16.3232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5347, device='cuda:0')



h[200].sum tensor(22.0979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6773, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0007],
        [0.0161, 0.0000, 0.0097,  ..., 0.0148, 0.0000, 0.0342],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37829.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0186, 0.0000, 0.0070,  ..., 0.0004, 0.0245, 0.0084],
        [0.0120, 0.0000, 0.0183,  ..., 0.0048, 0.0148, 0.0325],
        [0.0052, 0.0000, 0.0464,  ..., 0.0173, 0.0013, 0.0926],
        ...,
        [0.0205, 0.0000, 0.0037,  ..., 0.0000, 0.0319, 0.0015],
        [0.0205, 0.0000, 0.0037,  ..., 0.0000, 0.0319, 0.0015],
        [0.0205, 0.0000, 0.0037,  ..., 0.0000, 0.0319, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(307640.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3675.2612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.1595, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(187.7465, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(140.6184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1572],
        [-0.0713],
        [-0.0017],
        ...,
        [-0.2773],
        [-0.2762],
        [-0.2758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86517.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0030],
        ...,
        [1.0012],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366631.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0031],
        ...,
        [1.0012],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366645.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042, -0.0026,  0.0025,  ...,  0.0041, -0.0030,  0.0099],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(618.2555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0144, device='cuda:0')



h[100].sum tensor(-16.3159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5315, device='cuda:0')



h[200].sum tensor(22.2556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0000, 0.0091,  ..., 0.0144, 0.0000, 0.0327],
        [0.0042, 0.0000, 0.0025,  ..., 0.0084, 0.0000, 0.0105],
        [0.0266, 0.0000, 0.0161,  ..., 0.0201, 0.0000, 0.0537],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37793.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0602,  ..., 0.0248, 0.0000, 0.1220],
        [0.0000, 0.0000, 0.0498,  ..., 0.0203, 0.0000, 0.1010],
        [0.0000, 0.0000, 0.0703,  ..., 0.0359, 0.0000, 0.1492],
        ...,
        [0.0205, 0.0000, 0.0038,  ..., 0.0000, 0.0322, 0.0017],
        [0.0205, 0.0000, 0.0038,  ..., 0.0000, 0.0322, 0.0017],
        [0.0205, 0.0000, 0.0038,  ..., 0.0000, 0.0322, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(305998.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3682.0776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.5695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(185.9019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(139.4351, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0802],
        [ 0.0660],
        [ 0.0508],
        ...,
        [-0.2760],
        [-0.2749],
        [-0.2749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84051.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0013],
        [1.0031],
        ...,
        [1.0012],
        [1.0009],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366645.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0032],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366658.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.5133, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-17.1057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5606, device='cuda:0')



h[200].sum tensor(22.5196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39539.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0204, 0.0000, 0.0034,  ..., 0.0000, 0.0327, 0.0018],
        [0.0200, 0.0000, 0.0041,  ..., 0.0000, 0.0312, 0.0033],
        [0.0168, 0.0000, 0.0106,  ..., 0.0011, 0.0189, 0.0165],
        ...,
        [0.0205, 0.0000, 0.0034,  ..., 0.0000, 0.0330, 0.0018],
        [0.0205, 0.0000, 0.0034,  ..., 0.0000, 0.0330, 0.0018],
        [0.0205, 0.0000, 0.0034,  ..., 0.0000, 0.0330, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(318244.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3615.8538, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.0690, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.2199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(151.1659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2556],
        [-0.1687],
        [-0.0564],
        ...,
        [-0.2868],
        [-0.2856],
        [-0.2853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101418.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0032],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366658.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(505.4583, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0033],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366672.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [ 0.0100, -0.0054,  0.0060,  ...,  0.0070, -0.0062,  0.0205],
        [ 0.0045, -0.0028,  0.0026,  ...,  0.0043, -0.0031,  0.0105],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(812.8058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-21.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6825, device='cuda:0')



h[200].sum tensor(24.9885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0135, 0.0000, 0.0080,  ..., 0.0138, 0.0000, 0.0295],
        [0.0146, 0.0000, 0.0086,  ..., 0.0149, 0.0000, 0.0335],
        [0.0600, 0.0000, 0.0363,  ..., 0.0379, 0.0000, 0.1190],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41628.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0065, 0.0000, 0.0415,  ..., 0.0138, 0.0027, 0.0829],
        [0.0020, 0.0000, 0.0684,  ..., 0.0299, 0.0000, 0.1421],
        [0.0000, 0.0000, 0.1195,  ..., 0.0665, 0.0000, 0.2604],
        ...,
        [0.0206, 0.0000, 0.0032,  ..., 0.0000, 0.0339, 0.0019],
        [0.0206, 0.0000, 0.0032,  ..., 0.0000, 0.0339, 0.0019],
        [0.0206, 0.0000, 0.0032,  ..., 0.0000, 0.0339, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(319399.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3656.1743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(349.7919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.8731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(159.7046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0767],
        [ 0.1044],
        [ 0.1187],
        ...,
        [-0.2926],
        [-0.2913],
        [-0.2909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89763.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0014],
        [1.0033],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366672.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0033],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366685.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(965.9835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0218, device='cuda:0')



h[100].sum tensor(-24.6130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8065, device='cuda:0')



h[200].sum tensor(27.2967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50821.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0207, 0.0000, 0.0028,  ..., 0.0000, 0.0347, 0.0019],
        [0.0207, 0.0000, 0.0028,  ..., 0.0000, 0.0347, 0.0019],
        [0.0208, 0.0000, 0.0028,  ..., 0.0000, 0.0348, 0.0019],
        ...,
        [0.0208, 0.0000, 0.0028,  ..., 0.0000, 0.0349, 0.0019],
        [0.0208, 0.0000, 0.0028,  ..., 0.0000, 0.0349, 0.0019],
        [0.0208, 0.0000, 0.0028,  ..., 0.0000, 0.0349, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372123.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3550.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(387.8833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(272.8011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(202.8232, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2525],
        [-0.2899],
        [-0.3222],
        ...,
        [-0.3008],
        [-0.2995],
        [-0.2991]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90380.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0033],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366685.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0034],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366698.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1154.5493, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0255, device='cuda:0')



h[100].sum tensor(-28.8833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9426, device='cuda:0')



h[200].sum tensor(30.6941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0029, 0.0000, 0.0016,  ..., 0.0084, 0.0000, 0.0078],
        [0.0058, 0.0000, 0.0033,  ..., 0.0104, 0.0000, 0.0152],
        [0.0029, 0.0000, 0.0016,  ..., 0.0084, 0.0000, 0.0078],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53738.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0142, 0.0000, 0.0209,  ..., 0.0006, 0.0033, 0.0380],
        [0.0132, 0.0000, 0.0237,  ..., 0.0004, 0.0020, 0.0434],
        [0.0133, 0.0000, 0.0228,  ..., 0.0011, 0.0015, 0.0418],
        ...,
        [0.0210, 0.0000, 0.0025,  ..., 0.0000, 0.0357, 0.0020],
        [0.0210, 0.0000, 0.0025,  ..., 0.0000, 0.0357, 0.0020],
        [0.0210, 0.0000, 0.0025,  ..., 0.0000, 0.0357, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383272.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3552.2012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(396.5694, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.9237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(219.8362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1214],
        [ 0.1215],
        [ 0.1198],
        ...,
        [-0.3085],
        [-0.3072],
        [-0.3068]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106883.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0015],
        [1.0034],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366698.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0016],
        [1.0035],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366711.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0708e-03,  0.0000e+00, -8.0977e-04,  ...,  1.6370e-03,
          0.0000e+00,  8.6550e-05],
        [-1.0708e-03,  0.0000e+00, -8.0977e-04,  ...,  1.6370e-03,
          0.0000e+00,  8.6550e-05],
        [-1.0708e-03,  0.0000e+00, -8.0977e-04,  ...,  1.6370e-03,
          0.0000e+00,  8.6550e-05],
        ...,
        [-1.0708e-03,  0.0000e+00, -8.0977e-04,  ...,  1.6370e-03,
          0.0000e+00,  8.6550e-05],
        [-1.0708e-03,  0.0000e+00, -8.0977e-04,  ...,  1.6370e-03,
          0.0000e+00,  8.6550e-05],
        [-1.0708e-03,  0.0000e+00, -8.0977e-04,  ...,  1.6370e-03,
          0.0000e+00,  8.6550e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(720.5117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0171, device='cuda:0')



h[100].sum tensor(-19.2207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6343, device='cuda:0')



h[200].sum tensor(21.8823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41991.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0065, 0.0000, 0.0298,  ..., 0.0108, 0.0074, 0.0622],
        [0.0112, 0.0000, 0.0190,  ..., 0.0045, 0.0085, 0.0383],
        [0.0138, 0.0000, 0.0141,  ..., 0.0023, 0.0131, 0.0281],
        ...,
        [0.0211, 0.0000, 0.0022,  ..., 0.0000, 0.0364, 0.0020],
        [0.0211, 0.0000, 0.0022,  ..., 0.0000, 0.0364, 0.0020],
        [0.0211, 0.0000, 0.0022,  ..., 0.0000, 0.0364, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334483., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3606.7542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.9215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(218.5460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(167.7996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0570],
        [ 0.0468],
        [ 0.0438],
        ...,
        [-0.3138],
        [-0.3123],
        [-0.3111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112165.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0016],
        [1.0035],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366711.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0016],
        [1.0036],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366724.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0211e-02, -1.5236e-02,  1.8353e-02,  ...,  1.7151e-02,
         -1.7374e-02,  5.7596e-02],
        [ 2.1391e-02, -1.0940e-02,  1.2949e-02,  ...,  1.2783e-02,
         -1.2475e-02,  4.1375e-02],
        [ 1.0781e-02, -5.7720e-03,  6.4471e-03,  ...,  7.5283e-03,
         -6.5819e-03,  2.1862e-02],
        ...,
        [-1.0686e-03,  0.0000e+00, -8.1443e-04,  ...,  1.6599e-03,
          0.0000e+00,  6.7781e-05],
        [-1.0686e-03,  0.0000e+00, -8.1443e-04,  ...,  1.6599e-03,
          0.0000e+00,  6.7781e-05],
        [-1.0686e-03,  0.0000e+00, -8.1443e-04,  ...,  1.6599e-03,
          0.0000e+00,  6.7781e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(707.2122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-18.8716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6165, device='cuda:0')



h[200].sum tensor(21.7849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0915, 0.0000, 0.0555,  ..., 0.0541, 0.0000, 0.1765],
        [0.0838, 0.0000, 0.0507,  ..., 0.0503, 0.0000, 0.1623],
        [0.0687, 0.0000, 0.0414,  ..., 0.0428, 0.0000, 0.1344],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40021.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1648,  ..., 0.1096, 0.0000, 0.3809],
        [0.0000, 0.0000, 0.1450,  ..., 0.0941, 0.0000, 0.3333],
        [0.0000, 0.0000, 0.1196,  ..., 0.0728, 0.0000, 0.2711],
        ...,
        [0.0211, 0.0000, 0.0021,  ..., 0.0000, 0.0370, 0.0021],
        [0.0211, 0.0000, 0.0021,  ..., 0.0000, 0.0370, 0.0021],
        [0.0211, 0.0000, 0.0021,  ..., 0.0000, 0.0370, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(323044.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3604.8013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(346.8145, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(208.0799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(160.0423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0365],
        [ 0.0559],
        [ 0.0702],
        ...,
        [-0.3215],
        [-0.3201],
        [-0.3197]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122216.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0016],
        [1.0036],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366724.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0017],
        [1.0037],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366737.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4067e-02, -1.7075e-02,  2.0703e-02,  ...,  1.9072e-02,
         -1.9477e-02,  6.4694e-02],
        [ 9.6324e-03, -5.2018e-03,  5.7318e-03,  ...,  6.9735e-03,
         -5.9333e-03,  1.9753e-02],
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        ...,
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(701.5438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-18.6844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6177, device='cuda:0')



h[200].sum tensor(21.6284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0547, 0.0000, 0.0328,  ..., 0.0359, 0.0000, 0.1088],
        [0.0668, 0.0000, 0.0404,  ..., 0.0414, 0.0000, 0.1291],
        [0.0259, 0.0000, 0.0155,  ..., 0.0206, 0.0000, 0.0518],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41042.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1228,  ..., 0.0755, 0.0000, 0.2791],
        [0.0000, 0.0000, 0.1225,  ..., 0.0760, 0.0000, 0.2792],
        [0.0000, 0.0000, 0.0915,  ..., 0.0529, 0.0000, 0.2057],
        ...,
        [0.0212, 0.0000, 0.0020,  ..., 0.0000, 0.0376, 0.0021],
        [0.0212, 0.0000, 0.0020,  ..., 0.0000, 0.0376, 0.0021],
        [0.0212, 0.0000, 0.0020,  ..., 0.0000, 0.0376, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(330885.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3611.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(354.8331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.2926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(163.8508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0231],
        [ 0.0442],
        [ 0.0588],
        ...,
        [-0.3246],
        [-0.3232],
        [-0.3228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115980.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0017],
        [1.0037],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366737.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0017],
        [1.0037],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366737.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        ...,
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05],
        [-1.0721e-03,  0.0000e+00, -8.2688e-04,  ...,  1.6732e-03,
          0.0000e+00,  6.3618e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1153.0525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0259, device='cuda:0')



h[100].sum tensor(-28.6687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9573, device='cuda:0')



h[200].sum tensor(30.6752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.3252, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54064.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0210, 0.0000, 0.0020,  ..., 0.0000, 0.0373, 0.0021],
        [0.0210, 0.0000, 0.0020,  ..., 0.0000, 0.0373, 0.0021],
        [0.0211, 0.0000, 0.0020,  ..., 0.0000, 0.0374, 0.0021],
        ...,
        [0.0212, 0.0000, 0.0020,  ..., 0.0000, 0.0376, 0.0021],
        [0.0212, 0.0000, 0.0020,  ..., 0.0000, 0.0376, 0.0021],
        [0.0212, 0.0000, 0.0020,  ..., 0.0000, 0.0376, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391987.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3521.4866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(406.4918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(295.0029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(223.3891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4275],
        [-0.4457],
        [-0.4554],
        ...,
        [-0.3232],
        [-0.3218],
        [-0.3213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104276.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0017],
        [1.0037],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366737.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0037],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366750.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1640e-02, -6.1648e-03,  6.9496e-03,  ...,  7.9598e-03,
         -7.0336e-03,  2.3465e-02],
        [ 2.5422e-02, -1.2846e-02,  1.5393e-02,  ...,  1.4781e-02,
         -1.4656e-02,  4.8815e-02],
        [ 2.0096e-02, -1.0264e-02,  1.2130e-02,  ...,  1.2145e-02,
         -1.1711e-02,  3.9018e-02],
        ...,
        [-1.0764e-03,  0.0000e+00, -8.4110e-04,  ...,  1.6655e-03,
          0.0000e+00,  7.3847e-05],
        [-1.0764e-03,  0.0000e+00, -8.4110e-04,  ...,  1.6655e-03,
          0.0000e+00,  7.3847e-05],
        [-1.0764e-03,  0.0000e+00, -8.4110e-04,  ...,  1.6655e-03,
          0.0000e+00,  7.3847e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1036.3401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0228, device='cuda:0')



h[100].sum tensor(-25.8527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8451, device='cuda:0')



h[200].sum tensor(29.0438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.2938, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0608, 0.0000, 0.0365,  ..., 0.0389, 0.0000, 0.1201],
        [0.0649, 0.0000, 0.0390,  ..., 0.0409, 0.0000, 0.1276],
        [0.0813, 0.0000, 0.0491,  ..., 0.0491, 0.0000, 0.1579],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51234.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1020,  ..., 0.0600, 0.0000, 0.2296],
        [0.0000, 0.0000, 0.1240,  ..., 0.0751, 0.0000, 0.2804],
        [0.0000, 0.0000, 0.1396,  ..., 0.0864, 0.0000, 0.3173],
        ...,
        [0.0211, 0.0000, 0.0021,  ..., 0.0000, 0.0377, 0.0021],
        [0.0211, 0.0000, 0.0021,  ..., 0.0000, 0.0377, 0.0021],
        [0.0211, 0.0000, 0.0021,  ..., 0.0000, 0.0377, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(373474.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3463.5889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(398.7436, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.7920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(208.6537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0301],
        [ 0.0152],
        [-0.0047],
        ...,
        [-0.3251],
        [-0.3237],
        [-0.3233]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97086.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0037],
        ...,
        [1.0012],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366750.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0038],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366764.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039, -0.0024,  0.0022,  ...,  0.0041, -0.0027,  0.0092],
        [ 0.0154, -0.0080,  0.0092,  ...,  0.0098, -0.0091,  0.0304],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1374.2968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0294, device='cuda:0')



h[100].sum tensor(-33.0512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0864, device='cuda:0')



h[200].sum tensor(35.8043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.6611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0484, 0.0000, 0.0289,  ..., 0.0327, 0.0000, 0.0975],
        [0.0184, 0.0000, 0.0107,  ..., 0.0173, 0.0000, 0.0403],
        [0.0312, 0.0000, 0.0183,  ..., 0.0242, 0.0000, 0.0658],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59383.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1013,  ..., 0.0554, 0.0000, 0.2238],
        [0.0003, 0.0000, 0.0797,  ..., 0.0398, 0.0000, 0.1732],
        [0.0008, 0.0000, 0.0704,  ..., 0.0339, 0.0000, 0.1522],
        ...,
        [0.0213, 0.0000, 0.0024,  ..., 0.0000, 0.0378, 0.0023],
        [0.0213, 0.0000, 0.0024,  ..., 0.0000, 0.0378, 0.0023],
        [0.0213, 0.0000, 0.0024,  ..., 0.0000, 0.0378, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406138.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3398.6462, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.5226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(323.6975, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(242.0206, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1507],
        [ 0.1505],
        [ 0.1403],
        ...,
        [-0.3216],
        [-0.3202],
        [-0.3198]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89052.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0038],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366764.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0039],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366777., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [ 0.0042, -0.0026,  0.0024,  ...,  0.0043, -0.0029,  0.0098],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1302.7373, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0275, device='cuda:0')



h[100].sum tensor(-31.1118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0172, device='cuda:0')



h[200].sum tensor(35.1498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.4092, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0005],
        [0.0042, 0.0000, 0.0024,  ..., 0.0092, 0.0000, 0.0102],
        [0.0157, 0.0000, 0.0092,  ..., 0.0154, 0.0000, 0.0334],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58918.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0169, 0.0000, 0.0107,  ..., 0.0016, 0.0207, 0.0196],
        [0.0102, 0.0000, 0.0255,  ..., 0.0062, 0.0097, 0.0506],
        [0.0039, 0.0000, 0.0451,  ..., 0.0156, 0.0000, 0.0929],
        ...,
        [0.0212, 0.0000, 0.0025,  ..., 0.0000, 0.0377, 0.0023],
        [0.0212, 0.0000, 0.0025,  ..., 0.0000, 0.0377, 0.0023],
        [0.0212, 0.0000, 0.0025,  ..., 0.0000, 0.0377, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411067.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3352.0994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(425.5618, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(319.4330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(238.9854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0445],
        [ 0.0481],
        [ 0.1100],
        ...,
        [-0.3211],
        [-0.3197],
        [-0.3193]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89959.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0039],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366777., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(512.1863, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0040],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366789.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(846.8804, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-20.8309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6805, device='cuda:0')



h[200].sum tensor(27.4747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3150, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47555.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0000, 0.0060,  ..., 0.0000, 0.0300, 0.0094],
        [0.0204, 0.0000, 0.0046,  ..., 0.0000, 0.0333, 0.0064],
        [0.0176, 0.0000, 0.0100,  ..., 0.0013, 0.0211, 0.0179],
        ...,
        [0.0209, 0.0000, 0.0023,  ..., 0.0000, 0.0377, 0.0021],
        [0.0209, 0.0000, 0.0023,  ..., 0.0000, 0.0377, 0.0021],
        [0.0209, 0.0000, 0.0023,  ..., 0.0000, 0.0377, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372392.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3339.7881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(387.0534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(244.0435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(188.1454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0200],
        [ 0.0135],
        [ 0.0307],
        ...,
        [-0.3238],
        [-0.3225],
        [-0.3221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88826.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0040],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366789.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0041],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366802.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(735.8562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-18.1273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6048, device='cuda:0')



h[200].sum tensor(26.2340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40411.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0144, 0.0000, 0.0113,  ..., 0.0036, 0.0192, 0.0226],
        [0.0174, 0.0000, 0.0080,  ..., 0.0008, 0.0244, 0.0142],
        [0.0135, 0.0000, 0.0162,  ..., 0.0031, 0.0149, 0.0312],
        ...,
        [0.0206, 0.0000, 0.0022,  ..., 0.0000, 0.0379, 0.0019],
        [0.0206, 0.0000, 0.0022,  ..., 0.0000, 0.0379, 0.0019],
        [0.0206, 0.0000, 0.0022,  ..., 0.0000, 0.0379, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(325340.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3368.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(358.1274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(200.3867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(157.5339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0352],
        [-0.0207],
        [ 0.0095],
        ...,
        [-0.3263],
        [-0.3241],
        [-0.3190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109095.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0041],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366802.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0042],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366814.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6863e-03, -2.7649e-03,  2.6906e-03,  ...,  4.4830e-03,
         -3.1589e-03,  1.0717e-02],
        [ 3.5041e-03, -2.1989e-03,  1.9662e-03,  ...,  3.8992e-03,
         -2.5121e-03,  8.5424e-03],
        [ 9.2782e-03, -4.9638e-03,  5.5043e-03,  ...,  6.7508e-03,
         -5.6710e-03,  1.9162e-02],
        ...,
        [-1.0878e-03,  0.0000e+00, -8.4757e-04,  ...,  1.6315e-03,
          0.0000e+00,  9.7090e-05],
        [-1.0878e-03,  0.0000e+00, -8.4757e-04,  ...,  1.6315e-03,
          0.0000e+00,  9.7090e-05],
        [-1.0878e-03,  0.0000e+00, -8.4757e-04,  ...,  1.6315e-03,
          0.0000e+00,  9.7090e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(614.8188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0137, device='cuda:0')



h[100].sum tensor(-15.2799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5076, device='cuda:0')



h[200].sum tensor(24.2201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0150, 0.0000, 0.0088,  ..., 0.0150, 0.0000, 0.0320],
        [0.0333, 0.0000, 0.0197,  ..., 0.0251, 0.0000, 0.0696],
        [0.0140, 0.0000, 0.0080,  ..., 0.0151, 0.0000, 0.0321],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37166.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0434,  ..., 0.0145, 0.0000, 0.0890],
        [0.0000, 0.0000, 0.0600,  ..., 0.0236, 0.0000, 0.1253],
        [0.0019, 0.0000, 0.0474,  ..., 0.0156, 0.0000, 0.0969],
        ...,
        [0.0205, 0.0000, 0.0022,  ..., 0.0000, 0.0381, 0.0018],
        [0.0205, 0.0000, 0.0022,  ..., 0.0000, 0.0382, 0.0018],
        [0.0205, 0.0000, 0.0022,  ..., 0.0000, 0.0382, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(314883.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3372.4265, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.1776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(177.3144, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(142.7577, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0834],
        [ 0.0893],
        [ 0.0775],
        ...,
        [-0.3260],
        [-0.3188],
        [-0.2970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105045.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0042],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366814.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0021],
        [1.0043],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366828., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [ 5.2490e-03, -3.0286e-03,  3.0330e-03,  ...,  4.7624e-03,
         -3.4610e-03,  1.1759e-02],
        ...,
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1032.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0219, device='cuda:0')



h[100].sum tensor(-24.1683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8120, device='cuda:0')



h[200].sum tensor(31.9557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.6954, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0175, 0.0000, 0.0106,  ..., 0.0157, 0.0000, 0.0346],
        [0.0053, 0.0000, 0.0030,  ..., 0.0097, 0.0000, 0.0121],
        [0.0088, 0.0000, 0.0050,  ..., 0.0120, 0.0000, 0.0206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48854.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.7639e-03, 0.0000e+00, 4.4494e-02,  ..., 2.3411e-02, 0.0000e+00,
         9.8295e-02],
        [4.8210e-03, 0.0000e+00, 3.0201e-02,  ..., 1.1127e-02, 8.8117e-05,
         6.2849e-02],
        [5.2920e-03, 0.0000e+00, 3.7349e-02,  ..., 1.2584e-02, 8.8183e-05,
         7.6397e-02],
        ...,
        [2.0698e-02, 0.0000e+00, 2.3191e-03,  ..., 0.0000e+00, 3.8775e-02,
         1.6826e-03],
        [2.0702e-02, 0.0000e+00, 2.3196e-03,  ..., 0.0000e+00, 3.8783e-02,
         1.6829e-03],
        [2.0702e-02, 0.0000e+00, 2.3196e-03,  ..., 0.0000e+00, 3.8782e-02,
         1.6828e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367357.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3242.3442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(393.7500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(253.5829, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(195.2151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0170],
        [ 0.0363],
        [ 0.0580],
        ...,
        [-0.3345],
        [-0.3332],
        [-0.3328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110641.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0021],
        [1.0043],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366828., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0021],
        [1.0043],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366828., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [ 5.5292e-03, -3.1624e-03,  3.2047e-03,  ...,  4.9008e-03,
         -3.6140e-03,  1.2274e-02],
        ...,
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05],
        [-1.0911e-03,  0.0000e+00, -8.5189e-04,  ...,  1.6327e-03,
          0.0000e+00,  9.8616e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1120.1855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0234, device='cuda:0')



h[100].sum tensor(-26.0654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8642, device='cuda:0')



h[200].sum tensor(33.6906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0055, 0.0000, 0.0032,  ..., 0.0098, 0.0000, 0.0126],
        [0.0043, 0.0000, 0.0025,  ..., 0.0092, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53334.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0000, 0.0072,  ..., 0.0002, 0.0285, 0.0112],
        [0.0149, 0.0000, 0.0156,  ..., 0.0020, 0.0153, 0.0284],
        [0.0119, 0.0000, 0.0225,  ..., 0.0031, 0.0077, 0.0422],
        ...,
        [0.0207, 0.0000, 0.0023,  ..., 0.0000, 0.0388, 0.0017],
        [0.0207, 0.0000, 0.0023,  ..., 0.0000, 0.0388, 0.0017],
        [0.0207, 0.0000, 0.0023,  ..., 0.0000, 0.0388, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393354.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3154.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.8542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.0208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(215.1509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1168],
        [-0.0108],
        [ 0.0642],
        ...,
        [-0.3345],
        [-0.3332],
        [-0.3328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103917.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0021],
        [1.0043],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366828., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0022],
        [1.0044],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366840.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1374.9065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0278, device='cuda:0')



h[100].sum tensor(-31.3504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0273, device='cuda:0')



h[200].sum tensor(38.2574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.5923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58393.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0206, 0.0000, 0.0024,  ..., 0.0000, 0.0389, 0.0016],
        [0.0207, 0.0000, 0.0024,  ..., 0.0000, 0.0390, 0.0016],
        [0.0207, 0.0000, 0.0024,  ..., 0.0000, 0.0391, 0.0016],
        ...,
        [0.0208, 0.0000, 0.0024,  ..., 0.0000, 0.0393, 0.0016],
        [0.0208, 0.0000, 0.0024,  ..., 0.0000, 0.0393, 0.0016],
        [0.0208, 0.0000, 0.0024,  ..., 0.0000, 0.0393, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406249.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3230.9722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(436.9501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(311.6607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(237.8715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4540],
        [-0.4438],
        [-0.4189],
        ...,
        [-0.3336],
        [-0.3332],
        [-0.3332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95290.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0022],
        [1.0044],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366840.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0022],
        [1.0045],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366853.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.3448e-03, -2.5885e-03,  2.4825e-03,  ...,  4.3299e-03,
         -2.9597e-03,  1.0112e-02],
        [-1.0995e-03,  0.0000e+00, -8.5350e-04,  ...,  1.6431e-03,
          0.0000e+00,  9.8350e-05],
        [-1.0995e-03,  0.0000e+00, -8.5350e-04,  ...,  1.6431e-03,
          0.0000e+00,  9.8350e-05],
        ...,
        [-1.0995e-03,  0.0000e+00, -8.5350e-04,  ...,  1.6431e-03,
          0.0000e+00,  9.8350e-05],
        [-1.0995e-03,  0.0000e+00, -8.5350e-04,  ...,  1.6431e-03,
          0.0000e+00,  9.8350e-05],
        [-1.0995e-03,  0.0000e+00, -8.5350e-04,  ...,  1.6431e-03,
          0.0000e+00,  9.8350e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(857.4733, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-20.1443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6717, device='cuda:0')



h[200].sum tensor(27.3641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0000, 0.0094,  ..., 0.0156, 0.0000, 0.0339],
        [0.0043, 0.0000, 0.0025,  ..., 0.0093, 0.0000, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43716.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0037, 0.0000, 0.0462,  ..., 0.0166, 0.0000, 0.0940],
        [0.0099, 0.0000, 0.0257,  ..., 0.0066, 0.0111, 0.0498],
        [0.0161, 0.0000, 0.0115,  ..., 0.0018, 0.0206, 0.0204],
        ...,
        [0.0210, 0.0000, 0.0026,  ..., 0.0000, 0.0400, 0.0015],
        [0.0210, 0.0000, 0.0026,  ..., 0.0000, 0.0400, 0.0015],
        [0.0210, 0.0000, 0.0026,  ..., 0.0000, 0.0400, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342255.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3424.8274, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.8820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(217.6363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(170.7738, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0757],
        [ 0.0177],
        [-0.0627],
        ...,
        [-0.3407],
        [-0.3393],
        [-0.3389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105989.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0022],
        [1.0045],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366853.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0023],
        [1.0046],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366866.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0305, -0.0150,  0.0185,  ...,  0.0173, -0.0172,  0.0583],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [ 0.0176, -0.0089,  0.0106,  ...,  0.0109, -0.0102,  0.0346],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(991.3118, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0204, device='cuda:0')



h[100].sum tensor(-22.9415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7555, device='cuda:0')



h[200].sum tensor(28.6420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0524, 0.0000, 0.0315,  ..., 0.0341, 0.0000, 0.1029],
        [0.0767, 0.0000, 0.0462,  ..., 0.0466, 0.0000, 0.1497],
        [0.0142, 0.0000, 0.0085,  ..., 0.0142, 0.0000, 0.0287],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48016.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1325,  ..., 0.0857, 0.0000, 0.3017],
        [0.0000, 0.0000, 0.1078,  ..., 0.0682, 0.0000, 0.2439],
        [0.0027, 0.0000, 0.0520,  ..., 0.0293, 0.0000, 0.1143],
        ...,
        [0.0215, 0.0000, 0.0028,  ..., 0.0000, 0.0409, 0.0015],
        [0.0215, 0.0000, 0.0029,  ..., 0.0000, 0.0409, 0.0015],
        [0.0215, 0.0000, 0.0029,  ..., 0.0000, 0.0409, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368960.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3405.6792, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(396.3504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(244.9226, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(188.1037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0227],
        [ 0.0070],
        [-0.0252],
        ...,
        [-0.3423],
        [-0.3410],
        [-0.3406]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112932.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0023],
        [1.0046],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366866.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0023],
        [1.0047],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366879.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [ 0.0078, -0.0042,  0.0046,  ...,  0.0060, -0.0048,  0.0164],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1181.3975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0237, device='cuda:0')



h[100].sum tensor(-26.8702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8780, device='cuda:0')



h[200].sum tensor(31.3980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0078, 0.0000, 0.0046,  ..., 0.0110, 0.0000, 0.0168],
        [0.0287, 0.0000, 0.0172,  ..., 0.0219, 0.0000, 0.0573],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55572.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0121, 0.0000, 0.0191,  ..., 0.0061, 0.0157, 0.0366],
        [0.0062, 0.0000, 0.0494,  ..., 0.0252, 0.0072, 0.1056],
        [0.0006, 0.0000, 0.1000,  ..., 0.0613, 0.0000, 0.2238],
        ...,
        [0.0217, 0.0000, 0.0030,  ..., 0.0000, 0.0416, 0.0015],
        [0.0218, 0.0000, 0.0030,  ..., 0.0000, 0.0416, 0.0015],
        [0.0218, 0.0000, 0.0030,  ..., 0.0000, 0.0416, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403539.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3395.0002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.0414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.1183, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(221.9324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0201],
        [ 0.0029],
        [ 0.0145],
        ...,
        [-0.3449],
        [-0.3435],
        [-0.3431]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105089.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0023],
        [1.0047],
        ...,
        [1.0013],
        [1.0010],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366879.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0024],
        [1.0047],
        ...,
        [1.0014],
        [1.0011],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366891.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(614.8059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0134, device='cuda:0')



h[100].sum tensor(-14.7625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4948, device='cuda:0')



h[200].sum tensor(19.5686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9548, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0181, 0.0000, 0.0109,  ..., 0.0162, 0.0000, 0.0359],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36948.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0203, 0.0000, 0.0051,  ..., 0.0012, 0.0362, 0.0066],
        [0.0153, 0.0000, 0.0125,  ..., 0.0048, 0.0209, 0.0234],
        [0.0071, 0.0000, 0.0358,  ..., 0.0191, 0.0043, 0.0771],
        ...,
        [0.0220, 0.0000, 0.0030,  ..., 0.0000, 0.0422, 0.0015],
        [0.0220, 0.0000, 0.0030,  ..., 0.0000, 0.0422, 0.0015],
        [0.0220, 0.0000, 0.0030,  ..., 0.0000, 0.0422, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(318735.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3599.5471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.7412, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(175.3375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(136.3544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2741],
        [-0.1477],
        [-0.0202],
        ...,
        [-0.3029],
        [-0.3346],
        [-0.3426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120222.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0024],
        [1.0047],
        ...,
        [1.0014],
        [1.0011],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366891.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(482.9285, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0048],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366903.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [ 0.0043, -0.0025,  0.0024,  ...,  0.0043, -0.0029,  0.0100],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0017,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(666.7309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-15.8155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5263, device='cuda:0')



h[200].sum tensor(19.8453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0004],
        [0.0043, 0.0000, 0.0024,  ..., 0.0094, 0.0000, 0.0103],
        [0.0033, 0.0000, 0.0018,  ..., 0.0089, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38230.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0208, 0.0000, 0.0060,  ..., 0.0002, 0.0358, 0.0078],
        [0.0179, 0.0000, 0.0129,  ..., 0.0014, 0.0229, 0.0215],
        [0.0167, 0.0000, 0.0165,  ..., 0.0014, 0.0147, 0.0286],
        ...,
        [0.0223, 0.0000, 0.0030,  ..., 0.0000, 0.0429, 0.0015],
        [0.0223, 0.0000, 0.0030,  ..., 0.0000, 0.0429, 0.0015],
        [0.0222, 0.0000, 0.0033,  ..., 0.0000, 0.0422, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324850.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3618.4575, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.0276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(183.7162, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(141.3684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2237],
        [-0.1291],
        [-0.0169],
        ...,
        [-0.3440],
        [-0.3267],
        [-0.3021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117945.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0048],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366903.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0049],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366916.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7178e-03, -2.7439e-03,  2.7308e-03,  ...,  4.5818e-03,
         -3.1417e-03,  1.0834e-02],
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        [ 4.7178e-03, -2.7439e-03,  2.7308e-03,  ...,  4.5818e-03,
         -3.1417e-03,  1.0834e-02],
        ...,
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1318.3893, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0268, device='cuda:0')



h[100].sum tensor(-29.4639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9925, device='cuda:0')



h[200].sum tensor(32.1104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0157, 0.0000, 0.0093,  ..., 0.0157, 0.0000, 0.0334],
        [0.0253, 0.0000, 0.0149,  ..., 0.0215, 0.0000, 0.0552],
        [0.0079, 0.0000, 0.0047,  ..., 0.0113, 0.0000, 0.0170],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58907.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0000, 0.0508,  ..., 0.0194, 0.0000, 0.1035],
        [0.0010, 0.0000, 0.0512,  ..., 0.0182, 0.0000, 0.1032],
        [0.0085, 0.0000, 0.0323,  ..., 0.0092, 0.0032, 0.0630],
        ...,
        [0.0225, 0.0000, 0.0028,  ..., 0.0000, 0.0436, 0.0015],
        [0.0225, 0.0000, 0.0028,  ..., 0.0000, 0.0436, 0.0015],
        [0.0225, 0.0000, 0.0028,  ..., 0.0000, 0.0436, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(417129.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3389.2231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(452.9574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(315.6341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(234.5815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1583],
        [ 0.1475],
        [ 0.1095],
        ...,
        [-0.3584],
        [-0.3569],
        [-0.3564]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89610.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0049],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366916.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0049],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366916.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        ...,
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05],
        [-1.1192e-03,  0.0000e+00, -8.4577e-04,  ...,  1.6992e-03,
          0.0000e+00,  8.9642e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1063.3733, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0217, device='cuda:0')



h[100].sum tensor(-24.0995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8040, device='cuda:0')



h[200].sum tensor(27.1688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0004],
        [0.0038, 0.0000, 0.0022,  ..., 0.0093, 0.0000, 0.0094],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49543.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.0000, 0.0089,  ..., 0.0000, 0.0317, 0.0129],
        [0.0197, 0.0000, 0.0102,  ..., 0.0013, 0.0276, 0.0164],
        [0.0149, 0.0000, 0.0182,  ..., 0.0044, 0.0163, 0.0338],
        ...,
        [0.0221, 0.0000, 0.0037,  ..., 0.0001, 0.0416, 0.0033],
        [0.0225, 0.0000, 0.0028,  ..., 0.0000, 0.0436, 0.0015],
        [0.0225, 0.0000, 0.0028,  ..., 0.0000, 0.0436, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(374960.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3578.7915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.0489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(258.2153, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(193.1753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0103],
        [ 0.0072],
        [ 0.0275],
        ...,
        [-0.3201],
        [-0.3455],
        [-0.3546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-114545.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0024],
        [1.0049],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366916.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0025],
        [1.0049],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366927.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1025e-03,  0.0000e+00, -8.0650e-04,  ...,  1.7203e-03,
          0.0000e+00,  4.3552e-05],
        [ 7.0856e-03, -3.8399e-03,  4.2130e-03,  ...,  5.7641e-03,
         -4.3979e-03,  1.5114e-02],
        [ 5.9302e-03, -3.2981e-03,  3.5047e-03,  ...,  5.1935e-03,
         -3.7773e-03,  1.2987e-02],
        ...,
        [-1.1025e-03,  0.0000e+00, -8.0650e-04,  ...,  1.7203e-03,
          0.0000e+00,  4.3552e-05],
        [-1.1025e-03,  0.0000e+00, -8.0650e-04,  ...,  1.7203e-03,
          0.0000e+00,  4.3552e-05],
        [-1.1025e-03,  0.0000e+00, -8.0650e-04,  ...,  1.7203e-03,
          0.0000e+00,  4.3552e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(587.6099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0125, device='cuda:0')



h[100].sum tensor(-13.9046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4636, device='cuda:0')



h[200].sum tensor(19.5650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0000, 0.0042,  ..., 0.0109, 0.0000, 0.0153],
        [0.0173, 0.0000, 0.0103,  ..., 0.0165, 0.0000, 0.0361],
        [0.0438, 0.0000, 0.0263,  ..., 0.0307, 0.0000, 0.0890],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36097.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0375,  ..., 0.0175, 0.0000, 0.0794],
        [0.0016, 0.0000, 0.0539,  ..., 0.0273, 0.0000, 0.1161],
        [0.0000, 0.0000, 0.0824,  ..., 0.0453, 0.0000, 0.1809],
        ...,
        [0.0224, 0.0000, 0.0025,  ..., 0.0000, 0.0437, 0.0015],
        [0.0224, 0.0000, 0.0025,  ..., 0.0000, 0.0437, 0.0015],
        [0.0224, 0.0000, 0.0025,  ..., 0.0000, 0.0437, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(319495.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3651.2571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.1390, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(175.1053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(134.9511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0411],
        [ 0.0558],
        [ 0.0711],
        ...,
        [-0.3683],
        [-0.3666],
        [-0.3641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133531.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0025],
        [1.0049],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366927.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0025],
        [1.0050],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366938.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4005e-03, -4.9096e-03,  5.6565e-03,  ...,  6.9150e-03,
         -5.6246e-03,  1.9322e-02],
        [ 1.6720e-02, -8.3340e-03,  1.0145e-02,  ...,  1.0530e-02,
         -9.5476e-03,  3.2793e-02],
        [ 1.7450e-02, -8.6757e-03,  1.0593e-02,  ...,  1.0891e-02,
         -9.9390e-03,  3.4137e-02],
        ...,
        [-1.0934e-03,  0.0000e+00, -7.7857e-04,  ...,  1.7322e-03,
          0.0000e+00,  8.5592e-06],
        [-1.0934e-03,  0.0000e+00, -7.7857e-04,  ...,  1.7322e-03,
          0.0000e+00,  8.5592e-06],
        [-1.0934e-03,  0.0000e+00, -7.7857e-04,  ...,  1.7322e-03,
          0.0000e+00,  8.5592e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(793.3756, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0164, device='cuda:0')



h[100].sum tensor(-17.9818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6063, device='cuda:0')



h[200].sum tensor(25.0339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9722, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.1027e-02, 0.0000e+00, 3.0858e-02,  ..., 3.4296e-02, 0.0000e+00,
         1.0200e-01],
        [6.9493e-02, 0.0000e+00, 4.2182e-02,  ..., 4.3426e-02, 0.0000e+00,
         1.3600e-01],
        [6.0973e-02, 0.0000e+00, 3.6956e-02,  ..., 3.9229e-02, 0.0000e+00,
         1.2033e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0003e-03, 0.0000e+00,
         3.4591e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0019e-03, 0.0000e+00,
         3.4599e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0018e-03, 0.0000e+00,
         3.4598e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42758.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1234,  ..., 0.0756, 0.0000, 0.2786],
        [0.0000, 0.0000, 0.1298,  ..., 0.0831, 0.0000, 0.2961],
        [0.0000, 0.0000, 0.1100,  ..., 0.0694, 0.0000, 0.2503],
        ...,
        [0.0222, 0.0000, 0.0023,  ..., 0.0000, 0.0434, 0.0017],
        [0.0222, 0.0000, 0.0023,  ..., 0.0000, 0.0434, 0.0017],
        [0.0222, 0.0000, 0.0023,  ..., 0.0000, 0.0434, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355217.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3440.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(390.9449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(216.1213, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(167.7117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0749],
        [ 0.0634],
        [ 0.0436],
        ...,
        [-0.3723],
        [-0.3708],
        [-0.3703]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132346.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0025],
        [1.0050],
        ...,
        [1.0014],
        [1.0011],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366938.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0026],
        [1.0051],
        ...,
        [1.0014],
        [1.0010],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366950.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2461e-02, -6.3268e-03,  7.5502e-03,  ...,  8.4320e-03,
         -7.2501e-03,  2.4936e-02],
        [ 6.8645e-03, -3.7144e-03,  4.1174e-03,  ...,  5.6676e-03,
         -4.2564e-03,  1.4635e-02],
        [ 2.7066e-02, -1.3144e-02,  1.6508e-02,  ...,  1.5646e-02,
         -1.5062e-02,  5.1817e-02],
        ...,
        [-1.0931e-03,  0.0000e+00, -7.6322e-04,  ...,  1.7371e-03,
          0.0000e+00, -1.1578e-05],
        [-1.0931e-03,  0.0000e+00, -7.6322e-04,  ...,  1.7371e-03,
          0.0000e+00, -1.1578e-05],
        [-1.0931e-03,  0.0000e+00, -7.6322e-04,  ...,  1.7371e-03,
          0.0000e+00, -1.1578e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(851.6910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-18.9702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6370, device='cuda:0')



h[200].sum tensor(27.2830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0356, 0.0000, 0.0215,  ..., 0.0267, 0.0000, 0.0736],
        [0.0661, 0.0000, 0.0402,  ..., 0.0418, 0.0000, 0.1297],
        [0.0277, 0.0000, 0.0167,  ..., 0.0223, 0.0000, 0.0571],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43109.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0658,  ..., 0.0373, 0.0000, 0.1461],
        [0.0000, 0.0000, 0.0888,  ..., 0.0560, 0.0000, 0.2021],
        [0.0000, 0.0000, 0.0727,  ..., 0.0434, 0.0000, 0.1633],
        ...,
        [0.0220, 0.0000, 0.0023,  ..., 0.0000, 0.0431, 0.0019],
        [0.0220, 0.0000, 0.0023,  ..., 0.0000, 0.0432, 0.0019],
        [0.0220, 0.0000, 0.0023,  ..., 0.0000, 0.0432, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(348986.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3382.0674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(398.5985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(214.2587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(171.1229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0813],
        [-0.0450],
        [-0.0186],
        ...,
        [-0.3742],
        [-0.3727],
        [-0.3722]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122036.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0026],
        [1.0051],
        ...,
        [1.0014],
        [1.0010],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366950.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0026],
        [1.0052],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366962.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1039e-03,  0.0000e+00, -7.6549e-04,  ...,  1.7320e-03,
          0.0000e+00, -2.1182e-05],
        [-1.1039e-03,  0.0000e+00, -7.6549e-04,  ...,  1.7320e-03,
          0.0000e+00, -2.1182e-05],
        [-1.1039e-03,  0.0000e+00, -7.6549e-04,  ...,  1.7320e-03,
          0.0000e+00, -2.1182e-05],
        ...,
        [-1.1039e-03,  0.0000e+00, -7.6549e-04,  ...,  1.7320e-03,
          0.0000e+00, -2.1182e-05],
        [-1.1039e-03,  0.0000e+00, -7.6549e-04,  ...,  1.7320e-03,
          0.0000e+00, -2.1182e-05],
        [-1.1039e-03,  0.0000e+00, -7.6549e-04,  ...,  1.7320e-03,
          0.0000e+00, -2.1182e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(783.3761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0157, device='cuda:0')



h[100].sum tensor(-17.3521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5811, device='cuda:0')



h[200].sum tensor(26.4535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5170, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0000, 0.0031,  ..., 0.0101, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40784.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0000, 0.0201,  ..., 0.0050, 0.0132, 0.0394],
        [0.0187, 0.0000, 0.0091,  ..., 0.0015, 0.0275, 0.0159],
        [0.0212, 0.0000, 0.0040,  ..., 0.0002, 0.0393, 0.0051],
        ...,
        [0.0218, 0.0000, 0.0024,  ..., 0.0000, 0.0428, 0.0021],
        [0.0218, 0.0000, 0.0024,  ..., 0.0000, 0.0428, 0.0021],
        [0.0218, 0.0000, 0.0024,  ..., 0.0000, 0.0428, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(336667.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3380.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(392.4057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(195.2110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.6022, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0249],
        [-0.0382],
        [-0.1091],
        ...,
        [-0.3739],
        [-0.3724],
        [-0.3720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117354.4766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0026],
        [1.0052],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366962.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0027],
        [1.0053],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366974.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.2943e-03, -2.0527e-03,  1.9258e-03,  ...,  3.9045e-03,
         -2.3536e-03,  8.1129e-03],
        [ 3.2943e-03, -2.0527e-03,  1.9258e-03,  ...,  3.9045e-03,
         -2.3536e-03,  8.1129e-03],
        [-1.1231e-03,  0.0000e+00, -7.8301e-04,  ...,  1.7220e-03,
          0.0000e+00, -2.0941e-05],
        ...,
        [-1.1231e-03,  0.0000e+00, -7.8301e-04,  ...,  1.7220e-03,
          0.0000e+00, -2.0941e-05],
        [-1.1231e-03,  0.0000e+00, -7.8301e-04,  ...,  1.7220e-03,
          0.0000e+00, -2.0941e-05],
        [-1.1231e-03,  0.0000e+00, -7.8301e-04,  ...,  1.7220e-03,
          0.0000e+00, -2.0941e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1598.8179, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0310, device='cuda:0')



h[100].sum tensor(-34.0937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1460, device='cuda:0')



h[200].sum tensor(41.7362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.7397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0236, 0.0000, 0.0141,  ..., 0.0208, 0.0000, 0.0517],
        [0.0133, 0.0000, 0.0079,  ..., 0.0151, 0.0000, 0.0306],
        [0.0288, 0.0000, 0.0173,  ..., 0.0234, 0.0000, 0.0612],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66714.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0652,  ..., 0.0285, 0.0000, 0.1366],
        [0.0000, 0.0000, 0.0592,  ..., 0.0251, 0.0000, 0.1234],
        [0.0000, 0.0000, 0.0680,  ..., 0.0323, 0.0000, 0.1444],
        ...,
        [0.0219, 0.0000, 0.0028,  ..., 0.0000, 0.0426, 0.0023],
        [0.0219, 0.0000, 0.0028,  ..., 0.0000, 0.0426, 0.0023],
        [0.0219, 0.0000, 0.0028,  ..., 0.0000, 0.0426, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463701.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3152.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(487.7409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(360.4026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.6294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1358],
        [ 0.1334],
        [ 0.1211],
        ...,
        [-0.3724],
        [-0.3709],
        [-0.3705]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116900.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0027],
        [1.0053],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366974.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0028],
        [1.0054],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366986.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1397e-03,  0.0000e+00, -8.0279e-04,  ...,  1.7151e-03,
          0.0000e+00, -1.7747e-05],
        [-1.1397e-03,  0.0000e+00, -8.0279e-04,  ...,  1.7151e-03,
          0.0000e+00, -1.7747e-05],
        [-1.1397e-03,  0.0000e+00, -8.0279e-04,  ...,  1.7151e-03,
          0.0000e+00, -1.7747e-05],
        ...,
        [-1.1397e-03,  0.0000e+00, -8.0279e-04,  ...,  1.7151e-03,
          0.0000e+00, -1.7747e-05],
        [-1.1397e-03,  0.0000e+00, -8.0279e-04,  ...,  1.7151e-03,
          0.0000e+00, -1.7747e-05],
        [-1.1397e-03,  0.0000e+00, -8.0279e-04,  ...,  1.7151e-03,
          0.0000e+00, -1.7747e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1071.6465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0210, device='cuda:0')



h[100].sum tensor(-23.1101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7769, device='cuda:0')



h[200].sum tensor(31.0150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0593, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0185, 0.0000, 0.0112,  ..., 0.0166, 0.0000, 0.0362],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49213.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7129e-02, 0.0000e+00, 1.1216e-02,  ..., 2.8911e-03, 2.1993e-02,
         2.0512e-02],
        [1.0909e-02, 0.0000e+00, 1.9854e-02,  ..., 9.2761e-03, 1.4935e-02,
         4.0888e-02],
        [4.8287e-03, 0.0000e+00, 4.4469e-02,  ..., 2.6995e-02, 4.0777e-03,
         9.8584e-02],
        ...,
        [2.2168e-02, 0.0000e+00, 3.0348e-03,  ..., 8.0325e-05, 4.2577e-02,
         2.5620e-03],
        [2.2174e-02, 0.0000e+00, 3.0357e-03,  ..., 8.0079e-05, 4.2587e-02,
         2.5626e-03],
        [2.2174e-02, 0.0000e+00, 3.0358e-03,  ..., 7.9933e-05, 4.2587e-02,
         2.5625e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372551.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3326.2573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(428.1570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(240.2005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(202.2698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0731],
        [ 0.0732],
        [ 0.0799],
        ...,
        [-0.3270],
        [-0.3403],
        [-0.3527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101940.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0028],
        [1.0054],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366986.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0028],
        [1.0054],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366998.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1485e-03,  0.0000e+00, -8.1543e-04,  ...,  1.7171e-03,
          0.0000e+00, -1.7242e-05],
        [-1.1485e-03,  0.0000e+00, -8.1543e-04,  ...,  1.7171e-03,
          0.0000e+00, -1.7242e-05],
        [-1.1485e-03,  0.0000e+00, -8.1543e-04,  ...,  1.7171e-03,
          0.0000e+00, -1.7242e-05],
        ...,
        [ 1.0784e-02, -5.5189e-03,  6.4988e-03,  ...,  7.6112e-03,
         -6.3314e-03,  2.1961e-02],
        [-1.1485e-03,  0.0000e+00, -8.1543e-04,  ...,  1.7171e-03,
          0.0000e+00, -1.7242e-05],
        [-1.1485e-03,  0.0000e+00, -8.1543e-04,  ...,  1.7171e-03,
          0.0000e+00, -1.7242e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1309.0649, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0251, device='cuda:0')



h[100].sum tensor(-27.9707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9303, device='cuda:0')



h[200].sum tensor(34.9610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.8355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0160, 0.0000, 0.0095,  ..., 0.0166, 0.0000, 0.0358],
        [0.0146, 0.0000, 0.0087,  ..., 0.0153, 0.0000, 0.0312],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58587.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0206, 0.0000, 0.0060,  ..., 0.0015, 0.0355, 0.0089],
        [0.0222, 0.0000, 0.0031,  ..., 0.0003, 0.0425, 0.0027],
        [0.0223, 0.0000, 0.0031,  ..., 0.0003, 0.0426, 0.0027],
        ...,
        [0.0017, 0.0000, 0.0558,  ..., 0.0270, 0.0000, 0.1177],
        [0.0061, 0.0000, 0.0412,  ..., 0.0200, 0.0039, 0.0866],
        [0.0149, 0.0000, 0.0159,  ..., 0.0063, 0.0205, 0.0308]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424714.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3258.2842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(465.8518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(299.1204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(246.0526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0856],
        [-0.1755],
        [-0.2336],
        ...,
        [ 0.0946],
        [ 0.0282],
        [-0.0868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95388.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0028],
        [1.0054],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366998.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(474.4778, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0029],
        [1.0055],
        ...,
        [1.0013],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367010.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.2820e-03, -2.0381e-03,  1.8882e-03,  ...,  3.9079e-03,
         -2.3388e-03,  8.1153e-03],
        [ 4.5302e-03, -2.6138e-03,  2.6532e-03,  ...,  4.5239e-03,
         -2.9995e-03,  1.0414e-02],
        [-1.1369e-03,  0.0000e+00, -8.2007e-04,  ...,  1.7270e-03,
          0.0000e+00, -2.1783e-05],
        ...,
        [-1.1369e-03,  0.0000e+00, -8.2007e-04,  ...,  1.7270e-03,
          0.0000e+00, -2.1783e-05],
        [-1.1369e-03,  0.0000e+00, -8.2007e-04,  ...,  1.7270e-03,
          0.0000e+00, -2.1783e-05],
        [-1.1369e-03,  0.0000e+00, -8.2007e-04,  ...,  1.7270e-03,
          0.0000e+00, -2.1783e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1067.9924, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0211, device='cuda:0')



h[100].sum tensor(-22.9966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7812, device='cuda:0')



h[200].sum tensor(30.2350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0393, 0.0000, 0.0236,  ..., 0.0286, 0.0000, 0.0807],
        [0.0157, 0.0000, 0.0094,  ..., 0.0158, 0.0000, 0.0331],
        [0.0045, 0.0000, 0.0027,  ..., 0.0097, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48801.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0737,  ..., 0.0401, 0.0000, 0.1602],
        [0.0036, 0.0000, 0.0503,  ..., 0.0258, 0.0000, 0.1077],
        [0.0102, 0.0000, 0.0273,  ..., 0.0120, 0.0125, 0.0562],
        ...,
        [0.0225, 0.0000, 0.0031,  ..., 0.0003, 0.0432, 0.0028],
        [0.0225, 0.0000, 0.0031,  ..., 0.0003, 0.0432, 0.0028],
        [0.0225, 0.0000, 0.0031,  ..., 0.0003, 0.0432, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(373144.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3342.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(433.3609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(235.1616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(201.8200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1098],
        [ 0.0873],
        [ 0.0298],
        ...,
        [-0.3734],
        [-0.3720],
        [-0.3715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100838.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0029],
        [1.0055],
        ...,
        [1.0013],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367010.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0030],
        [1.0056],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367021.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1178e-03,  0.0000e+00, -8.0208e-04,  ...,  1.7422e-03,
          0.0000e+00, -2.9781e-05],
        [-1.1178e-03,  0.0000e+00, -8.0208e-04,  ...,  1.7422e-03,
          0.0000e+00, -2.9781e-05],
        [ 3.3111e-03, -2.0370e-03,  1.9129e-03,  ...,  3.9263e-03,
         -2.3382e-03,  8.1232e-03],
        ...,
        [-1.1178e-03,  0.0000e+00, -8.0208e-04,  ...,  1.7422e-03,
          0.0000e+00, -2.9781e-05],
        [-1.1178e-03,  0.0000e+00, -8.0208e-04,  ...,  1.7422e-03,
          0.0000e+00, -2.9781e-05],
        [-1.1178e-03,  0.0000e+00, -8.0208e-04,  ...,  1.7422e-03,
          0.0000e+00, -2.9781e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1381.8169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0265, device='cuda:0')



h[100].sum tensor(-29.2317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9812, device='cuda:0')



h[200].sum tensor(36.5784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.7583, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0025, 0.0000, 0.0014,  ..., 0.0087, 0.0000, 0.0066],
        [0.0153, 0.0000, 0.0090,  ..., 0.0162, 0.0000, 0.0342],
        [0.0289, 0.0000, 0.0174,  ..., 0.0229, 0.0000, 0.0593],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60038.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0108, 0.0000, 0.0256,  ..., 0.0091, 0.0121, 0.0523],
        [0.0042, 0.0000, 0.0488,  ..., 0.0218, 0.0000, 0.1036],
        [0.0000, 0.0000, 0.0668,  ..., 0.0337, 0.0000, 0.1446],
        ...,
        [0.0224, 0.0000, 0.0027,  ..., 0.0004, 0.0433, 0.0030],
        [0.0224, 0.0000, 0.0027,  ..., 0.0004, 0.0433, 0.0030],
        [0.0224, 0.0000, 0.0027,  ..., 0.0004, 0.0433, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(428717.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3158.2407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(484.2510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(306.8183, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(253.2396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0067],
        [ 0.0645],
        [ 0.0925],
        ...,
        [-0.3811],
        [-0.3796],
        [-0.3792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98315.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0030],
        [1.0056],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367021.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0031],
        [1.0057],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367032.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1011e-03,  0.0000e+00, -7.8388e-04,  ...,  1.7555e-03,
          0.0000e+00, -2.9416e-05],
        [-1.1011e-03,  0.0000e+00, -7.8388e-04,  ...,  1.7555e-03,
          0.0000e+00, -2.9416e-05],
        [-1.1011e-03,  0.0000e+00, -7.8388e-04,  ...,  1.7555e-03,
          0.0000e+00, -2.9416e-05],
        ...,
        [-1.1011e-03,  0.0000e+00, -7.8388e-04,  ...,  1.7555e-03,
          0.0000e+00, -2.9416e-05],
        [-1.1011e-03,  0.0000e+00, -7.8388e-04,  ...,  1.7555e-03,
          0.0000e+00, -2.9416e-05],
        [-1.1011e-03,  0.0000e+00, -7.8388e-04,  ...,  1.7555e-03,
          0.0000e+00, -2.9416e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(853.3972, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-18.3062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6185, device='cuda:0')



h[200].sum tensor(26.5481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43648.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0223, 0.0000, 0.0024,  ..., 0.0006, 0.0429, 0.0032],
        [0.0223, 0.0000, 0.0024,  ..., 0.0006, 0.0430, 0.0032],
        [0.0224, 0.0000, 0.0024,  ..., 0.0006, 0.0431, 0.0032],
        ...,
        [0.0225, 0.0000, 0.0024,  ..., 0.0005, 0.0434, 0.0032],
        [0.0225, 0.0000, 0.0024,  ..., 0.0005, 0.0434, 0.0032],
        [0.0225, 0.0000, 0.0024,  ..., 0.0005, 0.0434, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(354269.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3357.6040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(205.7768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(178.0488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4667],
        [-0.4146],
        [-0.3407],
        ...,
        [-0.3877],
        [-0.3862],
        [-0.3858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124917.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0031],
        [1.0057],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367032.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0031],
        [1.0058],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367044.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0928e-03,  0.0000e+00, -7.7676e-04,  ...,  1.7605e-03,
          0.0000e+00, -2.5268e-05],
        [-1.0928e-03,  0.0000e+00, -7.7676e-04,  ...,  1.7605e-03,
          0.0000e+00, -2.5268e-05],
        [ 4.2898e-03, -2.4623e-03,  2.5234e-03,  ...,  4.4116e-03,
         -2.8280e-03,  9.8802e-03],
        ...,
        [-1.0928e-03,  0.0000e+00, -7.7676e-04,  ...,  1.7605e-03,
          0.0000e+00, -2.5268e-05],
        [-1.0928e-03,  0.0000e+00, -7.7676e-04,  ...,  1.7605e-03,
          0.0000e+00, -2.5268e-05],
        [-1.0928e-03,  0.0000e+00, -7.7676e-04,  ...,  1.7605e-03,
          0.0000e+00, -2.5268e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1058.1060, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0206, device='cuda:0')



h[100].sum tensor(-22.3949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7625, device='cuda:0')



h[200].sum tensor(30.2316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0043, 0.0000, 0.0025,  ..., 0.0097, 0.0000, 0.0099],
        [0.0070, 0.0000, 0.0042,  ..., 0.0110, 0.0000, 0.0148],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49762.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0200, 0.0000, 0.0072,  ..., 0.0023, 0.0313, 0.0138],
        [0.0138, 0.0000, 0.0187,  ..., 0.0072, 0.0147, 0.0391],
        [0.0082, 0.0000, 0.0296,  ..., 0.0121, 0.0038, 0.0626],
        ...,
        [0.0228, 0.0000, 0.0023,  ..., 0.0006, 0.0436, 0.0033],
        [0.0228, 0.0000, 0.0023,  ..., 0.0006, 0.0436, 0.0033],
        [0.0228, 0.0000, 0.0023,  ..., 0.0006, 0.0436, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380901.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3307.4917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(447.6047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(242.1050, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(205.6321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2488],
        [-0.1207],
        [-0.0178],
        ...,
        [-0.3916],
        [-0.3901],
        [-0.3896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105139.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0031],
        [1.0058],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367044.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0032],
        [1.0059],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367055.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0999e-03,  0.0000e+00, -7.8471e-04,  ...,  1.7558e-03,
          0.0000e+00, -1.1293e-05],
        [-1.0999e-03,  0.0000e+00, -7.8471e-04,  ...,  1.7558e-03,
          0.0000e+00, -1.1293e-05],
        [-1.0999e-03,  0.0000e+00, -7.8471e-04,  ...,  1.7558e-03,
          0.0000e+00, -1.1293e-05],
        ...,
        [-1.0999e-03,  0.0000e+00, -7.8471e-04,  ...,  1.7558e-03,
          0.0000e+00, -1.1293e-05],
        [-1.0999e-03,  0.0000e+00, -7.8471e-04,  ...,  1.7558e-03,
          0.0000e+00, -1.1293e-05],
        [-1.0999e-03,  0.0000e+00, -7.8471e-04,  ...,  1.7558e-03,
          0.0000e+00, -1.1293e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(972.4385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0190, device='cuda:0')



h[100].sum tensor(-20.6793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7039, device='cuda:0')



h[200].sum tensor(27.9722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0079, 0.0000, 0.0046,  ..., 0.0120, 0.0000, 0.0186],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45481.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0220, 0.0000, 0.0045,  ..., 0.0016, 0.0384, 0.0079],
        [0.0187, 0.0000, 0.0109,  ..., 0.0040, 0.0230, 0.0217],
        [0.0102, 0.0000, 0.0269,  ..., 0.0110, 0.0082, 0.0565],
        ...,
        [0.0233, 0.0000, 0.0025,  ..., 0.0008, 0.0439, 0.0034],
        [0.0233, 0.0000, 0.0025,  ..., 0.0008, 0.0439, 0.0034],
        [0.0233, 0.0000, 0.0025,  ..., 0.0008, 0.0439, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359381.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3430.9084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.7683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(216.1105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(186.2427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2628],
        [-0.1376],
        [-0.0198],
        ...,
        [-0.3829],
        [-0.3789],
        [-0.3773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113417.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0032],
        [1.0059],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367055.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0033],
        [1.0059],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367067.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.7671e-02, -1.3104e-02,  1.6841e-02,  ...,  1.5916e-02,
         -1.5059e-02,  5.2995e-02],
        [ 6.0979e-03, -3.2826e-03,  3.6184e-03,  ...,  5.2935e-03,
         -3.7722e-03,  1.3281e-02],
        [ 2.2281e-02, -1.0650e-02,  1.3537e-02,  ...,  1.3261e-02,
         -1.2239e-02,  4.3072e-02],
        ...,
        [-1.1123e-03,  0.0000e+00, -8.0088e-04,  ...,  1.7434e-03,
          0.0000e+00,  8.1479e-06],
        [-1.1123e-03,  0.0000e+00, -8.0088e-04,  ...,  1.7434e-03,
          0.0000e+00,  8.1479e-06],
        [-1.1123e-03,  0.0000e+00, -8.0088e-04,  ...,  1.7434e-03,
          0.0000e+00,  8.1479e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1591.9143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0304, device='cuda:0')



h[100].sum tensor(-33.1778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1244, device='cuda:0')



h[200].sum tensor(38.7353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.7200e-02, 0.0000e+00, 2.8453e-02,  ..., 3.2414e-02, 0.0000e+00,
         9.5120e-02],
        [1.1174e-01, 0.0000e+00, 6.8011e-02,  ..., 6.4202e-02, 0.0000e+00,
         2.1394e-01],
        [6.9311e-02, 0.0000e+00, 4.2004e-02,  ..., 4.3323e-02, 0.0000e+00,
         1.3584e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0564e-03, 0.0000e+00,
         3.2979e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0579e-03, 0.0000e+00,
         3.2986e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0575e-03, 0.0000e+00,
         3.2984e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65441.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1291,  ..., 0.0913, 0.0000, 0.3050],
        [0.0000, 0.0000, 0.1709,  ..., 0.1290, 0.0000, 0.4116],
        [0.0000, 0.0000, 0.1576,  ..., 0.1173, 0.0000, 0.3776],
        ...,
        [0.0238, 0.0000, 0.0026,  ..., 0.0011, 0.0443, 0.0035],
        [0.0238, 0.0000, 0.0026,  ..., 0.0011, 0.0443, 0.0035],
        [0.0238, 0.0000, 0.0026,  ..., 0.0011, 0.0443, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451747.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3257.9380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(509.9181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(343.8801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(277.5891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0318],
        [ 0.0080],
        [-0.0124],
        ...,
        [-0.3844],
        [-0.3784],
        [-0.3732]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-96412.8828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0033],
        [1.0059],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367067.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0033],
        [1.0060],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367079., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1262e-03,  0.0000e+00, -8.1567e-04,  ...,  1.7368e-03,
          0.0000e+00,  2.6241e-05],
        [ 1.5506e-02, -7.5546e-03,  9.3766e-03,  ...,  9.9259e-03,
         -8.6839e-03,  3.0651e-02],
        [-1.1262e-03,  0.0000e+00, -8.1567e-04,  ...,  1.7368e-03,
          0.0000e+00,  2.6241e-05],
        ...,
        [-1.1262e-03,  0.0000e+00, -8.1567e-04,  ...,  1.7368e-03,
          0.0000e+00,  2.6241e-05],
        [-1.1262e-03,  0.0000e+00, -8.1567e-04,  ...,  1.7368e-03,
          0.0000e+00,  2.6241e-05],
        [-1.1262e-03,  0.0000e+00, -8.1567e-04,  ...,  1.7368e-03,
          0.0000e+00,  2.6241e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(872.9501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-18.7535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6431, device='cuda:0')



h[200].sum tensor(24.2121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6390, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.5522e-02, 0.0000e+00, 9.3864e-03,  ..., 1.5153e-02, 0.0000e+00,
         3.0762e-02],
        [1.2485e-02, 0.0000e+00, 7.5252e-03,  ..., 1.3666e-02, 0.0000e+00,
         2.5173e-02],
        [5.6123e-02, 0.0000e+00, 3.3888e-02,  ..., 3.6831e-02, 0.0000e+00,
         1.1177e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0306e-03, 0.0000e+00,
         1.0623e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0322e-03, 0.0000e+00,
         1.0625e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0318e-03, 0.0000e+00,
         1.0624e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44034.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0099, 0.0000, 0.0286,  ..., 0.0197, 0.0065, 0.0651],
        [0.0046, 0.0000, 0.0360,  ..., 0.0247, 0.0000, 0.0825],
        [0.0000, 0.0000, 0.0597,  ..., 0.0417, 0.0000, 0.1389],
        ...,
        [0.0241, 0.0000, 0.0028,  ..., 0.0014, 0.0448, 0.0036],
        [0.0241, 0.0000, 0.0028,  ..., 0.0014, 0.0449, 0.0036],
        [0.0241, 0.0000, 0.0028,  ..., 0.0014, 0.0449, 0.0036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355640.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3612.1736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(421.9297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(209.5553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(179.2395, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1679],
        [-0.1110],
        [-0.0638],
        ...,
        [-0.3889],
        [-0.3873],
        [-0.3869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112233.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0033],
        [1.0060],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367079., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0034],
        [1.0061],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367090.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1378e-03,  0.0000e+00, -8.2408e-04,  ...,  1.7380e-03,
          0.0000e+00,  3.5926e-05],
        [-1.1378e-03,  0.0000e+00, -8.2408e-04,  ...,  1.7380e-03,
          0.0000e+00,  3.5926e-05],
        [-1.1378e-03,  0.0000e+00, -8.2408e-04,  ...,  1.7380e-03,
          0.0000e+00,  3.5926e-05],
        ...,
        [-1.1378e-03,  0.0000e+00, -8.2408e-04,  ...,  1.7380e-03,
          0.0000e+00,  3.5926e-05],
        [-1.1378e-03,  0.0000e+00, -8.2408e-04,  ...,  1.7380e-03,
          0.0000e+00,  3.5926e-05],
        [-1.1378e-03,  0.0000e+00, -8.2408e-04,  ...,  1.7380e-03,
          0.0000e+00,  3.5926e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(900.8608, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0177, device='cuda:0')



h[100].sum tensor(-19.3500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6542, device='cuda:0')



h[200].sum tensor(23.7760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43960.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0241, 0.0000, 0.0027,  ..., 0.0017, 0.0450, 0.0036],
        [0.0242, 0.0000, 0.0031,  ..., 0.0017, 0.0445, 0.0042],
        [0.0243, 0.0000, 0.0039,  ..., 0.0017, 0.0431, 0.0058],
        ...,
        [0.0244, 0.0000, 0.0027,  ..., 0.0016, 0.0456, 0.0036],
        [0.0244, 0.0000, 0.0028,  ..., 0.0016, 0.0456, 0.0036],
        [0.0244, 0.0000, 0.0027,  ..., 0.0016, 0.0456, 0.0036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353664.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3699.3704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(424.8832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(209.7901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(177.7286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2754],
        [-0.2962],
        [-0.2897],
        ...,
        [-0.3725],
        [-0.3842],
        [-0.3864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105189.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0034],
        [1.0061],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367090.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0035],
        [1.0062],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367102.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1450e-03,  0.0000e+00, -8.2232e-04,  ...,  1.7406e-03,
          0.0000e+00,  3.9288e-05],
        [-1.1450e-03,  0.0000e+00, -8.2232e-04,  ...,  1.7406e-03,
          0.0000e+00,  3.9288e-05],
        [ 3.2899e-03, -2.0050e-03,  1.8950e-03,  ...,  3.9243e-03,
         -2.3060e-03,  8.2083e-03],
        ...,
        [-1.1450e-03,  0.0000e+00, -8.2232e-04,  ...,  1.7406e-03,
          0.0000e+00,  3.9288e-05],
        [-1.1450e-03,  0.0000e+00, -8.2232e-04,  ...,  1.7406e-03,
          0.0000e+00,  3.9288e-05],
        [-1.1450e-03,  0.0000e+00, -8.2232e-04,  ...,  1.7406e-03,
          0.0000e+00,  3.9288e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(810.8661, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-17.5196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6041, device='cuda:0')



h[200].sum tensor(21.4222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002],
        [0.0058, 0.0000, 0.0033,  ..., 0.0110, 0.0000, 0.0150],
        [0.0080, 0.0000, 0.0046,  ..., 0.0126, 0.0000, 0.0213],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41001.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0211, 0.0000, 0.0100,  ..., 0.0049, 0.0282, 0.0194],
        [0.0144, 0.0000, 0.0239,  ..., 0.0106, 0.0114, 0.0496],
        [0.0104, 0.0000, 0.0334,  ..., 0.0138, 0.0023, 0.0698],
        ...,
        [0.0247, 0.0000, 0.0027,  ..., 0.0021, 0.0462, 0.0037],
        [0.0247, 0.0000, 0.0027,  ..., 0.0021, 0.0462, 0.0037],
        [0.0247, 0.0000, 0.0027,  ..., 0.0021, 0.0462, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341168.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3753.2053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(403.0053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(196.9809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(163.6451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0760],
        [ 0.0454],
        [ 0.1270],
        ...,
        [-0.3906],
        [-0.3891],
        [-0.3886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128326.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0035],
        [1.0062],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367102.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0063],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367114.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1531e-03,  0.0000e+00, -8.1847e-04,  ...,  1.7423e-03,
          0.0000e+00,  4.0704e-05],
        [-1.1531e-03,  0.0000e+00, -8.1847e-04,  ...,  1.7423e-03,
          0.0000e+00,  4.0704e-05],
        [ 4.1880e-03, -2.4090e-03,  2.4541e-03,  ...,  4.3724e-03,
         -2.7715e-03,  9.8806e-03],
        ...,
        [-1.1531e-03,  0.0000e+00, -8.1847e-04,  ...,  1.7423e-03,
          0.0000e+00,  4.0704e-05],
        [ 1.3153e-02, -6.4526e-03,  7.9472e-03,  ...,  8.7871e-03,
         -7.4236e-03,  2.6397e-02],
        [-1.1531e-03,  0.0000e+00, -8.1847e-04,  ...,  1.7423e-03,
          0.0000e+00,  4.0704e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1140.8846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0222, device='cuda:0')



h[100].sum tensor(-23.9968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8223, device='cuda:0')



h[200].sum tensor(27.0912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0002],
        [0.0042, 0.0000, 0.0025,  ..., 0.0096, 0.0000, 0.0100],
        [0.0155, 0.0000, 0.0093,  ..., 0.0158, 0.0000, 0.0331],
        ...,
        [0.0133, 0.0000, 0.0080,  ..., 0.0142, 0.0000, 0.0269],
        [0.0107, 0.0000, 0.0064,  ..., 0.0129, 0.0000, 0.0220],
        [0.0480, 0.0000, 0.0289,  ..., 0.0330, 0.0000, 0.0971]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52245.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0163, 0.0000, 0.0168,  ..., 0.0105, 0.0113, 0.0358],
        [0.0112, 0.0000, 0.0272,  ..., 0.0150, 0.0094, 0.0584],
        [0.0054, 0.0000, 0.0442,  ..., 0.0247, 0.0000, 0.0971],
        ...,
        [0.0119, 0.0000, 0.0257,  ..., 0.0187, 0.0095, 0.0583],
        [0.0070, 0.0000, 0.0324,  ..., 0.0230, 0.0007, 0.0739],
        [0.0000, 0.0000, 0.0536,  ..., 0.0379, 0.0000, 0.1241]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(394655.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3642.5386, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(462.8173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.3278, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(212.3703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1414],
        [ 0.1639],
        [ 0.1817],
        ...,
        [-0.1163],
        [-0.0300],
        [ 0.0118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87316.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0035],
        [1.0063],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367114.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(565.4154, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0036],
        [1.0064],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367124.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1481e-03,  0.0000e+00, -7.9414e-04,  ...,  1.7693e-03,
          0.0000e+00,  5.8741e-06],
        [-1.1481e-03,  0.0000e+00, -7.9414e-04,  ...,  1.7693e-03,
          0.0000e+00,  5.8741e-06],
        [-1.1481e-03,  0.0000e+00, -7.9414e-04,  ...,  1.7693e-03,
          0.0000e+00,  5.8741e-06],
        ...,
        [-1.1481e-03,  0.0000e+00, -7.9414e-04,  ...,  1.7693e-03,
          0.0000e+00,  5.8741e-06],
        [-1.1481e-03,  0.0000e+00, -7.9414e-04,  ...,  1.7693e-03,
          0.0000e+00,  5.8741e-06],
        [-1.1481e-03,  0.0000e+00, -7.9414e-04,  ...,  1.7693e-03,
          0.0000e+00,  5.8741e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(928.1139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-19.7796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6771, device='cuda:0')



h[200].sum tensor(23.3302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0864e-03, 0.0000e+00,
         2.3527e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0945e-03, 0.0000e+00,
         2.3553e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.1048e-03, 0.0000e+00,
         2.3587e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.1662e-03, 0.0000e+00,
         2.3791e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.1679e-03, 0.0000e+00,
         2.3797e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.1673e-03, 0.0000e+00,
         2.3795e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47430.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0239, 0.0000, 0.0044,  ..., 0.0035, 0.0420, 0.0081],
        [0.0249, 0.0000, 0.0026,  ..., 0.0026, 0.0466, 0.0041],
        [0.0249, 0.0000, 0.0025,  ..., 0.0025, 0.0470, 0.0039],
        ...,
        [0.0251, 0.0000, 0.0025,  ..., 0.0026, 0.0474, 0.0039],
        [0.0251, 0.0000, 0.0025,  ..., 0.0026, 0.0474, 0.0039],
        [0.0251, 0.0000, 0.0025,  ..., 0.0026, 0.0474, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380032.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3674.6558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(433.5474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(237.1666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(190.7703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2944],
        [-0.4066],
        [-0.4859],
        ...,
        [-0.4022],
        [-0.4005],
        [-0.4000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122570.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0036],
        [1.0064],
        ...,
        [1.0014],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367124.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0037],
        [1.0065],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367134.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1359e-03,  0.0000e+00, -7.5981e-04,  ...,  1.7920e-03,
          0.0000e+00, -3.2684e-05],
        [-1.1359e-03,  0.0000e+00, -7.5981e-04,  ...,  1.7920e-03,
          0.0000e+00, -3.2684e-05],
        [-1.1359e-03,  0.0000e+00, -7.5981e-04,  ...,  1.7920e-03,
          0.0000e+00, -3.2684e-05],
        ...,
        [-1.1359e-03,  0.0000e+00, -7.5981e-04,  ...,  1.7920e-03,
          0.0000e+00, -3.2684e-05],
        [-1.1359e-03,  0.0000e+00, -7.5981e-04,  ...,  1.7920e-03,
          0.0000e+00, -3.2684e-05],
        [-1.1359e-03,  0.0000e+00, -7.5981e-04,  ...,  1.7920e-03,
          0.0000e+00, -3.2684e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(787.5387, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-16.9215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5757, device='cuda:0')



h[200].sum tensor(21.2708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0025,  ..., 0.0098, 0.0000, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0096, 0.0000, 0.0058,  ..., 0.0125, 0.0000, 0.0198],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42759., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0000, 0.0127,  ..., 0.0073, 0.0224, 0.0269],
        [0.0201, 0.0000, 0.0105,  ..., 0.0072, 0.0258, 0.0227],
        [0.0136, 0.0000, 0.0203,  ..., 0.0143, 0.0134, 0.0459],
        ...,
        [0.0253, 0.0000, 0.0023,  ..., 0.0025, 0.0477, 0.0039],
        [0.0253, 0.0000, 0.0023,  ..., 0.0025, 0.0477, 0.0039],
        [0.0253, 0.0000, 0.0023,  ..., 0.0025, 0.0477, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359062.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3773.9971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.3725, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.7542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(168.5312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3307],
        [-0.3404],
        [-0.3112],
        ...,
        [-0.4117],
        [-0.4100],
        [-0.4094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126917.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0037],
        [1.0065],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367134.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0038],
        [1.0066],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367145.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1333e-03,  0.0000e+00, -7.3725e-04,  ...,  1.7962e-03,
          0.0000e+00, -6.2262e-05],
        [-1.1333e-03,  0.0000e+00, -7.3725e-04,  ...,  1.7962e-03,
          0.0000e+00, -6.2262e-05],
        [-1.1333e-03,  0.0000e+00, -7.3725e-04,  ...,  1.7962e-03,
          0.0000e+00, -6.2262e-05],
        ...,
        [-1.1333e-03,  0.0000e+00, -7.3725e-04,  ...,  1.7962e-03,
          0.0000e+00, -6.2262e-05],
        [-1.1333e-03,  0.0000e+00, -7.3725e-04,  ...,  1.7962e-03,
          0.0000e+00, -6.2262e-05],
        [-1.1333e-03,  0.0000e+00, -7.3725e-04,  ...,  1.7962e-03,
          0.0000e+00, -6.2262e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1108.9412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0212, device='cuda:0')



h[100].sum tensor(-23.0761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7854, device='cuda:0')



h[200].sum tensor(27.8973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49395.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0246, 0.0000, 0.0031,  ..., 0.0029, 0.0454, 0.0055],
        [0.0247, 0.0000, 0.0037,  ..., 0.0028, 0.0443, 0.0067],
        [0.0234, 0.0000, 0.0073,  ..., 0.0038, 0.0362, 0.0143],
        ...,
        [0.0253, 0.0000, 0.0023,  ..., 0.0027, 0.0478, 0.0039],
        [0.0253, 0.0000, 0.0023,  ..., 0.0027, 0.0478, 0.0039],
        [0.0253, 0.0000, 0.0023,  ..., 0.0027, 0.0478, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(379543.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3639.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(456.5714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(244.5961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(197.5110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3030],
        [-0.2247],
        [-0.1106],
        ...,
        [-0.4157],
        [-0.4141],
        [-0.4137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103411.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0038],
        [1.0066],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367145.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0039],
        [1.0067],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367155.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1269e-03,  0.0000e+00, -7.0695e-04,  ...,  1.8093e-03,
          0.0000e+00, -9.7338e-05],
        [-1.1269e-03,  0.0000e+00, -7.0695e-04,  ...,  1.8093e-03,
          0.0000e+00, -9.7338e-05],
        [-1.1269e-03,  0.0000e+00, -7.0695e-04,  ...,  1.8093e-03,
          0.0000e+00, -9.7338e-05],
        ...,
        [-1.1269e-03,  0.0000e+00, -7.0695e-04,  ...,  1.8093e-03,
          0.0000e+00, -9.7338e-05],
        [-1.1269e-03,  0.0000e+00, -7.0695e-04,  ...,  1.8093e-03,
          0.0000e+00, -9.7338e-05],
        [-1.1269e-03,  0.0000e+00, -7.0695e-04,  ...,  1.8093e-03,
          0.0000e+00, -9.7338e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(891.3573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-18.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6370, device='cuda:0')



h[200].sum tensor(25.1217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0432, 0.0000, 0.0265,  ..., 0.0302, 0.0000, 0.0856],
        [0.0064, 0.0000, 0.0039,  ..., 0.0110, 0.0000, 0.0138],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43664.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0843,  ..., 0.0649, 0.0000, 0.2039],
        [0.0024, 0.0000, 0.0530,  ..., 0.0400, 0.0000, 0.1265],
        [0.0046, 0.0000, 0.0388,  ..., 0.0296, 0.0060, 0.0922],
        ...,
        [0.0250, 0.0000, 0.0023,  ..., 0.0029, 0.0478, 0.0039],
        [0.0250, 0.0000, 0.0023,  ..., 0.0029, 0.0478, 0.0039],
        [0.0250, 0.0000, 0.0023,  ..., 0.0029, 0.0478, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358366.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3669.0718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(429.3340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(207.8554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(172.4276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0576],
        [ 0.0552],
        [ 0.0477],
        ...,
        [-0.4202],
        [-0.4185],
        [-0.4180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124282.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0039],
        [1.0067],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367155.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0039],
        [1.0069],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367167.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(913.9482, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-18.7491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6448, device='cuda:0')



h[200].sum tensor(26.6758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45430.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0221, 0.0000, 0.0065,  ..., 0.0053, 0.0364, 0.0134],
        [0.0238, 0.0000, 0.0033,  ..., 0.0037, 0.0446, 0.0062],
        [0.0245, 0.0000, 0.0023,  ..., 0.0031, 0.0473, 0.0040],
        ...,
        [0.0247, 0.0000, 0.0023,  ..., 0.0032, 0.0477, 0.0040],
        [0.0247, 0.0000, 0.0023,  ..., 0.0032, 0.0477, 0.0040],
        [0.0247, 0.0000, 0.0023,  ..., 0.0032, 0.0477, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370002.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3466.5789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.0039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.5855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(180.4497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2828],
        [-0.4046],
        [-0.4963],
        ...,
        [-0.4209],
        [-0.4193],
        [-0.4187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117592.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0039],
        [1.0069],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367167.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0040],
        [1.0070],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367178.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(958.7003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0180, device='cuda:0')



h[100].sum tensor(-19.4282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6658, device='cuda:0')



h[200].sum tensor(28.0963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44945.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0165, 0.0000, 0.0151,  ..., 0.0107, 0.0156, 0.0330],
        [0.0188, 0.0000, 0.0116,  ..., 0.0086, 0.0234, 0.0249],
        [0.0212, 0.0000, 0.0081,  ..., 0.0062, 0.0332, 0.0167],
        ...,
        [0.0245, 0.0000, 0.0025,  ..., 0.0035, 0.0478, 0.0040],
        [0.0245, 0.0000, 0.0025,  ..., 0.0035, 0.0478, 0.0040],
        [0.0245, 0.0000, 0.0025,  ..., 0.0035, 0.0478, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(361512.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3500.3013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.7743, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.8277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(179.0139, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0443],
        [ 0.0335],
        [ 0.0172],
        ...,
        [-0.4186],
        [-0.4173],
        [-0.4174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108834.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0040],
        [1.0070],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367178.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0041],
        [1.0071],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367190.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(945.3335, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0175, device='cuda:0')



h[100].sum tensor(-18.9458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6489, device='cuda:0')



h[200].sum tensor(28.0282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7432, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44071.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0243, 0.0000, 0.0027,  ..., 0.0040, 0.0472, 0.0041],
        [0.0243, 0.0000, 0.0027,  ..., 0.0040, 0.0472, 0.0041],
        [0.0244, 0.0000, 0.0028,  ..., 0.0039, 0.0474, 0.0041],
        ...,
        [0.0246, 0.0000, 0.0028,  ..., 0.0040, 0.0478, 0.0041],
        [0.0246, 0.0000, 0.0028,  ..., 0.0040, 0.0478, 0.0041],
        [0.0246, 0.0000, 0.0028,  ..., 0.0040, 0.0478, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(360579.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3499.2581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.8515, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.3638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(174.6999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3845],
        [-0.4005],
        [-0.4075],
        ...,
        [-0.4165],
        [-0.4148],
        [-0.4144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125167.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0041],
        [1.0071],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367190.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0042],
        [1.0072],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367202.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(952.2747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0175, device='cuda:0')



h[100].sum tensor(-18.9112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6476, device='cuda:0')



h[200].sum tensor(27.9297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45156.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0232, 0.0000, 0.0050,  ..., 0.0057, 0.0421, 0.0087],
        [0.0187, 0.0000, 0.0114,  ..., 0.0101, 0.0244, 0.0239],
        [0.0136, 0.0000, 0.0191,  ..., 0.0152, 0.0100, 0.0419],
        ...,
        [0.0248, 0.0000, 0.0031,  ..., 0.0044, 0.0479, 0.0043],
        [0.0248, 0.0000, 0.0031,  ..., 0.0044, 0.0480, 0.0043],
        [0.0248, 0.0000, 0.0031,  ..., 0.0044, 0.0480, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(366108.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3462.4866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.2624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(202.3372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(177.9615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0027],
        [ 0.0533],
        [ 0.1071],
        ...,
        [-0.4106],
        [-0.4090],
        [-0.4085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99941.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0042],
        [1.0072],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367202.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0043],
        [1.0073],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367214.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(955.8807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0177, device='cuda:0')



h[100].sum tensor(-18.8347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6543, device='cuda:0')



h[200].sum tensor(27.8144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44561.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0245, 0.0000, 0.0051,  ..., 0.0045, 0.0444, 0.0076],
        [0.0245, 0.0000, 0.0049,  ..., 0.0045, 0.0447, 0.0074],
        [0.0237, 0.0000, 0.0058,  ..., 0.0054, 0.0422, 0.0096],
        ...,
        [0.0229, 0.0000, 0.0063,  ..., 0.0066, 0.0405, 0.0111],
        [0.0191, 0.0000, 0.0120,  ..., 0.0104, 0.0248, 0.0245],
        [0.0172, 0.0000, 0.0149,  ..., 0.0122, 0.0169, 0.0311]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(360764.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3495.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(434.4798, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(198.9176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(174.4641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2029],
        [-0.1639],
        [-0.0982],
        ...,
        [-0.2298],
        [-0.1337],
        [-0.0744]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108431.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0043],
        [1.0073],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367214.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0044],
        [1.0074],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367226.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [ 0.0123, -0.0059,  0.0075,  ...,  0.0084, -0.0068,  0.0246],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1695.4177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0308, device='cuda:0')



h[100].sum tensor(-32.9622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1386, device='cuda:0')



h[200].sum tensor(41.2751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6065, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0221, 0.0000, 0.0135,  ..., 0.0190, 0.0000, 0.0448],
        [0.0222, 0.0000, 0.0135,  ..., 0.0191, 0.0000, 0.0449],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63556.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0000, 0.0274,  ..., 0.0202, 0.0000, 0.0603],
        [0.0038, 0.0000, 0.0485,  ..., 0.0380, 0.0000, 0.1124],
        [0.0015, 0.0000, 0.0602,  ..., 0.0477, 0.0000, 0.1411],
        ...,
        [0.0248, 0.0000, 0.0035,  ..., 0.0047, 0.0490, 0.0044],
        [0.0249, 0.0000, 0.0035,  ..., 0.0047, 0.0490, 0.0044],
        [0.0249, 0.0000, 0.0035,  ..., 0.0047, 0.0490, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447579.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3282.7234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(512.2180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(320.5648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(262.2185, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1761],
        [ 0.1708],
        [ 0.1703],
        ...,
        [-0.4118],
        [-0.4101],
        [-0.4096]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91334.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0044],
        [1.0074],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367226.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(576.5854, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0045],
        [1.0075],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367237.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033, -0.0020,  0.0020,  ...,  0.0039, -0.0023,  0.0081],
        [ 0.0185, -0.0086,  0.0113,  ...,  0.0114, -0.0100,  0.0361],
        [ 0.0294, -0.0134,  0.0180,  ...,  0.0168, -0.0155,  0.0561],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1301.0323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0236, device='cuda:0')



h[100].sum tensor(-25.2431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8748, device='cuda:0')



h[200].sum tensor(34.1151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0599, 0.0000, 0.0367,  ..., 0.0387, 0.0000, 0.1182],
        [0.0678, 0.0000, 0.0414,  ..., 0.0426, 0.0000, 0.1327],
        [0.0916, 0.0000, 0.0560,  ..., 0.0543, 0.0000, 0.1765],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55713.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1430,  ..., 0.1164, 0.0000, 0.3470],
        [0.0000, 0.0000, 0.1544,  ..., 0.1268, 0.0000, 0.3762],
        [0.0000, 0.0000, 0.1728,  ..., 0.1441, 0.0000, 0.4235],
        ...,
        [0.0248, 0.0000, 0.0034,  ..., 0.0044, 0.0497, 0.0043],
        [0.0248, 0.0000, 0.0034,  ..., 0.0044, 0.0497, 0.0043],
        [0.0248, 0.0000, 0.0034,  ..., 0.0044, 0.0497, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420570.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3317.6858, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(486.3752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.4281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(226.9129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1629],
        [ 0.1588],
        [ 0.1469],
        ...,
        [-0.4245],
        [-0.4229],
        [-0.4225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103593.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0045],
        [1.0075],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367237.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0046],
        [1.0076],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367249.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0102, -0.0049,  0.0062,  ...,  0.0073, -0.0057,  0.0206],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [ 0.0298, -0.0135,  0.0183,  ...,  0.0169, -0.0156,  0.0568],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1645.6597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0295, device='cuda:0')



h[100].sum tensor(-31.7342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0914, device='cuda:0')



h[200].sum tensor(39.9592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0081, 0.0000, 0.0049,  ..., 0.0115, 0.0000, 0.0168],
        [0.0636, 0.0000, 0.0389,  ..., 0.0404, 0.0000, 0.1247],
        [0.0602, 0.0000, 0.0368,  ..., 0.0382, 0.0000, 0.1166],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60150.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0068, 0.0000, 0.0446,  ..., 0.0343, 0.0037, 0.1037],
        [0.0000, 0.0000, 0.0964,  ..., 0.0769, 0.0000, 0.2320],
        [0.0000, 0.0000, 0.1206,  ..., 0.0978, 0.0000, 0.2927],
        ...,
        [0.0249, 0.0000, 0.0032,  ..., 0.0041, 0.0507, 0.0041],
        [0.0231, 0.0000, 0.0064,  ..., 0.0057, 0.0425, 0.0112],
        [0.0204, 0.0000, 0.0110,  ..., 0.0080, 0.0306, 0.0216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423950.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3349.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.4862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(306.8551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(247.7062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0627],
        [ 0.0795],
        [ 0.0774],
        ...,
        [-0.3712],
        [-0.2897],
        [-0.1846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118035.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0046],
        [1.0076],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367249.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0077],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367261.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [ 0.0099, -0.0048,  0.0060,  ...,  0.0071, -0.0055,  0.0200],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(757.1150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-14.7934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5113, device='cuda:0')



h[200].sum tensor(23.2033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0128, 0.0000, 0.0078,  ..., 0.0144, 0.0000, 0.0273],
        [0.0106, 0.0000, 0.0064,  ..., 0.0139, 0.0000, 0.0251],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39481.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0197, 0.0000, 0.0115,  ..., 0.0083, 0.0287, 0.0233],
        [0.0101, 0.0000, 0.0295,  ..., 0.0182, 0.0091, 0.0643],
        [0.0054, 0.0000, 0.0372,  ..., 0.0207, 0.0014, 0.0808],
        ...,
        [0.0251, 0.0000, 0.0030,  ..., 0.0040, 0.0517, 0.0040],
        [0.0251, 0.0000, 0.0030,  ..., 0.0040, 0.0517, 0.0040],
        [0.0251, 0.0000, 0.0030,  ..., 0.0040, 0.0517, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347744.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3523.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(424.1171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(177.8273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(151.4545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1113],
        [ 0.0006],
        [ 0.0713],
        ...,
        [-0.4443],
        [-0.4426],
        [-0.4421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135130.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0077],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367261.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0047],
        [1.0078],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367273.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0187, -0.0086,  0.0114,  ...,  0.0115, -0.0100,  0.0363],
        [ 0.0188, -0.0087,  0.0115,  ...,  0.0115, -0.0100,  0.0365],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1248.7898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0227, device='cuda:0')



h[100].sum tensor(-24.1561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8393, device='cuda:0')



h[200].sum tensor(31.2923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1900, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0769, 0.0000, 0.0471,  ..., 0.0469, 0.0000, 0.1491],
        [0.0553, 0.0000, 0.0338,  ..., 0.0358, 0.0000, 0.1074],
        [0.0495, 0.0000, 0.0303,  ..., 0.0335, 0.0000, 0.0987],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53122.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1215,  ..., 0.0965, 0.0000, 0.2939],
        [0.0000, 0.0000, 0.1226,  ..., 0.0961, 0.0000, 0.2956],
        [0.0000, 0.0000, 0.1179,  ..., 0.0896, 0.0000, 0.2818],
        ...,
        [0.0253, 0.0000, 0.0029,  ..., 0.0040, 0.0526, 0.0039],
        [0.0253, 0.0000, 0.0029,  ..., 0.0040, 0.0527, 0.0040],
        [0.0253, 0.0000, 0.0029,  ..., 0.0040, 0.0527, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410839.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3442.8843, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(481.6845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(267.2544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(213.2768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0061],
        [-0.0072],
        [-0.0017],
        ...,
        [-0.4497],
        [-0.4479],
        [-0.4474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126747.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0047],
        [1.0078],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367273.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0048],
        [1.0079],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367284.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032, -0.0019,  0.0019,  ...,  0.0038, -0.0021,  0.0077],
        [ 0.0047, -0.0025,  0.0029,  ...,  0.0046, -0.0029,  0.0106],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0018,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(858.6985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-16.7697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5756, device='cuda:0')



h[200].sum tensor(23.4884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0338, 0.0000, 0.0206,  ..., 0.0258, 0.0000, 0.0699],
        [0.0103, 0.0000, 0.0062,  ..., 0.0132, 0.0000, 0.0228],
        [0.0047, 0.0000, 0.0029,  ..., 0.0099, 0.0000, 0.0106],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42330.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4063e-04, 0.0000e+00, 7.0852e-02,  ..., 4.8419e-02, 0.0000e+00,
         1.6291e-01],
        [7.5226e-03, 0.0000e+00, 4.0684e-02,  ..., 2.6545e-02, 5.1119e-03,
         9.0956e-02],
        [1.4643e-02, 0.0000e+00, 2.0587e-02,  ..., 1.3658e-02, 1.9935e-02,
         4.4066e-02],
        ...,
        [2.5693e-02, 0.0000e+00, 2.9406e-03,  ..., 4.0379e-03, 5.3350e-02,
         4.0501e-03],
        [2.5699e-02, 0.0000e+00, 2.9413e-03,  ..., 4.0385e-03, 5.3362e-02,
         4.0510e-03],
        [2.5697e-02, 0.0000e+00, 2.9413e-03,  ..., 4.0375e-03, 5.3360e-02,
         4.0507e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(362431.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3624.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(439.1720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(199.0464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.3982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0150],
        [-0.0109],
        [-0.0782],
        ...,
        [-0.4516],
        [-0.4499],
        [-0.4494]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130110.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0048],
        [1.0079],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367284.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0048],
        [1.0080],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367296.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(932.5903, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0171, device='cuda:0')



h[100].sum tensor(-18.0903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6345, device='cuda:0')



h[200].sum tensor(24.5603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0062, 0.0000, 0.0037,  ..., 0.0106, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44928.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0197, 0.0000, 0.0125,  ..., 0.0096, 0.0275, 0.0256],
        [0.0155, 0.0000, 0.0188,  ..., 0.0135, 0.0151, 0.0400],
        [0.0095, 0.0000, 0.0309,  ..., 0.0206, 0.0068, 0.0677],
        ...,
        [0.0260, 0.0000, 0.0031,  ..., 0.0042, 0.0537, 0.0042],
        [0.0260, 0.0000, 0.0031,  ..., 0.0042, 0.0537, 0.0042],
        [0.0260, 0.0000, 0.0031,  ..., 0.0042, 0.0537, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(379096.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3584.8452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(445.8986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(216.3138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(173.4927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0225],
        [ 0.0038],
        [ 0.0329],
        ...,
        [-0.4509],
        [-0.4492],
        [-0.4487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127855.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0048],
        [1.0080],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367296.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0080],
        ...,
        [1.0016],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367308.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(925.4802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-17.8905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6162, device='cuda:0')



h[200].sum tensor(24.1941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1519, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43943.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0251, 0.0000, 0.0059,  ..., 0.0049, 0.0470, 0.0101],
        [0.0258, 0.0000, 0.0042,  ..., 0.0044, 0.0512, 0.0064],
        [0.0261, 0.0000, 0.0033,  ..., 0.0043, 0.0532, 0.0047],
        ...,
        [0.0263, 0.0000, 0.0032,  ..., 0.0044, 0.0540, 0.0044],
        [0.0263, 0.0000, 0.0032,  ..., 0.0044, 0.0540, 0.0045],
        [0.0263, 0.0000, 0.0032,  ..., 0.0044, 0.0540, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368834.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3664.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.4706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(209.4590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(168.3723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1580],
        [-0.2452],
        [-0.3400],
        ...,
        [-0.4432],
        [-0.4444],
        [-0.4451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124837.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0080],
        ...,
        [1.0016],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367308.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0081],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367319.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0086, -0.0042,  0.0052,  ...,  0.0065, -0.0049,  0.0179],
        [ 0.0099, -0.0048,  0.0060,  ...,  0.0071, -0.0055,  0.0202],
        [ 0.0102, -0.0049,  0.0062,  ...,  0.0072, -0.0056,  0.0207],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(960.2737, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-18.4874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6434, device='cuda:0')



h[200].sum tensor(24.6048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0000, 0.0189,  ..., 0.0244, 0.0000, 0.0654],
        [0.0372, 0.0000, 0.0225,  ..., 0.0273, 0.0000, 0.0763],
        [0.0482, 0.0000, 0.0293,  ..., 0.0327, 0.0000, 0.0967],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45242.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1111,  ..., 0.0791, 0.0000, 0.2588],
        [0.0000, 0.0000, 0.1023,  ..., 0.0708, 0.0000, 0.2365],
        [0.0000, 0.0000, 0.0917,  ..., 0.0625, 0.0000, 0.2107],
        ...,
        [0.0267, 0.0000, 0.0032,  ..., 0.0047, 0.0544, 0.0047],
        [0.0267, 0.0000, 0.0032,  ..., 0.0047, 0.0544, 0.0047],
        [0.0267, 0.0000, 0.0032,  ..., 0.0047, 0.0544, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(376827.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3723.9878, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(444.1373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(217.5405, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(174.0848, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0311],
        [ 0.0593],
        [ 0.0714],
        ...,
        [-0.4491],
        [-0.4473],
        [-0.4466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115468.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0081],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367319.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0082],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367330.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0092, -0.0045,  0.0056,  ...,  0.0068, -0.0052,  0.0190],
        [ 0.0032, -0.0019,  0.0019,  ...,  0.0038, -0.0022,  0.0079],
        [ 0.0049, -0.0026,  0.0029,  ...,  0.0046, -0.0030,  0.0109],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(871.7177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-16.7124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5792, device='cuda:0')



h[200].sum tensor(23.4202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4814, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0137, 0.0000, 0.0082,  ..., 0.0152, 0.0000, 0.0312],
        [0.0339, 0.0000, 0.0205,  ..., 0.0256, 0.0000, 0.0703],
        [0.0149, 0.0000, 0.0090,  ..., 0.0153, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41786.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0000, 0.0459,  ..., 0.0273, 0.0000, 0.1008],
        [0.0000, 0.0000, 0.0584,  ..., 0.0366, 0.0000, 0.1302],
        [0.0066, 0.0000, 0.0420,  ..., 0.0264, 0.0014, 0.0926],
        ...,
        [0.0267, 0.0000, 0.0032,  ..., 0.0050, 0.0548, 0.0049],
        [0.0267, 0.0000, 0.0032,  ..., 0.0050, 0.0548, 0.0049],
        [0.0267, 0.0000, 0.0032,  ..., 0.0050, 0.0548, 0.0049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(362408.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3798.1941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(423.0280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(198.5058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(159.2018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1323],
        [ 0.1324],
        [ 0.0877],
        ...,
        [-0.4527],
        [-0.4510],
        [-0.4505]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140210.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0082],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367330.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0082],
        ...,
        [1.0016],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367341.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036, -0.0020,  0.0022,  ...,  0.0040, -0.0024,  0.0087],
        [ 0.0097, -0.0046,  0.0059,  ...,  0.0070, -0.0054,  0.0198],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1461.8080, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0261, device='cuda:0')



h[100].sum tensor(-27.6505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9676, device='cuda:0')



h[200].sum tensor(34.5888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.5104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0330, 0.0000, 0.0199,  ..., 0.0251, 0.0000, 0.0686],
        [0.0137, 0.0000, 0.0082,  ..., 0.0152, 0.0000, 0.0311],
        [0.0125, 0.0000, 0.0075,  ..., 0.0140, 0.0000, 0.0269],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62963.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0583,  ..., 0.0370, 0.0000, 0.1302],
        [0.0044, 0.0000, 0.0453,  ..., 0.0276, 0.0000, 0.0995],
        [0.0104, 0.0000, 0.0328,  ..., 0.0215, 0.0095, 0.0719],
        ...,
        [0.0267, 0.0000, 0.0031,  ..., 0.0053, 0.0552, 0.0050],
        [0.0267, 0.0000, 0.0031,  ..., 0.0053, 0.0552, 0.0050],
        [0.0267, 0.0000, 0.0031,  ..., 0.0053, 0.0552, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472198.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3408.0278, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.5933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.4853, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(255.4873, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1188],
        [ 0.0948],
        [ 0.0251],
        ...,
        [-0.4572],
        [-0.4552],
        [-0.4545]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115084.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0049],
        [1.0082],
        ...,
        [1.0016],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367341.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(524.5499, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0050],
        [1.0083],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367352.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0209, -0.0094,  0.0127,  ...,  0.0125, -0.0109,  0.0405],
        [ 0.0429, -0.0188,  0.0262,  ...,  0.0232, -0.0218,  0.0810],
        [ 0.0210, -0.0095,  0.0128,  ...,  0.0125, -0.0110,  0.0407],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(845.4595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0153, device='cuda:0')



h[100].sum tensor(-16.1703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5646, device='cuda:0')



h[200].sum tensor(24.0186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2185, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1032, 0.0000, 0.0630,  ..., 0.0594, 0.0000, 0.1979],
        [0.0958, 0.0000, 0.0585,  ..., 0.0558, 0.0000, 0.1844],
        [0.1048, 0.0000, 0.0640,  ..., 0.0602, 0.0000, 0.2010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41302.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1540,  ..., 0.1268, 0.0000, 0.3749],
        [0.0000, 0.0000, 0.1637,  ..., 0.1348, 0.0000, 0.3987],
        [0.0000, 0.0000, 0.1639,  ..., 0.1344, 0.0000, 0.3987],
        ...,
        [0.0267, 0.0000, 0.0030,  ..., 0.0056, 0.0558, 0.0051],
        [0.0267, 0.0000, 0.0030,  ..., 0.0056, 0.0558, 0.0051],
        [0.0267, 0.0000, 0.0030,  ..., 0.0056, 0.0558, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(361691.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3732.2002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(426.9997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(194.6858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(157.8759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0506],
        [ 0.0424],
        [ 0.0504],
        ...,
        [-0.4621],
        [-0.4603],
        [-0.4597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136928.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0050],
        [1.0083],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367352.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0050],
        [1.0084],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367362.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [ 0.0043, -0.0023,  0.0026,  ...,  0.0043, -0.0027,  0.0098],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(921.6095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-17.5233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6119, device='cuda:0')



h[200].sum tensor(25.7707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0237, 0.0000, 0.0143,  ..., 0.0206, 0.0000, 0.0515],
        [0.0033, 0.0000, 0.0020,  ..., 0.0090, 0.0000, 0.0080],
        [0.0043, 0.0000, 0.0026,  ..., 0.0095, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43164.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0052, 0.0000, 0.0409,  ..., 0.0259, 0.0000, 0.0903],
        [0.0156, 0.0000, 0.0217,  ..., 0.0151, 0.0150, 0.0470],
        [0.0165, 0.0000, 0.0193,  ..., 0.0146, 0.0150, 0.0420],
        ...,
        [0.0267, 0.0000, 0.0030,  ..., 0.0059, 0.0563, 0.0052],
        [0.0267, 0.0000, 0.0030,  ..., 0.0059, 0.0563, 0.0052],
        [0.0267, 0.0000, 0.0030,  ..., 0.0059, 0.0563, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368781.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3662.4641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(431.4369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(208.5567, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(166.8995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1304],
        [-0.1997],
        [-0.2365],
        ...,
        [-0.4663],
        [-0.4645],
        [-0.4639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141933.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0050],
        [1.0084],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367362.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0050],
        [1.0084],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367373.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [ 0.0128, -0.0059,  0.0078,  ...,  0.0085, -0.0069,  0.0256],
        [ 0.0197, -0.0089,  0.0120,  ...,  0.0119, -0.0103,  0.0383],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1102.4534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0196, device='cuda:0')



h[100].sum tensor(-20.7871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7268, device='cuda:0')



h[200].sum tensor(29.2134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0230, 0.0000, 0.0139,  ..., 0.0197, 0.0000, 0.0483],
        [0.0344, 0.0000, 0.0209,  ..., 0.0253, 0.0000, 0.0693],
        [0.0533, 0.0000, 0.0324,  ..., 0.0351, 0.0000, 0.1062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48079.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0941,  ..., 0.0730, 0.0000, 0.2229],
        [0.0000, 0.0000, 0.1105,  ..., 0.0874, 0.0000, 0.2637],
        [0.0000, 0.0000, 0.1309,  ..., 0.1045, 0.0000, 0.3138],
        ...,
        [0.0268, 0.0000, 0.0029,  ..., 0.0062, 0.0567, 0.0054],
        [0.0268, 0.0000, 0.0029,  ..., 0.0062, 0.0567, 0.0054],
        [0.0268, 0.0000, 0.0029,  ..., 0.0062, 0.0567, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389848.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3604.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.5255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(237.2309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(188.8051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1011],
        [-0.0900],
        [-0.0809],
        ...,
        [-0.4680],
        [-0.4660],
        [-0.4651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124604.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0050],
        [1.0084],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367373.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0050],
        [1.0085],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367384.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [ 0.0046, -0.0025,  0.0028,  ...,  0.0045, -0.0029,  0.0106],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(942.4465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0168, device='cuda:0')



h[100].sum tensor(-17.6758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6212, device='cuda:0')



h[200].sum tensor(26.6567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0047, 0.0000, 0.0028,  ..., 0.0097, 0.0000, 0.0106],
        [0.0295, 0.0000, 0.0180,  ..., 0.0224, 0.0000, 0.0585],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44246.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0170, 0.0000, 0.0169,  ..., 0.0157, 0.0237, 0.0381],
        [0.0091, 0.0000, 0.0356,  ..., 0.0282, 0.0097, 0.0819],
        [0.0012, 0.0000, 0.0694,  ..., 0.0547, 0.0000, 0.1641],
        ...,
        [0.0269, 0.0000, 0.0029,  ..., 0.0067, 0.0568, 0.0057],
        [0.0269, 0.0000, 0.0029,  ..., 0.0067, 0.0569, 0.0057],
        [0.0269, 0.0000, 0.0029,  ..., 0.0067, 0.0569, 0.0057]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(377344.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3628.3127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(439.8497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(213.2268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(171.7729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0032],
        [ 0.0748],
        [ 0.1140],
        ...,
        [-0.4699],
        [-0.4680],
        [-0.4675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128793.1797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0050],
        [1.0085],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367384.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0051],
        [1.0085],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367395.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(670.0911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0118, device='cuda:0')



h[100].sum tensor(-12.5838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4385, device='cuda:0')



h[200].sum tensor(22.3336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0241, 0.0000, 0.0147,  ..., 0.0197, 0.0000, 0.0484],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37888.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0057, 0.0000, 0.0427,  ..., 0.0371, 0.0015, 0.1013],
        [0.0165, 0.0000, 0.0161,  ..., 0.0170, 0.0256, 0.0375],
        [0.0229, 0.0000, 0.0077,  ..., 0.0106, 0.0419, 0.0174],
        ...,
        [0.0268, 0.0000, 0.0028,  ..., 0.0070, 0.0572, 0.0057],
        [0.0268, 0.0000, 0.0028,  ..., 0.0070, 0.0572, 0.0057],
        [0.0268, 0.0000, 0.0028,  ..., 0.0070, 0.0572, 0.0057]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(352982.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3654.5398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(419.5393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(172.3220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(143.4891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0155],
        [-0.1656],
        [-0.3325],
        ...,
        [-0.4751],
        [-0.4732],
        [-0.4726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131153.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0051],
        [1.0085],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367395.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0051],
        [1.0086],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367406.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0102, -0.0048,  0.0063,  ...,  0.0073, -0.0056,  0.0209],
        [ 0.0218, -0.0097,  0.0134,  ...,  0.0129, -0.0113,  0.0422],
        [ 0.0321, -0.0141,  0.0197,  ...,  0.0180, -0.0163,  0.0612],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(752.1064, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0133, device='cuda:0')



h[100].sum tensor(-13.9344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4908, device='cuda:0')



h[200].sum tensor(24.8015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0700, 0.0000, 0.0428,  ..., 0.0433, 0.0000, 0.1370],
        [0.1014, 0.0000, 0.0622,  ..., 0.0587, 0.0000, 0.1950],
        [0.1523, 0.0000, 0.0934,  ..., 0.0835, 0.0000, 0.2887],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38587.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1659,  ..., 0.1452, 0.0000, 0.4106],
        [0.0000, 0.0000, 0.2309,  ..., 0.2105, 0.0000, 0.5800],
        [0.0000, 0.0000, 0.2990,  ..., 0.2791, 0.0000, 0.7580],
        ...,
        [0.0266, 0.0000, 0.0027,  ..., 0.0073, 0.0573, 0.0058],
        [0.0266, 0.0000, 0.0027,  ..., 0.0073, 0.0573, 0.0058],
        [0.0266, 0.0000, 0.0027,  ..., 0.0073, 0.0573, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353429.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3544.2173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(420.9938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(181.1010, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(148.6570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0488],
        [-0.0078],
        [-0.0579],
        ...,
        [-0.4811],
        [-0.4792],
        [-0.4786]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149202.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0051],
        [1.0086],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367406.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0051],
        [1.0086],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367417.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(978.1353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-17.9061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6364, device='cuda:0')



h[200].sum tensor(29.5739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5177, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0033,  ..., 0.0101, 0.0000, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0069, 0.0000, 0.0042,  ..., 0.0109, 0.0000, 0.0147],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45678.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0000, 0.0141,  ..., 0.0141, 0.0272, 0.0322],
        [0.0208, 0.0000, 0.0107,  ..., 0.0124, 0.0341, 0.0245],
        [0.0144, 0.0000, 0.0201,  ..., 0.0192, 0.0203, 0.0468],
        ...,
        [0.0264, 0.0000, 0.0025,  ..., 0.0076, 0.0575, 0.0058],
        [0.0264, 0.0000, 0.0025,  ..., 0.0076, 0.0575, 0.0058],
        [0.0264, 0.0000, 0.0025,  ..., 0.0076, 0.0575, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390214.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3314.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.0495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(226.3125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(181.3009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2187],
        [-0.1716],
        [-0.0766],
        ...,
        [-0.4860],
        [-0.4840],
        [-0.4835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141095.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0051],
        [1.0086],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367417.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0051],
        [1.0087],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367428., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031, -0.0018,  0.0019,  ...,  0.0038, -0.0021,  0.0077],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1332.9812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0228, device='cuda:0')



h[100].sum tensor(-24.2608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8430, device='cuda:0')



h[200].sum tensor(36.0322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.2557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0126, 0.0000, 0.0077,  ..., 0.0142, 0.0000, 0.0272],
        [0.0237, 0.0000, 0.0146,  ..., 0.0202, 0.0000, 0.0497],
        [0.0419, 0.0000, 0.0257,  ..., 0.0291, 0.0000, 0.0832],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54588.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0690,  ..., 0.0578, 0.0000, 0.1664],
        [0.0000, 0.0000, 0.0903,  ..., 0.0772, 0.0000, 0.2198],
        [0.0000, 0.0000, 0.1136,  ..., 0.1000, 0.0000, 0.2796],
        ...,
        [0.0265, 0.0000, 0.0025,  ..., 0.0079, 0.0581, 0.0059],
        [0.0265, 0.0000, 0.0025,  ..., 0.0079, 0.0581, 0.0059],
        [0.0265, 0.0000, 0.0025,  ..., 0.0079, 0.0581, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427317.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3257.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(495.5360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(284.7997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(222.6195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1141],
        [ 0.1100],
        [ 0.1066],
        ...,
        [-0.4894],
        [-0.4874],
        [-0.4868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136180., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0051],
        [1.0087],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367428., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0052],
        [1.0088],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367439.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(809.9097, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0139, device='cuda:0')



h[100].sum tensor(-14.6636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5157, device='cuda:0')



h[200].sum tensor(26.8145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0028,  ..., 0.0103, 0.0000, 0.0123],
        [0.0023, 0.0000, 0.0014,  ..., 0.0087, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41537.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0209, 0.0000, 0.0158,  ..., 0.0120, 0.0245, 0.0355],
        [0.0224, 0.0000, 0.0119,  ..., 0.0110, 0.0343, 0.0268],
        [0.0243, 0.0000, 0.0073,  ..., 0.0096, 0.0457, 0.0167],
        ...,
        [0.0265, 0.0000, 0.0025,  ..., 0.0082, 0.0583, 0.0061],
        [0.0265, 0.0000, 0.0025,  ..., 0.0082, 0.0583, 0.0061],
        [0.0265, 0.0000, 0.0025,  ..., 0.0082, 0.0583, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372629.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3341.3911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.6819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(198.1892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(163.4496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3155],
        [-0.3558],
        [-0.4133],
        ...,
        [-0.4939],
        [-0.4919],
        [-0.4913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133066.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0052],
        [1.0088],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367439.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0088],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367450.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2081e-03, -2.2424e-03,  2.5933e-03,  ...,  4.3614e-03,
         -2.6028e-03,  9.7801e-03],
        [ 4.0680e-03, -2.1838e-03,  2.5074e-03,  ...,  4.2931e-03,
         -2.5347e-03,  9.5218e-03],
        [ 9.4209e-03, -4.4262e-03,  5.7934e-03,  ...,  6.9068e-03,
         -5.1375e-03,  1.9393e-02],
        ...,
        [-1.1448e-03,  0.0000e+00, -6.9272e-04,  ...,  1.7477e-03,
          0.0000e+00, -9.1370e-05],
        [-1.1448e-03,  0.0000e+00, -6.9272e-04,  ...,  1.7477e-03,
          0.0000e+00, -9.1370e-05],
        [-1.1448e-03,  0.0000e+00, -6.9272e-04,  ...,  1.7477e-03,
          0.0000e+00, -9.1370e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1079.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-19.3634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6817, device='cuda:0')



h[200].sum tensor(31.3658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3377, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0000, 0.0093,  ..., 0.0155, 0.0000, 0.0320],
        [0.0316, 0.0000, 0.0195,  ..., 0.0247, 0.0000, 0.0665],
        [0.0141, 0.0000, 0.0087,  ..., 0.0156, 0.0000, 0.0321],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46969.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0000, 0.0457,  ..., 0.0377, 0.0023, 0.1077],
        [0.0000, 0.0000, 0.0645,  ..., 0.0513, 0.0000, 0.1524],
        [0.0000, 0.0000, 0.0584,  ..., 0.0459, 0.0000, 0.1375],
        ...,
        [0.0267, 0.0000, 0.0026,  ..., 0.0086, 0.0583, 0.0064],
        [0.0267, 0.0000, 0.0026,  ..., 0.0086, 0.0583, 0.0064],
        [0.0267, 0.0000, 0.0026,  ..., 0.0086, 0.0583, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390229.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3313.0571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(475.6312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(230.1930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(189.3192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1837],
        [ 0.2013],
        [ 0.2081],
        ...,
        [-0.4937],
        [-0.4917],
        [-0.4913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126344.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0088],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367450.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(510.7689, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0089],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367462.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1491e-03,  0.0000e+00, -7.0047e-04,  ...,  1.7565e-03,
          0.0000e+00, -8.0758e-05],
        [-1.1491e-03,  0.0000e+00, -7.0047e-04,  ...,  1.7565e-03,
          0.0000e+00, -8.0758e-05],
        [-1.1491e-03,  0.0000e+00, -7.0047e-04,  ...,  1.7565e-03,
          0.0000e+00, -8.0758e-05],
        ...,
        [-1.1491e-03,  0.0000e+00, -7.0047e-04,  ...,  1.7565e-03,
          0.0000e+00, -8.0758e-05],
        [-1.1491e-03,  0.0000e+00, -7.0047e-04,  ...,  1.7565e-03,
          0.0000e+00, -8.0758e-05],
        [-1.1491e-03,  0.0000e+00, -7.0047e-04,  ...,  1.7565e-03,
          0.0000e+00, -8.0758e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1011.0384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-18.0330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6457, device='cuda:0')



h[200].sum tensor(29.7078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45824.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0265, 0.0000, 0.0027,  ..., 0.0088, 0.0576, 0.0066],
        [0.0265, 0.0000, 0.0031,  ..., 0.0088, 0.0571, 0.0073],
        [0.0265, 0.0000, 0.0039,  ..., 0.0086, 0.0556, 0.0090],
        ...,
        [0.0269, 0.0000, 0.0028,  ..., 0.0088, 0.0586, 0.0067],
        [0.0269, 0.0000, 0.0028,  ..., 0.0089, 0.0586, 0.0067],
        [0.0269, 0.0000, 0.0028,  ..., 0.0088, 0.0586, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393316.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3446.9429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(464.4353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(225.2915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(185.7544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3566],
        [-0.3632],
        [-0.3551],
        ...,
        [-0.4942],
        [-0.4928],
        [-0.4928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147783.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0052],
        [1.0089],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367462.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0089],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367474.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1566e-03,  0.0000e+00, -7.1619e-04,  ...,  1.7662e-03,
          0.0000e+00, -6.5105e-05],
        [-1.1566e-03,  0.0000e+00, -7.1619e-04,  ...,  1.7662e-03,
          0.0000e+00, -6.5105e-05],
        [-1.1566e-03,  0.0000e+00, -7.1619e-04,  ...,  1.7662e-03,
          0.0000e+00, -6.5105e-05],
        ...,
        [-1.1566e-03,  0.0000e+00, -7.1619e-04,  ...,  1.7662e-03,
          0.0000e+00, -6.5105e-05],
        [-1.1566e-03,  0.0000e+00, -7.1619e-04,  ...,  1.7662e-03,
          0.0000e+00, -6.5105e-05],
        [-1.1566e-03,  0.0000e+00, -7.1619e-04,  ...,  1.7662e-03,
          0.0000e+00, -6.5105e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(812.2952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-14.3870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5096, device='cuda:0')



h[200].sum tensor(25.2574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0022, 0.0000, 0.0014,  ..., 0.0087, 0.0000, 0.0062],
        [0.0045, 0.0000, 0.0027,  ..., 0.0104, 0.0000, 0.0124],
        [0.0022, 0.0000, 0.0014,  ..., 0.0088, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41966.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0209, 0.0000, 0.0161,  ..., 0.0137, 0.0245, 0.0363],
        [0.0194, 0.0000, 0.0198,  ..., 0.0150, 0.0154, 0.0444],
        [0.0216, 0.0000, 0.0153,  ..., 0.0132, 0.0271, 0.0345],
        ...,
        [0.0273, 0.0000, 0.0030,  ..., 0.0091, 0.0590, 0.0071],
        [0.0273, 0.0000, 0.0030,  ..., 0.0091, 0.0590, 0.0071],
        [0.0273, 0.0000, 0.0030,  ..., 0.0091, 0.0590, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380140.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3467.7979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(459.0179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(196.8644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(166.5406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0147],
        [ 0.0538],
        [ 0.0785],
        ...,
        [-0.4968],
        [-0.4948],
        [-0.4942]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128015.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0089],
        ...,
        [1.0014],
        [1.0010],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367474.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0090],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367486.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1543e-03,  0.0000e+00, -7.2126e-04,  ...,  1.7746e-03,
          0.0000e+00, -5.5341e-05],
        [-1.1543e-03,  0.0000e+00, -7.2126e-04,  ...,  1.7746e-03,
          0.0000e+00, -5.5341e-05],
        [ 1.0025e-02, -4.6488e-03,  6.1397e-03,  ...,  7.2323e-03,
         -5.4009e-03,  2.0564e-02],
        ...,
        [-1.1543e-03,  0.0000e+00, -7.2126e-04,  ...,  1.7746e-03,
          0.0000e+00, -5.5341e-05],
        [-1.1543e-03,  0.0000e+00, -7.2126e-04,  ...,  1.7746e-03,
          0.0000e+00, -5.5341e-05],
        [-1.1543e-03,  0.0000e+00, -7.2126e-04,  ...,  1.7746e-03,
          0.0000e+00, -5.5341e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1300.5398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0219, device='cuda:0')



h[100].sum tensor(-22.9890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8104, device='cuda:0')



h[200].sum tensor(33.1795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.6670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0000, 0.0030,  ..., 0.0101, 0.0000, 0.0112],
        [0.0296, 0.0000, 0.0181,  ..., 0.0233, 0.0000, 0.0609],
        [0.0465, 0.0000, 0.0285,  ..., 0.0316, 0.0000, 0.0921],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52415.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0092, 0.0000, 0.0417,  ..., 0.0365, 0.0088, 0.0986],
        [0.0011, 0.0000, 0.0864,  ..., 0.0752, 0.0000, 0.2087],
        [0.0000, 0.0000, 0.1241,  ..., 0.1102, 0.0000, 0.3036],
        ...,
        [0.0277, 0.0000, 0.0031,  ..., 0.0091, 0.0594, 0.0073],
        [0.0277, 0.0000, 0.0031,  ..., 0.0091, 0.0594, 0.0073],
        [0.0277, 0.0000, 0.0031,  ..., 0.0091, 0.0594, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(416445.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3441.7456, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(503.3675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(265.3328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(214.2172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1577],
        [ 0.2104],
        [ 0.2376],
        ...,
        [-0.5010],
        [-0.4991],
        [-0.4986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126436.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0090],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367486.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0092],
        ...,
        [1.0014],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367498.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.3022e-03, -3.0862e-03,  3.8634e-03,  ...,  5.4119e-03,
         -3.5865e-03,  1.3667e-02],
        [-1.1385e-03,  0.0000e+00, -7.0437e-04,  ...,  1.7807e-03,
          0.0000e+00, -5.3683e-05],
        [-1.1385e-03,  0.0000e+00, -7.0437e-04,  ...,  1.7807e-03,
          0.0000e+00, -5.3683e-05],
        ...,
        [-1.1385e-03,  0.0000e+00, -7.0437e-04,  ...,  1.7807e-03,
          0.0000e+00, -5.3683e-05],
        [-1.1385e-03,  0.0000e+00, -7.0437e-04,  ...,  1.7807e-03,
          0.0000e+00, -5.3683e-05],
        [-1.1385e-03,  0.0000e+00, -7.0437e-04,  ...,  1.7807e-03,
          0.0000e+00, -5.3683e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(839.3250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0140, device='cuda:0')



h[100].sum tensor(-14.5401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5179, device='cuda:0')



h[200].sum tensor(25.4422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3730, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0000, 0.0132,  ..., 0.0187, 0.0000, 0.0437],
        [0.0063, 0.0000, 0.0039,  ..., 0.0108, 0.0000, 0.0137],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40883.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.0636,  ..., 0.0555, 0.0007, 0.1519],
        [0.0108, 0.0000, 0.0321,  ..., 0.0296, 0.0161, 0.0756],
        [0.0212, 0.0000, 0.0129,  ..., 0.0146, 0.0310, 0.0297],
        ...,
        [0.0279, 0.0000, 0.0031,  ..., 0.0088, 0.0595, 0.0073],
        [0.0279, 0.0000, 0.0031,  ..., 0.0088, 0.0595, 0.0073],
        [0.0279, 0.0000, 0.0031,  ..., 0.0088, 0.0595, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370967., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3583.0757, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.4848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(191.9684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.5983, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1487],
        [ 0.0902],
        [-0.0071],
        ...,
        [-0.5098],
        [-0.5078],
        [-0.5072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139712.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0092],
        ...,
        [1.0014],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367498.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0093],
        ...,
        [1.0014],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367510.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1230e-03,  0.0000e+00, -6.8744e-04,  ...,  1.7821e-03,
          0.0000e+00, -5.3268e-05],
        [ 1.8777e-02, -8.2326e-03,  1.1533e-02,  ...,  1.1490e-02,
         -9.5703e-03,  3.6636e-02],
        [ 2.2192e-02, -9.6452e-03,  1.3629e-02,  ...,  1.3156e-02,
         -1.1212e-02,  4.2931e-02],
        ...,
        [-1.1230e-03,  0.0000e+00, -6.8744e-04,  ...,  1.7821e-03,
          0.0000e+00, -5.3268e-05],
        [-1.1230e-03,  0.0000e+00, -6.8744e-04,  ...,  1.7821e-03,
          0.0000e+00, -5.3268e-05],
        [-1.1230e-03,  0.0000e+00, -6.8744e-04,  ...,  1.7821e-03,
          0.0000e+00, -5.3268e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(889.3282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0145, device='cuda:0')



h[100].sum tensor(-15.1587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5363, device='cuda:0')



h[200].sum tensor(26.8150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7053, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0000, 0.0116,  ..., 0.0169, 0.0000, 0.0367],
        [0.0418, 0.0000, 0.0256,  ..., 0.0286, 0.0000, 0.0810],
        [0.1000, 0.0000, 0.0614,  ..., 0.0581, 0.0000, 0.1924],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42671.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0071, 0.0000, 0.0509,  ..., 0.0483, 0.0071, 0.1238],
        [0.0000, 0.0000, 0.0911,  ..., 0.0831, 0.0000, 0.2231],
        [0.0000, 0.0000, 0.1395,  ..., 0.1257, 0.0000, 0.3433],
        ...,
        [0.0280, 0.0000, 0.0032,  ..., 0.0084, 0.0592, 0.0072],
        [0.0280, 0.0000, 0.0032,  ..., 0.0084, 0.0593, 0.0072],
        [0.0280, 0.0000, 0.0032,  ..., 0.0084, 0.0593, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(384822.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3569.0681, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.7857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(204.8531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(171.5357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1083],
        [ 0.1399],
        [ 0.1562],
        ...,
        [-0.5165],
        [-0.5145],
        [-0.5139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156284.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0093],
        ...,
        [1.0014],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367510.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0094],
        ...,
        [1.0013],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367522.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1131e-03,  0.0000e+00, -6.8061e-04,  ...,  1.7803e-03,
          0.0000e+00, -5.0070e-05],
        [-1.1131e-03,  0.0000e+00, -6.8061e-04,  ...,  1.7803e-03,
          0.0000e+00, -5.0070e-05],
        [-1.1131e-03,  0.0000e+00, -6.8061e-04,  ...,  1.7803e-03,
          0.0000e+00, -5.0070e-05],
        ...,
        [-1.1131e-03,  0.0000e+00, -6.8061e-04,  ...,  1.7803e-03,
          0.0000e+00, -5.0070e-05],
        [-1.1131e-03,  0.0000e+00, -6.8061e-04,  ...,  1.7803e-03,
          0.0000e+00, -5.0070e-05],
        [-1.1131e-03,  0.0000e+00, -6.8061e-04,  ...,  1.7803e-03,
          0.0000e+00, -5.0070e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1338.6990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0220, device='cuda:0')



h[100].sum tensor(-22.8577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8154, device='cuda:0')



h[200].sum tensor(34.8710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51029.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0207, 0.0000, 0.0157,  ..., 0.0145, 0.0264, 0.0345],
        [0.0261, 0.0000, 0.0075,  ..., 0.0095, 0.0478, 0.0159],
        [0.0279, 0.0000, 0.0041,  ..., 0.0080, 0.0570, 0.0085],
        ...,
        [0.0283, 0.0000, 0.0034,  ..., 0.0081, 0.0591, 0.0072],
        [0.0283, 0.0000, 0.0034,  ..., 0.0081, 0.0591, 0.0072],
        [0.0283, 0.0000, 0.0034,  ..., 0.0081, 0.0591, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(408851.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3509.8818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.4409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(255.4363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(209.1749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0031],
        [-0.1217],
        [-0.2525],
        ...,
        [-0.5177],
        [-0.5159],
        [-0.5156]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131099.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0094],
        ...,
        [1.0013],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367522.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0095],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367534.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1103e-03,  0.0000e+00, -6.8348e-04,  ...,  1.7802e-03,
          0.0000e+00, -4.5218e-05],
        [-1.1103e-03,  0.0000e+00, -6.8348e-04,  ...,  1.7802e-03,
          0.0000e+00, -4.5218e-05],
        [-1.1103e-03,  0.0000e+00, -6.8348e-04,  ...,  1.7802e-03,
          0.0000e+00, -4.5218e-05],
        ...,
        [-1.1103e-03,  0.0000e+00, -6.8348e-04,  ...,  1.7802e-03,
          0.0000e+00, -4.5218e-05],
        [-1.1103e-03,  0.0000e+00, -6.8348e-04,  ...,  1.7802e-03,
          0.0000e+00, -4.5218e-05],
        [-1.1103e-03,  0.0000e+00, -6.8348e-04,  ...,  1.7802e-03,
          0.0000e+00, -4.5218e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(982.9510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-16.3934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5850, device='cuda:0')



h[200].sum tensor(28.5909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43856.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0281, 0.0000, 0.0036,  ..., 0.0080, 0.0582, 0.0071],
        [0.0268, 0.0000, 0.0053,  ..., 0.0094, 0.0527, 0.0113],
        [0.0251, 0.0000, 0.0077,  ..., 0.0112, 0.0457, 0.0169],
        ...,
        [0.0286, 0.0000, 0.0036,  ..., 0.0080, 0.0592, 0.0072],
        [0.0286, 0.0000, 0.0036,  ..., 0.0080, 0.0592, 0.0072],
        [0.0286, 0.0000, 0.0036,  ..., 0.0080, 0.0592, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(384258.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3624.0010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.7137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(208.0546, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(175.7796, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5131],
        [-0.4700],
        [-0.3766],
        ...,
        [-0.5240],
        [-0.5220],
        [-0.5214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133351.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0095],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367534.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0057],
        [1.0096],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367546.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.0411e-03, -2.5269e-03,  3.0858e-03,  ...,  4.7811e-03,
         -2.9402e-03,  1.1309e-02],
        [ 1.3416e-02, -5.9658e-03,  8.2297e-03,  ...,  8.8643e-03,
         -6.9416e-03,  2.6747e-02],
        [ 1.7246e-02, -7.5381e-03,  1.0582e-02,  ...,  1.0731e-02,
         -8.7710e-03,  3.3805e-02],
        ...,
        [-1.1133e-03,  0.0000e+00, -6.9391e-04,  ...,  1.7806e-03,
          0.0000e+00, -3.4581e-05],
        [-1.1133e-03,  0.0000e+00, -6.9391e-04,  ...,  1.7806e-03,
          0.0000e+00, -3.4581e-05],
        [-1.1133e-03,  0.0000e+00, -6.9391e-04,  ...,  1.7806e-03,
          0.0000e+00, -3.4581e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2017.0996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0322, device='cuda:0')



h[100].sum tensor(-34.4286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1932, device='cuda:0')



h[200].sum tensor(45.9929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0253, 0.0000, 0.0155,  ..., 0.0206, 0.0000, 0.0507],
        [0.0572, 0.0000, 0.0351,  ..., 0.0372, 0.0000, 0.1135],
        [0.0701, 0.0000, 0.0430,  ..., 0.0435, 0.0000, 0.1373],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71133.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0000, 0.0768,  ..., 0.0626, 0.0000, 0.1787],
        [0.0000, 0.0000, 0.1213,  ..., 0.1010, 0.0000, 0.2872],
        [0.0000, 0.0000, 0.1526,  ..., 0.1294, 0.0000, 0.3644],
        ...,
        [0.0291, 0.0000, 0.0039,  ..., 0.0081, 0.0595, 0.0072],
        [0.0291, 0.0000, 0.0039,  ..., 0.0081, 0.0595, 0.0072],
        [0.0291, 0.0000, 0.0039,  ..., 0.0081, 0.0595, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511338.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3575.9131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(587.8057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(387.0559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.7827, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1043],
        [ 0.0909],
        [ 0.0734],
        ...,
        [-0.5176],
        [-0.5200],
        [-0.5208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142710.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0057],
        [1.0096],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367546.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0097],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367559., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6212e-02, -7.1012e-03,  9.9370e-03,  ...,  1.0230e-02,
         -8.2652e-03,  3.1939e-02],
        [ 2.9290e-02, -1.2458e-02,  1.7968e-02,  ...,  1.6606e-02,
         -1.4500e-02,  5.6048e-02],
        [ 1.5329e-02, -6.7395e-03,  9.3947e-03,  ...,  9.7991e-03,
         -7.8442e-03,  3.0311e-02],
        ...,
        [-1.1234e-03,  0.0000e+00, -7.0919e-04,  ...,  1.7772e-03,
          0.0000e+00, -1.9790e-05],
        [-1.1234e-03,  0.0000e+00, -7.0919e-04,  ...,  1.7772e-03,
          0.0000e+00, -1.9790e-05],
        [-1.1234e-03,  0.0000e+00, -7.0919e-04,  ...,  1.7772e-03,
          0.0000e+00, -1.9790e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(967.5412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0153, device='cuda:0')



h[100].sum tensor(-15.8379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5677, device='cuda:0')



h[200].sum tensor(27.1447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0807, 0.0000, 0.0495,  ..., 0.0487, 0.0000, 0.1569],
        [0.0836, 0.0000, 0.0513,  ..., 0.0501, 0.0000, 0.1623],
        [0.0888, 0.0000, 0.0545,  ..., 0.0527, 0.0000, 0.1720],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42972.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1253,  ..., 0.1078, 0.0000, 0.2981],
        [0.0000, 0.0000, 0.1461,  ..., 0.1239, 0.0000, 0.3474],
        [0.0000, 0.0000, 0.1502,  ..., 0.1260, 0.0000, 0.3565],
        ...,
        [0.0295, 0.0000, 0.0042,  ..., 0.0083, 0.0598, 0.0074],
        [0.0295, 0.0000, 0.0042,  ..., 0.0083, 0.0598, 0.0074],
        [0.0295, 0.0000, 0.0042,  ..., 0.0083, 0.0598, 0.0074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380761.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3822.0688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(485.1113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(200.3174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(168.8308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0763],
        [ 0.1160],
        [ 0.1374],
        ...,
        [-0.5247],
        [-0.5228],
        [-0.5223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136516.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0097],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367559., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0097],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367559., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0918e-02, -9.0289e-03,  1.2827e-02,  ...,  1.2524e-02,
         -1.0509e-02,  4.0615e-02],
        [ 4.1751e-02, -1.7562e-02,  2.5620e-02,  ...,  2.2681e-02,
         -2.0441e-02,  7.9019e-02],
        [ 4.0829e-02, -1.7185e-02,  2.5054e-02,  ...,  2.2232e-02,
         -2.0002e-02,  7.7320e-02],
        ...,
        [-1.1234e-03,  0.0000e+00, -7.0919e-04,  ...,  1.7772e-03,
          0.0000e+00, -1.9790e-05],
        [-1.1234e-03,  0.0000e+00, -7.0919e-04,  ...,  1.7772e-03,
          0.0000e+00, -1.9790e-05],
        [-1.1234e-03,  0.0000e+00, -7.0919e-04,  ...,  1.7772e-03,
          0.0000e+00, -1.9790e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(997.3781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-16.3605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5829, device='cuda:0')



h[200].sum tensor(27.6609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5490, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1042, 0.0000, 0.0639,  ..., 0.0601, 0.0000, 0.2003],
        [0.1395, 0.0000, 0.0856,  ..., 0.0774, 0.0000, 0.2654],
        [0.1464, 0.0000, 0.0899,  ..., 0.0808, 0.0000, 0.2782],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44824.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1770,  ..., 0.1596, 0.0000, 0.4296],
        [0.0000, 0.0000, 0.2277,  ..., 0.2060, 0.0000, 0.5555],
        [0.0000, 0.0000, 0.2366,  ..., 0.2127, 0.0000, 0.5764],
        ...,
        [0.0295, 0.0000, 0.0042,  ..., 0.0083, 0.0598, 0.0074],
        [0.0295, 0.0000, 0.0042,  ..., 0.0083, 0.0598, 0.0074],
        [0.0295, 0.0000, 0.0042,  ..., 0.0083, 0.0598, 0.0074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393124.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3825.5403, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.5955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(210.9739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(177.6920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0383],
        [ 0.0716],
        [ 0.0985],
        ...,
        [-0.5236],
        [-0.5215],
        [-0.5203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130142.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0097],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367559., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(487.9748, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0098],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367571.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.9592e-03, -3.7151e-03,  4.8611e-03,  ...,  6.2174e-03,
         -4.3254e-03,  1.6752e-02],
        [ 2.0749e-02, -8.9420e-03,  1.2715e-02,  ...,  1.2455e-02,
         -1.0411e-02,  4.0335e-02],
        [ 1.7621e-02, -7.6634e-03,  1.0794e-02,  ...,  1.0929e-02,
         -8.9224e-03,  3.4566e-02],
        ...,
        [-1.1317e-03,  0.0000e+00, -7.2133e-04,  ...,  1.7842e-03,
          0.0000e+00, -9.1767e-06],
        [-1.1317e-03,  0.0000e+00, -7.2133e-04,  ...,  1.7842e-03,
          0.0000e+00, -9.1767e-06],
        [-1.1317e-03,  0.0000e+00, -7.2133e-04,  ...,  1.7842e-03,
          0.0000e+00, -9.1767e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1411.0432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0227, device='cuda:0')



h[100].sum tensor(-23.5263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8411, device='cuda:0')



h[200].sum tensor(34.1568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.2227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0567, 0.0000, 0.0347,  ..., 0.0370, 0.0000, 0.1128],
        [0.0563, 0.0000, 0.0344,  ..., 0.0368, 0.0000, 0.1121],
        [0.0589, 0.0000, 0.0361,  ..., 0.0381, 0.0000, 0.1170],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53955.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0918,  ..., 0.0764, 0.0000, 0.2131],
        [0.0000, 0.0000, 0.1023,  ..., 0.0846, 0.0000, 0.2379],
        [0.0000, 0.0000, 0.1018,  ..., 0.0839, 0.0000, 0.2366],
        ...,
        [0.0300, 0.0000, 0.0043,  ..., 0.0087, 0.0605, 0.0075],
        [0.0300, 0.0000, 0.0043,  ..., 0.0087, 0.0605, 0.0075],
        [0.0300, 0.0000, 0.0043,  ..., 0.0087, 0.0605, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433717.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3786.7815, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(531.5916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(269.7502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(216.7355, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1614],
        [ 0.1695],
        [ 0.1716],
        ...,
        [-0.5258],
        [-0.5248],
        [-0.5248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125354.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0058],
        [1.0098],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367571.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0059],
        [1.0099],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367583.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1101e-02, -4.9872e-03,  6.7843e-03,  ...,  7.7693e-03,
         -5.8083e-03,  2.2557e-02],
        [ 9.8899e-03, -4.4934e-03,  6.0406e-03,  ...,  7.1786e-03,
         -5.2332e-03,  2.0324e-02],
        [ 1.0876e-02, -4.8954e-03,  6.6460e-03,  ...,  7.6595e-03,
         -5.7014e-03,  2.2142e-02],
        ...,
        [-1.1321e-03,  0.0000e+00, -7.2737e-04,  ...,  1.8030e-03,
          0.0000e+00, -3.5383e-07],
        [ 1.2613e-02, -5.6035e-03,  7.7126e-03,  ...,  8.5066e-03,
         -6.5261e-03,  2.5345e-02],
        [-1.1321e-03,  0.0000e+00, -7.2737e-04,  ...,  1.8030e-03,
          0.0000e+00, -3.5383e-07]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1274.5474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0206, device='cuda:0')



h[100].sum tensor(-21.1615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7634, device='cuda:0')



h[200].sum tensor(31.1187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0378, 0.0000, 0.0231,  ..., 0.0279, 0.0000, 0.0780],
        [0.0389, 0.0000, 0.0238,  ..., 0.0284, 0.0000, 0.0802],
        [0.0376, 0.0000, 0.0230,  ..., 0.0278, 0.0000, 0.0778],
        ...,
        [0.0128, 0.0000, 0.0079,  ..., 0.0142, 0.0000, 0.0258],
        [0.0103, 0.0000, 0.0063,  ..., 0.0129, 0.0000, 0.0211],
        [0.0462, 0.0000, 0.0283,  ..., 0.0321, 0.0000, 0.0938]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50290.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0808,  ..., 0.0610, 0.0000, 0.1819],
        [0.0000, 0.0000, 0.0803,  ..., 0.0616, 0.0000, 0.1814],
        [0.0000, 0.0000, 0.0794,  ..., 0.0608, 0.0000, 0.1792],
        ...,
        [0.0156, 0.0000, 0.0268,  ..., 0.0259, 0.0156, 0.0600],
        [0.0105, 0.0000, 0.0334,  ..., 0.0305, 0.0034, 0.0751],
        [0.0000, 0.0000, 0.0542,  ..., 0.0462, 0.0000, 0.1235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413157.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3878.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(523.4761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.5910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(198.1099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2228],
        [ 0.2225],
        [ 0.2214],
        ...,
        [-0.1119],
        [-0.0618],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113830.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0059],
        [1.0099],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367583.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0060],
        [1.0100],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367595.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1258e-03,  0.0000e+00, -7.2296e-04,  ...,  1.8243e-03,
          0.0000e+00,  7.4270e-06],
        [-1.1258e-03,  0.0000e+00, -7.2296e-04,  ...,  1.8243e-03,
          0.0000e+00,  7.4270e-06],
        [-1.1258e-03,  0.0000e+00, -7.2296e-04,  ...,  1.8243e-03,
          0.0000e+00,  7.4270e-06],
        ...,
        [-1.1258e-03,  0.0000e+00, -7.2296e-04,  ...,  1.8243e-03,
          0.0000e+00,  7.4270e-06],
        [-1.1258e-03,  0.0000e+00, -7.2296e-04,  ...,  1.8243e-03,
          0.0000e+00,  7.4270e-06],
        [-1.1258e-03,  0.0000e+00, -7.2296e-04,  ...,  1.8243e-03,
          0.0000e+00,  7.4270e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1464.6567, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0235, device='cuda:0')



h[100].sum tensor(-24.4157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8693, device='cuda:0')



h[200].sum tensor(34.0201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.3165e-03, 0.0000e+00,
         2.9787e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.3295e-03, 0.0000e+00,
         2.9840e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.3447e-03, 0.0000e+00,
         2.9902e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4305e-03, 0.0000e+00,
         3.0251e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4319e-03, 0.0000e+00,
         3.0257e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.4313e-03, 0.0000e+00,
         3.0255e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54954.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0258, 0.0000, 0.0120,  ..., 0.0130, 0.0392, 0.0251],
        [0.0287, 0.0000, 0.0074,  ..., 0.0104, 0.0521, 0.0151],
        [0.0297, 0.0000, 0.0058,  ..., 0.0095, 0.0568, 0.0114],
        ...,
        [0.0308, 0.0000, 0.0039,  ..., 0.0091, 0.0624, 0.0074],
        [0.0308, 0.0000, 0.0039,  ..., 0.0091, 0.0624, 0.0074],
        [0.0308, 0.0000, 0.0039,  ..., 0.0091, 0.0624, 0.0074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433335.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3841.5471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(543.1766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(279.9478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(217.5051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0021],
        [-0.1200],
        [-0.2460],
        ...,
        [-0.5448],
        [-0.5427],
        [-0.5420]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120700.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0060],
        [1.0100],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367595.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0061],
        [1.0101],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367606.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0981e-03,  0.0000e+00, -6.8643e-04,  ...,  1.8589e-03,
          0.0000e+00, -2.7399e-05],
        [-1.0981e-03,  0.0000e+00, -6.8643e-04,  ...,  1.8589e-03,
          0.0000e+00, -2.7399e-05],
        [ 9.7730e-03, -4.4096e-03,  5.9912e-03,  ...,  7.1617e-03,
         -5.1388e-03,  2.0016e-02],
        ...,
        [-1.0981e-03,  0.0000e+00, -6.8643e-04,  ...,  1.8589e-03,
          0.0000e+00, -2.7399e-05],
        [-1.0981e-03,  0.0000e+00, -6.8643e-04,  ...,  1.8589e-03,
          0.0000e+00, -2.7399e-05],
        [-1.0981e-03,  0.0000e+00, -6.8643e-04,  ...,  1.8589e-03,
          0.0000e+00, -2.7399e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1389.6218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0222, device='cuda:0')



h[100].sum tensor(-22.9363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8213, device='cuda:0')



h[200].sum tensor(33.1128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0176, 0.0000, 0.0108,  ..., 0.0171, 0.0000, 0.0365],
        [0.0241, 0.0000, 0.0148,  ..., 0.0209, 0.0000, 0.0505],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0123, 0.0000, 0.0075,  ..., 0.0147, 0.0000, 0.0268]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51480.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0000, 0.0185,  ..., 0.0201, 0.0262, 0.0422],
        [0.0084, 0.0000, 0.0458,  ..., 0.0403, 0.0055, 0.1060],
        [0.0022, 0.0000, 0.0691,  ..., 0.0565, 0.0000, 0.1598],
        ...,
        [0.0281, 0.0000, 0.0079,  ..., 0.0119, 0.0502, 0.0175],
        [0.0224, 0.0000, 0.0171,  ..., 0.0172, 0.0275, 0.0380],
        [0.0103, 0.0000, 0.0382,  ..., 0.0307, 0.0064, 0.0861]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(418615.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3911.9158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(528.6052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.0119, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(202.9509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0308],
        [ 0.0519],
        [ 0.1161],
        ...,
        [-0.3158],
        [-0.1359],
        [ 0.0249]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138975.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0061],
        [1.0101],
        ...,
        [1.0013],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367606.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0062],
        [1.0102],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367618.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.8352e-03, -4.0119e-03,  5.4270e-03,  ...,  6.7093e-03,
         -4.6768e-03,  1.8230e-02],
        [ 2.5302e-02, -1.0674e-02,  1.5543e-02,  ...,  1.4742e-02,
         -1.2443e-02,  4.8589e-02],
        [ 3.3784e-03, -1.8040e-03,  2.0746e-03,  ...,  4.0475e-03,
         -2.1030e-03,  8.1691e-03],
        ...,
        [-1.0803e-03,  0.0000e+00, -6.6465e-04,  ...,  1.8725e-03,
          0.0000e+00, -5.1418e-05],
        [-1.0803e-03,  0.0000e+00, -6.6465e-04,  ...,  1.8725e-03,
          0.0000e+00, -5.1418e-05],
        [-1.0803e-03,  0.0000e+00, -6.6465e-04,  ...,  1.8725e-03,
          0.0000e+00, -5.1418e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1000.0570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-15.9653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5748, device='cuda:0')



h[200].sum tensor(26.9470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0895, 0.0000, 0.0550,  ..., 0.0533, 0.0000, 0.1728],
        [0.0431, 0.0000, 0.0265,  ..., 0.0306, 0.0000, 0.0872],
        [0.0485, 0.0000, 0.0298,  ..., 0.0333, 0.0000, 0.0973],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42036.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1358,  ..., 0.1200, 0.0000, 0.3274],
        [0.0000, 0.0000, 0.1081,  ..., 0.0920, 0.0000, 0.2572],
        [0.0000, 0.0000, 0.0913,  ..., 0.0773, 0.0000, 0.2161],
        ...,
        [0.0308, 0.0000, 0.0032,  ..., 0.0095, 0.0631, 0.0073],
        [0.0308, 0.0000, 0.0032,  ..., 0.0095, 0.0631, 0.0074],
        [0.0308, 0.0000, 0.0032,  ..., 0.0095, 0.0631, 0.0074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382356.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4000.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(488.2983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(204.3141, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(161.1897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1250],
        [ 0.1252],
        [ 0.1149],
        ...,
        [-0.5663],
        [-0.5641],
        [-0.5635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171476.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0062],
        [1.0102],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367618.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0063],
        [1.0103],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367630.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.6282e-03, -3.5132e-03,  4.6847e-03,  ...,  6.1103e-03,
         -4.0967e-03,  1.5983e-02],
        [ 1.6390e-02, -7.0494e-03,  1.0067e-02,  ...,  1.0384e-02,
         -8.2203e-03,  3.2138e-02],
        [ 7.6854e-03, -3.5362e-03,  4.7198e-03,  ...,  6.1382e-03,
         -4.1236e-03,  1.6089e-02],
        ...,
        [-1.0761e-03,  0.0000e+00, -6.6284e-04,  ...,  1.8645e-03,
          0.0000e+00, -6.5792e-05],
        [-1.0761e-03,  0.0000e+00, -6.6284e-04,  ...,  1.8645e-03,
          0.0000e+00, -6.5792e-05],
        [-1.0761e-03,  0.0000e+00, -6.6284e-04,  ...,  1.8645e-03,
          0.0000e+00, -6.5792e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1520.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0238, device='cuda:0')



h[100].sum tensor(-24.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8801, device='cuda:0')



h[200].sum tensor(36.5154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.9274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0545, 0.0000, 0.0335,  ..., 0.0356, 0.0000, 0.1062],
        [0.0534, 0.0000, 0.0328,  ..., 0.0357, 0.0000, 0.1062],
        [0.0540, 0.0000, 0.0332,  ..., 0.0354, 0.0000, 0.1054],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57560.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1516,  ..., 0.1401, 0.0000, 0.3700],
        [0.0000, 0.0000, 0.1661,  ..., 0.1524, 0.0000, 0.4051],
        [0.0000, 0.0000, 0.1595,  ..., 0.1471, 0.0000, 0.3893],
        ...,
        [0.0307, 0.0000, 0.0032,  ..., 0.0098, 0.0629, 0.0073],
        [0.0307, 0.0000, 0.0032,  ..., 0.0098, 0.0629, 0.0073],
        [0.0307, 0.0000, 0.0032,  ..., 0.0098, 0.0629, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(453660.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3782.3052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(554.2246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(301.0652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(232.8293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1180],
        [ 0.1232],
        [ 0.1246],
        ...,
        [-0.5680],
        [-0.5658],
        [-0.5652]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149695.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0063],
        [1.0103],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367630.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0065],
        [1.0105],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367641.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0798e-03,  0.0000e+00, -6.7207e-04,  ...,  1.8418e-03,
          0.0000e+00, -6.9669e-05],
        [-1.0798e-03,  0.0000e+00, -6.7207e-04,  ...,  1.8418e-03,
          0.0000e+00, -6.9669e-05],
        [-1.0798e-03,  0.0000e+00, -6.7207e-04,  ...,  1.8418e-03,
          0.0000e+00, -6.9669e-05],
        ...,
        [-1.0798e-03,  0.0000e+00, -6.7207e-04,  ...,  1.8418e-03,
          0.0000e+00, -6.9669e-05],
        [-1.0798e-03,  0.0000e+00, -6.7207e-04,  ...,  1.8418e-03,
          0.0000e+00, -6.9669e-05],
        [-1.0798e-03,  0.0000e+00, -6.7207e-04,  ...,  1.8418e-03,
          0.0000e+00, -6.9669e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1087.4591, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0164, device='cuda:0')



h[100].sum tensor(-16.9095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6088, device='cuda:0')



h[200].sum tensor(29.6289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45762.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0000, 0.0047,  ..., 0.0105, 0.0576, 0.0106],
        [0.0302, 0.0000, 0.0034,  ..., 0.0102, 0.0609, 0.0078],
        [0.0303, 0.0000, 0.0031,  ..., 0.0102, 0.0618, 0.0072],
        ...,
        [0.0306, 0.0000, 0.0031,  ..., 0.0103, 0.0625, 0.0073],
        [0.0306, 0.0000, 0.0031,  ..., 0.0103, 0.0625, 0.0073],
        [0.0306, 0.0000, 0.0031,  ..., 0.0103, 0.0625, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402392.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3817.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(511.6784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(218.8063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(178.0779, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2932],
        [-0.4355],
        [-0.5515],
        ...,
        [-0.5662],
        [-0.5641],
        [-0.5635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141223.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0065],
        [1.0105],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367641.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0066],
        [1.0106],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367654., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0929e-03,  0.0000e+00, -6.9375e-04,  ...,  1.8101e-03,
          0.0000e+00, -6.3837e-05],
        [-1.0929e-03,  0.0000e+00, -6.9375e-04,  ...,  1.8101e-03,
          0.0000e+00, -6.3837e-05],
        [-1.0929e-03,  0.0000e+00, -6.9375e-04,  ...,  1.8101e-03,
          0.0000e+00, -6.3837e-05],
        ...,
        [-1.0929e-03,  0.0000e+00, -6.9375e-04,  ...,  1.8101e-03,
          0.0000e+00, -6.3837e-05],
        [-1.0929e-03,  0.0000e+00, -6.9375e-04,  ...,  1.8101e-03,
          0.0000e+00, -6.3837e-05],
        [-1.0929e-03,  0.0000e+00, -6.9375e-04,  ...,  1.8101e-03,
          0.0000e+00, -6.3837e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1018.0929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0149, device='cuda:0')



h[100].sum tensor(-15.4165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5501, device='cuda:0')



h[200].sum tensor(28.7327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43047.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0301, 0.0000, 0.0031,  ..., 0.0109, 0.0610, 0.0072],
        [0.0302, 0.0000, 0.0031,  ..., 0.0109, 0.0611, 0.0072],
        [0.0303, 0.0000, 0.0031,  ..., 0.0108, 0.0613, 0.0073],
        ...,
        [0.0307, 0.0000, 0.0032,  ..., 0.0110, 0.0621, 0.0073],
        [0.0307, 0.0000, 0.0032,  ..., 0.0110, 0.0621, 0.0073],
        [0.0307, 0.0000, 0.0032,  ..., 0.0110, 0.0621, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387813.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3825.8347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(498.9489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(196.3201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(163.6699, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6312],
        [-0.6748],
        [-0.7087],
        ...,
        [-0.5605],
        [-0.5584],
        [-0.5578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138342.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0066],
        [1.0106],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367654., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0067],
        [1.0107],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367666.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1235e-02, -4.9467e-03,  6.8585e-03,  ...,  7.8013e-03,
         -5.7738e-03,  2.2725e-02],
        [ 1.9636e-02, -8.3132e-03,  1.2016e-02,  ...,  1.1899e-02,
         -9.7032e-03,  3.8225e-02],
        [ 1.3035e-02, -5.6679e-03,  7.9635e-03,  ...,  8.6793e-03,
         -6.6156e-03,  2.6045e-02],
        ...,
        [-1.1087e-03,  0.0000e+00, -7.2057e-04,  ...,  1.7796e-03,
          0.0000e+00, -5.1282e-05],
        [-1.1087e-03,  0.0000e+00, -7.2057e-04,  ...,  1.7796e-03,
          0.0000e+00, -5.1282e-05],
        [-1.1087e-03,  0.0000e+00, -7.2057e-04,  ...,  1.7796e-03,
          0.0000e+00, -5.1282e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2350.9702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.3860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0368, device='cuda:0')



h[100].sum tensor(-37.8715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.3617, device='cuda:0')



h[200].sum tensor(51.3187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0309, 0.0000, 0.0188,  ..., 0.0238, 0.0000, 0.0630],
        [0.0457, 0.0000, 0.0279,  ..., 0.0316, 0.0000, 0.0924],
        [0.0918, 0.0000, 0.0562,  ..., 0.0541, 0.0000, 0.1774],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83541.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0936,  ..., 0.0820, 0.0000, 0.2205],
        [0.0000, 0.0000, 0.1289,  ..., 0.1151, 0.0000, 0.3074],
        [0.0000, 0.0000, 0.1783,  ..., 0.1647, 0.0000, 0.4309],
        ...,
        [0.0309, 0.0000, 0.0033,  ..., 0.0116, 0.0617, 0.0075],
        [0.0309, 0.0000, 0.0033,  ..., 0.0116, 0.0618, 0.0075],
        [0.0309, 0.0000, 0.0033,  ..., 0.0116, 0.0618, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608381.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3538.8718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(659.3242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(452.1436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.6289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2818],
        [ 0.3045],
        [ 0.3336],
        ...,
        [-0.5528],
        [-0.5508],
        [-0.5503]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97925.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0067],
        [1.0107],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367666.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0069],
        [1.0108],
        ...,
        [1.0014],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367676.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.8187e-03, -4.3281e-03,  5.9737e-03,  ...,  6.9746e-03,
         -5.0533e-03,  1.9985e-02],
        [ 3.8923e-03, -1.9625e-03,  2.3330e-03,  ...,  4.0962e-03,
         -2.2913e-03,  9.0571e-03],
        [ 1.7837e-02, -7.5286e-03,  1.0899e-02,  ...,  1.0869e-02,
         -8.7902e-03,  3.4770e-02],
        ...,
        [-1.0244e-03,  0.0000e+00, -6.8737e-04,  ...,  1.7082e-03,
          0.0000e+00, -8.9043e-06],
        [-1.0244e-03,  0.0000e+00, -6.8737e-04,  ...,  1.7082e-03,
          0.0000e+00, -8.9043e-06],
        [-1.0244e-03,  0.0000e+00, -6.8737e-04,  ...,  1.7082e-03,
          0.0000e+00, -8.9043e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1498.1677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0220, device='cuda:0')



h[100].sum tensor(-22.7936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8143, device='cuda:0')



h[200].sum tensor(37.7797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7373, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0000, 0.0122,  ..., 0.0187, 0.0000, 0.0448],
        [0.0395, 0.0000, 0.0240,  ..., 0.0281, 0.0000, 0.0804],
        [0.0214, 0.0000, 0.0130,  ..., 0.0188, 0.0000, 0.0452],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57598.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0741,  ..., 0.0616, 0.0000, 0.1751],
        [0.0000, 0.0000, 0.0736,  ..., 0.0619, 0.0000, 0.1738],
        [0.0000, 0.0000, 0.0619,  ..., 0.0528, 0.0000, 0.1458],
        ...,
        [0.0308, 0.0000, 0.0027,  ..., 0.0117, 0.0610, 0.0073],
        [0.0308, 0.0000, 0.0027,  ..., 0.0117, 0.0610, 0.0073],
        [0.0308, 0.0000, 0.0027,  ..., 0.0117, 0.0610, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464934.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3604.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(569.0497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(286.0402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(227.0969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1083],
        [ 0.1160],
        [ 0.1161],
        ...,
        [-0.5593],
        [-0.5574],
        [-0.5569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112376.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0069],
        [1.0108],
        ...,
        [1.0014],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367676.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(580.5013, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0070],
        [1.0110],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367687.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.0997e-03, -3.2012e-03,  4.2840e-03,  ...,  5.5434e-03,
         -3.7388e-03,  1.4869e-02],
        [ 7.2702e-03, -3.2691e-03,  4.3888e-03,  ...,  5.6259e-03,
         -3.8180e-03,  1.5183e-02],
        [-9.5089e-04,  0.0000e+00, -6.6385e-04,  ...,  1.6486e-03,
          0.0000e+00,  3.1910e-05],
        ...,
        [-9.5089e-04,  0.0000e+00, -6.6385e-04,  ...,  1.6486e-03,
          0.0000e+00,  3.1910e-05],
        [-9.5089e-04,  0.0000e+00, -6.6385e-04,  ...,  1.6486e-03,
          0.0000e+00,  3.1910e-05],
        [-9.5089e-04,  0.0000e+00, -6.6385e-04,  ...,  1.6486e-03,
          0.0000e+00,  3.1910e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1486.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0217, device='cuda:0')



h[100].sum tensor(-22.1727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8021, device='cuda:0')



h[200].sum tensor(38.0098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0408, 0.0000, 0.0248,  ..., 0.0282, 0.0000, 0.0823],
        [0.0129, 0.0000, 0.0078,  ..., 0.0138, 0.0000, 0.0275],
        [0.0073, 0.0000, 0.0044,  ..., 0.0106, 0.0000, 0.0154],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55937.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0608,  ..., 0.0570, 0.0000, 0.1488],
        [0.0096, 0.0000, 0.0353,  ..., 0.0360, 0.0050, 0.0861],
        [0.0176, 0.0000, 0.0197,  ..., 0.0241, 0.0211, 0.0485],
        ...,
        [0.0308, 0.0000, 0.0023,  ..., 0.0117, 0.0603, 0.0070],
        [0.0308, 0.0000, 0.0023,  ..., 0.0117, 0.0604, 0.0070],
        [0.0308, 0.0000, 0.0023,  ..., 0.0117, 0.0604, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458370.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3617.9602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(563.8484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(280.3344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(217.2533, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1043],
        [-0.2123],
        [-0.3513],
        ...,
        [-0.5627],
        [-0.5608],
        [-0.5603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131865.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0070],
        [1.0110],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367687.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0071],
        [1.0111],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367696.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.0155e-04,  0.0000e+00, -6.6895e-04,  ...,  1.5993e-03,
          0.0000e+00,  8.7474e-05],
        [ 2.3773e-02, -9.7762e-03,  1.4499e-02,  ...,  1.3496e-02,
         -1.1422e-02,  4.5544e-02],
        [-9.0155e-04,  0.0000e+00, -6.6895e-04,  ...,  1.5993e-03,
          0.0000e+00,  8.7474e-05],
        ...,
        [-9.0155e-04,  0.0000e+00, -6.6895e-04,  ...,  1.5993e-03,
          0.0000e+00,  8.7474e-05],
        [-9.0155e-04,  0.0000e+00, -6.6895e-04,  ...,  1.5993e-03,
          0.0000e+00,  8.7474e-05],
        [-9.0155e-04,  0.0000e+00, -6.6895e-04,  ...,  1.5993e-03,
          0.0000e+00,  8.7474e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1467.2139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0211, device='cuda:0')



h[100].sum tensor(-21.6535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7798, device='cuda:0')



h[200].sum tensor(37.0237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1130, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0526, 0.0000, 0.0319,  ..., 0.0335, 0.0000, 0.1039],
        [0.0184, 0.0000, 0.0111,  ..., 0.0162, 0.0000, 0.0377],
        [0.0551, 0.0000, 0.0334,  ..., 0.0348, 0.0000, 0.1086],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54569.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0595,  ..., 0.0583, 0.0000, 0.1493],
        [0.0005, 0.0000, 0.0482,  ..., 0.0486, 0.0000, 0.1207],
        [0.0000, 0.0000, 0.0613,  ..., 0.0600, 0.0000, 0.1539],
        ...,
        [0.0311, 0.0000, 0.0022,  ..., 0.0119, 0.0603, 0.0067],
        [0.0311, 0.0000, 0.0022,  ..., 0.0119, 0.0603, 0.0067],
        [0.0311, 0.0000, 0.0022,  ..., 0.0119, 0.0603, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442114.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3715.4187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(559.1000, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(278.0970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(207.7320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1797],
        [-0.1939],
        [-0.2160],
        ...,
        [-0.5547],
        [-0.5533],
        [-0.5539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143334.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0071],
        [1.0111],
        ...,
        [1.0014],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367696.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0112],
        ...,
        [1.0013],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367705.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0025,  0.0032,  ...,  0.0046, -0.0029,  0.0119],
        [ 0.0181, -0.0075,  0.0110,  ...,  0.0107, -0.0088,  0.0351],
        [ 0.0226, -0.0092,  0.0137,  ...,  0.0128, -0.0108,  0.0433],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1270.3945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-18.3629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6629, device='cuda:0')



h[200].sum tensor(32.5476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.0000, 0.0156,  ..., 0.0196, 0.0000, 0.0516],
        [0.0618, 0.0000, 0.0374,  ..., 0.0377, 0.0000, 0.1209],
        [0.1048, 0.0000, 0.0638,  ..., 0.0583, 0.0000, 0.2001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49129.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0704,  ..., 0.0673, 0.0000, 0.1782],
        [0.0000, 0.0000, 0.1156,  ..., 0.1093, 0.0000, 0.2965],
        [0.0000, 0.0000, 0.1644,  ..., 0.1572, 0.0000, 0.4265],
        ...,
        [0.0317, 0.0000, 0.0024,  ..., 0.0122, 0.0605, 0.0062],
        [0.0317, 0.0000, 0.0024,  ..., 0.0122, 0.0606, 0.0062],
        [0.0317, 0.0000, 0.0024,  ..., 0.0122, 0.0606, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415067.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3923.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(543.1536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.5890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(180.8527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3674],
        [-0.5202],
        [-0.6908],
        ...,
        [-0.5598],
        [-0.5580],
        [-0.5575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124843.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0112],
        ...,
        [1.0013],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367705.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0072],
        [1.0112],
        ...,
        [1.0013],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367713.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0088, -0.0038,  0.0052,  ...,  0.0062, -0.0045,  0.0181],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0009,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0009,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1213.8337, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-17.6331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6454, device='cuda:0')



h[200].sum tensor(29.9977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0179, 0.0000, 0.0106,  ..., 0.0155, 0.0000, 0.0370],
        [0.0089, 0.0000, 0.0052,  ..., 0.0108, 0.0000, 0.0188],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47162.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0000, 0.0422,  ..., 0.0422, 0.0016, 0.1043],
        [0.0174, 0.0000, 0.0253,  ..., 0.0277, 0.0143, 0.0610],
        [0.0276, 0.0000, 0.0122,  ..., 0.0165, 0.0313, 0.0274],
        ...,
        [0.0327, 0.0000, 0.0029,  ..., 0.0121, 0.0608, 0.0055],
        [0.0327, 0.0000, 0.0029,  ..., 0.0122, 0.0608, 0.0055],
        [0.0327, 0.0000, 0.0029,  ..., 0.0121, 0.0608, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402109.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4116.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(525.9480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(238.8396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(171.7779, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0838],
        [-0.0898],
        [-0.1211],
        ...,
        [-0.5485],
        [-0.5468],
        [-0.5463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133966.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0072],
        [1.0112],
        ...,
        [1.0013],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367713.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0113],
        ...,
        [1.0013],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367721.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097, -0.0041,  0.0057,  ...,  0.0065, -0.0048,  0.0197],
        [ 0.0037, -0.0018,  0.0020,  ...,  0.0037, -0.0021,  0.0087],
        [ 0.0051, -0.0023,  0.0029,  ...,  0.0043, -0.0027,  0.0112],
        ...,
        [-0.0009,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1107.9741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-16.2052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5897, device='cuda:0')



h[200].sum tensor(26.4966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6731, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0149, 0.0000, 0.0083,  ..., 0.0143, 0.0000, 0.0333],
        [0.0352, 0.0000, 0.0205,  ..., 0.0244, 0.0000, 0.0723],
        [0.0158, 0.0000, 0.0092,  ..., 0.0143, 0.0000, 0.0335],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45161.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0000, 0.0423,  ..., 0.0348, 0.0000, 0.0984],
        [0.0020, 0.0000, 0.0539,  ..., 0.0437, 0.0000, 0.1275],
        [0.0119, 0.0000, 0.0399,  ..., 0.0336, 0.0017, 0.0924],
        ...,
        [0.0336, 0.0000, 0.0037,  ..., 0.0119, 0.0615, 0.0049],
        [0.0336, 0.0000, 0.0037,  ..., 0.0119, 0.0615, 0.0049],
        [0.0336, 0.0000, 0.0037,  ..., 0.0119, 0.0615, 0.0049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(395031.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4362.0396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.0363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(228.1597, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(163.2426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0124],
        [ 0.0395],
        [ 0.0407],
        ...,
        [-0.5347],
        [-0.5331],
        [-0.5327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130335.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0113],
        ...,
        [1.0013],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367721.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0113],
        ...,
        [1.0012],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367727.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044, -0.0021,  0.0024,  ...,  0.0040, -0.0024,  0.0100],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        ...,
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1153.9297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-17.4559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6371, device='cuda:0')



h[200].sum tensor(25.6038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0161, 0.0000, 0.0092,  ..., 0.0144, 0.0000, 0.0343],
        [0.0044, 0.0000, 0.0024,  ..., 0.0084, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47856.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0122, 0.0000, 0.0414,  ..., 0.0335, 0.0013, 0.0938],
        [0.0227, 0.0000, 0.0248,  ..., 0.0224, 0.0183, 0.0529],
        [0.0304, 0.0000, 0.0127,  ..., 0.0151, 0.0354, 0.0238],
        ...,
        [0.0346, 0.0000, 0.0043,  ..., 0.0116, 0.0622, 0.0046],
        [0.0346, 0.0000, 0.0043,  ..., 0.0116, 0.0622, 0.0046],
        [0.0346, 0.0000, 0.0043,  ..., 0.0116, 0.0622, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(404323.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4545.0146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(521.9587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(244.4721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(174.7683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0714],
        [ 0.0235],
        [-0.0497],
        ...,
        [-0.5037],
        [-0.5124],
        [-0.5153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111040.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0113],
        ...,
        [1.0012],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367727.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0114],
        ...,
        [1.0012],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367734.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0020,  0.0022,  ...,  0.0038, -0.0023,  0.0096],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        ...,
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0009,  ...,  0.0014,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1144.5400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-17.8880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6437, device='cuda:0')



h[200].sum tensor(23.9220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6496, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0000, 0.0049,  ..., 0.0118, 0.0000, 0.0246],
        [0.0074, 0.0000, 0.0038,  ..., 0.0102, 0.0000, 0.0183],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50314.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0216, 0.0000, 0.0341,  ..., 0.0237, 0.0045, 0.0721],
        [0.0257, 0.0000, 0.0260,  ..., 0.0200, 0.0150, 0.0533],
        [0.0323, 0.0000, 0.0134,  ..., 0.0139, 0.0369, 0.0239],
        ...,
        [0.0355, 0.0000, 0.0046,  ..., 0.0113, 0.0635, 0.0043],
        [0.0355, 0.0000, 0.0046,  ..., 0.0113, 0.0635, 0.0043],
        [0.0355, 0.0000, 0.0046,  ..., 0.0113, 0.0635, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423746.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4734.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(525.0770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(261.9701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(187.1353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1290],
        [ 0.0675],
        [-0.0353],
        ...,
        [-0.5189],
        [-0.5173],
        [-0.5169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107966.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0114],
        ...,
        [1.0012],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367734.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0114],
        ...,
        [1.0011],
        [1.0006],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367738.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0214, -0.0087,  0.0127,  ...,  0.0121, -0.0101,  0.0413],
        [ 0.0349, -0.0139,  0.0210,  ...,  0.0185, -0.0163,  0.0662],
        [ 0.0245, -0.0099,  0.0146,  ...,  0.0136, -0.0116,  0.0471],
        ...,
        [-0.0009,  0.0000, -0.0010,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0010,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0010,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(827.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0132, device='cuda:0')



h[100].sum tensor(-13.4812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4889, device='cuda:0')



h[200].sum tensor(17.2497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8471, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0929, 0.0000, 0.0553,  ..., 0.0518, 0.0000, 0.1789],
        [0.1202, 0.0000, 0.0721,  ..., 0.0648, 0.0000, 0.2292],
        [0.1277, 0.0000, 0.0767,  ..., 0.0684, 0.0000, 0.2430],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41588.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1398,  ..., 0.1259, 0.0000, 0.3548],
        [0.0000, 0.0000, 0.1741,  ..., 0.1571, 0.0000, 0.4458],
        [0.0000, 0.0000, 0.1804,  ..., 0.1611, 0.0000, 0.4610],
        ...,
        [0.0360, 0.0000, 0.0044,  ..., 0.0111, 0.0657, 0.0032],
        [0.0360, 0.0000, 0.0044,  ..., 0.0111, 0.0657, 0.0032],
        [0.0360, 0.0000, 0.0044,  ..., 0.0111, 0.0657, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382583.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4981.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(486.5558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.7336, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(154.2030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3431],
        [-0.4113],
        [-0.4107],
        ...,
        [-0.5378],
        [-0.5361],
        [-0.5357]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129809.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0114],
        ...,
        [1.0011],
        [1.0006],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367738.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0114],
        ...,
        [1.0010],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367743.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097, -0.0041,  0.0055,  ...,  0.0066, -0.0048,  0.0198],
        [ 0.0042, -0.0020,  0.0021,  ...,  0.0040, -0.0023,  0.0097],
        [ 0.0046, -0.0021,  0.0024,  ...,  0.0041, -0.0025,  0.0104],
        ...,
        [-0.0009,  0.0000, -0.0010,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0010,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0009,  0.0000, -0.0010,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1145.0808, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0192, device='cuda:0')



h[100].sum tensor(-19.5616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7101, device='cuda:0')



h[200].sum tensor(21.4206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0149, 0.0000, 0.0077,  ..., 0.0144, 0.0000, 0.0334],
        [0.0333, 0.0000, 0.0185,  ..., 0.0236, 0.0000, 0.0690],
        [0.0158, 0.0000, 0.0088,  ..., 0.0145, 0.0000, 0.0336],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51414.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0153, 0.0000, 0.0419,  ..., 0.0305, 0.0004, 0.0895],
        [0.0080, 0.0000, 0.0520,  ..., 0.0378, 0.0000, 0.1143],
        [0.0166, 0.0000, 0.0381,  ..., 0.0296, 0.0036, 0.0815],
        ...,
        [0.0366, 0.0000, 0.0041,  ..., 0.0110, 0.0678, 0.0020],
        [0.0366, 0.0000, 0.0041,  ..., 0.0110, 0.0678, 0.0020],
        [0.0366, 0.0000, 0.0041,  ..., 0.0110, 0.0678, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427755.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4991.6343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.0229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(280.0115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(203.2311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0567],
        [ 0.0449],
        [-0.0309],
        ...,
        [-0.5575],
        [-0.5557],
        [-0.5552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121793.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0114],
        ...,
        [1.0010],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367743.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0114],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367747., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0163, -0.0066,  0.0095,  ...,  0.0097, -0.0078,  0.0318],
        [-0.0008,  0.0000, -0.0010,  ...,  0.0016,  0.0000,  0.0002],
        [-0.0008,  0.0000, -0.0010,  ...,  0.0016,  0.0000,  0.0002],
        ...,
        [-0.0008,  0.0000, -0.0010,  ...,  0.0016,  0.0000,  0.0002],
        [-0.0008,  0.0000, -0.0010,  ...,  0.0016,  0.0000,  0.0002],
        [-0.0008,  0.0000, -0.0010,  ...,  0.0016,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1169.8254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0205, device='cuda:0')



h[100].sum tensor(-20.8024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7574, device='cuda:0')



h[200].sum tensor(20.4858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0609, 0.0000, 0.0358,  ..., 0.0364, 0.0000, 0.1176],
        [0.0406, 0.0000, 0.0233,  ..., 0.0268, 0.0000, 0.0802],
        [0.0232, 0.0000, 0.0127,  ..., 0.0186, 0.0000, 0.0484],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52595.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1181,  ..., 0.1008, 0.0000, 0.2888],
        [0.0000, 0.0000, 0.1011,  ..., 0.0842, 0.0000, 0.2429],
        [0.0000, 0.0000, 0.0848,  ..., 0.0673, 0.0000, 0.1982],
        ...,
        [0.0372, 0.0000, 0.0033,  ..., 0.0106, 0.0700, 0.0006],
        [0.0372, 0.0000, 0.0033,  ..., 0.0106, 0.0700, 0.0006],
        [0.0372, 0.0000, 0.0033,  ..., 0.0106, 0.0700, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433501.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5023.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(542.6775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(294.9997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(214.6104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4190],
        [-0.4042],
        [-0.3744],
        ...,
        [-0.5840],
        [-0.5820],
        [-0.5815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137459.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0114],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367747., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(592.4382, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0114],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367751.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0002],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0002],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0002],
        ...,
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0002],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0002],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1030.0720, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0192, device='cuda:0')



h[100].sum tensor(-19.1718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7093, device='cuda:0')



h[200].sum tensor(17.0717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0007],
        [0.0048, 0.0000, 0.0024,  ..., 0.0092, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50457.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0365, 0.0000, 0.0047,  ..., 0.0107, 0.0655, 0.0032],
        [0.0347, 0.0000, 0.0086,  ..., 0.0124, 0.0535, 0.0117],
        [0.0288, 0.0000, 0.0196,  ..., 0.0179, 0.0266, 0.0362],
        ...,
        [0.0374, 0.0000, 0.0028,  ..., 0.0107, 0.0712, 0.0002],
        [0.0374, 0.0000, 0.0028,  ..., 0.0107, 0.0712, 0.0002],
        [0.0374, 0.0000, 0.0028,  ..., 0.0107, 0.0712, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431702.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5059.4927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(530.8099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(285.5949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(213.3046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3572],
        [-0.2270],
        [-0.0997],
        ...,
        [-0.6005],
        [-0.5984],
        [-0.5978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172105.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0074],
        [1.0114],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367751.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0113],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367755.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057, -0.0025,  0.0029,  ...,  0.0047, -0.0029,  0.0121],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001],
        [ 0.0057, -0.0025,  0.0029,  ...,  0.0047, -0.0029,  0.0121],
        ...,
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2065.4614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.5000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0357, device='cuda:0')



h[100].sum tensor(-36.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.3210, device='cuda:0')



h[200].sum tensor(33.6543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9067, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0119, 0.0000, 0.0061,  ..., 0.0130, 0.0000, 0.0255],
        [0.0247, 0.0000, 0.0128,  ..., 0.0198, 0.0000, 0.0520],
        [0.0046, 0.0000, 0.0022,  ..., 0.0092, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82790.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0191, 0.0000, 0.0361,  ..., 0.0265, 0.0040, 0.0722],
        [0.0164, 0.0000, 0.0410,  ..., 0.0288, 0.0000, 0.0826],
        [0.0250, 0.0000, 0.0263,  ..., 0.0211, 0.0137, 0.0499],
        ...,
        [0.0375, 0.0000, 0.0025,  ..., 0.0108, 0.0720, 0.0000],
        [0.0375, 0.0000, 0.0025,  ..., 0.0108, 0.0720, 0.0000],
        [0.0375, 0.0000, 0.0025,  ..., 0.0108, 0.0720, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608693.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4980.5610, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(647.3627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(499.4468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(372.4331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0049],
        [ 0.0037],
        [-0.0461],
        ...,
        [-0.6115],
        [-0.6093],
        [-0.6087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211127.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0113],
        ...,
        [1.0010],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367755.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0113],
        ...,
        [1.0010],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367757.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0019,  0.0020,  ...,  0.0040, -0.0023,  0.0094],
        [ 0.0043, -0.0019,  0.0020,  ...,  0.0040, -0.0023,  0.0094],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0011,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1001.4269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.0509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-19.8067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7280, device='cuda:0')



h[200].sum tensor(15.5034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0138, 0.0000, 0.0066,  ..., 0.0143, 0.0000, 0.0303],
        [0.0103, 0.0000, 0.0044,  ..., 0.0127, 0.0000, 0.0239],
        [0.0076, 0.0000, 0.0034,  ..., 0.0110, 0.0000, 0.0175],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50563.9023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0000, 0.0405,  ..., 0.0247, 0.0000, 0.0760],
        [0.0238, 0.0000, 0.0350,  ..., 0.0217, 0.0045, 0.0640],
        [0.0277, 0.0000, 0.0269,  ..., 0.0184, 0.0130, 0.0472],
        ...,
        [0.0376, 0.0000, 0.0026,  ..., 0.0109, 0.0721, 0.0000],
        [0.0376, 0.0000, 0.0026,  ..., 0.0109, 0.0722, 0.0000],
        [0.0376, 0.0000, 0.0026,  ..., 0.0109, 0.0721, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429778.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5125.5015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(540.3572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.8194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(226.0488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1108],
        [ 0.0722],
        [ 0.0037],
        ...,
        [-0.6135],
        [-0.6113],
        [-0.6105]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167776.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0113],
        ...,
        [1.0010],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367757.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367760.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001],
        [ 0.0123, -0.0050,  0.0069,  ...,  0.0078, -0.0059,  0.0244],
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(772.6922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-16.4051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6005, device='cuda:0')



h[200].sum tensor(11.9775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0244, 0.0000, 0.0124,  ..., 0.0196, 0.0000, 0.0515],
        [0.0091, 0.0000, 0.0043,  ..., 0.0116, 0.0000, 0.0203],
        [0.0303, 0.0000, 0.0160,  ..., 0.0224, 0.0000, 0.0624],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46141.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0000, 0.0473,  ..., 0.0304, 0.0000, 0.0892],
        [0.0166, 0.0000, 0.0430,  ..., 0.0281, 0.0000, 0.0799],
        [0.0085, 0.0000, 0.0549,  ..., 0.0354, 0.0000, 0.1061],
        ...,
        [0.0375, 0.0000, 0.0030,  ..., 0.0111, 0.0714, 0.0000],
        [0.0375, 0.0000, 0.0030,  ..., 0.0111, 0.0714, 0.0000],
        [0.0375, 0.0000, 0.0030,  ..., 0.0111, 0.0714, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409859.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5140.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(522.4116, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(247.9459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(206.4427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1262],
        [-0.1227],
        [-0.1282],
        ...,
        [-0.6040],
        [-0.6015],
        [-0.6000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157969.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367760.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367760.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0112, -0.0046,  0.0062,  ...,  0.0073, -0.0054,  0.0223],
        [ 0.0098, -0.0041,  0.0054,  ...,  0.0066, -0.0048,  0.0197],
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001],
        ...,
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001],
        [-0.0008,  0.0000, -0.0012,  ...,  0.0016,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(970.3080, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0196, device='cuda:0')



h[100].sum tensor(-19.6298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7269, device='cuda:0')



h[200].sum tensor(15.2764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1548, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0452, 0.0000, 0.0252,  ..., 0.0294, 0.0000, 0.0899],
        [0.0191, 0.0000, 0.0104,  ..., 0.0163, 0.0000, 0.0387],
        [0.0099, 0.0000, 0.0054,  ..., 0.0115, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49233.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0580,  ..., 0.0462, 0.0000, 0.1214],
        [0.0130, 0.0000, 0.0401,  ..., 0.0338, 0.0038, 0.0803],
        [0.0230, 0.0000, 0.0251,  ..., 0.0235, 0.0220, 0.0459],
        ...,
        [0.0375, 0.0000, 0.0030,  ..., 0.0111, 0.0714, 0.0000],
        [0.0375, 0.0000, 0.0030,  ..., 0.0111, 0.0714, 0.0000],
        [0.0375, 0.0000, 0.0030,  ..., 0.0111, 0.0714, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(417469.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5078.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(527.2355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(270.1275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(220.9366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2259],
        [-0.2265],
        [-0.2382],
        ...,
        [-0.6054],
        [-0.6034],
        [-0.6028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174051.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0073],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367760.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367762.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039, -0.0018,  0.0018,  ...,  0.0038, -0.0022,  0.0090],
        [ 0.0108, -0.0045,  0.0060,  ...,  0.0071, -0.0052,  0.0217],
        [ 0.0054, -0.0024,  0.0027,  ...,  0.0045, -0.0028,  0.0117],
        ...,
        [-0.0009,  0.0000, -0.0012,  ...,  0.0015,  0.0000,  0.0001],
        [-0.0009,  0.0000, -0.0012,  ...,  0.0015,  0.0000,  0.0001],
        [-0.0009,  0.0000, -0.0012,  ...,  0.0015,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(690.2095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-15.2656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5587, device='cuda:0')



h[200].sum tensor(11.2884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1106, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0304, 0.0000, 0.0161,  ..., 0.0223, 0.0000, 0.0630],
        [0.0173, 0.0000, 0.0080,  ..., 0.0161, 0.0000, 0.0387],
        [0.0311, 0.0000, 0.0165,  ..., 0.0226, 0.0000, 0.0642],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44594.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0125, 0.0000, 0.0491,  ..., 0.0310, 0.0000, 0.0892],
        [0.0132, 0.0000, 0.0517,  ..., 0.0301, 0.0000, 0.0927],
        [0.0054, 0.0000, 0.0620,  ..., 0.0372, 0.0000, 0.1157],
        ...,
        [0.0368, 0.0000, 0.0044,  ..., 0.0118, 0.0677, 0.0007],
        [0.0373, 0.0000, 0.0036,  ..., 0.0114, 0.0702, 0.0000],
        [0.0373, 0.0000, 0.0036,  ..., 0.0114, 0.0702, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(400694.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5119.2466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(516.6752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(232.4021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(197.0324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1996],
        [-0.1302],
        [-0.0899],
        ...,
        [-0.4355],
        [-0.5060],
        [-0.5561]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142835.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367762.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367765.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.2488e-04,  0.0000e+00, -1.2010e-03,  ...,  1.4870e-03,
          0.0000e+00,  9.3010e-05],
        [-9.2488e-04,  0.0000e+00, -1.2010e-03,  ...,  1.4870e-03,
          0.0000e+00,  9.3010e-05],
        [-9.2488e-04,  0.0000e+00, -1.2010e-03,  ...,  1.4870e-03,
          0.0000e+00,  9.3010e-05],
        ...,
        [-9.2488e-04,  0.0000e+00, -1.2010e-03,  ...,  1.4870e-03,
          0.0000e+00,  9.3010e-05],
        [-9.2488e-04,  0.0000e+00, -1.2010e-03,  ...,  1.4870e-03,
          0.0000e+00,  9.3010e-05],
        [-9.2488e-04,  0.0000e+00, -1.2010e-03,  ...,  1.4870e-03,
          0.0000e+00,  9.3010e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(560.1468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0132, device='cuda:0')



h[100].sum tensor(-13.2738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4877, device='cuda:0')



h[200].sum tensor(9.9210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42308.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0364, 0.0000, 0.0040,  ..., 0.0115, 0.0671, 0.0000],
        [0.0362, 0.0000, 0.0047,  ..., 0.0117, 0.0653, 0.0008],
        [0.0360, 0.0000, 0.0058,  ..., 0.0118, 0.0627, 0.0016],
        ...,
        [0.0371, 0.0000, 0.0041,  ..., 0.0115, 0.0685, 0.0000],
        [0.0371, 0.0000, 0.0041,  ..., 0.0115, 0.0685, 0.0000],
        [0.0371, 0.0000, 0.0041,  ..., 0.0115, 0.0685, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(388421.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5094.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(508.5829, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.0682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(185.0069, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5229],
        [-0.4179],
        [-0.2826],
        ...,
        [-0.5664],
        [-0.5604],
        [-0.5557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128038.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0072],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367765.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367767.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.4582e-04,  0.0000e+00, -1.2020e-03,  ...,  1.4513e-03,
          0.0000e+00,  5.5820e-05],
        [-9.4582e-04,  0.0000e+00, -1.2020e-03,  ...,  1.4513e-03,
          0.0000e+00,  5.5820e-05],
        [-9.4582e-04,  0.0000e+00, -1.2020e-03,  ...,  1.4513e-03,
          0.0000e+00,  5.5820e-05],
        ...,
        [-9.4582e-04,  0.0000e+00, -1.2020e-03,  ...,  1.4513e-03,
          0.0000e+00,  5.5820e-05],
        [-9.4582e-04,  0.0000e+00, -1.2020e-03,  ...,  1.4513e-03,
          0.0000e+00,  5.5820e-05],
        [-9.4582e-04,  0.0000e+00, -1.2020e-03,  ...,  1.4513e-03,
          0.0000e+00,  5.5820e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(655.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-14.9036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5460, device='cuda:0')



h[200].sum tensor(12.2583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44340.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0363, 0.0000, 0.0041,  ..., 0.0116, 0.0656, 0.0000],
        [0.0364, 0.0000, 0.0041,  ..., 0.0116, 0.0657, 0.0000],
        [0.0365, 0.0000, 0.0041,  ..., 0.0115, 0.0660, 0.0000],
        ...,
        [0.0371, 0.0000, 0.0042,  ..., 0.0116, 0.0669, 0.0000],
        [0.0371, 0.0000, 0.0042,  ..., 0.0116, 0.0670, 0.0000],
        [0.0371, 0.0000, 0.0042,  ..., 0.0116, 0.0669, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393919.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5064.0264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(520.9373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(227.8742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(196.9005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5031],
        [-0.5729],
        [-0.6293],
        ...,
        [-0.5645],
        [-0.5629],
        [-0.5626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123530.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367767.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367769.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.4634e-04,  0.0000e+00, -1.1925e-03,  ...,  1.4532e-03,
          0.0000e+00, -1.5579e-05],
        [ 1.0241e-02, -4.2317e-03,  5.6531e-03,  ...,  6.7394e-03,
         -4.9695e-03,  2.0571e-02],
        [ 1.5738e-02, -6.3108e-03,  9.0164e-03,  ...,  9.3365e-03,
         -7.4110e-03,  3.0686e-02],
        ...,
        [-9.4634e-04,  0.0000e+00, -1.1925e-03,  ...,  1.4532e-03,
          0.0000e+00, -1.5579e-05],
        [-9.4634e-04,  0.0000e+00, -1.1925e-03,  ...,  1.4532e-03,
          0.0000e+00, -1.5579e-05],
        [-9.4634e-04,  0.0000e+00, -1.1925e-03,  ...,  1.4532e-03,
          0.0000e+00, -1.5579e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(538.5034, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0131, device='cuda:0')



h[100].sum tensor(-13.2143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4861, device='cuda:0')



h[200].sum tensor(10.9973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0277, 0.0000, 0.0145,  ..., 0.0207, 0.0000, 0.0579],
        [0.0241, 0.0000, 0.0135,  ..., 0.0181, 0.0000, 0.0477],
        [0.0231, 0.0000, 0.0129,  ..., 0.0177, 0.0000, 0.0460],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41959.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0103, 0.0000, 0.0496,  ..., 0.0314, 0.0000, 0.0822],
        [0.0064, 0.0000, 0.0559,  ..., 0.0375, 0.0000, 0.0977],
        [0.0030, 0.0000, 0.0625,  ..., 0.0411, 0.0000, 0.1107],
        ...,
        [0.0370, 0.0000, 0.0038,  ..., 0.0116, 0.0660, 0.0000],
        [0.0371, 0.0000, 0.0038,  ..., 0.0116, 0.0661, 0.0000],
        [0.0370, 0.0000, 0.0038,  ..., 0.0116, 0.0660, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383812.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5047.3706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(521.6863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(213.6137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(192.2967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1726],
        [ 0.2067],
        [ 0.2197],
        ...,
        [-0.5722],
        [-0.5705],
        [-0.5700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128640.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367769.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367770.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.3623e-04,  0.0000e+00, -1.1687e-03,  ...,  1.4727e-03,
          0.0000e+00, -9.4156e-05],
        [-9.3623e-04,  0.0000e+00, -1.1687e-03,  ...,  1.4727e-03,
          0.0000e+00, -9.4156e-05],
        [-9.3623e-04,  0.0000e+00, -1.1687e-03,  ...,  1.4727e-03,
          0.0000e+00, -9.4156e-05],
        ...,
        [-9.3623e-04,  0.0000e+00, -1.1687e-03,  ...,  1.4727e-03,
          0.0000e+00, -9.4156e-05],
        [-9.3623e-04,  0.0000e+00, -1.1687e-03,  ...,  1.4727e-03,
          0.0000e+00, -9.4156e-05],
        [-9.3623e-04,  0.0000e+00, -1.1687e-03,  ...,  1.4727e-03,
          0.0000e+00, -9.4156e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(705.6344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-16.0849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5882, device='cuda:0')



h[200].sum tensor(14.5267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48544.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0355, 0.0000, 0.0048,  ..., 0.0120, 0.0589, 0.0001],
        [0.0339, 0.0000, 0.0087,  ..., 0.0130, 0.0483, 0.0045],
        [0.0332, 0.0000, 0.0108,  ..., 0.0135, 0.0430, 0.0068],
        ...,
        [0.0370, 0.0000, 0.0031,  ..., 0.0116, 0.0653, 0.0000],
        [0.0370, 0.0000, 0.0031,  ..., 0.0116, 0.0653, 0.0000],
        [0.0370, 0.0000, 0.0031,  ..., 0.0116, 0.0653, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419368.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4940.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(560.1295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(260.6865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(230.3190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1308],
        [-0.1167],
        [-0.0797],
        ...,
        [-0.5852],
        [-0.5834],
        [-0.5829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134171.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367770.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(503.7529, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367773.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0146, -0.0059,  0.0084,  ...,  0.0088, -0.0069,  0.0285],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(716.0955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-16.3791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5984, device='cuda:0')



h[200].sum tensor(15.4623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0467, 0.0000, 0.0274,  ..., 0.0289, 0.0000, 0.0891],
        [0.0265, 0.0000, 0.0151,  ..., 0.0194, 0.0000, 0.0519],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48004.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0000, 0.0786,  ..., 0.0649, 0.0000, 0.1547],
        [0.0060, 0.0000, 0.0567,  ..., 0.0473, 0.0000, 0.1049],
        [0.0141, 0.0000, 0.0305,  ..., 0.0290, 0.0000, 0.0492],
        ...,
        [0.0370, 0.0000, 0.0025,  ..., 0.0116, 0.0644, 0.0000],
        [0.0370, 0.0000, 0.0025,  ..., 0.0116, 0.0644, 0.0000],
        [0.0370, 0.0000, 0.0025,  ..., 0.0116, 0.0644, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413138.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4890.7583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(572.5450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(260.4656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(234.3746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0960],
        [-0.0505],
        [ 0.0009],
        ...,
        [-0.5975],
        [-0.5956],
        [-0.5951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132538.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367773.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0069],
        [1.0110],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367776.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0168, -0.0066,  0.0097,  ...,  0.0099, -0.0078,  0.0323],
        [ 0.0070, -0.0030,  0.0037,  ...,  0.0053, -0.0035,  0.0144],
        [ 0.0093, -0.0038,  0.0051,  ...,  0.0063, -0.0045,  0.0185],
        ...,
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(997.7646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-20.9837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7748, device='cuda:0')



h[200].sum tensor(20.9090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0474, 0.0000, 0.0268,  ..., 0.0302, 0.0000, 0.0931],
        [0.0476, 0.0000, 0.0269,  ..., 0.0303, 0.0000, 0.0935],
        [0.0145, 0.0000, 0.0078,  ..., 0.0138, 0.0000, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54621.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1226,  ..., 0.0873, 0.0000, 0.2338],
        [0.0000, 0.0000, 0.1010,  ..., 0.0702, 0.0000, 0.1859],
        [0.0025, 0.0000, 0.0678,  ..., 0.0472, 0.0000, 0.1166],
        ...,
        [0.0363, 0.0000, 0.0035,  ..., 0.0119, 0.0601, 0.0000],
        [0.0355, 0.0000, 0.0065,  ..., 0.0122, 0.0529, 0.0006],
        [0.0350, 0.0000, 0.0080,  ..., 0.0124, 0.0493, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(437319.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4743.9351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(613.7846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(306.3074, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(271.6708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4465],
        [-0.3839],
        [-0.3316],
        ...,
        [-0.4996],
        [-0.4319],
        [-0.3893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130905.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0069],
        [1.0110],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367776.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367780.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(734.1725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0168, device='cuda:0')



h[100].sum tensor(-16.7942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6224, device='cuda:0')



h[200].sum tensor(17.3975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47179.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0229, 0.0000, 0.0199,  ..., 0.0207, 0.0251, 0.0261],
        [0.0327, 0.0000, 0.0075,  ..., 0.0136, 0.0447, 0.0034],
        [0.0353, 0.0000, 0.0031,  ..., 0.0119, 0.0582, 0.0000],
        ...,
        [0.0365, 0.0000, 0.0016,  ..., 0.0118, 0.0629, 0.0000],
        [0.0365, 0.0000, 0.0016,  ..., 0.0118, 0.0630, 0.0000],
        [0.0365, 0.0000, 0.0016,  ..., 0.0118, 0.0629, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(401779.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4823.5454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(582.8171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.2133, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(239.7904, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1110],
        [-0.1912],
        [-0.2617],
        ...,
        [-0.6112],
        [-0.6093],
        [-0.6087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152939.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367780.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367785.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084, -0.0035,  0.0046,  ...,  0.0059, -0.0041,  0.0168],
        [ 0.0059, -0.0025,  0.0031,  ...,  0.0047, -0.0030,  0.0122],
        [ 0.0038, -0.0018,  0.0018,  ...,  0.0038, -0.0021,  0.0084],
        ...,
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0009,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(597.3994, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-14.6285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5452, device='cuda:0')



h[200].sum tensor(15.9462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8669, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0287, 0.0000, 0.0155,  ..., 0.0214, 0.0000, 0.0584],
        [0.0198, 0.0000, 0.0101,  ..., 0.0173, 0.0000, 0.0421],
        [0.0152, 0.0000, 0.0073,  ..., 0.0151, 0.0000, 0.0336],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43983.5117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0000, 0.0538,  ..., 0.0340, 0.0000, 0.0809],
        [0.0036, 0.0000, 0.0553,  ..., 0.0318, 0.0000, 0.0805],
        [0.0074, 0.0000, 0.0520,  ..., 0.0284, 0.0000, 0.0728],
        ...,
        [0.0362, 0.0000, 0.0015,  ..., 0.0118, 0.0623, 0.0000],
        [0.0362, 0.0000, 0.0015,  ..., 0.0118, 0.0623, 0.0000],
        [0.0362, 0.0000, 0.0015,  ..., 0.0118, 0.0623, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389398.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4799.5088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(581.3170, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(241.8318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(227.7113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0624],
        [ 0.0980],
        [ 0.1119],
        ...,
        [-0.6125],
        [-0.6106],
        [-0.6100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144179.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367785.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367790.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0125, -0.0050,  0.0071,  ...,  0.0079, -0.0059,  0.0243],
        ...,
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1125.4949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0228, device='cuda:0')



h[100].sum tensor(-22.9957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8444, device='cuda:0')



h[200].sum tensor(25.3972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.2814, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0125, 0.0000, 0.0072,  ..., 0.0125, 0.0000, 0.0245],
        [0.0428, 0.0000, 0.0252,  ..., 0.0272, 0.0000, 0.0816],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58544.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0248, 0.0000, 0.0143,  ..., 0.0192, 0.0292, 0.0175],
        [0.0132, 0.0000, 0.0461,  ..., 0.0426, 0.0117, 0.0808],
        [0.0016, 0.0000, 0.0998,  ..., 0.0821, 0.0000, 0.1932],
        ...,
        [0.0360, 0.0000, 0.0014,  ..., 0.0119, 0.0619, 0.0000],
        [0.0360, 0.0000, 0.0014,  ..., 0.0120, 0.0619, 0.0000],
        [0.0360, 0.0000, 0.0014,  ..., 0.0119, 0.0619, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460413., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4594.8135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(645.3234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(336.2497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.5473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0705],
        [-0.0744],
        [-0.0921],
        ...,
        [-0.6116],
        [-0.6096],
        [-0.6091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142060.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367790.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367796.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(729.1891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-16.6676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6294, device='cuda:0')



h[200].sum tensor(19.4951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47750.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.4956e-02, 5.6175e-05, 1.4051e-03,  ..., 1.2062e-02, 6.0228e-02,
         0.0000e+00],
        [3.5036e-02, 4.0496e-05, 1.8209e-03,  ..., 1.1941e-02, 5.9743e-02,
         0.0000e+00],
        [3.5188e-02, 2.4777e-05, 2.8703e-03,  ..., 1.1555e-02, 5.8417e-02,
         0.0000e+00],
        ...,
        [3.5747e-02, 5.7911e-05, 1.4666e-03,  ..., 1.2086e-02, 6.1505e-02,
         0.0000e+00],
        [3.5752e-02, 5.7919e-05, 1.4667e-03,  ..., 1.2088e-02, 6.1514e-02,
         0.0000e+00],
        [3.5747e-02, 5.7912e-05, 1.4665e-03,  ..., 1.2086e-02, 6.1505e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406944.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4502.9565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(614.2990, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.9282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(248.0463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6115],
        [-0.5342],
        [-0.4274],
        ...,
        [-0.6075],
        [-0.6048],
        [-0.6036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119154.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0109],
        ...,
        [1.0008],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367796.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367802.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0149, -0.0059,  0.0087,  ...,  0.0090, -0.0069,  0.0289],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0010,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(428.9873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0119, device='cuda:0')



h[100].sum tensor(-11.8703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4407, device='cuda:0')



h[200].sum tensor(15.0504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9750, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0418, 0.0000, 0.0243,  ..., 0.0272, 0.0000, 0.0813],
        [0.0270, 0.0000, 0.0156,  ..., 0.0197, 0.0000, 0.0526],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40819.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0831,  ..., 0.0614, 0.0000, 0.1453],
        [0.0078, 0.0000, 0.0545,  ..., 0.0443, 0.0000, 0.0906],
        [0.0206, 0.0002, 0.0186,  ..., 0.0220, 0.0269, 0.0251],
        ...,
        [0.0356, 0.0009, 0.0016,  ..., 0.0123, 0.0614, 0.0000],
        [0.0356, 0.0009, 0.0016,  ..., 0.0123, 0.0614, 0.0000],
        [0.0356, 0.0009, 0.0016,  ..., 0.0123, 0.0614, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375745.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4639.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(579.2371, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(218.2927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(213.1080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0251],
        [-0.0360],
        [-0.1599],
        ...,
        [-0.6067],
        [-0.6049],
        [-0.6044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136034.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367802.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367809.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0043, -0.0020,  0.0022,  ...,  0.0040, -0.0023,  0.0095],
        [-0.0011,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0011,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1905.1265, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.8229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0355, device='cuda:0')



h[100].sum tensor(-35.0338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.3123, device='cuda:0')



h[200].sum tensor(39.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7501, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0000, 0.0173,  ..., 0.0225, 0.0000, 0.0630],
        [0.0034, 0.0000, 0.0016,  ..., 0.0080, 0.0000, 0.0077],
        [0.0309, 0.0000, 0.0176,  ..., 0.0220, 0.0000, 0.0613],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75773.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0828,  ..., 0.0550, 0.0000, 0.1370],
        [0.0004, 0.0000, 0.0735,  ..., 0.0510, 0.0000, 0.1208],
        [0.0000, 0.0000, 0.1013,  ..., 0.0708, 0.0000, 0.1769],
        ...,
        [0.0354, 0.0020, 0.0019,  ..., 0.0124, 0.0615, 0.0000],
        [0.0354, 0.0020, 0.0019,  ..., 0.0124, 0.0615, 0.0000],
        [0.0354, 0.0020, 0.0019,  ..., 0.0124, 0.0615, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529557.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4120.3970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(730.1811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(440.4644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(379.5085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2774],
        [-0.3302],
        [-0.3841],
        ...,
        [-0.5979],
        [-0.5965],
        [-0.5965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113270.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367809.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367816.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0194, -0.0076,  0.0115,  ...,  0.0112, -0.0089,  0.0373],
        [ 0.0212, -0.0082,  0.0126,  ...,  0.0120, -0.0097,  0.0405],
        [ 0.0196, -0.0076,  0.0116,  ...,  0.0113, -0.0090,  0.0377],
        ...,
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(976.8169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0206, device='cuda:0')



h[100].sum tensor(-20.3490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7615, device='cuda:0')



h[200].sum tensor(24.4245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7822, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0737, 0.0000, 0.0435,  ..., 0.0427, 0.0000, 0.1418],
        [0.0857, 0.0000, 0.0508,  ..., 0.0484, 0.0000, 0.1638],
        [0.0894, 0.0000, 0.0531,  ..., 0.0502, 0.0000, 0.1708],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54852.9492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1718,  ..., 0.1248, 0.0000, 0.3213],
        [0.0000, 0.0000, 0.1754,  ..., 0.1279, 0.0000, 0.3293],
        [0.0000, 0.0000, 0.1679,  ..., 0.1215, 0.0000, 0.3130],
        ...,
        [0.0352, 0.0030, 0.0020,  ..., 0.0128, 0.0618, 0.0000],
        [0.0352, 0.0030, 0.0020,  ..., 0.0128, 0.0618, 0.0000],
        [0.0352, 0.0030, 0.0020,  ..., 0.0128, 0.0618, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438741.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4206.2529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(652.8448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(300.4910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(279.2033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2129],
        [-0.1931],
        [-0.1551],
        ...,
        [-0.5989],
        [-0.5971],
        [-0.5966]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98413.2266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0110],
        ...,
        [1.0009],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367816.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367822.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0224, -0.0086,  0.0133,  ...,  0.0126, -0.0102,  0.0428],
        [ 0.0153, -0.0060,  0.0090,  ...,  0.0092, -0.0071,  0.0297],
        [ 0.0044, -0.0020,  0.0023,  ...,  0.0041, -0.0024,  0.0096],
        ...,
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(847.4509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-18.2486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6853, device='cuda:0')



h[200].sum tensor(22.1883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4019, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0810, 0.0000, 0.0481,  ..., 0.0462, 0.0000, 0.1553],
        [0.0433, 0.0000, 0.0251,  ..., 0.0284, 0.0000, 0.0861],
        [0.0366, 0.0000, 0.0209,  ..., 0.0253, 0.0000, 0.0737],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50452.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1432,  ..., 0.1005, 0.0000, 0.2593],
        [0.0000, 0.0000, 0.1131,  ..., 0.0744, 0.0000, 0.1937],
        [0.0000, 0.0000, 0.0945,  ..., 0.0583, 0.0000, 0.1531],
        ...,
        [0.0351, 0.0041, 0.0019,  ..., 0.0132, 0.0622, 0.0000],
        [0.0351, 0.0041, 0.0019,  ..., 0.0132, 0.0622, 0.0000],
        [0.0351, 0.0041, 0.0019,  ..., 0.0132, 0.0622, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(417570.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4207.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(637.9916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(269.8504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.5277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0666],
        [ 0.1091],
        [ 0.1504],
        ...,
        [-0.6022],
        [-0.6004],
        [-0.5999]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99538.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367822.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(551.8544, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367829.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [ 0.0052, -0.0023,  0.0029,  ...,  0.0045, -0.0027,  0.0112],
        ...,
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(724.6056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-16.2328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6095, device='cuda:0')



h[200].sum tensor(20.1785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0053, 0.0000, 0.0029,  ..., 0.0089, 0.0000, 0.0113],
        [0.0107, 0.0000, 0.0059,  ..., 0.0120, 0.0000, 0.0229],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49368.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0285, 0.0001, 0.0120,  ..., 0.0161, 0.0333, 0.0071],
        [0.0222, 0.0000, 0.0211,  ..., 0.0195, 0.0176, 0.0219],
        [0.0134, 0.0000, 0.0352,  ..., 0.0241, 0.0055, 0.0441],
        ...,
        [0.0348, 0.0052, 0.0015,  ..., 0.0136, 0.0625, 0.0000],
        [0.0348, 0.0052, 0.0015,  ..., 0.0136, 0.0625, 0.0000],
        [0.0348, 0.0052, 0.0015,  ..., 0.0136, 0.0625, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419425.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4191.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(634.9836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.4654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(256.5385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0137],
        [ 0.0512],
        [ 0.1054],
        ...,
        [-0.6114],
        [-0.6096],
        [-0.6090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109326.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367829.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367829.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0166, -0.0065,  0.0098,  ...,  0.0098, -0.0077,  0.0321],
        [ 0.0153, -0.0060,  0.0090,  ...,  0.0092, -0.0071,  0.0297],
        [ 0.0031, -0.0015,  0.0015,  ...,  0.0034, -0.0018,  0.0072],
        ...,
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(960.6224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0201, device='cuda:0')



h[100].sum tensor(-19.9123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7434, device='cuda:0')



h[200].sum tensor(24.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0758, 0.0000, 0.0450,  ..., 0.0438, 0.0000, 0.1457],
        [0.0545, 0.0000, 0.0320,  ..., 0.0338, 0.0000, 0.1066],
        [0.0705, 0.0000, 0.0418,  ..., 0.0413, 0.0000, 0.1360],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52262.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1515,  ..., 0.1084, 0.0000, 0.2765],
        [0.0000, 0.0000, 0.1425,  ..., 0.1004, 0.0000, 0.2565],
        [0.0000, 0.0000, 0.1436,  ..., 0.1008, 0.0000, 0.2579],
        ...,
        [0.0348, 0.0052, 0.0015,  ..., 0.0136, 0.0625, 0.0000],
        [0.0348, 0.0052, 0.0015,  ..., 0.0136, 0.0625, 0.0000],
        [0.0348, 0.0052, 0.0015,  ..., 0.0136, 0.0625, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424112.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4130.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(647.5108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.6491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(270.2426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0615],
        [-0.0678],
        [-0.0649],
        ...,
        [-0.6073],
        [-0.6054],
        [-0.6048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109847.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367829.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367836.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [ 0.0056, -0.0024,  0.0031,  ...,  0.0046, -0.0029,  0.0118],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        ...,
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0010,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(844.9326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-18.0197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6789, device='cuda:0')



h[200].sum tensor(22.0354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2870, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0056, 0.0000, 0.0031,  ..., 0.0091, 0.0000, 0.0118],
        [0.0044, 0.0000, 0.0024,  ..., 0.0086, 0.0000, 0.0096],
        [0.0237, 0.0000, 0.0133,  ..., 0.0193, 0.0000, 0.0498],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51962.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0235, 0.0009, 0.0181,  ..., 0.0189, 0.0231, 0.0182],
        [0.0183, 0.0000, 0.0269,  ..., 0.0216, 0.0108, 0.0315],
        [0.0066, 0.0000, 0.0441,  ..., 0.0278, 0.0000, 0.0591],
        ...,
        [0.0346, 0.0063, 0.0012,  ..., 0.0141, 0.0631, 0.0000],
        [0.0346, 0.0063, 0.0012,  ..., 0.0141, 0.0631, 0.0000],
        [0.0346, 0.0063, 0.0012,  ..., 0.0141, 0.0631, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(432665.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4097.9932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(645.1086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.3286, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(271.2379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0036],
        [ 0.0770],
        [ 0.1183],
        ...,
        [-0.6232],
        [-0.6214],
        [-0.6209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119104.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0111],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367836.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367843.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(953.1993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0199, device='cuda:0')



h[100].sum tensor(-19.6279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7365, device='cuda:0')



h[200].sum tensor(23.7471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50477.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0331, 0.0067, 0.0013,  ..., 0.0147, 0.0602, 0.0000],
        [0.0336, 0.0072, 0.0010,  ..., 0.0144, 0.0616, 0.0000],
        [0.0335, 0.0071, 0.0022,  ..., 0.0141, 0.0599, 0.0000],
        ...,
        [0.0343, 0.0074, 0.0006,  ..., 0.0146, 0.0637, 0.0000],
        [0.0343, 0.0074, 0.0006,  ..., 0.0146, 0.0637, 0.0000],
        [0.0343, 0.0074, 0.0006,  ..., 0.0146, 0.0637, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415163.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4039.9683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(644.5493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(271.2898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.9389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3859],
        [-0.4449],
        [-0.4539],
        ...,
        [-0.6341],
        [-0.6327],
        [-0.6328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129069.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367843.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367850.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [ 0.0041, -0.0019,  0.0023,  ...,  0.0040, -0.0023,  0.0091],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(711.0305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0160, device='cuda:0')



h[100].sum tensor(-15.8192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5924, device='cuda:0')



h[200].sum tensor(19.6827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7209, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0042, 0.0000, 0.0023,  ..., 0.0086, 0.0000, 0.0092],
        [0.0154, 0.0000, 0.0090,  ..., 0.0145, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47826.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0000, 0.0350,  ..., 0.0331, 0.0059, 0.0537],
        [0.0053, 0.0000, 0.0428,  ..., 0.0350, 0.0040, 0.0650],
        [0.0033, 0.0000, 0.0551,  ..., 0.0398, 0.0000, 0.0849],
        ...,
        [0.0341, 0.0085, 0.0002,  ..., 0.0151, 0.0643, 0.0000],
        [0.0309, 0.0051, 0.0047,  ..., 0.0168, 0.0509, 0.0033],
        [0.0233, 0.0022, 0.0148,  ..., 0.0210, 0.0249, 0.0175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413829., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4000.5461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(641.9719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(253.4679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.8066, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1172],
        [-0.0702],
        [-0.0229],
        ...,
        [-0.5092],
        [-0.3580],
        [-0.1801]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124989.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0112],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367850.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367858.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044, -0.0020,  0.0025,  ...,  0.0042, -0.0024,  0.0097],
        [ 0.0041, -0.0019,  0.0023,  ...,  0.0040, -0.0022,  0.0091],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(790.2066, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0171, device='cuda:0')



h[100].sum tensor(-16.9720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6330, device='cuda:0')



h[200].sum tensor(20.6987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0247, 0.0000, 0.0143,  ..., 0.0200, 0.0000, 0.0516],
        [0.0122, 0.0000, 0.0071,  ..., 0.0131, 0.0000, 0.0255],
        [0.0041, 0.0000, 0.0023,  ..., 0.0087, 0.0000, 0.0092],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49074.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0466,  ..., 0.0327, 0.0000, 0.0678],
        [0.0109, 0.0000, 0.0337,  ..., 0.0274, 0.0058, 0.0466],
        [0.0218, 0.0027, 0.0182,  ..., 0.0209, 0.0229, 0.0214],
        ...,
        [0.0340, 0.0092, 0.0000,  ..., 0.0156, 0.0652, 0.0000],
        [0.0340, 0.0092, 0.0000,  ..., 0.0156, 0.0652, 0.0000],
        [0.0340, 0.0092, 0.0000,  ..., 0.0156, 0.0652, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421854.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3923.7327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(644.4023, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.8976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(266.0346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0909],
        [ 0.0537],
        [-0.0285],
        ...,
        [-0.6585],
        [-0.6566],
        [-0.6565]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146458.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367858.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367866.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0009,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(720.7494, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-15.8378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5941, device='cuda:0')



h[200].sum tensor(19.1722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7527, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47306.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0306, 0.0074, 0.0071,  ..., 0.0162, 0.0486, 0.0040],
        [0.0310, 0.0078, 0.0061,  ..., 0.0162, 0.0508, 0.0030],
        [0.0303, 0.0068, 0.0066,  ..., 0.0167, 0.0486, 0.0032],
        ...,
        [0.0340, 0.0099, 0.0000,  ..., 0.0162, 0.0661, 0.0000],
        [0.0340, 0.0099, 0.0000,  ..., 0.0162, 0.0661, 0.0000],
        [0.0340, 0.0099, 0.0000,  ..., 0.0162, 0.0661, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413983.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3887.7544, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(642.4078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(250.3096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(259.3715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0699],
        [-0.0838],
        [-0.0782],
        ...,
        [-0.6720],
        [-0.6698],
        [-0.6693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144330.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0067],
        [1.0113],
        ...,
        [1.0009],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367866.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0068],
        [1.0114],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367875.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1377.7135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0268, device='cuda:0')



h[100].sum tensor(-25.8102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9908, device='cuda:0')



h[200].sum tensor(29.4630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63598.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0331, 0.0103, 0.0000,  ..., 0.0167, 0.0654, 0.0000],
        [0.0332, 0.0103, 0.0000,  ..., 0.0167, 0.0656, 0.0000],
        [0.0333, 0.0103, 0.0000,  ..., 0.0166, 0.0658, 0.0000],
        ...,
        [0.0339, 0.0105, 0.0000,  ..., 0.0168, 0.0669, 0.0000],
        [0.0339, 0.0105, 0.0000,  ..., 0.0168, 0.0669, 0.0000],
        [0.0339, 0.0105, 0.0000,  ..., 0.0168, 0.0669, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494362.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3791.2920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(705.6719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(354.8894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(338.4030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6676],
        [-0.7473],
        [-0.8041],
        ...,
        [-0.6779],
        [-0.6763],
        [-0.6763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150173.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0068],
        [1.0114],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367875.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0068],
        [1.0114],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367883.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051, -0.0022,  0.0030,  ...,  0.0045, -0.0027,  0.0109],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(679.9193, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0153, device='cuda:0')



h[100].sum tensor(-14.9974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5656, device='cuda:0')



h[200].sum tensor(17.9083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0108, 0.0000, 0.0063,  ..., 0.0125, 0.0000, 0.0230],
        [0.0051, 0.0000, 0.0030,  ..., 0.0093, 0.0000, 0.0110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45461.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0097, 0.0000, 0.0346,  ..., 0.0315, 0.0065, 0.0514],
        [0.0192, 0.0030, 0.0179,  ..., 0.0248, 0.0250, 0.0241],
        [0.0292, 0.0064, 0.0051,  ..., 0.0194, 0.0510, 0.0056],
        ...,
        [0.0337, 0.0110, 0.0000,  ..., 0.0174, 0.0676, 0.0000],
        [0.0338, 0.0111, 0.0000,  ..., 0.0174, 0.0677, 0.0000],
        [0.0337, 0.0110, 0.0000,  ..., 0.0174, 0.0676, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407070.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3878.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(635.9886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(232.7560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(250.8995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0313],
        [-0.2096],
        [-0.4168],
        ...,
        [-0.6839],
        [-0.6817],
        [-0.6811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147490.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0068],
        [1.0114],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367883.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0068],
        [1.0114],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367883.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [ 0.0102, -0.0041,  0.0061,  ...,  0.0070, -0.0048,  0.0204],
        [ 0.0087, -0.0036,  0.0052,  ...,  0.0063, -0.0042,  0.0176],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(807.9097, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-16.9533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6398, device='cuda:0')



h[200].sum tensor(19.9758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5793, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0103, 0.0000, 0.0062,  ..., 0.0118, 0.0000, 0.0205],
        [0.0170, 0.0000, 0.0101,  ..., 0.0155, 0.0000, 0.0344],
        [0.0559, 0.0000, 0.0337,  ..., 0.0350, 0.0000, 0.1094],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47223.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0148, 0.0012, 0.0254,  ..., 0.0314, 0.0183, 0.0400],
        [0.0064, 0.0000, 0.0465,  ..., 0.0443, 0.0021, 0.0787],
        [0.0000, 0.0000, 0.0848,  ..., 0.0709, 0.0000, 0.1528],
        ...,
        [0.0227, 0.0018, 0.0128,  ..., 0.0237, 0.0265, 0.0161],
        [0.0254, 0.0032, 0.0097,  ..., 0.0222, 0.0367, 0.0118],
        [0.0310, 0.0077, 0.0031,  ..., 0.0190, 0.0573, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410432.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3827.6526, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(641.6614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.0434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(259.1939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0068],
        [ 0.0520],
        [ 0.0901],
        ...,
        [-0.2275],
        [-0.3153],
        [-0.4479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147923.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0068],
        [1.0114],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367883.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(424.2849, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0115],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367892.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(451.5815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0116, device='cuda:0')



h[100].sum tensor(-11.3694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4301, device='cuda:0')



h[200].sum tensor(14.0115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7844, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41177.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0098, 0.0026,  ..., 0.0185, 0.0606, 0.0008],
        [0.0310, 0.0090, 0.0025,  ..., 0.0191, 0.0595, 0.0019],
        [0.0285, 0.0058, 0.0050,  ..., 0.0207, 0.0507, 0.0061],
        ...,
        [0.0336, 0.0116, 0.0002,  ..., 0.0181, 0.0683, 0.0000],
        [0.0336, 0.0116, 0.0002,  ..., 0.0181, 0.0683, 0.0000],
        [0.0336, 0.0116, 0.0002,  ..., 0.0181, 0.0683, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(392664., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3873.8738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(619.4694, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(200.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(229.9663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1802],
        [-0.2044],
        [-0.1759],
        ...,
        [-0.6864],
        [-0.6842],
        [-0.6835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142724.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0115],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367892.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0115],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367901.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [ 0.0041, -0.0019,  0.0024,  ...,  0.0041, -0.0023,  0.0093],
        [ 0.0043, -0.0020,  0.0025,  ...,  0.0042, -0.0023,  0.0095],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(887.5907, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-17.8137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6757, device='cuda:0')



h[200].sum tensor(20.5444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0025,  ..., 0.0089, 0.0000, 0.0093],
        [0.0154, 0.0000, 0.0092,  ..., 0.0148, 0.0000, 0.0318],
        [0.0378, 0.0000, 0.0228,  ..., 0.0265, 0.0000, 0.0764],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51028.5117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0169, 0.0027, 0.0244,  ..., 0.0272, 0.0222, 0.0346],
        [0.0074, 0.0000, 0.0464,  ..., 0.0387, 0.0029, 0.0729],
        [0.0000, 0.0000, 0.0699,  ..., 0.0512, 0.0000, 0.1141],
        ...,
        [0.0336, 0.0120, 0.0004,  ..., 0.0187, 0.0690, 0.0000],
        [0.0336, 0.0120, 0.0004,  ..., 0.0187, 0.0690, 0.0000],
        [0.0336, 0.0120, 0.0004,  ..., 0.0187, 0.0690, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431746.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3660.6138, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(661.6492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(261.0871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(278.4170, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0827],
        [ 0.1530],
        [ 0.1887],
        ...,
        [-0.6884],
        [-0.6861],
        [-0.6854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137249.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0115],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367901.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0069],
        [1.0116],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367911., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(945.0148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0191, device='cuda:0')



h[100].sum tensor(-18.5058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7084, device='cuda:0')



h[200].sum tensor(21.0860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52953.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0322, 0.0114, 0.0012,  ..., 0.0195, 0.0658, 0.0007],
        [0.0328, 0.0122, 0.0004,  ..., 0.0192, 0.0682, 0.0000],
        [0.0329, 0.0122, 0.0004,  ..., 0.0192, 0.0684, 0.0000],
        ...,
        [0.0335, 0.0124, 0.0005,  ..., 0.0194, 0.0697, 0.0000],
        [0.0335, 0.0124, 0.0005,  ..., 0.0194, 0.0697, 0.0000],
        [0.0335, 0.0124, 0.0005,  ..., 0.0194, 0.0697, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450576.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3604.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(670.9872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(269.7955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(287.6859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4928],
        [-0.6380],
        [-0.7396],
        ...,
        [-0.6925],
        [-0.6902],
        [-0.6895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141869.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0069],
        [1.0116],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367911., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0116],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367920.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [ 0.0093, -0.0038,  0.0057,  ...,  0.0066, -0.0044,  0.0189],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(819.6505, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-16.5854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6299, device='cuda:0')



h[200].sum tensor(18.9517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3994, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0000, 0.0057,  ..., 0.0115, 0.0000, 0.0190],
        [0.0158, 0.0000, 0.0096,  ..., 0.0145, 0.0000, 0.0307],
        [0.0760, 0.0000, 0.0462,  ..., 0.0447, 0.0000, 0.1469],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48301.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0000, 0.0597,  ..., 0.0551, 0.0005, 0.1054],
        [0.0000, 0.0000, 0.0883,  ..., 0.0766, 0.0000, 0.1623],
        [0.0000, 0.0000, 0.1406,  ..., 0.1140, 0.0000, 0.2650],
        ...,
        [0.0333, 0.0132, 0.0004,  ..., 0.0199, 0.0706, 0.0000],
        [0.0333, 0.0132, 0.0004,  ..., 0.0199, 0.0706, 0.0000],
        [0.0333, 0.0132, 0.0004,  ..., 0.0199, 0.0706, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421922.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3598.2729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(657.6524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(237.4438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(265.3648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1908],
        [ 0.1918],
        [ 0.1935],
        ...,
        [-0.7030],
        [-0.7006],
        [-0.6999]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139177.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0116],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367920.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0117],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367928.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0492, -0.0179,  0.0300,  ...,  0.0255, -0.0212,  0.0922],
        [ 0.0188, -0.0071,  0.0115,  ...,  0.0111, -0.0084,  0.0363],
        [ 0.0093, -0.0037,  0.0057,  ...,  0.0066, -0.0044,  0.0188],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(917.9778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-18.0304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6797, device='cuda:0')



h[200].sum tensor(21.1960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3008, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0804, 0.0000, 0.0490,  ..., 0.0468, 0.0000, 0.1547],
        [0.1038, 0.0000, 0.0634,  ..., 0.0580, 0.0000, 0.1980],
        [0.0584, 0.0000, 0.0356,  ..., 0.0365, 0.0000, 0.1143],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52013.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.8187e-01,  ..., 1.4876e-01, 0.0000e+00,
         3.5296e-01],
        [0.0000e+00, 0.0000e+00, 1.8568e-01,  ..., 1.5063e-01, 0.0000e+00,
         3.6002e-01],
        [0.0000e+00, 0.0000e+00, 1.4505e-01,  ..., 1.1354e-01, 0.0000e+00,
         2.7069e-01],
        ...,
        [3.3225e-02, 1.4102e-02, 2.9837e-04,  ..., 2.0252e-02, 7.1167e-02,
         0.0000e+00],
        [3.3229e-02, 1.4104e-02, 2.9826e-04,  ..., 2.0256e-02, 7.1177e-02,
         0.0000e+00],
        [3.3223e-02, 1.4101e-02, 2.9824e-04,  ..., 2.0252e-02, 7.1163e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(443494.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3421.6929, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(677.0667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.5548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.4718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1526],
        [ 0.1684],
        [ 0.1881],
        ...,
        [-0.7180],
        [-0.7156],
        [-0.7149]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138658.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0070],
        [1.0117],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367928.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0071],
        [1.0117],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367937.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0272, -0.0100,  0.0166,  ...,  0.0150, -0.0119,  0.0516],
        [ 0.0192, -0.0072,  0.0117,  ...,  0.0113, -0.0085,  0.0369],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [ 0.0094, -0.0037,  0.0057,  ...,  0.0066, -0.0044,  0.0189]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1486.4241, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0274, device='cuda:0')



h[100].sum tensor(-26.5115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0126, device='cuda:0')



h[200].sum tensor(30.9562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.3255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1029, 0.0000, 0.0629,  ..., 0.0575, 0.0000, 0.1960],
        [0.0486, 0.0000, 0.0297,  ..., 0.0313, 0.0000, 0.0944],
        [0.0260, 0.0000, 0.0159,  ..., 0.0200, 0.0000, 0.0512],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0096, 0.0000, 0.0059,  ..., 0.0118, 0.0000, 0.0194],
        [0.0171, 0.0000, 0.0105,  ..., 0.0160, 0.0000, 0.0349]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66593.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1814,  ..., 0.1545, 0.0000, 0.3603],
        [0.0000, 0.0000, 0.1135,  ..., 0.1004, 0.0000, 0.2199],
        [0.0055, 0.0000, 0.0610,  ..., 0.0611, 0.0033, 0.1145],
        ...,
        [0.0244, 0.0050, 0.0101,  ..., 0.0258, 0.0401, 0.0153],
        [0.0144, 0.0020, 0.0258,  ..., 0.0356, 0.0205, 0.0443],
        [0.0062, 0.0000, 0.0468,  ..., 0.0484, 0.0033, 0.0829]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(520402.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3238.1841, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(743.5475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(356.1523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(357.8485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0302],
        [-0.0113],
        [-0.1416],
        ...,
        [-0.2208],
        [-0.0946],
        [ 0.0338]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-138831.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0071],
        [1.0117],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367937.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0118],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367947.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1239.4662, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0234, device='cuda:0')



h[100].sum tensor(-22.7055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8664, device='cuda:0')



h[200].sum tensor(27.6739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56269.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4911e-02, 5.1725e-03, 1.0710e-02,  ..., 2.4759e-02, 4.0095e-02,
         1.6068e-02],
        [2.4901e-02, 5.1811e-03, 1.0787e-02,  ..., 2.4828e-02, 3.9981e-02,
         1.6187e-02],
        [2.4995e-02, 4.9177e-03, 1.0599e-02,  ..., 2.4839e-02, 4.0475e-02,
         1.5951e-02],
        ...,
        [3.3063e-02, 1.5347e-02, 8.1531e-05,  ..., 2.1117e-02, 7.1998e-02,
         0.0000e+00],
        [3.3067e-02, 1.5349e-02, 8.1388e-05,  ..., 2.1121e-02, 7.2007e-02,
         0.0000e+00],
        [3.3061e-02, 1.5346e-02, 8.1376e-05,  ..., 2.1117e-02, 7.1993e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456691.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3420.2390, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(696.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(291.5287, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(306.8615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0610],
        [-0.0233],
        [ 0.0118],
        ...,
        [-0.7434],
        [-0.7409],
        [-0.7402]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161225.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0071],
        [1.0118],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367947.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0072],
        [1.0119],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367956.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [ 0.0128, -0.0049,  0.0079,  ...,  0.0083, -0.0058,  0.0252],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1407.9871, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0252, device='cuda:0')



h[100].sum tensor(-25.1107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9311, device='cuda:0')



h[200].sum tensor(30.8431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.8511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0464, 0.0000, 0.0286,  ..., 0.0308, 0.0000, 0.0918],
        [0.0103, 0.0000, 0.0064,  ..., 0.0121, 0.0000, 0.0206],
        [0.0129, 0.0000, 0.0080,  ..., 0.0133, 0.0000, 0.0254],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64232.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0500,  ..., 0.0521, 0.0000, 0.0913],
        [0.0080, 0.0000, 0.0292,  ..., 0.0392, 0.0045, 0.0528],
        [0.0145, 0.0017, 0.0227,  ..., 0.0355, 0.0185, 0.0410],
        ...,
        [0.0331, 0.0160, 0.0001,  ..., 0.0216, 0.0723, 0.0000],
        [0.0331, 0.0160, 0.0001,  ..., 0.0216, 0.0723, 0.0000],
        [0.0331, 0.0160, 0.0001,  ..., 0.0216, 0.0723, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508176.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3359.7981, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(740.4311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(338.2921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(347.5161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2042],
        [-0.2728],
        [-0.3762],
        ...,
        [-0.7501],
        [-0.7475],
        [-0.7466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143070.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0072],
        [1.0119],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367956.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0119],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367966.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0021,  0.0030,  ...,  0.0045, -0.0025,  0.0105],
        [ 0.0107, -0.0042,  0.0066,  ...,  0.0073, -0.0050,  0.0214],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(634.0334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-13.4103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5117, device='cuda:0')



h[200].sum tensor(18.8289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0270, 0.0000, 0.0167,  ..., 0.0210, 0.0000, 0.0545],
        [0.0135, 0.0000, 0.0083,  ..., 0.0141, 0.0000, 0.0280],
        [0.0389, 0.0000, 0.0240,  ..., 0.0272, 0.0000, 0.0780],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44323.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 8.0162e-02,  ..., 6.8411e-02, 0.0000e+00,
         1.4545e-01],
        [1.2532e-03, 0.0000e+00, 5.4622e-02,  ..., 5.1901e-02, 0.0000e+00,
         9.6891e-02],
        [0.0000e+00, 0.0000e+00, 5.2487e-02,  ..., 5.0974e-02, 0.0000e+00,
         9.2842e-02],
        ...,
        [3.3139e-02, 1.6705e-02, 2.9887e-04,  ..., 2.2236e-02, 7.2320e-02,
         4.2725e-05],
        [3.3143e-02, 1.6707e-02, 2.9873e-04,  ..., 2.2239e-02, 7.2329e-02,
         4.2698e-05],
        [3.3136e-02, 1.6704e-02, 2.9868e-04,  ..., 2.2235e-02, 7.2314e-02,
         4.2930e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415760.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3501.2668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(653.3497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(210.3321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(249.3828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1553],
        [ 0.1333],
        [ 0.0946],
        ...,
        [-0.7522],
        [-0.7497],
        [-0.7490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168159.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0119],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367966.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0119],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367966.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1063.0254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0201, device='cuda:0')



h[100].sum tensor(-19.8001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7437, device='cuda:0')



h[200].sum tensor(25.6631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55932.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.2366e-02, 1.6383e-02, 2.7574e-04,  ..., 2.1973e-02, 7.0618e-02,
         1.2671e-04],
        [3.2431e-02, 1.6410e-02, 2.7835e-04,  ..., 2.1996e-02, 7.0761e-02,
         1.2183e-04],
        [3.2530e-02, 1.6433e-02, 2.9781e-04,  ..., 2.1960e-02, 7.0984e-02,
         1.1722e-04],
        ...,
        [3.2674e-02, 1.6012e-02, 9.5483e-04,  ..., 2.2477e-02, 7.0350e-02,
         1.2295e-03],
        [3.3143e-02, 1.6707e-02, 2.9873e-04,  ..., 2.2239e-02, 7.2329e-02,
         4.2698e-05],
        [3.3136e-02, 1.6704e-02, 2.9868e-04,  ..., 2.2235e-02, 7.2314e-02,
         4.2930e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466860.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3199.9150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(722.5826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(278.6557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(310.1210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9204],
        [-0.9147],
        [-0.8934],
        ...,
        [-0.7052],
        [-0.7348],
        [-0.7459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127004.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0119],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367966.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(488.5871, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0120],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367977., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0102, -0.0040,  0.0063,  ...,  0.0070, -0.0047,  0.0204],
        [ 0.0105, -0.0041,  0.0065,  ...,  0.0071, -0.0048,  0.0209],
        [ 0.0143, -0.0054,  0.0088,  ...,  0.0089, -0.0064,  0.0279],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(933.7245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-17.7314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6799, device='cuda:0')



h[200].sum tensor(23.7041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0471, 0.0000, 0.0290,  ..., 0.0311, 0.0000, 0.0932],
        [0.0446, 0.0000, 0.0275,  ..., 0.0299, 0.0000, 0.0885],
        [0.0493, 0.0000, 0.0303,  ..., 0.0316, 0.0000, 0.0956],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50514.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0843,  ..., 0.0741, 0.0000, 0.1552],
        [0.0000, 0.0000, 0.0989,  ..., 0.0843, 0.0000, 0.1839],
        [0.0000, 0.0000, 0.1043,  ..., 0.0909, 0.0000, 0.1979],
        ...,
        [0.0333, 0.0174, 0.0005,  ..., 0.0228, 0.0724, 0.0003],
        [0.0333, 0.0174, 0.0005,  ..., 0.0228, 0.0725, 0.0003],
        [0.0333, 0.0174, 0.0005,  ..., 0.0228, 0.0724, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438972.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3379.7505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(677.9459, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(249.3206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(279.0828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1439],
        [ 0.1544],
        [ 0.1550],
        ...,
        [-0.7528],
        [-0.7504],
        [-0.7496]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164424.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0073],
        [1.0120],
        ...,
        [1.0009],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367977., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0074],
        [1.0121],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367987.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0176, -0.0066,  0.0108,  ...,  0.0105, -0.0078,  0.0340],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [ 0.0068, -0.0028,  0.0042,  ...,  0.0054, -0.0033,  0.0141],
        [ 0.0164, -0.0062,  0.0101,  ...,  0.0100, -0.0073,  0.0319],
        [ 0.0113, -0.0044,  0.0070,  ...,  0.0075, -0.0052,  0.0224]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1073.4095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0202, device='cuda:0')



h[100].sum tensor(-19.6814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7482, device='cuda:0')



h[200].sum tensor(25.7739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0661, 0.0000, 0.0406,  ..., 0.0395, 0.0000, 0.1266],
        [0.0415, 0.0000, 0.0255,  ..., 0.0278, 0.0000, 0.0813],
        [0.0180, 0.0000, 0.0111,  ..., 0.0162, 0.0000, 0.0365],
        ...,
        [0.0307, 0.0000, 0.0189,  ..., 0.0223, 0.0000, 0.0598],
        [0.0534, 0.0000, 0.0328,  ..., 0.0342, 0.0000, 0.1049],
        [0.0636, 0.0000, 0.0391,  ..., 0.0390, 0.0000, 0.1237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53303.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1653,  ..., 0.1422, 0.0000, 0.3270],
        [0.0000, 0.0000, 0.1294,  ..., 0.1113, 0.0000, 0.2498],
        [0.0000, 0.0000, 0.1068,  ..., 0.0934, 0.0000, 0.2031],
        ...,
        [0.0019, 0.0000, 0.0693,  ..., 0.0647, 0.0000, 0.1269],
        [0.0000, 0.0000, 0.0968,  ..., 0.0818, 0.0000, 0.1783],
        [0.0000, 0.0000, 0.0981,  ..., 0.0845, 0.0000, 0.1827]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451868.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3448.4204, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(692.2307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(264.0859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.0424, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0391],
        [0.0672],
        [0.0899],
        ...,
        [0.0544],
        [0.1576],
        [0.1467]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162747.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0074],
        [1.0121],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367987.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0122],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367997.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0239, -0.0087,  0.0146,  ...,  0.0135, -0.0104,  0.0456],
        [ 0.0259, -0.0094,  0.0159,  ...,  0.0144, -0.0112,  0.0493],
        [ 0.0237, -0.0087,  0.0145,  ...,  0.0134, -0.0103,  0.0452],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(855.0626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-16.3053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6247, device='cuda:0')



h[200].sum tensor(22.1949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3053, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0841, 0.0000, 0.0516,  ..., 0.0485, 0.0000, 0.1615],
        [0.0866, 0.0000, 0.0531,  ..., 0.0497, 0.0000, 0.1661],
        [0.0796, 0.0000, 0.0489,  ..., 0.0464, 0.0000, 0.1532],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48863.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1484,  ..., 0.1290, 0.0000, 0.2921],
        [0.0000, 0.0000, 0.1427,  ..., 0.1255, 0.0000, 0.2810],
        [0.0000, 0.0000, 0.1269,  ..., 0.1130, 0.0000, 0.2481],
        ...,
        [0.0337, 0.0189, 0.0015,  ..., 0.0240, 0.0728, 0.0007],
        [0.0337, 0.0189, 0.0015,  ..., 0.0240, 0.0728, 0.0007],
        [0.0337, 0.0189, 0.0015,  ..., 0.0240, 0.0728, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(432288.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3461.9631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(681.2372, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(230.9531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(269.4186, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1496],
        [ 0.1230],
        [ 0.0697],
        ...,
        [-0.7513],
        [-0.7489],
        [-0.7482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140558.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0074],
        [1.0122],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367997.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0122],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368008.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053, -0.0023,  0.0033,  ...,  0.0047, -0.0027,  0.0114],
        [ 0.0089, -0.0035,  0.0055,  ...,  0.0064, -0.0042,  0.0181],
        [ 0.0203, -0.0075,  0.0124,  ...,  0.0117, -0.0089,  0.0390],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(982.3410, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0187, device='cuda:0')



h[100].sum tensor(-18.0855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6935, device='cuda:0')



h[200].sum tensor(23.7694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0000, 0.0122,  ..., 0.0175, 0.0000, 0.0414],
        [0.0488, 0.0000, 0.0300,  ..., 0.0318, 0.0000, 0.0966],
        [0.0632, 0.0000, 0.0388,  ..., 0.0386, 0.0000, 0.1231],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52190.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0000, 0.0701,  ..., 0.0580, 0.0000, 0.1221],
        [0.0000, 0.0000, 0.1081,  ..., 0.0866, 0.0000, 0.1981],
        [0.0000, 0.0000, 0.1341,  ..., 0.1075, 0.0000, 0.2511],
        ...,
        [0.0340, 0.0197, 0.0019,  ..., 0.0247, 0.0732, 0.0009],
        [0.0340, 0.0197, 0.0019,  ..., 0.0247, 0.0732, 0.0009],
        [0.0340, 0.0197, 0.0019,  ..., 0.0247, 0.0732, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448117.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3487.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(694.9761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(250.3706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.9027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1939],
        [ 0.2052],
        [ 0.2049],
        ...,
        [-0.7517],
        [-0.7491],
        [-0.7482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139168.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0122],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368008.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0123],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368018.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0086, -0.0034,  0.0053,  ...,  0.0062, -0.0040,  0.0174],
        [ 0.0170, -0.0063,  0.0104,  ...,  0.0102, -0.0075,  0.0329],
        [ 0.0033, -0.0016,  0.0021,  ...,  0.0037, -0.0019,  0.0078],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(868.0676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-16.3603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6242, device='cuda:0')



h[200].sum tensor(21.4239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2972, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0696, 0.0000, 0.0427,  ..., 0.0416, 0.0000, 0.1350],
        [0.0357, 0.0000, 0.0220,  ..., 0.0256, 0.0000, 0.0726],
        [0.0333, 0.0000, 0.0205,  ..., 0.0245, 0.0000, 0.0682],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49608.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1144,  ..., 0.0925, 0.0000, 0.2116],
        [0.0000, 0.0000, 0.0998,  ..., 0.0773, 0.0000, 0.1786],
        [0.0000, 0.0000, 0.0872,  ..., 0.0659, 0.0000, 0.1520],
        ...,
        [0.0343, 0.0206, 0.0022,  ..., 0.0252, 0.0739, 0.0012],
        [0.0343, 0.0206, 0.0022,  ..., 0.0252, 0.0739, 0.0012],
        [0.0343, 0.0206, 0.0022,  ..., 0.0252, 0.0739, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440243.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3605.7607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(679.4045, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(234.3700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(269.8156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2540],
        [ 0.2584],
        [ 0.2557],
        ...,
        [-0.7567],
        [-0.7543],
        [-0.7536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145913.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0075],
        [1.0123],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368018.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0123],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368029., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0228, -0.0083,  0.0140,  ...,  0.0130, -0.0099,  0.0437],
        [ 0.0237, -0.0086,  0.0145,  ...,  0.0134, -0.0102,  0.0453],
        [ 0.0100, -0.0039,  0.0061,  ...,  0.0069, -0.0046,  0.0201],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(958.7003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-17.6545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6820, device='cuda:0')



h[200].sum tensor(22.3624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0690, 0.0000, 0.0423,  ..., 0.0414, 0.0000, 0.1339],
        [0.0693, 0.0000, 0.0425,  ..., 0.0415, 0.0000, 0.1345],
        [0.0661, 0.0000, 0.0405,  ..., 0.0400, 0.0000, 0.1285],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51785.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1137,  ..., 0.1001, 0.0000, 0.2184],
        [0.0000, 0.0000, 0.1148,  ..., 0.1005, 0.0000, 0.2203],
        [0.0000, 0.0000, 0.1016,  ..., 0.0898, 0.0000, 0.1932],
        ...,
        [0.0346, 0.0215, 0.0023,  ..., 0.0257, 0.0747, 0.0014],
        [0.0346, 0.0215, 0.0023,  ..., 0.0257, 0.0747, 0.0014],
        [0.0346, 0.0215, 0.0023,  ..., 0.0257, 0.0747, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448495.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3579.6006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(696.5800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.9437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.6529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1890],
        [ 0.2041],
        [ 0.2143],
        ...,
        [-0.7635],
        [-0.7571],
        [-0.7411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127446.7578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0123],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368029., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0124],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368039.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0019,  0.0027,  ...,  0.0042, -0.0023,  0.0096],
        [ 0.0039, -0.0018,  0.0025,  ...,  0.0040, -0.0021,  0.0089],
        [ 0.0094, -0.0037,  0.0058,  ...,  0.0066, -0.0044,  0.0190],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(617.2566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0133, device='cuda:0')



h[100].sum tensor(-12.6751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4906, device='cuda:0')



h[200].sum tensor(16.4778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0000, 0.0094,  ..., 0.0148, 0.0000, 0.0314],
        [0.0322, 0.0000, 0.0199,  ..., 0.0240, 0.0000, 0.0660],
        [0.0141, 0.0000, 0.0087,  ..., 0.0149, 0.0000, 0.0310],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44211.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0000, 0.0468,  ..., 0.0454, 0.0043, 0.0794],
        [0.0002, 0.0000, 0.0640,  ..., 0.0545, 0.0000, 0.1101],
        [0.0042, 0.0000, 0.0514,  ..., 0.0471, 0.0006, 0.0867],
        ...,
        [0.0348, 0.0224, 0.0020,  ..., 0.0261, 0.0758, 0.0014],
        [0.0348, 0.0224, 0.0020,  ..., 0.0261, 0.0758, 0.0014],
        [0.0348, 0.0224, 0.0020,  ..., 0.0261, 0.0758, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421175.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3722.5198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(657.9660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(203.0272, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(244.9892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1609],
        [ 0.1881],
        [ 0.1418],
        ...,
        [-0.7781],
        [-0.7754],
        [-0.7746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170669.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0076],
        [1.0124],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368039.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0077],
        [1.0125],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368049.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1112.7100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0208, device='cuda:0')



h[100].sum tensor(-19.9414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7680, device='cuda:0')



h[200].sum tensor(23.7776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8985, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55207.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0326, 0.0200, 0.0047,  ..., 0.0267, 0.0675, 0.0063],
        [0.0322, 0.0192, 0.0056,  ..., 0.0270, 0.0655, 0.0077],
        [0.0319, 0.0186, 0.0064,  ..., 0.0272, 0.0639, 0.0089],
        ...,
        [0.0305, 0.0159, 0.0073,  ..., 0.0291, 0.0599, 0.0112],
        [0.0339, 0.0215, 0.0030,  ..., 0.0270, 0.0729, 0.0036],
        [0.0350, 0.0233, 0.0018,  ..., 0.0263, 0.0768, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467336.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3561.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(718.5615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(272.2058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.7178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4907],
        [-0.4187],
        [-0.3519],
        ...,
        [-0.5958],
        [-0.7025],
        [-0.7595]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151832.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0077],
        [1.0125],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368049.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0077],
        [1.0126],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368060.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099, -0.0038,  0.0061,  ...,  0.0069, -0.0045,  0.0198],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(727.2046, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-14.3344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5597, device='cuda:0')



h[200].sum tensor(17.3836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0148, 0.0000, 0.0092,  ..., 0.0148, 0.0000, 0.0306],
        [0.0099, 0.0000, 0.0061,  ..., 0.0119, 0.0000, 0.0199],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47764.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0002, 0.0633,  ..., 0.0657, 0.0053, 0.1203],
        [0.0073, 0.0040, 0.0447,  ..., 0.0551, 0.0141, 0.0860],
        [0.0123, 0.0044, 0.0297,  ..., 0.0458, 0.0199, 0.0575],
        ...,
        [0.0352, 0.0242, 0.0017,  ..., 0.0266, 0.0776, 0.0014],
        [0.0352, 0.0242, 0.0017,  ..., 0.0266, 0.0776, 0.0014],
        [0.0352, 0.0242, 0.0017,  ..., 0.0266, 0.0776, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442316.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3725.3787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(680.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(229.1460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(265.8026, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1441],
        [ 0.1312],
        [ 0.1280],
        ...,
        [-0.8043],
        [-0.8017],
        [-0.8009]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186329.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0077],
        [1.0126],
        ...,
        [1.0009],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368060.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0078],
        [1.0127],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368071.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052, -0.0022,  0.0032,  ...,  0.0047, -0.0026,  0.0112],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0017,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(957.6619, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-17.6735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6852, device='cuda:0')



h[200].sum tensor(20.7558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0000, 0.0049,  ..., 0.0115, 0.0000, 0.0178],
        [0.0052, 0.0000, 0.0033,  ..., 0.0097, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54776.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0043, 0.0272,  ..., 0.0363, 0.0189, 0.0453],
        [0.0237, 0.0089, 0.0180,  ..., 0.0332, 0.0352, 0.0299],
        [0.0298, 0.0154, 0.0101,  ..., 0.0294, 0.0540, 0.0161],
        ...,
        [0.0356, 0.0250, 0.0016,  ..., 0.0269, 0.0783, 0.0016],
        [0.0356, 0.0250, 0.0016,  ..., 0.0269, 0.0783, 0.0016],
        [0.0356, 0.0250, 0.0016,  ..., 0.0269, 0.0783, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(484467.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3639.0662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(721.8987, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(271.6276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(301.8781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3988],
        [-0.4119],
        [-0.3753],
        ...,
        [-0.8133],
        [-0.8106],
        [-0.8098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164586.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0078],
        [1.0127],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368071.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(509.1487, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0079],
        [1.0128],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368082.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0093, -0.0036,  0.0058,  ...,  0.0066, -0.0043,  0.0188],
        [ 0.0171, -0.0062,  0.0105,  ...,  0.0103, -0.0074,  0.0331],
        [ 0.0047, -0.0020,  0.0029,  ...,  0.0044, -0.0024,  0.0103],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(960.4626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-17.6549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6792, device='cuda:0')



h[200].sum tensor(20.8446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0786, 0.0000, 0.0483,  ..., 0.0461, 0.0000, 0.1513],
        [0.0530, 0.0000, 0.0326,  ..., 0.0340, 0.0000, 0.1042],
        [0.0289, 0.0000, 0.0178,  ..., 0.0215, 0.0000, 0.0566],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53540.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1336,  ..., 0.1163, 0.0000, 0.2623],
        [0.0000, 0.0000, 0.1135,  ..., 0.0996, 0.0000, 0.2198],
        [0.0033, 0.0000, 0.0770,  ..., 0.0744, 0.0000, 0.1475],
        ...,
        [0.0359, 0.0257, 0.0018,  ..., 0.0273, 0.0787, 0.0019],
        [0.0359, 0.0257, 0.0018,  ..., 0.0273, 0.0788, 0.0019],
        [0.0359, 0.0257, 0.0018,  ..., 0.0273, 0.0787, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469811.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3633.9304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(724.5296, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(260.8961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(295.8335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1945],
        [ 0.1858],
        [ 0.1516],
        ...,
        [-0.8186],
        [-0.8159],
        [-0.8151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148811.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0079],
        [1.0128],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368082.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0080],
        [1.0128],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368093.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [ 0.0088, -0.0034,  0.0054,  ...,  0.0064, -0.0040,  0.0178],
        [ 0.0182, -0.0066,  0.0112,  ...,  0.0108, -0.0079,  0.0352],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1141.9452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0211, device='cuda:0')



h[100].sum tensor(-20.1611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7810, device='cuda:0')



h[200].sum tensor(24.0787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0158, 0.0000, 0.0098,  ..., 0.0152, 0.0000, 0.0323],
        [0.0319, 0.0000, 0.0197,  ..., 0.0235, 0.0000, 0.0637],
        [0.0486, 0.0000, 0.0299,  ..., 0.0319, 0.0000, 0.0960],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57058.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0009, 0.0462,  ..., 0.0527, 0.0094, 0.0852],
        [0.0020, 0.0000, 0.0807,  ..., 0.0768, 0.0000, 0.1541],
        [0.0000, 0.0000, 0.1127,  ..., 0.0995, 0.0000, 0.2183],
        ...,
        [0.0328, 0.0205, 0.0067,  ..., 0.0297, 0.0651, 0.0106],
        [0.0361, 0.0264, 0.0018,  ..., 0.0276, 0.0790, 0.0021],
        [0.0361, 0.0264, 0.0018,  ..., 0.0276, 0.0790, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(483852.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3648.5706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(735.9186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(284.7470, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.3974, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1677],
        [ 0.1731],
        [ 0.1746],
        ...,
        [-0.5675],
        [-0.7085],
        [-0.7840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155726.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0080],
        [1.0128],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368093.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0081],
        [1.0129],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368105.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045, -0.0019,  0.0028,  ...,  0.0043, -0.0023,  0.0099],
        [ 0.0134, -0.0050,  0.0083,  ...,  0.0086, -0.0059,  0.0264],
        [ 0.0167, -0.0061,  0.0103,  ...,  0.0101, -0.0072,  0.0323],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1250.6265, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0226, device='cuda:0')



h[100].sum tensor(-21.6145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8356, device='cuda:0')



h[200].sum tensor(25.9267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0000, 0.0162,  ..., 0.0208, 0.0000, 0.0532],
        [0.0503, 0.0000, 0.0310,  ..., 0.0327, 0.0000, 0.0992],
        [0.0759, 0.0000, 0.0466,  ..., 0.0448, 0.0000, 0.1462],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58721.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0879,  ..., 0.0815, 0.0000, 0.1690],
        [0.0000, 0.0000, 0.1086,  ..., 0.0969, 0.0000, 0.2117],
        [0.0000, 0.0000, 0.1233,  ..., 0.1090, 0.0000, 0.2429],
        ...,
        [0.0364, 0.0270, 0.0018,  ..., 0.0280, 0.0794, 0.0024],
        [0.0364, 0.0270, 0.0018,  ..., 0.0280, 0.0794, 0.0024],
        [0.0364, 0.0270, 0.0018,  ..., 0.0280, 0.0793, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489034.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3692.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(739.1492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(296.6057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.5816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0646],
        [ 0.0865],
        [ 0.1104],
        ...,
        [-0.8311],
        [-0.8284],
        [-0.8275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173475.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0081],
        [1.0129],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368105.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0081],
        [1.0130],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368116.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(725.9612, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-13.9520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5466, device='cuda:0')



h[200].sum tensor(18.1715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8913, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46487.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0358, 0.0270, 0.0018,  ..., 0.0279, 0.0775, 0.0027],
        [0.0359, 0.0270, 0.0018,  ..., 0.0279, 0.0777, 0.0027],
        [0.0360, 0.0270, 0.0018,  ..., 0.0279, 0.0779, 0.0027],
        ...,
        [0.0367, 0.0275, 0.0019,  ..., 0.0283, 0.0795, 0.0027],
        [0.0367, 0.0275, 0.0019,  ..., 0.0283, 0.0795, 0.0027],
        [0.0367, 0.0275, 0.0019,  ..., 0.0283, 0.0795, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438232.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3967.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(691.4686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(214.6565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.7316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0138],
        [-1.0230],
        [-1.0255],
        ...,
        [-0.8308],
        [-0.8296],
        [-0.8290]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169364.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0081],
        [1.0130],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368116.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0082],
        [1.0131],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368128.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(710.4165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-13.5450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5241, device='cuda:0')



h[200].sum tensor(18.6368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4847, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45711.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0360, 0.0274, 0.0020,  ..., 0.0283, 0.0773, 0.0032],
        [0.0361, 0.0274, 0.0021,  ..., 0.0284, 0.0775, 0.0032],
        [0.0362, 0.0275, 0.0021,  ..., 0.0284, 0.0777, 0.0032],
        ...,
        [0.0370, 0.0280, 0.0022,  ..., 0.0288, 0.0793, 0.0032],
        [0.0370, 0.0280, 0.0022,  ..., 0.0288, 0.0793, 0.0032],
        [0.0369, 0.0280, 0.0022,  ..., 0.0288, 0.0793, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434393.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4025.7400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(680.4682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(209.8153, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(252.1174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8761],
        [-0.9407],
        [-0.9907],
        ...,
        [-0.8346],
        [-0.8318],
        [-0.8310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177805.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0082],
        [1.0131],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368128.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0083],
        [1.0132],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368139.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [ 0.0048, -0.0020,  0.0030,  ...,  0.0044, -0.0024,  0.0105],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(898.8213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0168, device='cuda:0')



h[100].sum tensor(-16.0365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6223, device='cuda:0')



h[200].sum tensor(22.1256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0000, 0.0041,  ..., 0.0106, 0.0000, 0.0154],
        [0.0049, 0.0000, 0.0030,  ..., 0.0093, 0.0000, 0.0106],
        [0.0038, 0.0000, 0.0023,  ..., 0.0088, 0.0000, 0.0086],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49678.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0199, 0.0030, 0.0325,  ..., 0.0387, 0.0128, 0.0544],
        [0.0245, 0.0048, 0.0238,  ..., 0.0361, 0.0219, 0.0397],
        [0.0265, 0.0086, 0.0206,  ..., 0.0350, 0.0307, 0.0341],
        ...,
        [0.0373, 0.0284, 0.0026,  ..., 0.0293, 0.0791, 0.0038],
        [0.0373, 0.0284, 0.0026,  ..., 0.0294, 0.0791, 0.0038],
        [0.0373, 0.0284, 0.0026,  ..., 0.0293, 0.0791, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450830.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4051.3547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(695.1780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(230.9027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(268.0943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0936],
        [-0.0244],
        [-0.1755],
        ...,
        [-0.8309],
        [-0.8282],
        [-0.8274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166013.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0083],
        [1.0132],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368139.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0083],
        [1.0133],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368151.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0015,  0.0021,  ...,  0.0037, -0.0018,  0.0079],
        [ 0.0034, -0.0015,  0.0021,  ...,  0.0037, -0.0018,  0.0079],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1260.4910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0223, device='cuda:0')



h[100].sum tensor(-20.9526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8263, device='cuda:0')



h[200].sum tensor(28.3419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.9535, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0132, 0.0000, 0.0081,  ..., 0.0142, 0.0000, 0.0292],
        [0.0090, 0.0000, 0.0056,  ..., 0.0123, 0.0000, 0.0215],
        [0.0060, 0.0000, 0.0037,  ..., 0.0103, 0.0000, 0.0144],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60450.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0978e-02, 8.9331e-05, 4.6408e-02,  ..., 4.5268e-02, 2.6697e-03,
         7.8844e-02],
        [1.5338e-02, 1.9912e-03, 4.2066e-02,  ..., 4.2388e-02, 6.8264e-03,
         7.0590e-02],
        [2.1669e-02, 6.7548e-03, 3.1296e-02,  ..., 3.8560e-02, 2.0945e-02,
         5.2310e-02],
        ...,
        [3.7715e-02, 2.8885e-02, 3.1407e-03,  ..., 2.9859e-02, 7.8869e-02,
         4.4160e-03],
        [3.7719e-02, 2.8888e-02, 3.1408e-03,  ..., 2.9863e-02, 7.8877e-02,
         4.4167e-03],
        [3.7711e-02, 2.8882e-02, 3.1398e-03,  ..., 2.9857e-02, 7.8860e-02,
         4.4160e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511848.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4018.7917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(730.1175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(298.7810, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.7020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1466],
        [ 0.1174],
        [-0.0054],
        ...,
        [-0.8257],
        [-0.8231],
        [-0.8223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155784.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0083],
        [1.0133],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368151.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0084],
        [1.0133],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368162.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(725.2727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-13.1448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5091, device='cuda:0')



h[200].sum tensor(20.5827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47042.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0373, 0.0284, 0.0045,  ..., 0.0295, 0.0748, 0.0067],
        [0.0373, 0.0287, 0.0037,  ..., 0.0298, 0.0762, 0.0055],
        [0.0374, 0.0289, 0.0034,  ..., 0.0299, 0.0771, 0.0049],
        ...,
        [0.0381, 0.0294, 0.0035,  ..., 0.0303, 0.0787, 0.0049],
        [0.0381, 0.0294, 0.0035,  ..., 0.0303, 0.0787, 0.0049],
        [0.0381, 0.0294, 0.0035,  ..., 0.0303, 0.0786, 0.0049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441993.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4234.6572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(679.2852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.9810, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(247.9659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5423],
        [-0.6916],
        [-0.8092],
        ...,
        [-0.8164],
        [-0.8137],
        [-0.8127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143115.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0084],
        [1.0133],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368162.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0085],
        [1.0134],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368173.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(883.9061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-15.2350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5949, device='cuda:0')



h[200].sum tensor(23.5704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49683.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0375, 0.0290, 0.0044,  ..., 0.0300, 0.0752, 0.0064],
        [0.0371, 0.0276, 0.0061,  ..., 0.0302, 0.0717, 0.0091],
        [0.0352, 0.0230, 0.0105,  ..., 0.0313, 0.0611, 0.0163],
        ...,
        [0.0384, 0.0299, 0.0037,  ..., 0.0307, 0.0786, 0.0051],
        [0.0384, 0.0299, 0.0037,  ..., 0.0307, 0.0786, 0.0051],
        [0.0384, 0.0299, 0.0037,  ..., 0.0307, 0.0786, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451572.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4224.2886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(691.7591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(221.8627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(259.9138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5748],
        [-0.4074],
        [-0.2112],
        ...,
        [-0.8231],
        [-0.8206],
        [-0.8198]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131402.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0085],
        [1.0134],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368173.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0085],
        [1.0135],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368185., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1975.2952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0317, device='cuda:0')



h[100].sum tensor(-30.5272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1715, device='cuda:0')



h[200].sum tensor(40.9266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78207.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0342, 0.0234, 0.0076,  ..., 0.0326, 0.0639, 0.0127],
        [0.0371, 0.0288, 0.0043,  ..., 0.0308, 0.0746, 0.0065],
        [0.0357, 0.0259, 0.0065,  ..., 0.0317, 0.0687, 0.0103],
        ...,
        [0.0385, 0.0305, 0.0036,  ..., 0.0309, 0.0788, 0.0050],
        [0.0385, 0.0305, 0.0036,  ..., 0.0309, 0.0788, 0.0050],
        [0.0385, 0.0304, 0.0036,  ..., 0.0309, 0.0787, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598874.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4026.9795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(807.2311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(408.0546, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(400.1732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3259],
        [-0.4911],
        [-0.5619],
        ...,
        [-0.8328],
        [-0.8302],
        [-0.8294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130435.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0085],
        [1.0135],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368185., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(545.3647, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0086],
        [1.0135],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368195.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065, -0.0025,  0.0040,  ...,  0.0052, -0.0030,  0.0135],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1262.1948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0215, device='cuda:0')



h[100].sum tensor(-20.5742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7972, device='cuda:0')



h[200].sum tensor(29.8301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0000, 0.0105,  ..., 0.0155, 0.0000, 0.0347],
        [0.0065, 0.0000, 0.0040,  ..., 0.0099, 0.0000, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59866.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0006, 0.0582,  ..., 0.0636, 0.0047, 0.1102],
        [0.0193, 0.0099, 0.0281,  ..., 0.0447, 0.0281, 0.0518],
        [0.0321, 0.0196, 0.0101,  ..., 0.0345, 0.0559, 0.0177],
        ...,
        [0.0386, 0.0311, 0.0031,  ..., 0.0311, 0.0796, 0.0046],
        [0.0386, 0.0311, 0.0031,  ..., 0.0311, 0.0796, 0.0046],
        [0.0386, 0.0311, 0.0031,  ..., 0.0311, 0.0796, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509471.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4111.8408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(727.6920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(297.3255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(313.0406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0355],
        [-0.1318],
        [-0.3002],
        ...,
        [-0.8521],
        [-0.8494],
        [-0.8486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169612.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0086],
        [1.0135],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368195.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0086],
        [1.0136],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368205.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [ 0.0056, -0.0023,  0.0035,  ...,  0.0048, -0.0027,  0.0120],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1558.7611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0264, device='cuda:0')



h[100].sum tensor(-24.8201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9761, device='cuda:0')



h[200].sum tensor(34.2157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.6656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0057, 0.0000, 0.0035,  ..., 0.0096, 0.0000, 0.0121],
        [0.0044, 0.0000, 0.0027,  ..., 0.0090, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64971.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0351, 0.0257, 0.0067,  ..., 0.0324, 0.0670, 0.0116],
        [0.0291, 0.0152, 0.0163,  ..., 0.0363, 0.0420, 0.0282],
        [0.0258, 0.0091, 0.0243,  ..., 0.0381, 0.0269, 0.0412],
        ...,
        [0.0388, 0.0318, 0.0027,  ..., 0.0312, 0.0806, 0.0043],
        [0.0388, 0.0318, 0.0027,  ..., 0.0312, 0.0806, 0.0043],
        [0.0388, 0.0318, 0.0027,  ..., 0.0312, 0.0806, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529356.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3910.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(764.5502, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(329.8292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(342.0021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6360],
        [-0.4017],
        [-0.1590],
        ...,
        [-0.8648],
        [-0.8621],
        [-0.8614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151558.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0086],
        [1.0136],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368205.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0087],
        [1.0136],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368216.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0085, -0.0032,  0.0052,  ...,  0.0062, -0.0038,  0.0172],
        [ 0.0159, -0.0056,  0.0097,  ...,  0.0097, -0.0067,  0.0309],
        [ 0.0297, -0.0102,  0.0182,  ...,  0.0162, -0.0122,  0.0562],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(991.1395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0180, device='cuda:0')



h[100].sum tensor(-16.9350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6662, device='cuda:0')



h[200].sum tensor(25.3970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0562, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0292, 0.0000, 0.0179,  ..., 0.0214, 0.0000, 0.0571],
        [0.0696, 0.0000, 0.0427,  ..., 0.0417, 0.0000, 0.1346],
        [0.0868, 0.0000, 0.0532,  ..., 0.0498, 0.0000, 0.1664],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52196.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0000, 0.0809,  ..., 0.0807, 0.0000, 0.1590],
        [0.0000, 0.0000, 0.1317,  ..., 0.1203, 0.0000, 0.2655],
        [0.0000, 0.0000, 0.1683,  ..., 0.1513, 0.0000, 0.3448],
        ...,
        [0.0332, 0.0213, 0.0099,  ..., 0.0353, 0.0586, 0.0177],
        [0.0391, 0.0324, 0.0025,  ..., 0.0314, 0.0813, 0.0041],
        [0.0391, 0.0324, 0.0025,  ..., 0.0314, 0.0813, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469763., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4204.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(711.2076, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(250.1582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(279.9698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1856],
        [ 0.1968],
        [ 0.1949],
        ...,
        [-0.4136],
        [-0.6450],
        [-0.7881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176612.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0087],
        [1.0136],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368216.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0088],
        [1.0137],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368226.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0073, -0.0028,  0.0045,  ...,  0.0057, -0.0034,  0.0152],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [ 0.0073, -0.0028,  0.0045,  ...,  0.0057, -0.0034,  0.0152],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1165.1028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0208, device='cuda:0')



h[100].sum tensor(-19.4047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7688, device='cuda:0')



h[200].sum tensor(27.9425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0000, 0.0036,  ..., 0.0098, 0.0000, 0.0123],
        [0.0265, 0.0000, 0.0163,  ..., 0.0213, 0.0000, 0.0553],
        [0.0058, 0.0000, 0.0036,  ..., 0.0098, 0.0000, 0.0124],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57405.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0232, 0.0080, 0.0227,  ..., 0.0411, 0.0254, 0.0410],
        [0.0101, 0.0000, 0.0417,  ..., 0.0496, 0.0000, 0.0746],
        [0.0159, 0.0007, 0.0365,  ..., 0.0457, 0.0040, 0.0645],
        ...,
        [0.0393, 0.0331, 0.0024,  ..., 0.0318, 0.0820, 0.0041],
        [0.0393, 0.0331, 0.0024,  ..., 0.0318, 0.0820, 0.0041],
        [0.0393, 0.0331, 0.0024,  ..., 0.0317, 0.0820, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500786.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4206.6597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(736.4251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(282.9994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(305.7943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1912],
        [ 0.0181],
        [ 0.1418],
        ...,
        [-0.8942],
        [-0.8915],
        [-0.8907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160641.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0088],
        [1.0137],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368226.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0088],
        [1.0137],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368237.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [ 0.0041, -0.0017,  0.0025,  ...,  0.0041, -0.0021,  0.0091],
        [ 0.0038, -0.0016,  0.0023,  ...,  0.0040, -0.0020,  0.0086],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(835.3660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0157, device='cuda:0')



h[100].sum tensor(-14.7772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5801, device='cuda:0')



h[200].sum tensor(23.0467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4991, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0000, 0.0025,  ..., 0.0089, 0.0000, 0.0092],
        [0.0148, 0.0000, 0.0091,  ..., 0.0146, 0.0000, 0.0307],
        [0.0312, 0.0000, 0.0191,  ..., 0.0235, 0.0000, 0.0642],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49051.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0114, 0.0263,  ..., 0.0414, 0.0283, 0.0467],
        [0.0108, 0.0017, 0.0464,  ..., 0.0523, 0.0059, 0.0836],
        [0.0015, 0.0000, 0.0638,  ..., 0.0616, 0.0000, 0.1152],
        ...,
        [0.0394, 0.0337, 0.0024,  ..., 0.0322, 0.0822, 0.0043],
        [0.0394, 0.0337, 0.0024,  ..., 0.0322, 0.0822, 0.0043],
        [0.0394, 0.0337, 0.0024,  ..., 0.0322, 0.0822, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457517.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4286.1724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(700.2448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(230.0892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(263.8077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0300],
        [ 0.1144],
        [ 0.1776],
        ...,
        [-0.8988],
        [-0.8960],
        [-0.8952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173028.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0088],
        [1.0137],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368237.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0089],
        [1.0137],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368248.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1220.0326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0214, device='cuda:0')



h[100].sum tensor(-20.0048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7908, device='cuda:0')



h[200].sum tensor(29.4964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55578.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0385, 0.0336, 0.0024,  ..., 0.0322, 0.0799, 0.0046],
        [0.0386, 0.0336, 0.0025,  ..., 0.0322, 0.0801, 0.0046],
        [0.0387, 0.0337, 0.0025,  ..., 0.0322, 0.0803, 0.0045],
        ...,
        [0.0395, 0.0343, 0.0026,  ..., 0.0327, 0.0820, 0.0045],
        [0.0395, 0.0343, 0.0026,  ..., 0.0327, 0.0820, 0.0045],
        [0.0395, 0.0343, 0.0026,  ..., 0.0327, 0.0820, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478874.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4234.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(724.5068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(271.4242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.9420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8134],
        [-0.9381],
        [-1.0244],
        ...,
        [-0.8989],
        [-0.8961],
        [-0.8952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162316.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0089],
        [1.0137],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368248.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0090],
        [1.0138],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368260.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0016,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1544.5422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0258, device='cuda:0')



h[100].sum tensor(-24.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9531, device='cuda:0')



h[200].sum tensor(35.1588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.2482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66059.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0312, 0.0194, 0.0133,  ..., 0.0374, 0.0494, 0.0234],
        [0.0326, 0.0221, 0.0112,  ..., 0.0367, 0.0550, 0.0200],
        [0.0188, 0.0142, 0.0282,  ..., 0.0498, 0.0335, 0.0555],
        ...,
        [0.0395, 0.0348, 0.0030,  ..., 0.0332, 0.0816, 0.0049],
        [0.0395, 0.0348, 0.0030,  ..., 0.0332, 0.0816, 0.0049],
        [0.0395, 0.0348, 0.0030,  ..., 0.0332, 0.0816, 0.0049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543641.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4034.2810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(770.7343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(336.3143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(344.3809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0809],
        [ 0.0917],
        [ 0.1333],
        ...,
        [-0.8967],
        [-0.8940],
        [-0.8932]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140898.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0090],
        [1.0138],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368260.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0091],
        [1.0139],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368271.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115, -0.0041,  0.0070,  ...,  0.0076, -0.0049,  0.0229],
        [ 0.0174, -0.0061,  0.0106,  ...,  0.0104, -0.0072,  0.0337],
        [ 0.0115, -0.0041,  0.0070,  ...,  0.0076, -0.0049,  0.0229],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1479.7584, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0248, device='cuda:0')



h[100].sum tensor(-23.2312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9174, device='cuda:0')



h[200].sum tensor(35.3495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0635, 0.0000, 0.0388,  ..., 0.0386, 0.0000, 0.1239],
        [0.0683, 0.0000, 0.0417,  ..., 0.0409, 0.0000, 0.1329],
        [0.1206, 0.0000, 0.0737,  ..., 0.0657, 0.0000, 0.2292],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62703.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1436,  ..., 0.1315, 0.0000, 0.2866],
        [0.0000, 0.0000, 0.1655,  ..., 0.1513, 0.0000, 0.3355],
        [0.0000, 0.0000, 0.2098,  ..., 0.1912, 0.0000, 0.4352],
        ...,
        [0.0395, 0.0353, 0.0033,  ..., 0.0334, 0.0811, 0.0049],
        [0.0395, 0.0353, 0.0033,  ..., 0.0334, 0.0811, 0.0049],
        [0.0379, 0.0320, 0.0058,  ..., 0.0344, 0.0742, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513815.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4074.5063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(756.9332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(314.3177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.4459, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2636],
        [ 0.2440],
        [ 0.2190],
        ...,
        [-0.8599],
        [-0.7928],
        [-0.6815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151602.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0091],
        [1.0139],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368271.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0092],
        [1.0139],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368281.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1214.0349, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0205, device='cuda:0')



h[100].sum tensor(-19.3454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7574, device='cuda:0')



h[200].sum tensor(32.9229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57140.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0358, 0.0294, 0.0076,  ..., 0.0344, 0.0670, 0.0121],
        [0.0382, 0.0342, 0.0043,  ..., 0.0329, 0.0767, 0.0061],
        [0.0387, 0.0351, 0.0035,  ..., 0.0327, 0.0788, 0.0048],
        ...,
        [0.0395, 0.0357, 0.0037,  ..., 0.0332, 0.0805, 0.0048],
        [0.0395, 0.0357, 0.0037,  ..., 0.0332, 0.0805, 0.0048],
        [0.0395, 0.0357, 0.0037,  ..., 0.0332, 0.0805, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491002.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4074.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(742.2666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(276.0919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(303.7274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3929],
        [-0.6035],
        [-0.7660],
        ...,
        [-0.8938],
        [-0.8910],
        [-0.8901]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142796.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0092],
        [1.0139],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368281.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0092],
        [1.0140],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368292.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0265, -0.0090,  0.0162,  ...,  0.0146, -0.0108,  0.0506],
        [ 0.0293, -0.0099,  0.0179,  ...,  0.0160, -0.0118,  0.0557],
        [ 0.0182, -0.0063,  0.0111,  ...,  0.0107, -0.0075,  0.0353],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(773.1975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0140, device='cuda:0')



h[100].sum tensor(-13.0755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5198, device='cuda:0')



h[200].sum tensor(27.8156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1246, 0.0000, 0.0761,  ..., 0.0674, 0.0000, 0.2362],
        [0.1034, 0.0000, 0.0631,  ..., 0.0574, 0.0000, 0.1973],
        [0.0776, 0.0000, 0.0473,  ..., 0.0451, 0.0000, 0.1498],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46475.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.2132,  ..., 0.1910, 0.0000, 0.4351],
        [0.0000, 0.0000, 0.1965,  ..., 0.1752, 0.0000, 0.3967],
        [0.0000, 0.0000, 0.1673,  ..., 0.1489, 0.0000, 0.3314],
        ...,
        [0.0396, 0.0360, 0.0042,  ..., 0.0330, 0.0799, 0.0048],
        [0.0396, 0.0360, 0.0042,  ..., 0.0330, 0.0799, 0.0048],
        [0.0396, 0.0360, 0.0042,  ..., 0.0330, 0.0798, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442940.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4287.8286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(696.1555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(208.7417, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(251.5565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1870],
        [ 0.1847],
        [ 0.1898],
        ...,
        [-0.9023],
        [-0.8996],
        [-0.8988]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158419.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0092],
        [1.0140],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368292.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(527.4631, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0141],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368303.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030, -0.0013,  0.0018,  ...,  0.0035, -0.0016,  0.0072],
        [ 0.0030, -0.0013,  0.0018,  ...,  0.0035, -0.0016,  0.0072],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(947.1307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-15.3150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6120, device='cuda:0')



h[200].sum tensor(31.4259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0000, 0.0031,  ..., 0.0096, 0.0000, 0.0130],
        [0.0052, 0.0000, 0.0031,  ..., 0.0096, 0.0000, 0.0130],
        [0.0053, 0.0000, 0.0031,  ..., 0.0096, 0.0000, 0.0130],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49344.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0303, 0.0135, 0.0281,  ..., 0.0361, 0.0258, 0.0419],
        [0.0297, 0.0117, 0.0304,  ..., 0.0365, 0.0212, 0.0454],
        [0.0306, 0.0137, 0.0285,  ..., 0.0361, 0.0258, 0.0424],
        ...,
        [0.0398, 0.0364, 0.0045,  ..., 0.0330, 0.0797, 0.0047],
        [0.0398, 0.0364, 0.0045,  ..., 0.0330, 0.0797, 0.0047],
        [0.0398, 0.0364, 0.0045,  ..., 0.0330, 0.0797, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451997.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4276.4248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(718.5218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(224.1995, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.6367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0298],
        [ 0.0291],
        [ 0.0122],
        ...,
        [-0.9083],
        [-0.9057],
        [-0.9049]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132501.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0093],
        [1.0141],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368303.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0094],
        [1.0141],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368314., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [ 0.0087, -0.0032,  0.0052,  ...,  0.0061, -0.0038,  0.0176],
        [ 0.0176, -0.0060,  0.0107,  ...,  0.0104, -0.0073,  0.0341],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(740.0610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0135, device='cuda:0')



h[100].sum tensor(-12.4189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4980, device='cuda:0')



h[200].sum tensor(28.5732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0134, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0087, 0.0000, 0.0053,  ..., 0.0107, 0.0000, 0.0177],
        [0.0322, 0.0000, 0.0196,  ..., 0.0224, 0.0000, 0.0626],
        [0.0806, 0.0000, 0.0491,  ..., 0.0464, 0.0000, 0.1551],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45654.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0159, 0.0106, 0.0394,  ..., 0.0553, 0.0248, 0.0716],
        [0.0035, 0.0000, 0.0843,  ..., 0.0880, 0.0000, 0.1615],
        [0.0000, 0.0000, 0.1470,  ..., 0.1355, 0.0000, 0.2894],
        ...,
        [0.0401, 0.0370, 0.0047,  ..., 0.0330, 0.0801, 0.0046],
        [0.0401, 0.0370, 0.0047,  ..., 0.0331, 0.0801, 0.0046],
        [0.0401, 0.0370, 0.0047,  ..., 0.0330, 0.0801, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440873.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4408.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(696.8039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(205.5195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(248.9848, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1899],
        [-0.0042],
        [ 0.1048],
        ...,
        [-0.9194],
        [-0.9167],
        [-0.9159]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159106.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0094],
        [1.0141],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368314., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0095],
        [1.0142],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368325.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0021,  0.0033,  ...,  0.0047, -0.0026,  0.0118],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [ 0.0098, -0.0035,  0.0059,  ...,  0.0067, -0.0042,  0.0196],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1117.3188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0190, device='cuda:0')



h[100].sum tensor(-17.5502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7017, device='cuda:0')



h[200].sum tensor(34.1337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6985, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0311, 0.0000, 0.0188,  ..., 0.0224, 0.0000, 0.0623],
        [0.0275, 0.0000, 0.0166,  ..., 0.0213, 0.0000, 0.0574],
        [0.0201, 0.0000, 0.0122,  ..., 0.0167, 0.0000, 0.0404],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0133, 0.0000, 0.0081,  ..., 0.0130, 0.0000, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52711.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0992,  ..., 0.0887, 0.0000, 0.1796],
        [0.0000, 0.0000, 0.0818,  ..., 0.0736, 0.0000, 0.1428],
        [0.0000, 0.0000, 0.0743,  ..., 0.0710, 0.0000, 0.1309],
        ...,
        [0.0389, 0.0343, 0.0065,  ..., 0.0345, 0.0748, 0.0080],
        [0.0320, 0.0214, 0.0145,  ..., 0.0393, 0.0487, 0.0228],
        [0.0171, 0.0089, 0.0375,  ..., 0.0546, 0.0219, 0.0669]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471297.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4346.0142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(731.9333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.8468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.8062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2494],
        [ 0.2536],
        [ 0.2542],
        ...,
        [-0.7724],
        [-0.5827],
        [-0.3393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156836.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0095],
        [1.0142],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368325.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0095],
        [1.0143],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368336.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106, -0.0038,  0.0064,  ...,  0.0071, -0.0045,  0.0212],
        [ 0.0091, -0.0033,  0.0055,  ...,  0.0063, -0.0039,  0.0184],
        [ 0.0089, -0.0032,  0.0054,  ...,  0.0062, -0.0039,  0.0180],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1095.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-17.2523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6799, device='cuda:0')



h[200].sum tensor(33.3856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3040, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0381, 0.0000, 0.0230,  ..., 0.0262, 0.0000, 0.0768],
        [0.0517, 0.0000, 0.0314,  ..., 0.0327, 0.0000, 0.1020],
        [0.0492, 0.0000, 0.0298,  ..., 0.0316, 0.0000, 0.0974],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53403.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1070,  ..., 0.0917, 0.0000, 0.1919],
        [0.0000, 0.0000, 0.1306,  ..., 0.1096, 0.0000, 0.2387],
        [0.0000, 0.0000, 0.1341,  ..., 0.1122, 0.0000, 0.2454],
        ...,
        [0.0410, 0.0382, 0.0047,  ..., 0.0337, 0.0817, 0.0046],
        [0.0410, 0.0382, 0.0047,  ..., 0.0337, 0.0817, 0.0046],
        [0.0410, 0.0382, 0.0047,  ..., 0.0337, 0.0817, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477264.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4463.4639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(736.8689, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(258.1327, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.0431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1966],
        [ 0.2252],
        [ 0.2390],
        ...,
        [-0.8783],
        [-0.8278],
        [-0.7948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154658.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0095],
        [1.0143],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368336.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0096],
        [1.0143],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368347.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040, -0.0016,  0.0023,  ...,  0.0039, -0.0020,  0.0090],
        [ 0.0041, -0.0017,  0.0024,  ...,  0.0040, -0.0020,  0.0091],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(883.4442, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-14.3546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5735, device='cuda:0')



h[200].sum tensor(29.6538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3785, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0000, 0.0188,  ..., 0.0230, 0.0000, 0.0641],
        [0.0150, 0.0000, 0.0091,  ..., 0.0143, 0.0000, 0.0310],
        [0.0041, 0.0000, 0.0024,  ..., 0.0085, 0.0000, 0.0092],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47870.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0678,  ..., 0.0621, 0.0000, 0.1133],
        [0.0123, 0.0013, 0.0510,  ..., 0.0534, 0.0035, 0.0840],
        [0.0241, 0.0094, 0.0325,  ..., 0.0436, 0.0204, 0.0512],
        ...,
        [0.0415, 0.0388, 0.0046,  ..., 0.0340, 0.0826, 0.0046],
        [0.0415, 0.0388, 0.0046,  ..., 0.0340, 0.0826, 0.0046],
        [0.0415, 0.0388, 0.0046,  ..., 0.0340, 0.0826, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450599.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4630.0420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(707.1288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(227.9865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(259.2916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2119],
        [ 0.2073],
        [ 0.1884],
        ...,
        [-0.9604],
        [-0.9573],
        [-0.9560]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180113.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0096],
        [1.0143],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368347.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0097],
        [1.0144],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368358.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1264.1521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-19.4824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7742, device='cuda:0')



h[200].sum tensor(34.8542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0116, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0027,  ..., 0.0094, 0.0000, 0.0120],
        [0.0024, 0.0000, 0.0014,  ..., 0.0077, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56853.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0351, 0.0214, 0.0229,  ..., 0.0365, 0.0418, 0.0331],
        [0.0368, 0.0264, 0.0174,  ..., 0.0358, 0.0535, 0.0247],
        [0.0389, 0.0323, 0.0110,  ..., 0.0350, 0.0673, 0.0149],
        ...,
        [0.0420, 0.0394, 0.0045,  ..., 0.0344, 0.0835, 0.0047],
        [0.0420, 0.0394, 0.0045,  ..., 0.0344, 0.0835, 0.0047],
        [0.0420, 0.0394, 0.0045,  ..., 0.0344, 0.0835, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496113.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4593.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(747.3682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(286.5225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.8438, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5254],
        [-0.6373],
        [-0.7686],
        ...,
        [-0.9728],
        [-0.9699],
        [-0.9690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167671.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0097],
        [1.0144],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368358.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0097],
        [1.0145],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368370.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(906.6802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0157, device='cuda:0')



h[100].sum tensor(-14.6424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5801, device='cuda:0')



h[200].sum tensor(29.0368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50591.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0414, 0.0392, 0.0041,  ..., 0.0341, 0.0821, 0.0048],
        [0.0415, 0.0392, 0.0041,  ..., 0.0341, 0.0823, 0.0048],
        [0.0416, 0.0393, 0.0042,  ..., 0.0341, 0.0825, 0.0047],
        ...,
        [0.0425, 0.0400, 0.0044,  ..., 0.0347, 0.0843, 0.0048],
        [0.0425, 0.0400, 0.0044,  ..., 0.0347, 0.0843, 0.0048],
        [0.0425, 0.0400, 0.0044,  ..., 0.0347, 0.0843, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472211.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4755.2056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(726.7484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(246.8717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(272.7420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0899],
        [-1.1379],
        [-1.1746],
        ...,
        [-0.9796],
        [-0.9778],
        [-0.9776]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173221.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0097],
        [1.0145],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368370.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0098],
        [1.0145],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368381.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(831.1119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.5205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0146, device='cuda:0')



h[100].sum tensor(-13.5282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5415, device='cuda:0')



h[200].sum tensor(27.9870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7998, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0025,  ..., 0.0086, 0.0000, 0.0094],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47588.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0239, 0.0156, 0.0302,  ..., 0.0463, 0.0322, 0.0505],
        [0.0359, 0.0257, 0.0143,  ..., 0.0380, 0.0545, 0.0222],
        [0.0380, 0.0301, 0.0116,  ..., 0.0366, 0.0631, 0.0173],
        ...,
        [0.0429, 0.0406, 0.0043,  ..., 0.0349, 0.0847, 0.0049],
        [0.0429, 0.0406, 0.0043,  ..., 0.0349, 0.0847, 0.0049],
        [0.0429, 0.0406, 0.0043,  ..., 0.0349, 0.0847, 0.0049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457723.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4876.7261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(706.3881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(231.9578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(256.9499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0367],
        [-0.1981],
        [-0.2908],
        ...,
        [-0.9973],
        [-0.9942],
        [-0.9933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199528.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0098],
        [1.0145],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368381.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0099],
        [1.0146],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368392.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1113.9913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-17.1903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6942, device='cuda:0')



h[200].sum tensor(32.3787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51827.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0420, 0.0403, 0.0040,  ..., 0.0346, 0.0828, 0.0051],
        [0.0421, 0.0403, 0.0040,  ..., 0.0346, 0.0829, 0.0051],
        [0.0423, 0.0404, 0.0041,  ..., 0.0346, 0.0832, 0.0050],
        ...,
        [0.0432, 0.0412, 0.0043,  ..., 0.0352, 0.0850, 0.0051],
        [0.0432, 0.0412, 0.0043,  ..., 0.0352, 0.0850, 0.0051],
        [0.0432, 0.0412, 0.0043,  ..., 0.0352, 0.0850, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(468438.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4879.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(731.5225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(256.9332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(278.9180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1666],
        [-1.1912],
        [-1.2069],
        ...,
        [-1.0045],
        [-1.0010],
        [-0.9998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175853.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0099],
        [1.0146],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368392.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0100],
        [1.0147],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368403.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0154, -0.0052,  0.0093,  ...,  0.0093, -0.0062,  0.0299],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1015.7421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-15.6772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6277, device='cuda:0')



h[200].sum tensor(31.3975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0279, 0.0000, 0.0169,  ..., 0.0204, 0.0000, 0.0548],
        [0.0572, 0.0000, 0.0347,  ..., 0.0348, 0.0000, 0.1104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50888.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0190, 0.0133, 0.0334,  ..., 0.0526, 0.0280, 0.0597],
        [0.0070, 0.0010, 0.0761,  ..., 0.0812, 0.0031, 0.1415],
        [0.0000, 0.0000, 0.1196,  ..., 0.1116, 0.0000, 0.2267],
        ...,
        [0.0434, 0.0416, 0.0044,  ..., 0.0355, 0.0849, 0.0054],
        [0.0434, 0.0416, 0.0044,  ..., 0.0355, 0.0849, 0.0054],
        [0.0434, 0.0416, 0.0044,  ..., 0.0355, 0.0849, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470206.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4938.7524, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(723.7880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.5762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(273.6304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0904],
        [ 0.1565],
        [ 0.1935],
        ...,
        [-1.0120],
        [-1.0089],
        [-1.0079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187990.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0100],
        [1.0147],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368403.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(508.5532, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0100],
        [1.0148],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368415.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035, -0.0015,  0.0021,  ...,  0.0037, -0.0018,  0.0082],
        [ 0.0046, -0.0018,  0.0027,  ...,  0.0042, -0.0021,  0.0101],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1394.7866, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0223, device='cuda:0')



h[100].sum tensor(-20.4968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8258, device='cuda:0')



h[200].sum tensor(37.8830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.9454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0331, 0.0000, 0.0199,  ..., 0.0238, 0.0000, 0.0676],
        [0.0151, 0.0000, 0.0091,  ..., 0.0143, 0.0000, 0.0312],
        [0.0091, 0.0000, 0.0054,  ..., 0.0114, 0.0000, 0.0202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62000.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0000, 0.0711,  ..., 0.0661, 0.0000, 0.1205],
        [0.0125, 0.0003, 0.0550,  ..., 0.0572, 0.0007, 0.0919],
        [0.0189, 0.0018, 0.0434,  ..., 0.0511, 0.0047, 0.0712],
        ...,
        [0.0435, 0.0421, 0.0046,  ..., 0.0357, 0.0846, 0.0058],
        [0.0435, 0.0421, 0.0046,  ..., 0.0357, 0.0846, 0.0058],
        [0.0435, 0.0421, 0.0046,  ..., 0.0357, 0.0846, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540058.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4785.7803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(764.9776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(322.7984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(326.8903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2184],
        [ 0.2110],
        [ 0.1864],
        ...,
        [-1.0148],
        [-1.0116],
        [-1.0106]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185738.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0100],
        [1.0148],
        ...,
        [1.0008],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368415.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0101],
        [1.0148],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368427.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1512.1052, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0234, device='cuda:0')



h[100].sum tensor(-21.8091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8656, device='cuda:0')



h[200].sum tensor(40.4416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62836.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0425, 0.0416, 0.0046,  ..., 0.0353, 0.0820, 0.0062],
        [0.0424, 0.0410, 0.0054,  ..., 0.0355, 0.0806, 0.0073],
        [0.0418, 0.0390, 0.0073,  ..., 0.0359, 0.0766, 0.0102],
        ...,
        [0.0437, 0.0426, 0.0049,  ..., 0.0360, 0.0842, 0.0063],
        [0.0423, 0.0395, 0.0066,  ..., 0.0370, 0.0786, 0.0094],
        [0.0378, 0.0294, 0.0124,  ..., 0.0403, 0.0602, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531470.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4864.6411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(766.4360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(326.2570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.1529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8220],
        [-0.7563],
        [-0.6570],
        ...,
        [-0.9704],
        [-0.8763],
        [-0.7046]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187832.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0101],
        [1.0148],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368427.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0102],
        [1.0149],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368438.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0049, -0.0019,  0.0029,  ...,  0.0043, -0.0023,  0.0106],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1014.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-14.9652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6026, device='cuda:0')



h[200].sum tensor(33.8497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9056, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0049, 0.0000, 0.0029,  ..., 0.0088, 0.0000, 0.0107],
        [0.0038, 0.0000, 0.0022,  ..., 0.0083, 0.0000, 0.0087],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52199.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0401, 0.0358, 0.0097,  ..., 0.0373, 0.0692, 0.0144],
        [0.0359, 0.0251, 0.0176,  ..., 0.0401, 0.0485, 0.0272],
        [0.0344, 0.0206, 0.0215,  ..., 0.0411, 0.0396, 0.0331],
        ...,
        [0.0438, 0.0430, 0.0052,  ..., 0.0363, 0.0837, 0.0067],
        [0.0438, 0.0430, 0.0052,  ..., 0.0363, 0.0837, 0.0067],
        [0.0438, 0.0430, 0.0052,  ..., 0.0363, 0.0836, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481645.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4909.0381, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(728.5860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(253.5766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(279.5914, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5412],
        [-0.6264],
        [-0.6711],
        ...,
        [-1.0173],
        [-1.0141],
        [-1.0132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163323.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0102],
        [1.0149],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368438.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0103],
        [1.0149],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368450.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1590.1316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0243, device='cuda:0')



h[100].sum tensor(-22.3411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9005, device='cuda:0')



h[200].sum tensor(43.2606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62970.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0427, 0.0426, 0.0050,  ..., 0.0359, 0.0812, 0.0071],
        [0.0428, 0.0427, 0.0050,  ..., 0.0360, 0.0814, 0.0071],
        [0.0429, 0.0428, 0.0051,  ..., 0.0360, 0.0816, 0.0071],
        ...,
        [0.0324, 0.0172, 0.0210,  ..., 0.0447, 0.0352, 0.0346],
        [0.0330, 0.0185, 0.0202,  ..., 0.0444, 0.0376, 0.0333],
        [0.0357, 0.0248, 0.0165,  ..., 0.0424, 0.0490, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531504.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4784.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(770.5991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(321.4895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(332.2384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0800],
        [-1.1228],
        [-1.1421],
        ...,
        [-0.1619],
        [-0.1676],
        [-0.1985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153820.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0103],
        [1.0149],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368450.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0104],
        [1.0150],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368461.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0064, -0.0023,  0.0038,  ...,  0.0050, -0.0028,  0.0135]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1170.5931, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0181, device='cuda:0')



h[100].sum tensor(-16.6483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6716, device='cuda:0')



h[200].sum tensor(37.4665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1549, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0066, 0.0000, 0.0040,  ..., 0.0097, 0.0000, 0.0139],
        [0.0052, 0.0000, 0.0031,  ..., 0.0091, 0.0000, 0.0113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54009.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0391, 0.0341, 0.0110,  ..., 0.0389, 0.0641, 0.0178],
        [0.0413, 0.0391, 0.0081,  ..., 0.0374, 0.0733, 0.0125],
        [0.0399, 0.0350, 0.0115,  ..., 0.0384, 0.0653, 0.0177],
        ...,
        [0.0410, 0.0368, 0.0101,  ..., 0.0391, 0.0697, 0.0156],
        [0.0342, 0.0222, 0.0209,  ..., 0.0437, 0.0429, 0.0337],
        [0.0316, 0.0161, 0.0256,  ..., 0.0454, 0.0299, 0.0414]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485319.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4922.6533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(734.9971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.2831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(289.1746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1565],
        [-0.2370],
        [-0.2490],
        ...,
        [-0.8306],
        [-0.6742],
        [-0.5430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172315.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0104],
        [1.0150],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368461.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0104],
        [1.0151],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368473.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1050.8833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-14.9595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6020, device='cuda:0')



h[200].sum tensor(35.9258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51066.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0383, 0.0314, 0.0142,  ..., 0.0399, 0.0574, 0.0229],
        [0.0414, 0.0389, 0.0091,  ..., 0.0379, 0.0716, 0.0144],
        [0.0414, 0.0386, 0.0095,  ..., 0.0380, 0.0711, 0.0149],
        ...,
        [0.0443, 0.0448, 0.0050,  ..., 0.0374, 0.0838, 0.0076],
        [0.0443, 0.0448, 0.0050,  ..., 0.0374, 0.0838, 0.0076],
        [0.0442, 0.0448, 0.0050,  ..., 0.0374, 0.0838, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473187.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4981.3198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(730.4445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(241.2913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(276.3738, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0700],
        [-0.1850],
        [-0.2505],
        ...,
        [-1.0421],
        [-1.0389],
        [-1.0379]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161311.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0104],
        [1.0151],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368473.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0105],
        [1.0151],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368484.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(859.5001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0135, device='cuda:0')



h[100].sum tensor(-12.4061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4991, device='cuda:0')



h[200].sum tensor(33.1949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47051.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0433, 0.0446, 0.0043,  ..., 0.0371, 0.0823, 0.0076],
        [0.0434, 0.0447, 0.0043,  ..., 0.0372, 0.0825, 0.0076],
        [0.0435, 0.0447, 0.0044,  ..., 0.0372, 0.0827, 0.0076],
        ...,
        [0.0445, 0.0456, 0.0046,  ..., 0.0379, 0.0846, 0.0077],
        [0.0445, 0.0456, 0.0046,  ..., 0.0378, 0.0846, 0.0077],
        [0.0445, 0.0456, 0.0046,  ..., 0.0378, 0.0845, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460661.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5088.1650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(711.6978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(217.0456, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(257.6218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0793],
        [-0.9919],
        [-0.8556],
        ...,
        [-1.0586],
        [-1.0553],
        [-1.0542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180476.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0105],
        [1.0151],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368484.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0106],
        [1.0152],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368495.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0293, -0.0094,  0.0178,  ...,  0.0159, -0.0113,  0.0555],
        [ 0.0228, -0.0074,  0.0139,  ...,  0.0128, -0.0089,  0.0436],
        [ 0.0193, -0.0063,  0.0117,  ...,  0.0111, -0.0076,  0.0371],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1365.3132, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-18.9649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7733, device='cuda:0')



h[200].sum tensor(40.7693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9941, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0858, 0.0000, 0.0521,  ..., 0.0487, 0.0000, 0.1644],
        [0.1087, 0.0000, 0.0661,  ..., 0.0596, 0.0000, 0.2066],
        [0.1150, 0.0000, 0.0700,  ..., 0.0626, 0.0000, 0.2183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55128.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1978,  ..., 0.1791, 0.0000, 0.3964],
        [0.0000, 0.0000, 0.2320,  ..., 0.2094, 0.0000, 0.4713],
        [0.0000, 0.0000, 0.2465,  ..., 0.2225, 0.0000, 0.5033],
        ...,
        [0.0447, 0.0464, 0.0043,  ..., 0.0383, 0.0853, 0.0077],
        [0.0447, 0.0464, 0.0043,  ..., 0.0383, 0.0853, 0.0078],
        [0.0447, 0.0464, 0.0043,  ..., 0.0383, 0.0853, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488628.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5050.0713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(738.2169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.2061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.2470, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1469],
        [ 0.0915],
        [ 0.0448],
        ...,
        [-1.0739],
        [-1.0705],
        [-1.0695]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208285.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0106],
        [1.0152],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368495.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0106],
        [1.0152],
        ...,
        [1.0010],
        [1.0004],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368506.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0235, -0.0075,  0.0143,  ...,  0.0131, -0.0091,  0.0448],
        [ 0.0154, -0.0051,  0.0094,  ...,  0.0093, -0.0061,  0.0300],
        [ 0.0324, -0.0103,  0.0197,  ...,  0.0173, -0.0124,  0.0612],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1594.9110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0241, device='cuda:0')



h[100].sum tensor(-21.8749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8928, device='cuda:0')



h[200].sum tensor(44.3004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0668, 0.0000, 0.0405,  ..., 0.0398, 0.0000, 0.1293],
        [0.1012, 0.0000, 0.0616,  ..., 0.0561, 0.0000, 0.1928],
        [0.0534, 0.0000, 0.0323,  ..., 0.0335, 0.0000, 0.1048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61700.8633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1435,  ..., 0.1331, 0.0000, 0.2799],
        [0.0000, 0.0000, 0.1696,  ..., 0.1544, 0.0000, 0.3354],
        [0.0000, 0.0000, 0.1383,  ..., 0.1282, 0.0000, 0.2685],
        ...,
        [0.0450, 0.0471, 0.0040,  ..., 0.0387, 0.0860, 0.0079],
        [0.0450, 0.0471, 0.0040,  ..., 0.0387, 0.0860, 0.0079],
        [0.0450, 0.0471, 0.0040,  ..., 0.0387, 0.0860, 0.0079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522792.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4913.8892, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(778.4442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(310.5852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.3280, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2126],
        [ 0.2136],
        [ 0.1941],
        ...,
        [-1.0815],
        [-1.0778],
        [-1.0763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171986.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0106],
        [1.0152],
        ...,
        [1.0010],
        [1.0004],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368506.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0107],
        [1.0153],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368518.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0157, -0.0051,  0.0095,  ...,  0.0095, -0.0062,  0.0305],
        [ 0.0253, -0.0081,  0.0154,  ...,  0.0140, -0.0097,  0.0481],
        [ 0.0289, -0.0092,  0.0176,  ...,  0.0157, -0.0111,  0.0548],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1135.3175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0175, device='cuda:0')



h[100].sum tensor(-15.7995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6462, device='cuda:0')



h[200].sum tensor(37.4166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6950, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0764, 0.0000, 0.0464,  ..., 0.0443, 0.0000, 0.1470],
        [0.0793, 0.0000, 0.0482,  ..., 0.0457, 0.0000, 0.1524],
        [0.0903, 0.0000, 0.0549,  ..., 0.0509, 0.0000, 0.1727],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50780.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1772,  ..., 0.1645, 0.0000, 0.3547],
        [0.0000, 0.0000, 0.1737,  ..., 0.1611, 0.0000, 0.3466],
        [0.0000, 0.0000, 0.1650,  ..., 0.1557, 0.0000, 0.3303],
        ...,
        [0.0453, 0.0478, 0.0039,  ..., 0.0393, 0.0864, 0.0083],
        [0.0453, 0.0478, 0.0039,  ..., 0.0393, 0.0864, 0.0083],
        [0.0452, 0.0478, 0.0039,  ..., 0.0393, 0.0864, 0.0083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474138.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5166.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(723.9741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(242.6723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(276.3526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1738],
        [ 0.1700],
        [ 0.1495],
        ...,
        [-1.0933],
        [-1.0900],
        [-1.0892]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200401.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0107],
        [1.0153],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368518.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(534.5040, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0108],
        [1.0154],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368530.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1065.2615, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-14.7381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5976, device='cuda:0')



h[200].sum tensor(36.5273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50401.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0442, 0.0465, 0.0048,  ..., 0.0392, 0.0822, 0.0102],
        [0.0390, 0.0334, 0.0128,  ..., 0.0434, 0.0582, 0.0244],
        [0.0310, 0.0168, 0.0239,  ..., 0.0498, 0.0292, 0.0444],
        ...,
        [0.0456, 0.0484, 0.0041,  ..., 0.0400, 0.0866, 0.0087],
        [0.0456, 0.0484, 0.0041,  ..., 0.0400, 0.0866, 0.0087],
        [0.0456, 0.0484, 0.0041,  ..., 0.0400, 0.0865, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(475229.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5194.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(718.9334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(239.2532, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(271.7820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5866],
        [-0.3252],
        [-0.0647],
        ...,
        [-1.0969],
        [-1.0932],
        [-1.0900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207355.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0108],
        [1.0154],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368530.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0109],
        [1.0155],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368542., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(915.7574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-12.6511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5115, device='cuda:0')



h[200].sum tensor(34.6045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0000, 0.0025,  ..., 0.0086, 0.0000, 0.0096],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0046, 0.0000, 0.0028,  ..., 0.0088, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47423.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.0314, 0.0165,  ..., 0.0440, 0.0524, 0.0294],
        [0.0412, 0.0381, 0.0115,  ..., 0.0424, 0.0653, 0.0213],
        [0.0384, 0.0300, 0.0172,  ..., 0.0446, 0.0503, 0.0307],
        ...,
        [0.0459, 0.0490, 0.0043,  ..., 0.0405, 0.0866, 0.0091],
        [0.0459, 0.0490, 0.0043,  ..., 0.0405, 0.0866, 0.0091],
        [0.0459, 0.0490, 0.0043,  ..., 0.0405, 0.0866, 0.0091]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467046.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5283.3921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(702.7150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(217.8926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(253.7524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7862],
        [-0.8873],
        [-0.9020],
        ...,
        [-1.1036],
        [-1.1001],
        [-1.0990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206499.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0109],
        [1.0155],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368542., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0110],
        [1.0156],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368553.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1390.0007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0208, device='cuda:0')



h[100].sum tensor(-18.6612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7684, device='cuda:0')



h[200].sum tensor(41.4598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56237.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0450, 0.0480, 0.0049,  ..., 0.0403, 0.0830, 0.0108],
        [0.0452, 0.0476, 0.0058,  ..., 0.0402, 0.0817, 0.0121],
        [0.0442, 0.0439, 0.0091,  ..., 0.0410, 0.0743, 0.0172],
        ...,
        [0.0463, 0.0496, 0.0044,  ..., 0.0412, 0.0869, 0.0096],
        [0.0463, 0.0496, 0.0044,  ..., 0.0412, 0.0869, 0.0096],
        [0.0463, 0.0496, 0.0044,  ..., 0.0412, 0.0869, 0.0096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498503.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5177.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(743.0614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(270.1794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(294.8522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8377],
        [-0.6955],
        [-0.5253],
        ...,
        [-1.1056],
        [-1.1021],
        [-1.1009]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178390.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0110],
        [1.0156],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368553.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0111],
        [1.0157],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368565.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0018,  0.0029,  ...,  0.0043, -0.0022,  0.0106],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0049, -0.0018,  0.0029,  ...,  0.0043, -0.0022,  0.0106],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1849.7448, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0272, device='cuda:0')



h[100].sum tensor(-24.4592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0062, device='cuda:0')



h[200].sum tensor(48.2561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2097, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0000, 0.0022,  ..., 0.0083, 0.0000, 0.0087],
        [0.0174, 0.0000, 0.0103,  ..., 0.0164, 0.0000, 0.0388],
        [0.0038, 0.0000, 0.0022,  ..., 0.0084, 0.0000, 0.0087],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67405.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0335, 0.0159, 0.0276,  ..., 0.0500, 0.0222, 0.0489],
        [0.0303, 0.0050, 0.0369,  ..., 0.0525, 0.0033, 0.0636],
        [0.0353, 0.0173, 0.0295,  ..., 0.0487, 0.0225, 0.0508],
        ...,
        [0.0466, 0.0502, 0.0045,  ..., 0.0418, 0.0872, 0.0099],
        [0.0466, 0.0502, 0.0045,  ..., 0.0418, 0.0871, 0.0099],
        [0.0466, 0.0502, 0.0045,  ..., 0.0417, 0.0871, 0.0099]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554981.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4982.7842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(788.8820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(340.5683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(347.9735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0738],
        [ 0.0725],
        [ 0.1029],
        ...,
        [-1.1131],
        [-1.1096],
        [-1.1085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166836.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0111],
        [1.0157],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368565.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0112],
        [1.0158],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368576.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0052, -0.0019,  0.0031,  ...,  0.0045, -0.0023,  0.0112],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1089.6240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-14.6632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5991, device='cuda:0')



h[200].sum tensor(37.4466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0000, 0.0092,  ..., 0.0149, 0.0000, 0.0335],
        [0.0143, 0.0000, 0.0086,  ..., 0.0139, 0.0000, 0.0297],
        [0.0220, 0.0000, 0.0131,  ..., 0.0186, 0.0000, 0.0473],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50724.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0226, 0.0044, 0.0499,  ..., 0.0589, 0.0053, 0.0867],
        [0.0209, 0.0006, 0.0514,  ..., 0.0603, 0.0000, 0.0900],
        [0.0210, 0.0002, 0.0528,  ..., 0.0603, 0.0000, 0.0918],
        ...,
        [0.0467, 0.0510, 0.0043,  ..., 0.0418, 0.0877, 0.0100],
        [0.0467, 0.0510, 0.0043,  ..., 0.0418, 0.0877, 0.0100],
        [0.0467, 0.0510, 0.0043,  ..., 0.0418, 0.0877, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479550.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5341.3413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(717.2687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(235.0631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(266.2566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0694],
        [ 0.1400],
        [ 0.1068],
        ...,
        [-1.1259],
        [-1.1223],
        [-1.1211]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187125., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0112],
        [1.0158],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368576.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0113],
        [1.0159],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368587.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0183, -0.0058,  0.0111,  ...,  0.0107, -0.0071,  0.0353],
        [ 0.0117, -0.0038,  0.0071,  ...,  0.0076, -0.0047,  0.0231],
        [ 0.0138, -0.0045,  0.0084,  ...,  0.0086, -0.0054,  0.0271],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1132.5426, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-15.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6277, device='cuda:0')



h[200].sum tensor(38.5512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0499, 0.0000, 0.0302,  ..., 0.0318, 0.0000, 0.0985],
        [0.0580, 0.0000, 0.0351,  ..., 0.0357, 0.0000, 0.1135],
        [0.0522, 0.0000, 0.0316,  ..., 0.0329, 0.0000, 0.1029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53738.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1235,  ..., 0.1180, 0.0000, 0.2378],
        [0.0000, 0.0000, 0.1230,  ..., 0.1183, 0.0000, 0.2377],
        [0.0000, 0.0000, 0.1147,  ..., 0.1115, 0.0000, 0.2198],
        ...,
        [0.0469, 0.0517, 0.0040,  ..., 0.0419, 0.0883, 0.0100],
        [0.0468, 0.0517, 0.0040,  ..., 0.0419, 0.0883, 0.0100],
        [0.0468, 0.0517, 0.0040,  ..., 0.0419, 0.0883, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(502766.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5240.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(730.4765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(256.7929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.2093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3221],
        [ 0.3292],
        [ 0.3372],
        ...,
        [-1.1429],
        [-1.1393],
        [-1.1381]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195813.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0113],
        [1.0159],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368587.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0114],
        [1.0160],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368599.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(996.5862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0149, device='cuda:0')



h[100].sum tensor(-13.5431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5531, device='cuda:0')



h[200].sum tensor(37.1681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0000, 0.0019,  ..., 0.0082, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48944., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0318, 0.0150, 0.0303,  ..., 0.0524, 0.0205, 0.0555],
        [0.0385, 0.0311, 0.0175,  ..., 0.0471, 0.0485, 0.0338],
        [0.0433, 0.0442, 0.0084,  ..., 0.0434, 0.0733, 0.0183],
        ...,
        [0.0470, 0.0524, 0.0038,  ..., 0.0419, 0.0888, 0.0101],
        [0.0470, 0.0524, 0.0038,  ..., 0.0419, 0.0888, 0.0101],
        [0.0470, 0.0524, 0.0038,  ..., 0.0419, 0.0888, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476173.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5414.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(710.8729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(226.3006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(262.5977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0454],
        [-0.1810],
        [-0.4357],
        ...,
        [-1.1565],
        [-1.1528],
        [-1.1516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211385.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0114],
        [1.0160],
        ...,
        [1.0011],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368599.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0114],
        [1.0161],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368610.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2087.5195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0309, device='cuda:0')



h[100].sum tensor(-27.3312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1422, device='cuda:0')



h[200].sum tensor(54.4082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6719, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0252, 0.0000, 0.0153,  ..., 0.0191, 0.0000, 0.0496],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70800.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0110, 0.0052, 0.0806,  ..., 0.0964, 0.0072, 0.1641],
        [0.0264, 0.0248, 0.0277,  ..., 0.0577, 0.0413, 0.0574],
        [0.0410, 0.0396, 0.0103,  ..., 0.0453, 0.0649, 0.0228],
        ...,
        [0.0470, 0.0530, 0.0038,  ..., 0.0418, 0.0888, 0.0103],
        [0.0470, 0.0530, 0.0038,  ..., 0.0418, 0.0888, 0.0103],
        [0.0470, 0.0530, 0.0038,  ..., 0.0418, 0.0888, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(567711., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5155.5415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(802.3195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(365.2896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(373.3051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0638],
        [-0.0455],
        [-0.1535],
        ...,
        [-1.1665],
        [-1.1627],
        [-1.1615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199637.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0114],
        [1.0161],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368610.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0115],
        [1.0162],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368622.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0018,  0.0030,  ...,  0.0044, -0.0022,  0.0108],
        [ 0.0130, -0.0042,  0.0079,  ...,  0.0082, -0.0051,  0.0256],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1245.2544, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-16.4795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6857, device='cuda:0')



h[200].sum tensor(43.3628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4097, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0627, 0.0000, 0.0381,  ..., 0.0378, 0.0000, 0.1218],
        [0.0314, 0.0000, 0.0190,  ..., 0.0225, 0.0000, 0.0626],
        [0.0329, 0.0000, 0.0199,  ..., 0.0232, 0.0000, 0.0654],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52649.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1316,  ..., 0.1248, 0.0000, 0.2563],
        [0.0021, 0.0000, 0.0995,  ..., 0.0994, 0.0000, 0.1896],
        [0.0071, 0.0000, 0.0800,  ..., 0.0861, 0.0000, 0.1516],
        ...,
        [0.0471, 0.0534, 0.0040,  ..., 0.0417, 0.0885, 0.0105],
        [0.0471, 0.0534, 0.0040,  ..., 0.0417, 0.0885, 0.0105],
        [0.0471, 0.0534, 0.0040,  ..., 0.0417, 0.0885, 0.0105]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486608.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5356.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(734.3221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(244.5404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(287.2138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2726],
        [ 0.2451],
        [ 0.1418],
        ...,
        [-1.1720],
        [-1.1683],
        [-1.1671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200887.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0115],
        [1.0162],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368622.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0116],
        [1.0163],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368633.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(957.9658, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-12.7114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5239, device='cuda:0')



h[200].sum tensor(40.1550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4816, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0140, 0.0000, 0.0084,  ..., 0.0137, 0.0000, 0.0289],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48709.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0442, 0.0475, 0.0073,  ..., 0.0425, 0.0765, 0.0163],
        [0.0320, 0.0257, 0.0234,  ..., 0.0532, 0.0418, 0.0476],
        [0.0166, 0.0089, 0.0583,  ..., 0.0747, 0.0131, 0.1135],
        ...,
        [0.0473, 0.0538, 0.0044,  ..., 0.0418, 0.0882, 0.0107],
        [0.0473, 0.0538, 0.0044,  ..., 0.0418, 0.0882, 0.0107],
        [0.0473, 0.0538, 0.0044,  ..., 0.0418, 0.0881, 0.0107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477474.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5416.0615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(720.3498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(216.5768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.9180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1616],
        [-0.0301],
        [ 0.1073],
        ...,
        [-1.1745],
        [-1.1708],
        [-1.1696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203275.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0116],
        [1.0163],
        ...,
        [1.0010],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368633.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(515.7340, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0117],
        [1.0164],
        ...,
        [1.0009],
        [1.0003],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368645.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1144.5107, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-14.9210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6122, device='cuda:0')



h[200].sum tensor(43.7632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0138, 0.0000, 0.0083,  ..., 0.0136, 0.0000, 0.0286],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52463.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0413, 0.0394, 0.0120,  ..., 0.0454, 0.0619, 0.0248],
        [0.0283, 0.0204, 0.0307,  ..., 0.0568, 0.0322, 0.0598],
        [0.0121, 0.0060, 0.0615,  ..., 0.0742, 0.0081, 0.1160],
        ...,
        [0.0476, 0.0540, 0.0048,  ..., 0.0421, 0.0877, 0.0111],
        [0.0476, 0.0540, 0.0048,  ..., 0.0421, 0.0877, 0.0111],
        [0.0476, 0.0540, 0.0048,  ..., 0.0420, 0.0877, 0.0111]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492788.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5405.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(730.2769, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(240.1046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.8701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1522],
        [ 0.0410],
        [ 0.1750],
        ...,
        [-1.1442],
        [-1.1537],
        [-1.1600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209253.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0117],
        [1.0164],
        ...,
        [1.0009],
        [1.0003],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368645.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0117],
        [1.0165],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368656.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1241.9917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0178, device='cuda:0')



h[100].sum tensor(-16.0091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6575, device='cuda:0')



h[200].sum tensor(45.7962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9000, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53895.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0337, 0.0180, 0.0244,  ..., 0.0525, 0.0259, 0.0470],
        [0.0320, 0.0136, 0.0275,  ..., 0.0541, 0.0193, 0.0523],
        [0.0322, 0.0136, 0.0275,  ..., 0.0541, 0.0193, 0.0521],
        ...,
        [0.0480, 0.0542, 0.0052,  ..., 0.0423, 0.0873, 0.0115],
        [0.0480, 0.0542, 0.0052,  ..., 0.0423, 0.0873, 0.0115],
        [0.0479, 0.0542, 0.0052,  ..., 0.0423, 0.0873, 0.0115]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496166., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5473.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(736.8102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.7086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.2000, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0514],
        [ 0.1376],
        [ 0.1878],
        ...,
        [-1.1727],
        [-1.1691],
        [-1.1680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200199.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0117],
        [1.0165],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368656.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0118],
        [1.0167],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368668.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0018,  0.0029,  ...,  0.0043, -0.0021,  0.0106],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        [ 0.0049, -0.0018,  0.0029,  ...,  0.0043, -0.0021,  0.0106],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1212.0640, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-15.5210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6400, device='cuda:0')



h[200].sum tensor(45.7098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0000, 0.0049,  ..., 0.0102, 0.0000, 0.0167],
        [0.0263, 0.0000, 0.0157,  ..., 0.0204, 0.0000, 0.0549],
        [0.0082, 0.0000, 0.0049,  ..., 0.0103, 0.0000, 0.0167],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53274.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0307, 0.0118, 0.0376,  ..., 0.0558, 0.0150, 0.0670],
        [0.0213, 0.0000, 0.0552,  ..., 0.0639, 0.0000, 0.0970],
        [0.0304, 0.0100, 0.0394,  ..., 0.0563, 0.0119, 0.0697],
        ...,
        [0.0484, 0.0544, 0.0056,  ..., 0.0427, 0.0871, 0.0119],
        [0.0484, 0.0544, 0.0056,  ..., 0.0427, 0.0871, 0.0119],
        [0.0484, 0.0544, 0.0056,  ..., 0.0426, 0.0871, 0.0119]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492619.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5615.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(741.1913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(236.1683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.0131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0266],
        [ 0.1015],
        [ 0.0881],
        ...,
        [-1.1683],
        [-1.1646],
        [-1.1612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170208.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0118],
        [1.0167],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368668.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0119],
        [1.0168],
        ...,
        [1.0008],
        [1.0002],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368679.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0122, -0.0039,  0.0074,  ...,  0.0078, -0.0048,  0.0242],
        [ 0.0050, -0.0018,  0.0030,  ...,  0.0043, -0.0022,  0.0109],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1013.3634, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0145, device='cuda:0')



h[100].sum tensor(-13.0264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5371, device='cuda:0')



h[200].sum tensor(42.7480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0149, 0.0000, 0.0090,  ..., 0.0139, 0.0000, 0.0308],
        [0.0163, 0.0000, 0.0098,  ..., 0.0146, 0.0000, 0.0333],
        [0.0261, 0.0000, 0.0156,  ..., 0.0203, 0.0000, 0.0546],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48611.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0124, 0.0000, 0.0631,  ..., 0.0742, 0.0000, 0.1178],
        [0.0159, 0.0000, 0.0614,  ..., 0.0703, 0.0000, 0.1116],
        [0.0125, 0.0000, 0.0711,  ..., 0.0726, 0.0000, 0.1254],
        ...,
        [0.0488, 0.0549, 0.0057,  ..., 0.0430, 0.0875, 0.0122],
        [0.0488, 0.0549, 0.0057,  ..., 0.0430, 0.0875, 0.0122],
        [0.0488, 0.0548, 0.0057,  ..., 0.0430, 0.0875, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473535.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5736.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(719.4722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(207.2144, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.0049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2984],
        [ 0.3085],
        [ 0.3189],
        ...,
        [-1.1798],
        [-1.1762],
        [-1.1752]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183692.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0119],
        [1.0168],
        ...,
        [1.0008],
        [1.0002],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368679.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0120],
        [1.0169],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368690.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0075, -0.0025,  0.0045,  ...,  0.0055, -0.0031,  0.0155],
        [ 0.0106, -0.0034,  0.0064,  ...,  0.0070, -0.0042,  0.0211],
        [ 0.0104, -0.0034,  0.0062,  ...,  0.0069, -0.0041,  0.0207],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0014,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1866.7611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0266, device='cuda:0')



h[100].sum tensor(-23.7149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9828, device='cuda:0')



h[200].sum tensor(54.7704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.7857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0000, 0.0100,  ..., 0.0148, 0.0000, 0.0340],
        [0.0266, 0.0000, 0.0160,  ..., 0.0200, 0.0000, 0.0540],
        [0.0611, 0.0000, 0.0370,  ..., 0.0369, 0.0000, 0.1193],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67760.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0000, 0.0724,  ..., 0.0777, 0.0000, 0.1318],
        [0.0031, 0.0000, 0.0951,  ..., 0.0960, 0.0000, 0.1791],
        [0.0000, 0.0000, 0.1291,  ..., 0.1261, 0.0000, 0.2532],
        ...,
        [0.0493, 0.0554, 0.0057,  ..., 0.0433, 0.0882, 0.0124],
        [0.0493, 0.0554, 0.0057,  ..., 0.0433, 0.0882, 0.0124],
        [0.0493, 0.0554, 0.0057,  ..., 0.0433, 0.0882, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563205.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5598.1748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(790.2152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.6621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(348.9938, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2815],
        [ 0.2901],
        [ 0.2781],
        ...,
        [-1.1916],
        [-1.1880],
        [-1.1869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194362.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0120],
        [1.0169],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368690.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0121],
        [1.0170],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368701.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0091, -0.0030,  0.0055,  ...,  0.0063, -0.0036,  0.0184],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1033.4888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-13.3378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5572, device='cuda:0')



h[200].sum tensor(42.4333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0839, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0092, 0.0000, 0.0055,  ..., 0.0108, 0.0000, 0.0186],
        [0.0073, 0.0000, 0.0044,  ..., 0.0099, 0.0000, 0.0152],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49187.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0435, 0.0406, 0.0128,  ..., 0.0469, 0.0631, 0.0258],
        [0.0330, 0.0234, 0.0279,  ..., 0.0564, 0.0361, 0.0543],
        [0.0201, 0.0118, 0.0464,  ..., 0.0682, 0.0173, 0.0891],
        ...,
        [0.0497, 0.0559, 0.0057,  ..., 0.0436, 0.0889, 0.0125],
        [0.0497, 0.0559, 0.0057,  ..., 0.0436, 0.0889, 0.0125],
        [0.0497, 0.0559, 0.0057,  ..., 0.0436, 0.0889, 0.0125]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480315.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5847.6216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(715.3840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.5629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.1859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5308],
        [-0.4376],
        [-0.2385],
        ...,
        [-1.2031],
        [-1.1995],
        [-1.1942]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207011.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0121],
        [1.0170],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368701.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0122],
        [1.0171],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368712.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0143, -0.0045,  0.0086,  ...,  0.0087, -0.0054,  0.0279],
        [ 0.0344, -0.0104,  0.0210,  ...,  0.0183, -0.0126,  0.0651],
        [ 0.0134, -0.0042,  0.0081,  ...,  0.0083, -0.0051,  0.0264],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1292.7915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-16.6100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6859, device='cuda:0')



h[200].sum tensor(46.0206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0711, 0.0000, 0.0431,  ..., 0.0416, 0.0000, 0.1375],
        [0.0551, 0.0000, 0.0333,  ..., 0.0341, 0.0000, 0.1081],
        [0.0573, 0.0000, 0.0348,  ..., 0.0346, 0.0000, 0.1106],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54459.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1308,  ..., 0.1288, 0.0000, 0.2584],
        [0.0000, 0.0000, 0.1286,  ..., 0.1271, 0.0000, 0.2530],
        [0.0025, 0.0000, 0.1143,  ..., 0.1192, 0.0000, 0.2274],
        ...,
        [0.0501, 0.0564, 0.0055,  ..., 0.0438, 0.0897, 0.0125],
        [0.0501, 0.0564, 0.0055,  ..., 0.0438, 0.0897, 0.0125],
        [0.0500, 0.0564, 0.0055,  ..., 0.0438, 0.0897, 0.0125]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500504.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5822.1675, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(744.3276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(249.5038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(286.4793, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2459],
        [ 0.2063],
        [ 0.1061],
        ...,
        [-1.2225],
        [-1.2188],
        [-1.2177]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196568.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0122],
        [1.0171],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368712.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0123],
        [1.0172],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368723.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101, -0.0033,  0.0061,  ...,  0.0068, -0.0040,  0.0202],
        [ 0.0054, -0.0019,  0.0032,  ...,  0.0045, -0.0023,  0.0115],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1095.1479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-14.2638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5948, device='cuda:0')



h[200].sum tensor(43.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7645, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0333, 0.0000, 0.0200,  ..., 0.0238, 0.0000, 0.0679],
        [0.0184, 0.0000, 0.0111,  ..., 0.0157, 0.0000, 0.0371],
        [0.0054, 0.0000, 0.0032,  ..., 0.0091, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50834.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0103, 0.0000, 0.0734,  ..., 0.0782, 0.0000, 0.1336],
        [0.0216, 0.0059, 0.0534,  ..., 0.0681, 0.0079, 0.0982],
        [0.0358, 0.0205, 0.0304,  ..., 0.0554, 0.0313, 0.0564],
        ...,
        [0.0504, 0.0570, 0.0053,  ..., 0.0440, 0.0906, 0.0124],
        [0.0504, 0.0570, 0.0053,  ..., 0.0440, 0.0906, 0.0124],
        [0.0504, 0.0570, 0.0053,  ..., 0.0440, 0.0905, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488479.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5968.2588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(726.2745, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(229.2442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(268.9759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2283],
        [ 0.1439],
        [-0.0023],
        ...,
        [-1.2409],
        [-1.2371],
        [-1.2360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213689.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0123],
        [1.0172],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368723.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0123],
        [1.0172],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368733.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(998.1805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-13.1383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5437, device='cuda:0')



h[200].sum tensor(41.6317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8390, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48437.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0562, 0.0046,  ..., 0.0433, 0.0887, 0.0121],
        [0.0495, 0.0563, 0.0047,  ..., 0.0433, 0.0889, 0.0121],
        [0.0497, 0.0564, 0.0048,  ..., 0.0434, 0.0892, 0.0121],
        ...,
        [0.0508, 0.0576, 0.0050,  ..., 0.0442, 0.0913, 0.0123],
        [0.0508, 0.0576, 0.0050,  ..., 0.0442, 0.0912, 0.0123],
        [0.0508, 0.0575, 0.0050,  ..., 0.0442, 0.0912, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477873.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6074.8940, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(718.7974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(214.7638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(257.6343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7029],
        [-0.9567],
        [-1.1435],
        ...,
        [-1.2560],
        [-1.2521],
        [-1.2510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221216.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0123],
        [1.0172],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368733.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0124],
        [1.0173],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368745.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1214.9741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0180, device='cuda:0')



h[100].sum tensor(-15.8170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6664, device='cuda:0')



h[200].sum tensor(44.9208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0610, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54245.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0497, 0.0567, 0.0046,  ..., 0.0435, 0.0890, 0.0122],
        [0.0499, 0.0565, 0.0051,  ..., 0.0435, 0.0883, 0.0130],
        [0.0502, 0.0557, 0.0063,  ..., 0.0435, 0.0865, 0.0147],
        ...,
        [0.0506, 0.0555, 0.0070,  ..., 0.0450, 0.0868, 0.0157],
        [0.0494, 0.0502, 0.0111,  ..., 0.0462, 0.0773, 0.0222],
        [0.0488, 0.0476, 0.0132,  ..., 0.0468, 0.0725, 0.0254]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506498.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6090.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(748.2900, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(250.3206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(286.5429, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2633],
        [-1.1388],
        [-0.9580],
        ...,
        [-1.1048],
        [-0.9845],
        [-0.9057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200640.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0124],
        [1.0173],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368745.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(506.0364, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0124],
        [1.0174],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368756.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0069, -0.0023,  0.0041,  ...,  0.0053, -0.0028,  0.0144],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(813.8554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0121, device='cuda:0')



h[100].sum tensor(-10.8132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4496, device='cuda:0')



h[200].sum tensor(39.5037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0070, 0.0000, 0.0042,  ..., 0.0099, 0.0000, 0.0146],
        [0.0249, 0.0000, 0.0151,  ..., 0.0189, 0.0000, 0.0491],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46227.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0423, 0.0303, 0.0209,  ..., 0.0510, 0.0428, 0.0408],
        [0.0305, 0.0195, 0.0373,  ..., 0.0623, 0.0287, 0.0726],
        [0.0156, 0.0035, 0.0761,  ..., 0.0908, 0.0038, 0.1502],
        ...,
        [0.0515, 0.0586, 0.0050,  ..., 0.0447, 0.0915, 0.0128],
        [0.0514, 0.0586, 0.0050,  ..., 0.0447, 0.0915, 0.0128],
        [0.0514, 0.0585, 0.0050,  ..., 0.0447, 0.0915, 0.0128]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476638.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6203.9639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(707.0710, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(201.6890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(244.6286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0037],
        [ 0.0557],
        [ 0.1335],
        ...,
        [-1.2705],
        [-1.2666],
        [-1.2654]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229949.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0124],
        [1.0174],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368756.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0125],
        [1.0174],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368767.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0232, -0.0070,  0.0141,  ...,  0.0130, -0.0085,  0.0443],
        [ 0.0211, -0.0064,  0.0128,  ...,  0.0120, -0.0078,  0.0405],
        [ 0.0197, -0.0060,  0.0120,  ...,  0.0113, -0.0073,  0.0380],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1331.3103, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0193, device='cuda:0')



h[100].sum tensor(-17.0839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7141, device='cuda:0')



h[200].sum tensor(47.2062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0478, 0.0000, 0.0290,  ..., 0.0302, 0.0000, 0.0930],
        [0.0680, 0.0000, 0.0413,  ..., 0.0402, 0.0000, 0.1319],
        [0.0741, 0.0000, 0.0450,  ..., 0.0432, 0.0000, 0.1432],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55882.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1413,  ..., 0.1371, 0.0000, 0.2796],
        [0.0000, 0.0000, 0.1579,  ..., 0.1487, 0.0000, 0.3118],
        [0.0000, 0.0000, 0.1640,  ..., 0.1523, 0.0000, 0.3226],
        ...,
        [0.0518, 0.0590, 0.0051,  ..., 0.0451, 0.0916, 0.0131],
        [0.0517, 0.0590, 0.0051,  ..., 0.0451, 0.0916, 0.0131],
        [0.0517, 0.0590, 0.0051,  ..., 0.0451, 0.0915, 0.0131]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515799.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6061.5737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(749.4650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.0369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.8758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2334],
        [ 0.2365],
        [ 0.2419],
        ...,
        [-1.2754],
        [-1.2715],
        [-1.2703]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210842.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0125],
        [1.0174],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368767.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0126],
        [1.0175],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368778.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111, -0.0035,  0.0067,  ...,  0.0072, -0.0043,  0.0221],
        [ 0.0042, -0.0015,  0.0025,  ...,  0.0040, -0.0019,  0.0094],
        [ 0.0040, -0.0015,  0.0024,  ...,  0.0039, -0.0018,  0.0091],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1059.5381, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0154, device='cuda:0')



h[100].sum tensor(-13.6527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5709, device='cuda:0')



h[200].sum tensor(43.5781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0196, 0.0000, 0.0117,  ..., 0.0174, 0.0000, 0.0429],
        [0.0288, 0.0000, 0.0173,  ..., 0.0217, 0.0000, 0.0597],
        [0.0229, 0.0000, 0.0137,  ..., 0.0190, 0.0000, 0.0490],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50588.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0131, 0.0000, 0.0833,  ..., 0.0804, 0.0000, 0.1476],
        [0.0100, 0.0000, 0.0868,  ..., 0.0834, 0.0000, 0.1547],
        [0.0155, 0.0000, 0.0788,  ..., 0.0786, 0.0000, 0.1397],
        ...,
        [0.0520, 0.0594, 0.0052,  ..., 0.0455, 0.0915, 0.0135],
        [0.0520, 0.0594, 0.0052,  ..., 0.0455, 0.0915, 0.0135],
        [0.0520, 0.0594, 0.0052,  ..., 0.0455, 0.0915, 0.0135]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491854.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6222.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(724.8077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(227.3928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(264.0041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3229],
        [ 0.3183],
        [ 0.3099],
        ...,
        [-1.2788],
        [-1.2750],
        [-1.2738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213816.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0126],
        [1.0175],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368778.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0126],
        [1.0175],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368790.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0161, -0.0049,  0.0098,  ...,  0.0096, -0.0060,  0.0314],
        [ 0.0067, -0.0022,  0.0040,  ...,  0.0051, -0.0027,  0.0140],
        [ 0.0075, -0.0025,  0.0045,  ...,  0.0055, -0.0030,  0.0154],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1132.8643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-14.4234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6036, device='cuda:0')



h[200].sum tensor(45.1182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0382, 0.0000, 0.0231,  ..., 0.0261, 0.0000, 0.0772],
        [0.0422, 0.0000, 0.0255,  ..., 0.0280, 0.0000, 0.0845],
        [0.0243, 0.0000, 0.0146,  ..., 0.0190, 0.0000, 0.0498],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51684.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0000, 0.0814,  ..., 0.0888, 0.0000, 0.1534],
        [0.0054, 0.0000, 0.0813,  ..., 0.0897, 0.0000, 0.1540],
        [0.0127, 0.0000, 0.0689,  ..., 0.0815, 0.0000, 0.1300],
        ...,
        [0.0522, 0.0599, 0.0053,  ..., 0.0458, 0.0913, 0.0138],
        [0.0522, 0.0599, 0.0053,  ..., 0.0458, 0.0913, 0.0138],
        [0.0522, 0.0599, 0.0053,  ..., 0.0458, 0.0913, 0.0138]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494645., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6279.0918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(733.5840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(231.4518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(269.9164, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2749],
        [ 0.2147],
        [ 0.1141],
        ...,
        [-1.2823],
        [-1.2785],
        [-1.2773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195229.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0126],
        [1.0175],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368790.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0127],
        [1.0176],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368801.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0080, -0.0026,  0.0048,  ...,  0.0058, -0.0031,  0.0164],
        [ 0.0048, -0.0017,  0.0028,  ...,  0.0042, -0.0020,  0.0105],
        [ 0.0034, -0.0013,  0.0020,  ...,  0.0036, -0.0016,  0.0080],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1557.8586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0223, device='cuda:0')



h[100].sum tensor(-19.4904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8267, device='cuda:0')



h[200].sum tensor(51.5843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.9609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0279, 0.0000, 0.0168,  ..., 0.0207, 0.0000, 0.0565],
        [0.0233, 0.0000, 0.0140,  ..., 0.0186, 0.0000, 0.0481],
        [0.0426, 0.0000, 0.0257,  ..., 0.0282, 0.0000, 0.0854],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60105.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0000, 0.1157,  ..., 0.1147, 0.0000, 0.2242],
        [0.0025, 0.0000, 0.0984,  ..., 0.0989, 0.0000, 0.1854],
        [0.0019, 0.0000, 0.1075,  ..., 0.1043, 0.0000, 0.2020],
        ...,
        [0.0525, 0.0604, 0.0054,  ..., 0.0461, 0.0913, 0.0141],
        [0.0525, 0.0604, 0.0054,  ..., 0.0461, 0.0913, 0.0141],
        [0.0525, 0.0604, 0.0054,  ..., 0.0461, 0.0913, 0.0141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530466.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6131.9229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(760.7302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(288.6007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(309.6890, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2207],
        [ 0.2437],
        [ 0.2630],
        ...,
        [-1.2877],
        [-1.2839],
        [-1.2827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212495.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0127],
        [1.0176],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368801.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0128],
        [1.0177],
        ...,
        [1.0008],
        [1.0001],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368812.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(927.3398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0132, device='cuda:0')



h[100].sum tensor(-11.7140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4901, device='cuda:0')



h[200].sum tensor(42.8649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0301, 0.0000, 0.0182,  ..., 0.0212, 0.0000, 0.0589],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47775.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0155, 0.0038, 0.0717,  ..., 0.0878, 0.0039, 0.1411],
        [0.0368, 0.0246, 0.0300,  ..., 0.0597, 0.0356, 0.0601],
        [0.0462, 0.0407, 0.0157,  ..., 0.0507, 0.0579, 0.0331],
        ...,
        [0.0528, 0.0610, 0.0054,  ..., 0.0463, 0.0913, 0.0145],
        [0.0528, 0.0610, 0.0054,  ..., 0.0463, 0.0913, 0.0145],
        [0.0528, 0.0609, 0.0054,  ..., 0.0463, 0.0913, 0.0145]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479388.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6366.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(711.3282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(208.1499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(248.5757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1613],
        [-0.0362],
        [-0.3082],
        ...,
        [-1.2912],
        [-1.2866],
        [-1.2841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215872.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0128],
        [1.0177],
        ...,
        [1.0008],
        [1.0001],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368812.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0129],
        [1.0178],
        ...,
        [1.0008],
        [1.0001],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368823.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1540.1791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0216, device='cuda:0')



h[100].sum tensor(-19.0092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7996, device='cuda:0')



h[200].sum tensor(52.1176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0141, 0.0000, 0.0085,  ..., 0.0137, 0.0000, 0.0295],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60996.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0474, 0.0456, 0.0129,  ..., 0.0495, 0.0652, 0.0284],
        [0.0435, 0.0326, 0.0204,  ..., 0.0536, 0.0467, 0.0419],
        [0.0282, 0.0135, 0.0495,  ..., 0.0690, 0.0171, 0.0941],
        ...,
        [0.0530, 0.0615, 0.0054,  ..., 0.0465, 0.0912, 0.0147],
        [0.0530, 0.0615, 0.0054,  ..., 0.0465, 0.0912, 0.0147],
        [0.0530, 0.0614, 0.0054,  ..., 0.0465, 0.0912, 0.0147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537702.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6165.0615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(774.8818, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(289.0279, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.6450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5456],
        [-0.3995],
        [-0.1339],
        ...,
        [-1.2998],
        [-1.2960],
        [-1.2948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164796.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0129],
        [1.0178],
        ...,
        [1.0008],
        [1.0001],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368823.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0130],
        [1.0179],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368834.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0015,  0.0024,  ...,  0.0039, -0.0018,  0.0092],
        [ 0.0043, -0.0015,  0.0026,  ...,  0.0040, -0.0019,  0.0097],
        [ 0.0095, -0.0030,  0.0057,  ...,  0.0065, -0.0037,  0.0192],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1713.0485, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0238, device='cuda:0')



h[100].sum tensor(-20.9823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8823, device='cuda:0')



h[200].sum tensor(55.1824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.9676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0000, 0.0094,  ..., 0.0143, 0.0000, 0.0320],
        [0.0317, 0.0000, 0.0191,  ..., 0.0230, 0.0000, 0.0652],
        [0.0248, 0.0000, 0.0148,  ..., 0.0198, 0.0000, 0.0525],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64970.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0149, 0.0000, 0.0732,  ..., 0.0824, 0.0000, 0.1384],
        [0.0140, 0.0000, 0.0800,  ..., 0.0833, 0.0000, 0.1471],
        [0.0121, 0.0000, 0.0831,  ..., 0.0854, 0.0000, 0.1534],
        ...,
        [0.0530, 0.0621, 0.0051,  ..., 0.0465, 0.0914, 0.0147],
        [0.0530, 0.0621, 0.0051,  ..., 0.0465, 0.0914, 0.0147],
        [0.0530, 0.0621, 0.0051,  ..., 0.0465, 0.0914, 0.0147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563593.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6074.3350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(788.1828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(317.0708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.4045, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2960],
        [ 0.3091],
        [ 0.3095],
        ...,
        [-1.3111],
        [-1.3065],
        [-1.3047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197890.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0130],
        [1.0179],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368834.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0131],
        [1.0180],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368844.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0091, -0.0029,  0.0055,  ...,  0.0063, -0.0035,  0.0184],
        [ 0.0209, -0.0062,  0.0127,  ...,  0.0119, -0.0076,  0.0402],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1295.5649, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0181, device='cuda:0')



h[100].sum tensor(-15.7916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6682, device='cuda:0')



h[200].sum tensor(49.9754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0928, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0237, 0.0000, 0.0144,  ..., 0.0182, 0.0000, 0.0470],
        [0.0420, 0.0000, 0.0254,  ..., 0.0274, 0.0000, 0.0823],
        [0.0326, 0.0000, 0.0197,  ..., 0.0229, 0.0000, 0.0649],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54526.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0103, 0.0000, 0.0815,  ..., 0.0927, 0.0000, 0.1594],
        [0.0035, 0.0000, 0.1016,  ..., 0.1072, 0.0000, 0.1999],
        [0.0029, 0.0000, 0.0966,  ..., 0.1024, 0.0000, 0.1884],
        ...,
        [0.0530, 0.0626, 0.0048,  ..., 0.0465, 0.0914, 0.0148],
        [0.0530, 0.0626, 0.0048,  ..., 0.0465, 0.0914, 0.0148],
        [0.0530, 0.0626, 0.0048,  ..., 0.0465, 0.0914, 0.0148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507471.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6207.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(751.7732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(247.1865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(290.0902, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2711],
        [ 0.2695],
        [ 0.2589],
        ...,
        [-1.3245],
        [-1.3206],
        [-1.3194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195428.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0131],
        [1.0180],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368844.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0131],
        [1.0180],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368855.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [ 0.0089, -0.0028,  0.0054,  ...,  0.0062, -0.0034,  0.0180],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1681.2756, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0233, device='cuda:0')



h[100].sum tensor(-20.3049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8612, device='cuda:0')



h[200].sum tensor(55.9575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.5858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0323, 0.0000, 0.0194,  ..., 0.0233, 0.0000, 0.0661],
        [0.0072, 0.0000, 0.0043,  ..., 0.0099, 0.0000, 0.0148],
        [0.0090, 0.0000, 0.0054,  ..., 0.0108, 0.0000, 0.0183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61593.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0237, 0.0000, 0.0579,  ..., 0.0738, 0.0000, 0.1106],
        [0.0350, 0.0092, 0.0377,  ..., 0.0625, 0.0063, 0.0737],
        [0.0290, 0.0025, 0.0466,  ..., 0.0689, 0.0017, 0.0914],
        ...,
        [0.0531, 0.0631, 0.0045,  ..., 0.0466, 0.0915, 0.0149],
        [0.0531, 0.0631, 0.0045,  ..., 0.0466, 0.0915, 0.0149],
        [0.0531, 0.0631, 0.0045,  ..., 0.0466, 0.0915, 0.0149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538161.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6124.3623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(782.1833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(292.8093, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(326.9308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2189],
        [ 0.2263],
        [ 0.2352],
        ...,
        [-1.3354],
        [-1.3315],
        [-1.3303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207001.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0131],
        [1.0180],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368855.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(514.1584, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0132],
        [1.0181],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368865.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1399.4734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0192, device='cuda:0')



h[100].sum tensor(-16.7794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7107, device='cuda:0')



h[200].sum tensor(52.2493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57039.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0517, 0.0620, 0.0040,  ..., 0.0458, 0.0890, 0.0147],
        [0.0518, 0.0621, 0.0040,  ..., 0.0459, 0.0892, 0.0148],
        [0.0520, 0.0623, 0.0041,  ..., 0.0460, 0.0894, 0.0148],
        ...,
        [0.0533, 0.0635, 0.0044,  ..., 0.0468, 0.0915, 0.0151],
        [0.0530, 0.0623, 0.0053,  ..., 0.0471, 0.0894, 0.0164],
        [0.0522, 0.0582, 0.0082,  ..., 0.0481, 0.0824, 0.0211]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(521480.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6187.8618, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(765.3207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(263.2645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(304.8983, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2455],
        [-1.3642],
        [-1.4583],
        ...,
        [-1.3233],
        [-1.2767],
        [-1.1898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205180.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0132],
        [1.0181],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368865.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0132],
        [1.0182],
        ...,
        [1.0007],
        [1.0000],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368876.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0169, -0.0050,  0.0103,  ...,  0.0100, -0.0061,  0.0328],
        [ 0.0097, -0.0030,  0.0058,  ...,  0.0066, -0.0037,  0.0195],
        [ 0.0042, -0.0015,  0.0025,  ...,  0.0040, -0.0018,  0.0094],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1049.4304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-12.4721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5271, device='cuda:0')



h[200].sum tensor(47.3237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0480, 0.0000, 0.0290,  ..., 0.0307, 0.0000, 0.0951],
        [0.0466, 0.0000, 0.0282,  ..., 0.0301, 0.0000, 0.0924],
        [0.0406, 0.0000, 0.0245,  ..., 0.0273, 0.0000, 0.0815],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49127.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1416,  ..., 0.1347, 0.0000, 0.2798],
        [0.0000, 0.0000, 0.1314,  ..., 0.1253, 0.0000, 0.2568],
        [0.0000, 0.0000, 0.1236,  ..., 0.1177, 0.0000, 0.2384],
        ...,
        [0.0536, 0.0640, 0.0044,  ..., 0.0471, 0.0916, 0.0153],
        [0.0536, 0.0640, 0.0044,  ..., 0.0471, 0.0916, 0.0153],
        [0.0536, 0.0640, 0.0044,  ..., 0.0471, 0.0916, 0.0153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486696.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6448.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(730.8343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.7122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(263.7427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2351],
        [ 0.2559],
        [ 0.2683],
        ...,
        [-1.3506],
        [-1.3467],
        [-1.3455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213767.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0132],
        [1.0182],
        ...,
        [1.0007],
        [1.0000],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368876.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0133],
        [1.0183],
        ...,
        [1.0007],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368887.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0134, -0.0040,  0.0081,  ...,  0.0083, -0.0049,  0.0264],
        [ 0.0142, -0.0042,  0.0086,  ...,  0.0087, -0.0052,  0.0278],
        [ 0.0197, -0.0058,  0.0120,  ...,  0.0113, -0.0071,  0.0379],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1638.6694, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0221, device='cuda:0')



h[100].sum tensor(-19.4052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8182, device='cuda:0')



h[200].sum tensor(55.4626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8081, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0764, 0.0000, 0.0464,  ..., 0.0442, 0.0000, 0.1476],
        [0.0570, 0.0000, 0.0346,  ..., 0.0345, 0.0000, 0.1101],
        [0.0305, 0.0000, 0.0185,  ..., 0.0215, 0.0000, 0.0595],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0056, 0.0000, 0.0033,  ..., 0.0093, 0.0000, 0.0121]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61387.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1810,  ..., 0.1756, 0.0000, 0.3760],
        [0.0000, 0.0000, 0.1410,  ..., 0.1435, 0.0000, 0.2903],
        [0.0057, 0.0000, 0.0977,  ..., 0.1097, 0.0000, 0.1992],
        ...,
        [0.0537, 0.0627, 0.0054,  ..., 0.0480, 0.0890, 0.0175],
        [0.0513, 0.0531, 0.0107,  ..., 0.0506, 0.0737, 0.0268],
        [0.0428, 0.0271, 0.0282,  ..., 0.0595, 0.0363, 0.0579]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539559.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6368.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(787.3317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(288.1419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(322.4326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2174],
        [ 0.2422],
        [ 0.2659],
        ...,
        [-1.1694],
        [-0.8969],
        [-0.5033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178244.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0133],
        [1.0183],
        ...,
        [1.0007],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368887.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0133],
        [1.0184],
        ...,
        [1.0007],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368898.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0142, -0.0043,  0.0086,  ...,  0.0087, -0.0052,  0.0279],
        [ 0.0078, -0.0025,  0.0047,  ...,  0.0057, -0.0030,  0.0161],
        [ 0.0042, -0.0015,  0.0025,  ...,  0.0040, -0.0018,  0.0094],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1202.6111, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-14.1623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5995, device='cuda:0')



h[200].sum tensor(48.7768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0408, 0.0000, 0.0247,  ..., 0.0274, 0.0000, 0.0822],
        [0.0325, 0.0000, 0.0196,  ..., 0.0235, 0.0000, 0.0668],
        [0.0170, 0.0000, 0.0101,  ..., 0.0162, 0.0000, 0.0383],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52308.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0000, 0.0838,  ..., 0.0917, 0.0000, 0.1635],
        [0.0176, 0.0000, 0.0784,  ..., 0.0856, 0.0000, 0.1507],
        [0.0276, 0.0000, 0.0695,  ..., 0.0759, 0.0000, 0.1295],
        ...,
        [0.0546, 0.0650, 0.0043,  ..., 0.0480, 0.0923, 0.0161],
        [0.0546, 0.0650, 0.0043,  ..., 0.0480, 0.0923, 0.0161],
        [0.0546, 0.0650, 0.0043,  ..., 0.0480, 0.0923, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500543.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6524.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(741.4254, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(234.9296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(273.9717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3158],
        [ 0.3247],
        [ 0.3124],
        ...,
        [-1.3657],
        [-1.3618],
        [-1.3607]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211375.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0133],
        [1.0184],
        ...,
        [1.0007],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368898.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0133],
        [1.0185],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368909., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [ 0.0134, -0.0040,  0.0081,  ...,  0.0083, -0.0049,  0.0264],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1316.8767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-15.4916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6617, device='cuda:0')



h[200].sum tensor(49.9163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0182, 0.0000, 0.0110,  ..., 0.0157, 0.0000, 0.0370],
        [0.0366, 0.0000, 0.0221,  ..., 0.0249, 0.0000, 0.0727],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53917.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0450, 0.0349, 0.0196,  ..., 0.0564, 0.0477, 0.0456],
        [0.0260, 0.0134, 0.0552,  ..., 0.0794, 0.0158, 0.1148],
        [0.0120, 0.0000, 0.0950,  ..., 0.1075, 0.0000, 0.1946],
        ...,
        [0.0551, 0.0656, 0.0041,  ..., 0.0484, 0.0929, 0.0164],
        [0.0551, 0.0656, 0.0041,  ..., 0.0484, 0.0929, 0.0164],
        [0.0551, 0.0656, 0.0041,  ..., 0.0483, 0.0929, 0.0164]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508470.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6579.8271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(752.5239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(244.5715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(280.5155, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6867],
        [-0.3764],
        [-0.0884],
        ...,
        [-1.3774],
        [-1.3733],
        [-1.3719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201794.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0133],
        [1.0185],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368909., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0134],
        [1.0185],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368919.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111, -0.0034,  0.0067,  ...,  0.0073, -0.0041,  0.0222],
        [ 0.0098, -0.0030,  0.0059,  ...,  0.0067, -0.0037,  0.0199],
        [ 0.0100, -0.0031,  0.0060,  ...,  0.0067, -0.0037,  0.0201],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1548.6151, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-18.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7752, device='cuda:0')



h[200].sum tensor(52.8973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0440, 0.0000, 0.0266,  ..., 0.0290, 0.0000, 0.0883],
        [0.0281, 0.0000, 0.0169,  ..., 0.0215, 0.0000, 0.0590],
        [0.0293, 0.0000, 0.0176,  ..., 0.0220, 0.0000, 0.0611],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61015.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0061, 0.0000, 0.1036,  ..., 0.1131, 0.0000, 0.2119],
        [0.0136, 0.0000, 0.0816,  ..., 0.0929, 0.0000, 0.1622],
        [0.0201, 0.0000, 0.0719,  ..., 0.0854, 0.0000, 0.1418],
        ...,
        [0.0555, 0.0661, 0.0039,  ..., 0.0486, 0.0934, 0.0167],
        [0.0555, 0.0661, 0.0039,  ..., 0.0486, 0.0934, 0.0167],
        [0.0555, 0.0661, 0.0039,  ..., 0.0486, 0.0934, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547625.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6584.3433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(777.3527, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.4756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.8895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0530],
        [ 0.0204],
        [-0.0232],
        ...,
        [-1.3888],
        [-1.3848],
        [-1.3835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206801.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0134],
        [1.0185],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368919.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0134],
        [1.0185],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368919.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053, -0.0018,  0.0032,  ...,  0.0045, -0.0022,  0.0115],
        [ 0.0031, -0.0012,  0.0018,  ...,  0.0035, -0.0014,  0.0075],
        [ 0.0095, -0.0029,  0.0057,  ...,  0.0065, -0.0036,  0.0193],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1059.9902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-12.3921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5274, device='cuda:0')



h[200].sum tensor(45.9641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0000, 0.0093,  ..., 0.0144, 0.0000, 0.0321],
        [0.0406, 0.0000, 0.0246,  ..., 0.0274, 0.0000, 0.0820],
        [0.0232, 0.0000, 0.0140,  ..., 0.0186, 0.0000, 0.0481],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49522.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0273, 0.0064, 0.0579,  ..., 0.0770, 0.0067, 0.1158],
        [0.0126, 0.0000, 0.0847,  ..., 0.0932, 0.0000, 0.1660],
        [0.0190, 0.0000, 0.0754,  ..., 0.0866, 0.0000, 0.1471],
        ...,
        [0.0555, 0.0661, 0.0039,  ..., 0.0486, 0.0934, 0.0167],
        [0.0555, 0.0661, 0.0039,  ..., 0.0486, 0.0934, 0.0167],
        [0.0555, 0.0661, 0.0039,  ..., 0.0486, 0.0934, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494720.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6693.9927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(727.6194, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(221.6084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(256.4644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1161],
        [ 0.2668],
        [ 0.2890],
        ...,
        [-1.3888],
        [-1.3848],
        [-1.3835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238755.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0134],
        [1.0185],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368919.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0134],
        [1.0186],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368930.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0116, -0.0035,  0.0070,  ...,  0.0075, -0.0043,  0.0230],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1749.4231, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0233, device='cuda:0')



h[100].sum tensor(-20.4228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8639, device='cuda:0')



h[200].sum tensor(56.0564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0093, 0.0000, 0.0056,  ..., 0.0110, 0.0000, 0.0190],
        [0.0117, 0.0000, 0.0071,  ..., 0.0121, 0.0000, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63878.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0273, 0.0091, 0.0515,  ..., 0.0772, 0.0100, 0.1080],
        [0.0368, 0.0126, 0.0359,  ..., 0.0669, 0.0145, 0.0774],
        [0.0438, 0.0231, 0.0251,  ..., 0.0594, 0.0265, 0.0557],
        ...,
        [0.0557, 0.0667, 0.0037,  ..., 0.0487, 0.0937, 0.0167],
        [0.0557, 0.0667, 0.0037,  ..., 0.0487, 0.0937, 0.0167],
        [0.0557, 0.0667, 0.0037,  ..., 0.0487, 0.0937, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560897.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6489.4399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(790.4845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.9594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(329.0098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2776],
        [ 0.2528],
        [ 0.2330],
        ...,
        [-1.4018],
        [-1.3977],
        [-1.3964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219277.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0134],
        [1.0186],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368930.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0134],
        [1.0187],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368941.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1364.2642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0180, device='cuda:0')



h[100].sum tensor(-15.7216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6644, device='cuda:0')



h[200].sum tensor(51.2078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0041, 0.0000, 0.0024,  ..., 0.0085, 0.0000, 0.0093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55856.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0642, 0.0039,  ..., 0.0478, 0.0890, 0.0177],
        [0.0522, 0.0561, 0.0087,  ..., 0.0500, 0.0761, 0.0258],
        [0.0481, 0.0359, 0.0211,  ..., 0.0551, 0.0475, 0.0468],
        ...,
        [0.0558, 0.0672, 0.0035,  ..., 0.0486, 0.0938, 0.0167],
        [0.0558, 0.0672, 0.0035,  ..., 0.0486, 0.0938, 0.0167],
        [0.0558, 0.0672, 0.0035,  ..., 0.0486, 0.0937, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(519550.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6590.4370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(765.6921, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.0362, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.4957, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3201],
        [-1.0555],
        [-0.6929],
        ...,
        [-1.4127],
        [-1.4087],
        [-1.4074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216937.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0134],
        [1.0187],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368941.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0135],
        [1.0188],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368952.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [ 0.0096, -0.0029,  0.0058,  ...,  0.0065, -0.0036,  0.0193],
        [ 0.0052, -0.0017,  0.0031,  ...,  0.0044, -0.0021,  0.0112],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1584.6362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0211, device='cuda:0')



h[100].sum tensor(-18.1085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7801, device='cuda:0')



h[200].sum tensor(55.1951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0000, 0.0125,  ..., 0.0180, 0.0000, 0.0455],
        [0.0119, 0.0000, 0.0070,  ..., 0.0132, 0.0000, 0.0270],
        [0.0354, 0.0000, 0.0214,  ..., 0.0248, 0.0000, 0.0721],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60944.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0364, 0.0000, 0.0536,  ..., 0.0687, 0.0000, 0.1030],
        [0.0339, 0.0046, 0.0583,  ..., 0.0719, 0.0027, 0.1123],
        [0.0144, 0.0000, 0.0903,  ..., 0.0947, 0.0000, 0.1761],
        ...,
        [0.0558, 0.0675, 0.0036,  ..., 0.0483, 0.0935, 0.0167],
        [0.0559, 0.0675, 0.0035,  ..., 0.0483, 0.0936, 0.0167],
        [0.0558, 0.0675, 0.0035,  ..., 0.0483, 0.0935, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(545170.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6482.2642, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(791.3149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.7928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(321.3463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3029],
        [ 0.3077],
        [ 0.3147],
        ...,
        [-1.4208],
        [-1.4167],
        [-1.4155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208876.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0135],
        [1.0188],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368952.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(470.9245, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0135],
        [1.0189],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368964., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1100.4875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0141, device='cuda:0')



h[100].sum tensor(-12.2294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5221, device='cuda:0')



h[200].sum tensor(49.1272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49487.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0544, 0.0643, 0.0051,  ..., 0.0472, 0.0871, 0.0191],
        [0.0544, 0.0660, 0.0035,  ..., 0.0472, 0.0904, 0.0168],
        [0.0546, 0.0661, 0.0037,  ..., 0.0473, 0.0906, 0.0168],
        ...,
        [0.0559, 0.0677, 0.0037,  ..., 0.0482, 0.0932, 0.0168],
        [0.0559, 0.0677, 0.0037,  ..., 0.0482, 0.0932, 0.0168],
        [0.0559, 0.0677, 0.0037,  ..., 0.0482, 0.0932, 0.0168]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493276.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6720.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(742.8841, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(221.1304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(265.0227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7993],
        [-0.7862],
        [-0.7005],
        ...,
        [-1.4251],
        [-1.4210],
        [-1.4197]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231839.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0135],
        [1.0189],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368964., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0136],
        [1.0190],
        ...,
        [1.0006],
        [0.9999],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368975.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2392.5513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0319, device='cuda:0')



h[100].sum tensor(-27.1525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1817, device='cuda:0')



h[200].sum tensor(67.9472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74758.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0544, 0.0663, 0.0034,  ..., 0.0471, 0.0904, 0.0165],
        [0.0545, 0.0664, 0.0034,  ..., 0.0472, 0.0906, 0.0165],
        [0.0547, 0.0665, 0.0036,  ..., 0.0473, 0.0909, 0.0166],
        ...,
        [0.0561, 0.0679, 0.0038,  ..., 0.0482, 0.0931, 0.0169],
        [0.0561, 0.0680, 0.0038,  ..., 0.0482, 0.0931, 0.0169],
        [0.0561, 0.0679, 0.0038,  ..., 0.0482, 0.0930, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605453.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6438.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(838.1678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.1624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(387.5912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4360],
        [-1.4736],
        [-1.4668],
        ...,
        [-1.4301],
        [-1.4260],
        [-1.4248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225145.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0136],
        [1.0190],
        ...,
        [1.0006],
        [0.9999],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368975.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0137],
        [1.0191],
        ...,
        [1.0006],
        [0.9999],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368985.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [ 0.0044, -0.0015,  0.0026,  ...,  0.0040, -0.0018,  0.0097],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(976.8912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0123, device='cuda:0')



h[100].sum tensor(-10.5094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4546, device='cuda:0')



h[200].sum tensor(48.2085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0044, 0.0000, 0.0026,  ..., 0.0086, 0.0000, 0.0099],
        [0.0149, 0.0000, 0.0090,  ..., 0.0141, 0.0000, 0.0309],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47148.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0505, 0.0480, 0.0131,  ..., 0.0521, 0.0621, 0.0338],
        [0.0425, 0.0239, 0.0332,  ..., 0.0618, 0.0307, 0.0688],
        [0.0307, 0.0063, 0.0594,  ..., 0.0756, 0.0063, 0.1156],
        ...,
        [0.0563, 0.0683, 0.0039,  ..., 0.0483, 0.0932, 0.0170],
        [0.0564, 0.0683, 0.0039,  ..., 0.0483, 0.0932, 0.0170],
        [0.0563, 0.0683, 0.0039,  ..., 0.0483, 0.0932, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485156.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6798.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(732.0872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.8222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(252.5923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1383],
        [ 0.0454],
        [ 0.1888],
        ...,
        [-1.4371],
        [-1.4330],
        [-1.4318]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236570.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0137],
        [1.0191],
        ...,
        [1.0006],
        [0.9999],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368985.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0138],
        [1.0192],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368996.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0222, -0.0063,  0.0135,  ...,  0.0124, -0.0077,  0.0425],
        [ 0.0187, -0.0053,  0.0113,  ...,  0.0108, -0.0066,  0.0361],
        [ 0.0178, -0.0051,  0.0108,  ...,  0.0104, -0.0063,  0.0346],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1298.5811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0164, device='cuda:0')



h[100].sum tensor(-14.1318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6061, device='cuda:0')



h[200].sum tensor(52.9302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0578, 0.0000, 0.0350,  ..., 0.0353, 0.0000, 0.1132],
        [0.0817, 0.0000, 0.0497,  ..., 0.0466, 0.0000, 0.1574],
        [0.0557, 0.0000, 0.0338,  ..., 0.0344, 0.0000, 0.1096],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53360.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0000, 0.1209,  ..., 0.1197, 0.0000, 0.2406],
        [0.0000, 0.0000, 0.1383,  ..., 0.1345, 0.0000, 0.2780],
        [0.0028, 0.0000, 0.1203,  ..., 0.1205, 0.0000, 0.2393],
        ...,
        [0.0567, 0.0685, 0.0040,  ..., 0.0485, 0.0932, 0.0173],
        [0.0567, 0.0685, 0.0040,  ..., 0.0485, 0.0932, 0.0173],
        [0.0567, 0.0685, 0.0040,  ..., 0.0485, 0.0932, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509258.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6758.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(760.2808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.0288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(282.1864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2901],
        [ 0.2732],
        [ 0.2575],
        ...,
        [-1.4411],
        [-1.4370],
        [-1.4358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221490.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0138],
        [1.0192],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368996.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0138],
        [1.0193],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369007.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1720.0734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0218, device='cuda:0')



h[100].sum tensor(-18.9335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8076, device='cuda:0')



h[200].sum tensor(58.8913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.6150, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0109, 0.0000, 0.0065,  ..., 0.0122, 0.0000, 0.0236],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63152.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0533, 0.0532, 0.0126,  ..., 0.0504, 0.0686, 0.0317],
        [0.0481, 0.0288, 0.0253,  ..., 0.0568, 0.0357, 0.0538],
        [0.0382, 0.0115, 0.0496,  ..., 0.0688, 0.0139, 0.0960],
        ...,
        [0.0570, 0.0688, 0.0040,  ..., 0.0486, 0.0935, 0.0175],
        [0.0570, 0.0688, 0.0040,  ..., 0.0486, 0.0935, 0.0176],
        [0.0570, 0.0688, 0.0040,  ..., 0.0486, 0.0934, 0.0175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555319.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6714.6987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(809.2625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(303.9203, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.5756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3187],
        [-0.0512],
        [ 0.1369],
        ...,
        [-1.4466],
        [-1.4424],
        [-1.4409]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176003.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0138],
        [1.0193],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369007.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0139],
        [1.0193],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369018.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0160, -0.0046,  0.0097,  ...,  0.0096, -0.0056,  0.0313],
        [ 0.0087, -0.0026,  0.0052,  ...,  0.0061, -0.0032,  0.0178],
        [ 0.0129, -0.0037,  0.0078,  ...,  0.0081, -0.0046,  0.0254],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [ 0.0074, -0.0023,  0.0044,  ...,  0.0055, -0.0028,  0.0154],
        [ 0.0074, -0.0023,  0.0044,  ...,  0.0055, -0.0028,  0.0154]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1343.7600, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-14.4818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6197, device='cuda:0')



h[200].sum tensor(53.5403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2156, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0207, 0.0000, 0.0124,  ..., 0.0173, 0.0000, 0.0435],
        [0.0473, 0.0000, 0.0286,  ..., 0.0304, 0.0000, 0.0943],
        [0.0263, 0.0000, 0.0158,  ..., 0.0200, 0.0000, 0.0537],
        ...,
        [0.0137, 0.0000, 0.0082,  ..., 0.0137, 0.0000, 0.0289],
        [0.0137, 0.0000, 0.0082,  ..., 0.0137, 0.0000, 0.0289],
        [0.0137, 0.0000, 0.0082,  ..., 0.0137, 0.0000, 0.0289]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53369.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0245, 0.0000, 0.0740,  ..., 0.0849, 0.0000, 0.1432],
        [0.0161, 0.0000, 0.0869,  ..., 0.0951, 0.0000, 0.1700],
        [0.0246, 0.0000, 0.0733,  ..., 0.0855, 0.0000, 0.1429],
        ...,
        [0.0447, 0.0189, 0.0367,  ..., 0.0642, 0.0235, 0.0739],
        [0.0420, 0.0076, 0.0436,  ..., 0.0674, 0.0083, 0.0857],
        [0.0420, 0.0076, 0.0435,  ..., 0.0674, 0.0083, 0.0857]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508511.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6909.1709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(757.5478, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(245.8880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(278.8911, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2458],
        [ 0.2636],
        [ 0.2523],
        ...,
        [-0.6413],
        [-0.4526],
        [-0.4516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224313.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0139],
        [1.0193],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369018.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0139],
        [1.0193],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369018.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1148.7998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-12.2276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5247, device='cuda:0')



h[200].sum tensor(50.8010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49910.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0556, 0.0673, 0.0036,  ..., 0.0478, 0.0910, 0.0174],
        [0.0559, 0.0672, 0.0040,  ..., 0.0478, 0.0905, 0.0180],
        [0.0555, 0.0616, 0.0082,  ..., 0.0488, 0.0813, 0.0244],
        ...,
        [0.0574, 0.0690, 0.0040,  ..., 0.0489, 0.0937, 0.0178],
        [0.0574, 0.0690, 0.0040,  ..., 0.0489, 0.0937, 0.0178],
        [0.0574, 0.0690, 0.0040,  ..., 0.0489, 0.0937, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496530.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6942.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(742.9612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(223.9447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(261.8121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3258],
        [-1.1246],
        [-0.8363],
        ...,
        [-1.4547],
        [-1.4507],
        [-1.4495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234150.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0139],
        [1.0193],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369018.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0140],
        [1.0194],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369028.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [ 0.0032, -0.0011,  0.0019,  ...,  0.0035, -0.0014,  0.0077],
        [ 0.0060, -0.0019,  0.0036,  ...,  0.0048, -0.0023,  0.0128],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1344.3887, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0168, device='cuda:0')



h[100].sum tensor(-14.4381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6225, device='cuda:0')



h[200].sum tensor(53.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0357, 0.0000, 0.0215,  ..., 0.0251, 0.0000, 0.0730],
        [0.0119, 0.0000, 0.0071,  ..., 0.0129, 0.0000, 0.0255],
        [0.0062, 0.0000, 0.0037,  ..., 0.0097, 0.0000, 0.0133]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55896.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0512, 0.0430, 0.0163,  ..., 0.0542, 0.0546, 0.0400],
        [0.0556, 0.0633, 0.0063,  ..., 0.0492, 0.0847, 0.0222],
        [0.0564, 0.0679, 0.0036,  ..., 0.0483, 0.0919, 0.0177],
        ...,
        [0.0240, 0.0000, 0.0816,  ..., 0.0904, 0.0000, 0.1570],
        [0.0366, 0.0096, 0.0545,  ..., 0.0753, 0.0113, 0.1079],
        [0.0470, 0.0276, 0.0296,  ..., 0.0625, 0.0365, 0.0637]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529872.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6910.4043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(770.3299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.2717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.0633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3802],
        [-0.7804],
        [-1.1225],
        ...,
        [ 0.1819],
        [-0.0748],
        [-0.4889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215819.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0140],
        [1.0194],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369028.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0140],
        [1.0195],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369039.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1040.4180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0127, device='cuda:0')



h[100].sum tensor(-10.9230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4716, device='cuda:0')



h[200].sum tensor(48.8837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48255.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0564, 0.0679, 0.0032,  ..., 0.0485, 0.0919, 0.0178],
        [0.0566, 0.0681, 0.0032,  ..., 0.0486, 0.0921, 0.0178],
        [0.0568, 0.0682, 0.0033,  ..., 0.0487, 0.0924, 0.0179],
        ...,
        [0.0556, 0.0544, 0.0122,  ..., 0.0531, 0.0717, 0.0325],
        [0.0574, 0.0646, 0.0065,  ..., 0.0508, 0.0870, 0.0230],
        [0.0582, 0.0697, 0.0036,  ..., 0.0496, 0.0946, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(491185.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7136.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(740.1386, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.0018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(253.6128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3066],
        [-1.1801],
        [-0.9723],
        ...,
        [-0.9270],
        [-1.1583],
        [-1.3348]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227986.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0140],
        [1.0195],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369039.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0141],
        [1.0196],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369049.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [ 0.0115, -0.0033,  0.0069,  ...,  0.0074, -0.0041,  0.0229]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2295.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0296, device='cuda:0')



h[100].sum tensor(-25.3290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0956, device='cuda:0')



h[200].sum tensor(66.4244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.8277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0119, 0.0000, 0.0072,  ..., 0.0124, 0.0000, 0.0238],
        [0.0215, 0.0000, 0.0130,  ..., 0.0175, 0.0000, 0.0432]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74736.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0567, 0.0684, 0.0029,  ..., 0.0487, 0.0925, 0.0178],
        [0.0551, 0.0583, 0.0085,  ..., 0.0512, 0.0774, 0.0273],
        [0.0475, 0.0326, 0.0253,  ..., 0.0612, 0.0424, 0.0587],
        ...,
        [0.0540, 0.0493, 0.0127,  ..., 0.0555, 0.0651, 0.0359],
        [0.0415, 0.0232, 0.0362,  ..., 0.0710, 0.0304, 0.0816],
        [0.0262, 0.0049, 0.0646,  ..., 0.0899, 0.0048, 0.1371]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615128.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6677.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(854.2108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.2171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(387.7336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0794],
        [-0.7498],
        [-0.3292],
        ...,
        [-1.0300],
        [-0.6511],
        [-0.2680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204448.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0141],
        [1.0196],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369049.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(459.3038, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0142],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369060.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1002.7194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0122, device='cuda:0')



h[100].sum tensor(-10.4788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4524, device='cuda:0')



h[200].sum tensor(48.3470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0024,  ..., 0.0092, 0.0000, 0.0113],
        [0.0021, 0.0000, 0.0012,  ..., 0.0077, 0.0000, 0.0057],
        [0.0262, 0.0000, 0.0159,  ..., 0.0196, 0.0000, 0.0519],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46999.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0543, 0.0348, 0.0263,  ..., 0.0551, 0.0378, 0.0562],
        [0.0482, 0.0219, 0.0344,  ..., 0.0624, 0.0255, 0.0731],
        [0.0283, 0.0030, 0.0681,  ..., 0.0870, 0.0011, 0.1404],
        ...,
        [0.0589, 0.0706, 0.0031,  ..., 0.0500, 0.0957, 0.0184],
        [0.0589, 0.0706, 0.0031,  ..., 0.0500, 0.0957, 0.0184],
        [0.0589, 0.0706, 0.0031,  ..., 0.0500, 0.0956, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489078.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7194.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(732.9680, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(213.5159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(250.6168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4338],
        [-0.0971],
        [ 0.1541],
        ...,
        [-1.5073],
        [-1.5031],
        [-1.5017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267185.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0142],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369060.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369070.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2128.7632, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0273, device='cuda:0')



h[100].sum tensor(-23.2963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0107, device='cuda:0')



h[200].sum tensor(64.2105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2917, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0151, 0.0000, 0.0091,  ..., 0.0143, 0.0000, 0.0314],
        [0.0157, 0.0000, 0.0094,  ..., 0.0146, 0.0000, 0.0325],
        [0.0051, 0.0000, 0.0030,  ..., 0.0091, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68863.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0432, 0.0057, 0.0466,  ..., 0.0688, 0.0044, 0.0939],
        [0.0430, 0.0054, 0.0466,  ..., 0.0692, 0.0039, 0.0942],
        [0.0489, 0.0221, 0.0311,  ..., 0.0615, 0.0268, 0.0666],
        ...,
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0960, 0.0185],
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0960, 0.0185],
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0959, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586304.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6949.5732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(837.2565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(348.4216, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.7652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2366],
        [-0.2975],
        [-0.5341],
        ...,
        [-1.5189],
        [-1.5146],
        [-1.5131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193471.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369070.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369070.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044, -0.0015,  0.0026,  ...,  0.0041, -0.0018,  0.0099],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2042.3263, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0259, device='cuda:0')



h[100].sum tensor(-22.3095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9581, device='cuda:0')



h[200].sum tensor(63.0035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.3395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0000, 0.0097,  ..., 0.0148, 0.0000, 0.0332],
        [0.0045, 0.0000, 0.0027,  ..., 0.0088, 0.0000, 0.0100],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71554.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0393, 0.0068, 0.0532,  ..., 0.0735, 0.0071, 0.1072],
        [0.0463, 0.0146, 0.0354,  ..., 0.0644, 0.0170, 0.0753],
        [0.0479, 0.0194, 0.0276,  ..., 0.0620, 0.0227, 0.0627],
        ...,
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0960, 0.0185],
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0960, 0.0185],
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0959, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614119.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6882.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(834.5543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(371.2440, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(372.4783, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2243],
        [ 0.1435],
        [ 0.0604],
        ...,
        [-1.5136],
        [-1.5092],
        [-1.5077]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235375.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369070.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369070.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0090, -0.0027,  0.0055,  ...,  0.0063, -0.0033,  0.0184],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1378.2361, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0171, device='cuda:0')



h[100].sum tensor(-14.7272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6325, device='cuda:0')



h[200].sum tensor(53.7306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0436, 0.0000, 0.0265,  ..., 0.0283, 0.0000, 0.0858],
        [0.0151, 0.0000, 0.0091,  ..., 0.0143, 0.0000, 0.0314],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55966.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0054, 0.0000, 0.1208,  ..., 0.1270, 0.0000, 0.2494],
        [0.0161, 0.0000, 0.0870,  ..., 0.1024, 0.0000, 0.1802],
        [0.0286, 0.0000, 0.0624,  ..., 0.0865, 0.0000, 0.1324],
        ...,
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0960, 0.0185],
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0960, 0.0185],
        [0.0592, 0.0710, 0.0029,  ..., 0.0501, 0.0959, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528791.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7159.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(785.7250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(266.0527, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.8973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2766],
        [ 0.2816],
        [ 0.2872],
        ...,
        [-1.5189],
        [-1.5146],
        [-1.5132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207984.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0143],
        [1.0197],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369070.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0143],
        [1.0198],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369081.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2072.3665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0263, device='cuda:0')



h[100].sum tensor(-22.5696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9741, device='cuda:0')



h[200].sum tensor(63.6620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.6298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72999.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0579, 0.0688, 0.0036,  ..., 0.0488, 0.0914, 0.0199],
        [0.0578, 0.0695, 0.0028,  ..., 0.0491, 0.0929, 0.0188],
        [0.0579, 0.0699, 0.0025,  ..., 0.0492, 0.0939, 0.0183],
        ...,
        [0.0594, 0.0714, 0.0028,  ..., 0.0502, 0.0962, 0.0187],
        [0.0594, 0.0714, 0.0028,  ..., 0.0502, 0.0962, 0.0187],
        [0.0594, 0.0714, 0.0028,  ..., 0.0502, 0.0962, 0.0186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631140.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6831.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.7917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(380.8285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(381.4757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8458],
        [-1.0934],
        [-1.3148],
        ...,
        [-1.5296],
        [-1.5253],
        [-1.5239]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240649.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0143],
        [1.0198],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369081.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0144],
        [1.0199],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369092.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0012,  0.0020,  ...,  0.0036, -0.0014,  0.0080],
        [ 0.0113, -0.0032,  0.0068,  ...,  0.0074, -0.0040,  0.0226],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1397.3529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-14.7766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6457, device='cuda:0')



h[200].sum tensor(54.6545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0623, 0.0000, 0.0378,  ..., 0.0376, 0.0000, 0.1220],
        [0.0162, 0.0000, 0.0098,  ..., 0.0149, 0.0000, 0.0335],
        [0.0114, 0.0000, 0.0069,  ..., 0.0121, 0.0000, 0.0229],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54583.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0115, 0.0000, 0.1035,  ..., 0.1114, 0.0000, 0.2114],
        [0.0321, 0.0061, 0.0624,  ..., 0.0839, 0.0062, 0.1307],
        [0.0438, 0.0222, 0.0368,  ..., 0.0685, 0.0278, 0.0825],
        ...,
        [0.0597, 0.0718, 0.0028,  ..., 0.0503, 0.0962, 0.0189],
        [0.0597, 0.0718, 0.0028,  ..., 0.0503, 0.0962, 0.0189],
        [0.0597, 0.0717, 0.0028,  ..., 0.0502, 0.0962, 0.0189]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(521791.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7245.4814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(779.3813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(258.2747, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.1881, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2705],
        [ 0.1105],
        [-0.1800],
        ...,
        [-1.5283],
        [-1.5304],
        [-1.5303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223836.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0144],
        [1.0199],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369092.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0144],
        [1.0199],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369103.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [ 0.0058, -0.0018,  0.0034,  ...,  0.0048, -0.0022,  0.0124],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1765.3215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0224, device='cuda:0')



h[100].sum tensor(-18.8237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8298, device='cuda:0')



h[200].sum tensor(60.4795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0208, 0.0000, 0.0124,  ..., 0.0180, 0.0000, 0.0455],
        [0.0046, 0.0000, 0.0027,  ..., 0.0089, 0.0000, 0.0102],
        [0.0212, 0.0000, 0.0128,  ..., 0.0172, 0.0000, 0.0427],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59990.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0480, 0.0091, 0.0365,  ..., 0.0635, 0.0062, 0.0764],
        [0.0445, 0.0120, 0.0369,  ..., 0.0678, 0.0124, 0.0820],
        [0.0293, 0.0000, 0.0645,  ..., 0.0879, 0.0000, 0.1384],
        ...,
        [0.0598, 0.0721, 0.0028,  ..., 0.0501, 0.0961, 0.0191],
        [0.0597, 0.0720, 0.0028,  ..., 0.0501, 0.0961, 0.0191],
        [0.0597, 0.0720, 0.0028,  ..., 0.0501, 0.0961, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538833., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7126.5474, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(800.3637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.2342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(321.6565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6024],
        [-0.3379],
        [-0.0537],
        ...,
        [-1.5442],
        [-1.5399],
        [-1.5385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229602.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0144],
        [1.0199],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369103.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0145],
        [1.0200],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369114., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0016,  0.0030,  ...,  0.0044, -0.0020,  0.0110],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1318.9476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0160, device='cuda:0')



h[100].sum tensor(-13.5848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5916, device='cuda:0')



h[200].sum tensor(54.8943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0000, 0.0101,  ..., 0.0151, 0.0000, 0.0344],
        [0.0050, 0.0000, 0.0030,  ..., 0.0091, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54222.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0249, 0.0053, 0.0778,  ..., 0.0941, 0.0050, 0.1609],
        [0.0403, 0.0162, 0.0454,  ..., 0.0735, 0.0205, 0.0982],
        [0.0477, 0.0233, 0.0264,  ..., 0.0635, 0.0281, 0.0639],
        ...,
        [0.0598, 0.0721, 0.0030,  ..., 0.0501, 0.0957, 0.0195],
        [0.0598, 0.0721, 0.0030,  ..., 0.0501, 0.0957, 0.0195],
        [0.0598, 0.0721, 0.0030,  ..., 0.0501, 0.0957, 0.0194]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523683.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7221.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(779.3181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(253.7435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.6830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1693],
        [ 0.1395],
        [ 0.1183],
        ...,
        [-1.5391],
        [-1.5346],
        [-1.5331]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224480.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0145],
        [1.0200],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369114., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0146],
        [1.0201],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369124.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1584.4172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0192, device='cuda:0')



h[100].sum tensor(-16.4109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7121, device='cuda:0')



h[200].sum tensor(59.0140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0085, 0.0000, 0.0050,  ..., 0.0112, 0.0000, 0.0194],
        [0.0021, 0.0000, 0.0012,  ..., 0.0077, 0.0000, 0.0057],
        [0.0048, 0.0000, 0.0028,  ..., 0.0090, 0.0000, 0.0106],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60069.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0511, 0.0167, 0.0391,  ..., 0.0617, 0.0168, 0.0792],
        [0.0525, 0.0191, 0.0338,  ..., 0.0598, 0.0183, 0.0703],
        [0.0512, 0.0130, 0.0367,  ..., 0.0616, 0.0110, 0.0752],
        ...,
        [0.0600, 0.0723, 0.0031,  ..., 0.0503, 0.0955, 0.0198],
        [0.0600, 0.0723, 0.0031,  ..., 0.0503, 0.0955, 0.0198],
        [0.0599, 0.0723, 0.0031,  ..., 0.0503, 0.0955, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553353.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7134.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(805.3465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(289.5631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.4897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0692],
        [ 0.1609],
        [ 0.2089],
        ...,
        [-1.5478],
        [-1.5435],
        [-1.5421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211215.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0146],
        [1.0201],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369124.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0147],
        [1.0202],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369135.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [ 0.0036, -0.0012,  0.0021,  ...,  0.0037, -0.0015,  0.0084],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1726.9934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-17.8712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7740, device='cuda:0')



h[200].sum tensor(61.0041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0083, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0105, 0.0000, 0.0063,  ..., 0.0122, 0.0000, 0.0232],
        [0.0136, 0.0000, 0.0081,  ..., 0.0142, 0.0000, 0.0308],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61218.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0473, 0.0063, 0.0393,  ..., 0.0666, 0.0047, 0.0839],
        [0.0478, 0.0053, 0.0477,  ..., 0.0674, 0.0037, 0.0949],
        [0.0451, 0.0057, 0.0599,  ..., 0.0725, 0.0047, 0.1155],
        ...,
        [0.0601, 0.0725, 0.0029,  ..., 0.0507, 0.0957, 0.0200],
        [0.0601, 0.0725, 0.0029,  ..., 0.0507, 0.0957, 0.0200],
        [0.0601, 0.0724, 0.0029,  ..., 0.0506, 0.0956, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554671.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7074.9189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(807.7989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(297.7057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.0749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3020],
        [ 0.3053],
        [ 0.3113],
        ...,
        [-1.5559],
        [-1.5516],
        [-1.5502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223479.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0147],
        [1.0202],
        ...,
        [1.0006],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369135.2812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(412.9435, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0147],
        [1.0203],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369145.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0227, -0.0062,  0.0138,  ...,  0.0127, -0.0076,  0.0437],
        [ 0.0149, -0.0041,  0.0091,  ...,  0.0091, -0.0051,  0.0294],
        [ 0.0074, -0.0022,  0.0045,  ...,  0.0055, -0.0027,  0.0155],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1556.7256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0186, device='cuda:0')



h[100].sum tensor(-15.8120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6877, device='cuda:0')



h[200].sum tensor(58.6167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4463, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0686, 0.0000, 0.0416,  ..., 0.0406, 0.0000, 0.1338],
        [0.0675, 0.0000, 0.0409,  ..., 0.0401, 0.0000, 0.1317],
        [0.0332, 0.0000, 0.0201,  ..., 0.0229, 0.0000, 0.0649],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58160.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1446,  ..., 0.1448, 0.0000, 0.3001],
        [0.0047, 0.0000, 0.1366,  ..., 0.1384, 0.0000, 0.2830],
        [0.0166, 0.0000, 0.1001,  ..., 0.1121, 0.0000, 0.2084],
        ...,
        [0.0604, 0.0726, 0.0028,  ..., 0.0511, 0.0958, 0.0202],
        [0.0604, 0.0726, 0.0028,  ..., 0.0511, 0.0958, 0.0202],
        [0.0604, 0.0726, 0.0028,  ..., 0.0511, 0.0957, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543420.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7228.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(797.9606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(276.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.8960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2764],
        [ 0.2739],
        [ 0.2606],
        ...,
        [-1.5617],
        [-1.5574],
        [-1.5560]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220763.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0147],
        [1.0203],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369145.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0148],
        [1.0203],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369156.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1717.5623, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0208, device='cuda:0')



h[100].sum tensor(-17.4857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7715, device='cuda:0')



h[200].sum tensor(60.7011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9632, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60550.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0588, 0.0709, 0.0024,  ..., 0.0504, 0.0931, 0.0199],
        [0.0590, 0.0710, 0.0024,  ..., 0.0505, 0.0933, 0.0200],
        [0.0592, 0.0712, 0.0025,  ..., 0.0505, 0.0936, 0.0200],
        ...,
        [0.0607, 0.0727, 0.0028,  ..., 0.0516, 0.0960, 0.0205],
        [0.0607, 0.0727, 0.0028,  ..., 0.0516, 0.0960, 0.0205],
        [0.0607, 0.0727, 0.0028,  ..., 0.0516, 0.0959, 0.0205]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552902.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7256.5913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(802.5413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.0988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(326.4670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6840],
        [-1.6083],
        [-1.4768],
        ...,
        [-1.5670],
        [-1.5626],
        [-1.5612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218525.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0148],
        [1.0203],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369156.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0148],
        [1.0204],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369166.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0001],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0001],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1357.1033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-13.3379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5782, device='cuda:0')



h[200].sum tensor(55.4727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54616.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0567, 0.0539, 0.0107,  ..., 0.0550, 0.0691, 0.0351],
        [0.0593, 0.0698, 0.0033,  ..., 0.0513, 0.0914, 0.0217],
        [0.0596, 0.0713, 0.0025,  ..., 0.0510, 0.0938, 0.0203],
        ...,
        [0.0612, 0.0729, 0.0028,  ..., 0.0521, 0.0962, 0.0208],
        [0.0612, 0.0729, 0.0028,  ..., 0.0521, 0.0962, 0.0208],
        [0.0612, 0.0728, 0.0028,  ..., 0.0521, 0.0961, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529033.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7387.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(776.4794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(255.5087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(295.3558, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7289],
        [-1.0299],
        [-1.2032],
        ...,
        [-1.5713],
        [-1.5667],
        [-1.5652]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227974.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0148],
        [1.0204],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369166.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0149],
        [1.0205],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369177.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.1098e-03, -2.1102e-03,  4.2752e-03,  ...,  5.3899e-03,
         -2.6095e-03,  1.5058e-02],
        [ 8.4786e-03, -2.4614e-03,  5.1124e-03,  ...,  6.0351e-03,
         -3.0438e-03,  1.7579e-02],
        [-1.1134e-03,  0.0000e+00, -7.5422e-04,  ...,  1.5137e-03,
          0.0000e+00, -8.2518e-05],
        ...,
        [-1.1134e-03,  0.0000e+00, -7.5422e-04,  ...,  1.5137e-03,
          0.0000e+00, -8.2518e-05],
        [-1.1134e-03,  0.0000e+00, -7.5422e-04,  ...,  1.5137e-03,
          0.0000e+00, -8.2518e-05],
        [-1.1134e-03,  0.0000e+00, -7.5422e-04,  ...,  1.5137e-03,
          0.0000e+00, -8.2518e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1826.2859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0218, device='cuda:0')



h[100].sum tensor(-18.4810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8057, device='cuda:0')



h[200].sum tensor(61.5925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5816, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0788, 0.0000, 0.0479,  ..., 0.0454, 0.0000, 0.1531],
        [0.0268, 0.0000, 0.0163,  ..., 0.0199, 0.0000, 0.0534],
        [0.0086, 0.0000, 0.0052,  ..., 0.0107, 0.0000, 0.0178],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62691.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0000, 0.1931,  ..., 0.1914, 0.0000, 0.4158],
        [0.0170, 0.0000, 0.1153,  ..., 0.1309, 0.0000, 0.2497],
        [0.0346, 0.0121, 0.0619,  ..., 0.0904, 0.0147, 0.1371],
        ...,
        [0.0617, 0.0731, 0.0027,  ..., 0.0527, 0.0965, 0.0211],
        [0.0617, 0.0731, 0.0027,  ..., 0.0527, 0.0965, 0.0211],
        [0.0617, 0.0731, 0.0027,  ..., 0.0527, 0.0965, 0.0210]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562092.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7403.6436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(810.1135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(306.3409, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.0624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0576],
        [ 0.1034],
        [ 0.1225],
        ...,
        [-1.5682],
        [-1.5641],
        [-1.5633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210113.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0149],
        [1.0205],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369177.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0150],
        [1.0206],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369187.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1194e-03,  0.0000e+00, -7.5788e-04,  ...,  1.5120e-03,
          0.0000e+00, -6.2099e-05],
        [-1.1194e-03,  0.0000e+00, -7.5788e-04,  ...,  1.5120e-03,
          0.0000e+00, -6.2099e-05],
        [-1.1194e-03,  0.0000e+00, -7.5788e-04,  ...,  1.5120e-03,
          0.0000e+00, -6.2099e-05],
        ...,
        [-1.1194e-03,  0.0000e+00, -7.5788e-04,  ...,  1.5120e-03,
          0.0000e+00, -6.2099e-05],
        [-1.1194e-03,  0.0000e+00, -7.5788e-04,  ...,  1.5120e-03,
          0.0000e+00, -6.2099e-05],
        [-1.1194e-03,  0.0000e+00, -7.5788e-04,  ...,  1.5120e-03,
          0.0000e+00, -6.2099e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1533.0338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-15.1304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6607, device='cuda:0')



h[200].sum tensor(57.3212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9568, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0062, 0.0000, 0.0036,  ..., 0.0101, 0.0000, 0.0155],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58778.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0483, 0.0206, 0.0365,  ..., 0.0712, 0.0239, 0.0845],
        [0.0433, 0.0114, 0.0500,  ..., 0.0795, 0.0137, 0.1101],
        [0.0386, 0.0061, 0.0682,  ..., 0.0882, 0.0061, 0.1422],
        ...,
        [0.0622, 0.0733, 0.0027,  ..., 0.0531, 0.0968, 0.0213],
        [0.0622, 0.0733, 0.0027,  ..., 0.0531, 0.0968, 0.0213],
        [0.0621, 0.0733, 0.0027,  ..., 0.0531, 0.0968, 0.0213]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548110.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7543.7476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(795.0468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.7489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.4766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1304],
        [ 0.1754],
        [ 0.2066],
        ...,
        [-1.5864],
        [-1.5819],
        [-1.5804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202092.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0150],
        [1.0206],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369187.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0150],
        [1.0207],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369198.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1224e-03,  0.0000e+00, -7.5877e-04,  ...,  1.5114e-03,
          0.0000e+00, -4.4533e-05],
        [-1.1224e-03,  0.0000e+00, -7.5877e-04,  ...,  1.5114e-03,
          0.0000e+00, -4.4533e-05],
        [ 1.5306e-02, -4.1903e-03,  9.2898e-03,  ...,  9.2539e-03,
         -5.1862e-03,  3.0207e-02],
        ...,
        [-1.1224e-03,  0.0000e+00, -7.5877e-04,  ...,  1.5114e-03,
          0.0000e+00, -4.4533e-05],
        [-1.1224e-03,  0.0000e+00, -7.5877e-04,  ...,  1.5114e-03,
          0.0000e+00, -4.4533e-05],
        [-1.1224e-03,  0.0000e+00, -7.5877e-04,  ...,  1.5114e-03,
          0.0000e+00, -4.4533e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1362.7271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0157, device='cuda:0')



h[100].sum tensor(-13.1574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5806, device='cuda:0')



h[200].sum tensor(54.7941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0155, 0.0000, 0.0094,  ..., 0.0140, 0.0000, 0.0307],
        [0.0125, 0.0000, 0.0076,  ..., 0.0126, 0.0000, 0.0251],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52589.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0567, 0.0490, 0.0114,  ..., 0.0582, 0.0634, 0.0389],
        [0.0473, 0.0208, 0.0333,  ..., 0.0733, 0.0266, 0.0826],
        [0.0419, 0.0071, 0.0462,  ..., 0.0820, 0.0080, 0.1081],
        ...,
        [0.0626, 0.0736, 0.0026,  ..., 0.0535, 0.0972, 0.0214],
        [0.0626, 0.0735, 0.0026,  ..., 0.0535, 0.0972, 0.0214],
        [0.0625, 0.0735, 0.0026,  ..., 0.0535, 0.0972, 0.0214]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515040.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7745.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(766.4089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(243.3591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(280.3094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2678],
        [-0.9554],
        [-0.6492],
        ...,
        [-1.5953],
        [-1.5910],
        [-1.5897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220425.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0150],
        [1.0207],
        ...,
        [1.0006],
        [0.9998],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369198.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0151],
        [1.0208],
        ...,
        [1.0006],
        [0.9998],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369208.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2531e-02, -3.4719e-03,  7.5952e-03,  ...,  7.9434e-03,
         -4.2988e-03,  2.5112e-02],
        [ 9.5972e-03, -2.7258e-03,  5.8006e-03,  ...,  6.5609e-03,
         -3.3751e-03,  1.9709e-02],
        [ 2.8519e-02, -7.5371e-03,  1.7374e-02,  ...,  1.5477e-02,
         -9.3324e-03,  5.4551e-02],
        ...,
        [-1.1227e-03,  0.0000e+00, -7.5631e-04,  ...,  1.5095e-03,
          0.0000e+00, -3.0046e-05],
        [-1.1227e-03,  0.0000e+00, -7.5631e-04,  ...,  1.5095e-03,
          0.0000e+00, -3.0046e-05],
        [-1.1227e-03,  0.0000e+00, -7.5631e-04,  ...,  1.5095e-03,
          0.0000e+00, -3.0046e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1692.0891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-16.6953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7279, device='cuda:0')



h[200].sum tensor(59.2013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1727, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0287, 0.0000, 0.0174,  ..., 0.0207, 0.0000, 0.0570],
        [0.0792, 0.0000, 0.0482,  ..., 0.0456, 0.0000, 0.1541],
        [0.0742, 0.0000, 0.0451,  ..., 0.0432, 0.0000, 0.1448],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59618.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0241, 0.0000, 0.0865,  ..., 0.1108, 0.0000, 0.1906],
        [0.0067, 0.0000, 0.1459,  ..., 0.1534, 0.0000, 0.3121],
        [0.0007, 0.0000, 0.1704,  ..., 0.1690, 0.0000, 0.3597],
        ...,
        [0.0629, 0.0739, 0.0026,  ..., 0.0538, 0.0975, 0.0216],
        [0.0629, 0.0738, 0.0026,  ..., 0.0538, 0.0975, 0.0216],
        [0.0629, 0.0738, 0.0026,  ..., 0.0538, 0.0975, 0.0216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550202.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7636.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(789.2360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(291.9739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.0503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0590],
        [ 0.2340],
        [ 0.2907],
        ...,
        [-1.6054],
        [-1.6009],
        [-1.5995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232999.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0151],
        [1.0208],
        ...,
        [1.0006],
        [0.9998],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369208.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0152],
        [1.0209],
        ...,
        [1.0006],
        [0.9998],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369218.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9569e-03, -2.0480e-03,  4.1877e-03,  ...,  5.3156e-03,
         -2.5370e-03,  1.4859e-02],
        [-1.1222e-03,  0.0000e+00, -7.5407e-04,  ...,  1.5090e-03,
          0.0000e+00, -1.7804e-05],
        [ 3.1089e-03, -1.0726e-03,  1.8340e-03,  ...,  3.5026e-03,
         -1.3286e-03,  7.7735e-03],
        ...,
        [ 1.2925e-02, -3.5610e-03,  7.8384e-03,  ...,  8.1278e-03,
         -4.4111e-03,  2.5850e-02],
        [-1.1222e-03,  0.0000e+00, -7.5407e-04,  ...,  1.5090e-03,
          0.0000e+00, -1.7804e-05],
        [-1.1222e-03,  0.0000e+00, -7.5407e-04,  ...,  1.5090e-03,
          0.0000e+00, -1.7804e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2139.6763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0253, device='cuda:0')



h[100].sum tensor(-21.5091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9353, device='cuda:0')



h[200].sum tensor(65.4082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.9270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0000, 0.0159,  ..., 0.0201, 0.0000, 0.0547],
        [0.0181, 0.0000, 0.0108,  ..., 0.0168, 0.0000, 0.0417],
        [0.0086, 0.0000, 0.0051,  ..., 0.0118, 0.0000, 0.0221],
        ...,
        [0.0108, 0.0000, 0.0065,  ..., 0.0119, 0.0000, 0.0220],
        [0.0135, 0.0000, 0.0082,  ..., 0.0132, 0.0000, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67119.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0311, 0.0000, 0.0955,  ..., 0.1050, 0.0000, 0.1946],
        [0.0427, 0.0000, 0.0779,  ..., 0.0885, 0.0000, 0.1548],
        [0.0513, 0.0000, 0.0605,  ..., 0.0758, 0.0000, 0.1193],
        ...,
        [0.0501, 0.0110, 0.0377,  ..., 0.0758, 0.0134, 0.0889],
        [0.0528, 0.0256, 0.0296,  ..., 0.0711, 0.0334, 0.0739],
        [0.0599, 0.0537, 0.0110,  ..., 0.0594, 0.0700, 0.0380]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582363.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7623.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(821.8872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(339.5500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(351.7082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3549],
        [ 0.3675],
        [ 0.3701],
        ...,
        [-0.6129],
        [-0.8682],
        [-1.1751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224070.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0152],
        [1.0209],
        ...,
        [1.0006],
        [0.9998],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369218.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0153],
        [1.0210],
        ...,
        [1.0006],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369228.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1173e-03,  0.0000e+00, -7.5059e-04,  ...,  1.5128e-03,
          0.0000e+00, -1.0512e-05],
        [-1.1173e-03,  0.0000e+00, -7.5059e-04,  ...,  1.5128e-03,
          0.0000e+00, -1.0512e-05],
        [-1.1173e-03,  0.0000e+00, -7.5059e-04,  ...,  1.5128e-03,
          0.0000e+00, -1.0512e-05],
        ...,
        [-1.1173e-03,  0.0000e+00, -7.5059e-04,  ...,  1.5128e-03,
          0.0000e+00, -1.0512e-05],
        [-1.1173e-03,  0.0000e+00, -7.5059e-04,  ...,  1.5128e-03,
          0.0000e+00, -1.0512e-05],
        [-1.1173e-03,  0.0000e+00, -7.5059e-04,  ...,  1.5128e-03,
          0.0000e+00, -1.0512e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1732.3589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0202, device='cuda:0')



h[100].sum tensor(-16.9500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7487, device='cuda:0')



h[200].sum tensor(59.8931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5496, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62243.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0613, 0.0726, 0.0020,  ..., 0.0529, 0.0952, 0.0211],
        [0.0615, 0.0727, 0.0020,  ..., 0.0530, 0.0954, 0.0211],
        [0.0617, 0.0729, 0.0021,  ..., 0.0531, 0.0957, 0.0211],
        ...,
        [0.0633, 0.0744, 0.0022,  ..., 0.0542, 0.0982, 0.0216],
        [0.0633, 0.0744, 0.0022,  ..., 0.0542, 0.0981, 0.0216],
        [0.0633, 0.0744, 0.0022,  ..., 0.0542, 0.0981, 0.0216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571583.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7646.6377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(807.7587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(309.5309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.8253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8055],
        [-1.8135],
        [-1.8065],
        ...,
        [-1.6290],
        [-1.6245],
        [-1.6231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228287.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0153],
        [1.0210],
        ...,
        [1.0006],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369228.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0154],
        [1.0211],
        ...,
        [1.0006],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369238.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1103e-03,  0.0000e+00, -7.4187e-04,  ...,  1.5221e-03,
          0.0000e+00, -7.5164e-06],
        [-1.1103e-03,  0.0000e+00, -7.4187e-04,  ...,  1.5221e-03,
          0.0000e+00, -7.5164e-06],
        [-1.1103e-03,  0.0000e+00, -7.4187e-04,  ...,  1.5221e-03,
          0.0000e+00, -7.5164e-06],
        ...,
        [-1.1103e-03,  0.0000e+00, -7.4187e-04,  ...,  1.5221e-03,
          0.0000e+00, -7.5164e-06],
        [-1.1103e-03,  0.0000e+00, -7.4187e-04,  ...,  1.5221e-03,
          0.0000e+00, -7.5164e-06],
        [-1.1103e-03,  0.0000e+00, -7.4187e-04,  ...,  1.5221e-03,
          0.0000e+00, -7.5164e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2146.1248, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0254, device='cuda:0')



h[100].sum tensor(-21.4244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9388, device='cuda:0')



h[200].sum tensor(65.6365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.9908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68704.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0730, 0.0018,  ..., 0.0530, 0.0956, 0.0210],
        [0.0617, 0.0730, 0.0020,  ..., 0.0531, 0.0955, 0.0214],
        [0.0620, 0.0725, 0.0028,  ..., 0.0531, 0.0945, 0.0224],
        ...,
        [0.0635, 0.0749, 0.0020,  ..., 0.0543, 0.0986, 0.0216],
        [0.0635, 0.0749, 0.0020,  ..., 0.0543, 0.0986, 0.0216],
        [0.0635, 0.0749, 0.0020,  ..., 0.0543, 0.0986, 0.0216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595454.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7635.5400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(836.8629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(351.6736, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(367.5780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6786],
        [-1.5635],
        [-1.3939],
        ...,
        [-1.6454],
        [-1.6408],
        [-1.6393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224809.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0154],
        [1.0211],
        ...,
        [1.0006],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369238.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(515.4195, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0155],
        [1.0212],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369248.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1071e-03,  0.0000e+00, -7.3806e-04,  ...,  1.5311e-03,
          0.0000e+00,  6.4099e-07],
        [-1.1071e-03,  0.0000e+00, -7.3806e-04,  ...,  1.5311e-03,
          0.0000e+00,  6.4099e-07],
        [-1.1071e-03,  0.0000e+00, -7.3806e-04,  ...,  1.5311e-03,
          0.0000e+00,  6.4099e-07],
        ...,
        [ 6.1836e-03, -1.8314e-03,  3.7220e-03,  ...,  4.9675e-03,
         -2.2716e-03,  1.3429e-02],
        [-1.1071e-03,  0.0000e+00, -7.3806e-04,  ...,  1.5311e-03,
          0.0000e+00,  6.4099e-07],
        [-1.1071e-03,  0.0000e+00, -7.3806e-04,  ...,  1.5311e-03,
          0.0000e+00,  6.4099e-07]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1945.8721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0229, device='cuda:0')



h[100].sum tensor(-19.1534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8466, device='cuda:0')



h[200].sum tensor(62.8688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.3212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.2007e-03, 0.0000e+00,
         2.5958e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.2154e-03, 0.0000e+00,
         2.6020e-06],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.2295e-03, 0.0000e+00,
         2.6079e-06],
        ...,
        [2.0766e-02, 0.0000e+00, 1.2577e-02,  ..., 1.7256e-02, 0.0000e+00,
         4.2500e-02],
        [6.4429e-03, 0.0000e+00, 3.8780e-03,  ..., 9.9605e-03, 0.0000e+00,
         1.3994e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3780e-03, 0.0000e+00,
         2.6701e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65786.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0642, 0.0054,  ..., 0.0555, 0.0836, 0.0285],
        [0.0578, 0.0407, 0.0168,  ..., 0.0613, 0.0511, 0.0490],
        [0.0546, 0.0197, 0.0312,  ..., 0.0685, 0.0246, 0.0749],
        ...,
        [0.0381, 0.0027, 0.0759,  ..., 0.1008, 0.0022, 0.1649],
        [0.0510, 0.0242, 0.0402,  ..., 0.0776, 0.0312, 0.0948],
        [0.0599, 0.0477, 0.0137,  ..., 0.0614, 0.0616, 0.0441]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592697.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7626.8521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(830.4825, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.2415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(356.7979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4971],
        [-0.1714],
        [ 0.0918],
        ...,
        [ 0.0398],
        [-0.3857],
        [-0.8979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222352.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0155],
        [1.0212],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369248.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0156],
        [1.0213],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369258.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1104e-03,  0.0000e+00, -7.4063e-04,  ...,  1.5339e-03,
          0.0000e+00,  2.8982e-05],
        [-1.1104e-03,  0.0000e+00, -7.4063e-04,  ...,  1.5339e-03,
          0.0000e+00,  2.8982e-05],
        [-1.1104e-03,  0.0000e+00, -7.4063e-04,  ...,  1.5339e-03,
          0.0000e+00,  2.8982e-05],
        ...,
        [-1.1104e-03,  0.0000e+00, -7.4063e-04,  ...,  1.5339e-03,
          0.0000e+00,  2.8982e-05],
        [-1.1104e-03,  0.0000e+00, -7.4063e-04,  ...,  1.5339e-03,
          0.0000e+00,  2.8982e-05],
        [-1.1104e-03,  0.0000e+00, -7.4063e-04,  ...,  1.5339e-03,
          0.0000e+00,  2.8982e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1388.7291, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0157, device='cuda:0')



h[100].sum tensor(-12.9448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5794, device='cuda:0')



h[200].sum tensor(55.2238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0000, 0.0028,  ..., 0.0089, 0.0000, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52486., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0581, 0.0355, 0.0224,  ..., 0.0622, 0.0447, 0.0573],
        [0.0610, 0.0622, 0.0077,  ..., 0.0560, 0.0799, 0.0317],
        [0.0619, 0.0703, 0.0033,  ..., 0.0544, 0.0916, 0.0244],
        ...,
        [0.0640, 0.0756, 0.0019,  ..., 0.0547, 0.0992, 0.0217],
        [0.0640, 0.0756, 0.0019,  ..., 0.0547, 0.0992, 0.0217],
        [0.0639, 0.0756, 0.0019,  ..., 0.0547, 0.0992, 0.0217]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(521490.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7859.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(771.3906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.5214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(289.2133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4285],
        [-0.7584],
        [-0.9390],
        ...,
        [-1.6659],
        [-1.6612],
        [-1.6597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251901.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0156],
        [1.0213],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369258.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0157],
        [1.0214],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369268.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5417e-03, -1.9122e-03,  3.9383e-03,  ...,  5.1435e-03,
         -2.3738e-03,  1.4179e-02],
        [-1.1169e-03,  0.0000e+00, -7.4635e-04,  ...,  1.5329e-03,
          0.0000e+00,  7.0451e-05],
        [ 2.0698e-02, -5.4468e-03,  1.2598e-02,  ...,  1.1818e-02,
         -6.7616e-03,  4.0259e-02],
        ...,
        [-1.1169e-03,  0.0000e+00, -7.4635e-04,  ...,  1.5329e-03,
          0.0000e+00,  7.0451e-05],
        [-1.1169e-03,  0.0000e+00, -7.4635e-04,  ...,  1.5329e-03,
          0.0000e+00,  7.0451e-05],
        [-1.1169e-03,  0.0000e+00, -7.4635e-04,  ...,  1.5329e-03,
          0.0000e+00,  7.0451e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1760.7181, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0202, device='cuda:0')



h[100].sum tensor(-16.8415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7482, device='cuda:0')



h[200].sum tensor(60.3012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0000, 0.0031,  ..., 0.0092, 0.0000, 0.0119],
        [0.0381, 0.0000, 0.0230,  ..., 0.0263, 0.0000, 0.0788],
        [0.0190, 0.0000, 0.0115,  ..., 0.0163, 0.0000, 0.0394],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61467.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0428, 0.0000, 0.0649,  ..., 0.0909, 0.0000, 0.1410],
        [0.0361, 0.0000, 0.0878,  ..., 0.1045, 0.0000, 0.1843],
        [0.0361, 0.0000, 0.0857,  ..., 0.1044, 0.0000, 0.1813],
        ...,
        [0.0643, 0.0759, 0.0021,  ..., 0.0549, 0.0991, 0.0219],
        [0.0643, 0.0759, 0.0021,  ..., 0.0549, 0.0991, 0.0219],
        [0.0642, 0.0758, 0.0020,  ..., 0.0549, 0.0991, 0.0219]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562547.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7876.3057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(813.3121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(305.5536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(329.9474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3240],
        [ 0.3349],
        [ 0.3433],
        ...,
        [-1.6681],
        [-1.6634],
        [-1.6619]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209740.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0157],
        [1.0214],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369268.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0158],
        [1.0215],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369279.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0092, -0.0026,  0.0056,  ...,  0.0064, -0.0032,  0.0191],
        [ 0.0054, -0.0016,  0.0032,  ...,  0.0046, -0.0020,  0.0121],
        [ 0.0040, -0.0013,  0.0024,  ...,  0.0039, -0.0016,  0.0095],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0001],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1370.7603, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-12.3413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5473, device='cuda:0')



h[200].sum tensor(55.4943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9057, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0279, 0.0000, 0.0168,  ..., 0.0215, 0.0000, 0.0602],
        [0.0249, 0.0000, 0.0149,  ..., 0.0201, 0.0000, 0.0546],
        [0.0085, 0.0000, 0.0051,  ..., 0.0113, 0.0000, 0.0204],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52482.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0483, 0.0000, 0.0675,  ..., 0.0843, 0.0000, 0.1361],
        [0.0514, 0.0012, 0.0567,  ..., 0.0788, 0.0000, 0.1167],
        [0.0573, 0.0226, 0.0347,  ..., 0.0673, 0.0274, 0.0769],
        ...,
        [0.0644, 0.0760, 0.0023,  ..., 0.0550, 0.0985, 0.0220],
        [0.0644, 0.0760, 0.0023,  ..., 0.0550, 0.0985, 0.0220],
        [0.0644, 0.0759, 0.0023,  ..., 0.0549, 0.0985, 0.0220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523375.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7934.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(768.6293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.7648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.5401, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2627],
        [ 0.0302],
        [-0.3638],
        ...,
        [-1.6625],
        [-1.6584],
        [-1.6575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235378.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0158],
        [1.0215],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369279.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0159],
        [1.0216],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369289.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1755.2939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0194, device='cuda:0')



h[100].sum tensor(-16.2572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7190, device='cuda:0')



h[200].sum tensor(61.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0113, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0006],
        [0.0203, 0.0000, 0.0123,  ..., 0.0168, 0.0000, 0.0422],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60718.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0610, 0.0624, 0.0074,  ..., 0.0566, 0.0791, 0.0310],
        [0.0569, 0.0351, 0.0216,  ..., 0.0650, 0.0447, 0.0575],
        [0.0428, 0.0093, 0.0641,  ..., 0.0919, 0.0105, 0.1393],
        ...,
        [0.0646, 0.0761, 0.0028,  ..., 0.0550, 0.0979, 0.0221],
        [0.0646, 0.0761, 0.0027,  ..., 0.0550, 0.0979, 0.0221],
        [0.0646, 0.0761, 0.0027,  ..., 0.0550, 0.0979, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557783.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7884.6748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(802.0971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(303.5936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(318.5738, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7354],
        [-0.2945],
        [ 0.0686],
        ...,
        [-1.6629],
        [-1.6585],
        [-1.6570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211272.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0159],
        [1.0216],
        ...,
        [1.0005],
        [0.9997],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369289.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0160],
        [1.0217],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369299.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0013,  0.0026,  ...,  0.0041, -0.0017,  0.0101],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1433.9465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0152, device='cuda:0')



h[100].sum tensor(-12.5692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5609, device='cuda:0')



h[200].sum tensor(57.3383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0145, 0.0000, 0.0087,  ..., 0.0145, 0.0000, 0.0337],
        [0.0117, 0.0000, 0.0070,  ..., 0.0127, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55459.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0061, 0.0524,  ..., 0.0736, 0.0056, 0.1045],
        [0.0569, 0.0174, 0.0413,  ..., 0.0692, 0.0197, 0.0861],
        [0.0612, 0.0453, 0.0193,  ..., 0.0595, 0.0537, 0.0487],
        ...,
        [0.0647, 0.0763, 0.0030,  ..., 0.0550, 0.0976, 0.0222],
        [0.0647, 0.0763, 0.0030,  ..., 0.0550, 0.0975, 0.0221],
        [0.0647, 0.0763, 0.0030,  ..., 0.0550, 0.0975, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542950.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7808.8232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(775.4224, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(274.0819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.5603, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1535],
        [ 0.0183],
        [-0.2290],
        ...,
        [-1.6656],
        [-1.6612],
        [-1.6597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232929.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0160],
        [1.0217],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369299.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0161],
        [1.0218],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369309.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1825.3323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0200, device='cuda:0')



h[100].sum tensor(-16.6072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7416, device='cuda:0')



h[200].sum tensor(63.3549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0044, 0.0000, 0.0026,  ..., 0.0092, 0.0000, 0.0130],
        [0.0084, 0.0000, 0.0050,  ..., 0.0111, 0.0000, 0.0204],
        [0.0124, 0.0000, 0.0074,  ..., 0.0130, 0.0000, 0.0278],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61703.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0593, 0.0140, 0.0375,  ..., 0.0654, 0.0114, 0.0781],
        [0.0577, 0.0134, 0.0395,  ..., 0.0677, 0.0127, 0.0825],
        [0.0560, 0.0099, 0.0424,  ..., 0.0705, 0.0084, 0.0882],
        ...,
        [0.0648, 0.0766, 0.0033,  ..., 0.0549, 0.0973, 0.0221],
        [0.0648, 0.0766, 0.0033,  ..., 0.0549, 0.0973, 0.0221],
        [0.0647, 0.0766, 0.0033,  ..., 0.0549, 0.0972, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565435.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7794.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(804.0402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.2473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.4850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2369],
        [ 0.1837],
        [ 0.0899],
        ...,
        [-1.6712],
        [-1.6668],
        [-1.6653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218516.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0161],
        [1.0218],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369309.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0161],
        [1.0218],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369309.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052, -0.0016,  0.0031,  ...,  0.0045, -0.0019,  0.0118],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [ 0.0184, -0.0048,  0.0112,  ...,  0.0107, -0.0060,  0.0360],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1552.3853, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0164, device='cuda:0')



h[100].sum tensor(-13.6654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6065, device='cuda:0')



h[200].sum tensor(59.6396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0000, 0.0024,  ..., 0.0086, 0.0000, 0.0104],
        [0.0240, 0.0000, 0.0145,  ..., 0.0184, 0.0000, 0.0490],
        [0.0151, 0.0000, 0.0091,  ..., 0.0137, 0.0000, 0.0306],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55283.5742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0557, 0.0042, 0.0420,  ..., 0.0705, 0.0027, 0.0890],
        [0.0476, 0.0000, 0.0560,  ..., 0.0838, 0.0000, 0.1206],
        [0.0464, 0.0000, 0.0532,  ..., 0.0852, 0.0000, 0.1186],
        ...,
        [0.0648, 0.0766, 0.0033,  ..., 0.0549, 0.0973, 0.0221],
        [0.0648, 0.0766, 0.0033,  ..., 0.0549, 0.0973, 0.0221],
        [0.0647, 0.0766, 0.0033,  ..., 0.0549, 0.0972, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531798.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7919.2710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(781.0460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(270.9489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.3982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2607],
        [ 0.2501],
        [ 0.1425],
        ...,
        [-1.6712],
        [-1.6668],
        [-1.6653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219347.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0161],
        [1.0218],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369309.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0161],
        [1.0219],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369320.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0015,  0.0030,  ...,  0.0044, -0.0019,  0.0113],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1564.5222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0164, device='cuda:0')



h[100].sum tensor(-13.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6086, device='cuda:0')



h[200].sum tensor(60.3978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0134, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0108, 0.0000, 0.0064,  ..., 0.0122, 0.0000, 0.0247],
        [0.0050, 0.0000, 0.0030,  ..., 0.0090, 0.0000, 0.0121],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56447.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0497, 0.0149, 0.0563,  ..., 0.0807, 0.0160, 0.1172],
        [0.0582, 0.0334, 0.0284,  ..., 0.0646, 0.0394, 0.0653],
        [0.0626, 0.0607, 0.0117,  ..., 0.0560, 0.0736, 0.0355],
        ...,
        [0.0648, 0.0770, 0.0034,  ..., 0.0548, 0.0972, 0.0220],
        [0.0648, 0.0770, 0.0034,  ..., 0.0548, 0.0972, 0.0220],
        [0.0648, 0.0770, 0.0034,  ..., 0.0548, 0.0972, 0.0220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539771.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7910.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(790.8241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(278.0630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.6178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0889],
        [-0.1947],
        [-0.5465],
        ...,
        [-1.6808],
        [-1.6764],
        [-1.6749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211516.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0161],
        [1.0219],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369320.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0162],
        [1.0219],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369330.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045, -0.0014,  0.0027,  ...,  0.0041, -0.0017,  0.0105],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1453.1655, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0149, device='cuda:0')



h[100].sum tensor(-12.3985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5521, device='cuda:0')



h[200].sum tensor(59.2571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0115, 0.0000, 0.0069,  ..., 0.0125, 0.0000, 0.0260],
        [0.0046, 0.0000, 0.0027,  ..., 0.0087, 0.0000, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54516.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0544, 0.0136, 0.0471,  ..., 0.0728, 0.0142, 0.0965],
        [0.0582, 0.0259, 0.0296,  ..., 0.0650, 0.0295, 0.0668],
        [0.0602, 0.0434, 0.0189,  ..., 0.0606, 0.0508, 0.0491],
        ...,
        [0.0649, 0.0775, 0.0033,  ..., 0.0547, 0.0976, 0.0220],
        [0.0649, 0.0775, 0.0033,  ..., 0.0547, 0.0975, 0.0220],
        [0.0648, 0.0775, 0.0033,  ..., 0.0547, 0.0975, 0.0220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533991.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7959.2139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(791.2639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(265.4154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.0414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1761],
        [ 0.0872],
        [ 0.0307],
        ...,
        [-1.6941],
        [-1.6899],
        [-1.6887]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210493.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0162],
        [1.0219],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369330.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(471.1086, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0163],
        [1.0220],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369340.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0215, -0.0055,  0.0130,  ...,  0.0121, -0.0068,  0.0416],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [ 0.0129, -0.0034,  0.0078,  ...,  0.0081, -0.0043,  0.0259],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1999.5186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0221, device='cuda:0')



h[100].sum tensor(-18.2254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8181, device='cuda:0')



h[200].sum tensor(66.8527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8065, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0591, 0.0000, 0.0359,  ..., 0.0354, 0.0000, 0.1156],
        [0.0697, 0.0000, 0.0423,  ..., 0.0410, 0.0000, 0.1371],
        [0.0150, 0.0000, 0.0090,  ..., 0.0142, 0.0000, 0.0324],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63998.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0071, 0.0000, 0.1763,  ..., 0.1741, 0.0000, 0.3697],
        [0.0150, 0.0000, 0.1446,  ..., 0.1476, 0.0000, 0.3003],
        [0.0382, 0.0000, 0.0890,  ..., 0.1033, 0.0000, 0.1815],
        ...,
        [0.0650, 0.0781, 0.0031,  ..., 0.0546, 0.0981, 0.0219],
        [0.0650, 0.0781, 0.0031,  ..., 0.0546, 0.0981, 0.0219],
        [0.0650, 0.0781, 0.0031,  ..., 0.0546, 0.0981, 0.0219]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574412.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7762.5010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(826.8515, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(330.4615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(345.3274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1745],
        [ 0.2123],
        [ 0.2559],
        ...,
        [-1.7130],
        [-1.7085],
        [-1.7069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232198.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0163],
        [1.0220],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369340.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0164],
        [1.0221],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369350.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [ 0.0118, -0.0031,  0.0072,  ...,  0.0076, -0.0039,  0.0239],
        [ 0.0118, -0.0031,  0.0071,  ...,  0.0076, -0.0039,  0.0239],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1699.6533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-14.9468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6740, device='cuda:0')



h[200].sum tensor(63.0199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0000, 0.0073,  ..., 0.0122, 0.0000, 0.0248],
        [0.0216, 0.0000, 0.0131,  ..., 0.0173, 0.0000, 0.0446],
        [0.0692, 0.0000, 0.0420,  ..., 0.0407, 0.0000, 0.1362],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57184.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0518, 0.0235, 0.0365,  ..., 0.0745, 0.0281, 0.0861],
        [0.0417, 0.0057, 0.0653,  ..., 0.0938, 0.0054, 0.1424],
        [0.0231, 0.0000, 0.1135,  ..., 0.1281, 0.0000, 0.2399],
        ...,
        [0.0652, 0.0786, 0.0031,  ..., 0.0546, 0.0984, 0.0220],
        [0.0652, 0.0785, 0.0031,  ..., 0.0546, 0.0984, 0.0220],
        [0.0651, 0.0785, 0.0031,  ..., 0.0546, 0.0983, 0.0220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544680.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7939.2104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(805.2744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(286.1297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.9098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4018],
        [-0.1496],
        [ 0.0935],
        ...,
        [-1.7242],
        [-1.7196],
        [-1.7179]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219583.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0164],
        [1.0221],
        ...,
        [1.0005],
        [0.9997],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369350.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0165],
        [1.0221],
        ...,
        [1.0005],
        [0.9996],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369360.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072, -0.0020,  0.0043,  ...,  0.0054, -0.0025,  0.0155],
        [ 0.0264, -0.0067,  0.0161,  ...,  0.0144, -0.0083,  0.0508],
        [ 0.0118, -0.0031,  0.0071,  ...,  0.0075, -0.0039,  0.0238],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1399.0187, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-11.6930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5277, device='cuda:0')



h[200].sum tensor(58.8595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0786, 0.0000, 0.0477,  ..., 0.0451, 0.0000, 0.1534],
        [0.0575, 0.0000, 0.0348,  ..., 0.0352, 0.0000, 0.1146],
        [0.0587, 0.0000, 0.0356,  ..., 0.0358, 0.0000, 0.1169],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53242.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0211, 0.0000, 0.1265,  ..., 0.1336, 0.0000, 0.2621],
        [0.0203, 0.0000, 0.1332,  ..., 0.1360, 0.0000, 0.2720],
        [0.0209, 0.0000, 0.1325,  ..., 0.1354, 0.0000, 0.2702],
        ...,
        [0.0654, 0.0790, 0.0030,  ..., 0.0547, 0.0988, 0.0222],
        [0.0654, 0.0790, 0.0030,  ..., 0.0547, 0.0988, 0.0222],
        [0.0654, 0.0790, 0.0030,  ..., 0.0547, 0.0987, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532723., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7939.0396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(784.9422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(264.3952, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.3951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2437],
        [ 0.2439],
        [ 0.2316],
        ...,
        [-1.7355],
        [-1.7309],
        [-1.7292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242686.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0165],
        [1.0221],
        ...,
        [1.0005],
        [0.9996],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369360.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0165],
        [1.0222],
        ...,
        [1.0005],
        [0.9996],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369370.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2191.3286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0244, device='cuda:0')



h[100].sum tensor(-20.0207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9036, device='cuda:0')



h[200].sum tensor(69.5845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.3526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0000, 0.0029,  ..., 0.0088, 0.0000, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0009],
        [0.0208, 0.0000, 0.0125,  ..., 0.0169, 0.0000, 0.0431],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68197.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0560, 0.0063, 0.0417,  ..., 0.0715, 0.0045, 0.0893],
        [0.0512, 0.0099, 0.0479,  ..., 0.0794, 0.0091, 0.1058],
        [0.0317, 0.0041, 0.0975,  ..., 0.1153, 0.0031, 0.2065],
        ...,
        [0.0657, 0.0794, 0.0030,  ..., 0.0549, 0.0989, 0.0224],
        [0.0657, 0.0794, 0.0030,  ..., 0.0549, 0.0989, 0.0224],
        [0.0656, 0.0793, 0.0030,  ..., 0.0549, 0.0989, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604378.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7789.0483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.2029, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(361.3793, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(362.8389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2682],
        [ 0.2189],
        [ 0.1707],
        ...,
        [-1.7425],
        [-1.7379],
        [-1.7362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234328.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0165],
        [1.0222],
        ...,
        [1.0005],
        [0.9996],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369370.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0166],
        [1.0222],
        ...,
        [1.0005],
        [0.9996],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369380.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0087, -0.0024,  0.0052,  ...,  0.0061, -0.0029,  0.0182],
        [ 0.0037, -0.0011,  0.0022,  ...,  0.0037, -0.0014,  0.0090],
        [ 0.0144, -0.0037,  0.0087,  ...,  0.0088, -0.0047,  0.0287],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1810.5110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0195, device='cuda:0')



h[100].sum tensor(-15.8800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7200, device='cuda:0')



h[200].sum tensor(64.3059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0280, 0.0000, 0.0168,  ..., 0.0213, 0.0000, 0.0604],
        [0.0374, 0.0000, 0.0225,  ..., 0.0257, 0.0000, 0.0778],
        [0.0259, 0.0000, 0.0155,  ..., 0.0203, 0.0000, 0.0567],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59384.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0447, 0.0000, 0.0956,  ..., 0.0984, 0.0000, 0.1843],
        [0.0425, 0.0000, 0.0996,  ..., 0.1024, 0.0000, 0.1932],
        [0.0445, 0.0000, 0.0970,  ..., 0.0995, 0.0000, 0.1866],
        ...,
        [0.0660, 0.0797, 0.0031,  ..., 0.0552, 0.0991, 0.0227],
        [0.0659, 0.0797, 0.0031,  ..., 0.0552, 0.0991, 0.0227],
        [0.0659, 0.0797, 0.0030,  ..., 0.0551, 0.0990, 0.0227]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552441., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7978.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(807.1503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(305.2352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(317.2020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3429],
        [ 0.3153],
        [ 0.2709],
        ...,
        [-1.7490],
        [-1.7448],
        [-1.7432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238835.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0166],
        [1.0222],
        ...,
        [1.0005],
        [0.9996],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369380.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0148],
        [1.0167],
        [1.0222],
        ...,
        [1.0005],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369390.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0127, -0.0033,  0.0077,  ...,  0.0080, -0.0041,  0.0256],
        [ 0.0185, -0.0047,  0.0113,  ...,  0.0107, -0.0059,  0.0364],
        [ 0.0161, -0.0041,  0.0098,  ...,  0.0096, -0.0052,  0.0319],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1939.9055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0210, device='cuda:0')



h[100].sum tensor(-17.1593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7789, device='cuda:0')



h[200].sum tensor(65.8739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0509, 0.0000, 0.0308,  ..., 0.0321, 0.0000, 0.1028],
        [0.0623, 0.0000, 0.0377,  ..., 0.0374, 0.0000, 0.1238],
        [0.0654, 0.0000, 0.0396,  ..., 0.0389, 0.0000, 0.1295],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60509.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0333, 0.0000, 0.1143,  ..., 0.1189, 0.0000, 0.2303],
        [0.0258, 0.0000, 0.1347,  ..., 0.1337, 0.0000, 0.2718],
        [0.0230, 0.0000, 0.1408,  ..., 0.1389, 0.0000, 0.2847],
        ...,
        [0.0661, 0.0760, 0.0054,  ..., 0.0562, 0.0937, 0.0268],
        [0.0660, 0.0739, 0.0066,  ..., 0.0566, 0.0908, 0.0286],
        [0.0660, 0.0760, 0.0054,  ..., 0.0562, 0.0936, 0.0268]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556114.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7973.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(809.3504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.5963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.1945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2945],
        [ 0.3040],
        [ 0.2988],
        ...,
        [-1.4978],
        [-1.4462],
        [-1.4460]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242425.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0148],
        [1.0167],
        [1.0222],
        ...,
        [1.0005],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369390.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0167],
        [1.0223],
        ...,
        [1.0005],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369400.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0271, -0.0068,  0.0165,  ...,  0.0148, -0.0085,  0.0522],
        [ 0.0098, -0.0026,  0.0059,  ...,  0.0066, -0.0033,  0.0204],
        [ 0.0140, -0.0036,  0.0085,  ...,  0.0086, -0.0045,  0.0281],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1712.8204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-14.6856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6620, device='cuda:0')



h[200].sum tensor(62.5528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0505, 0.0000, 0.0305,  ..., 0.0319, 0.0000, 0.1022],
        [0.0631, 0.0000, 0.0382,  ..., 0.0378, 0.0000, 0.1254],
        [0.0569, 0.0000, 0.0345,  ..., 0.0350, 0.0000, 0.1141],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57850.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0232, 0.0000, 0.1295,  ..., 0.1370, 0.0000, 0.2704],
        [0.0247, 0.0000, 0.1289,  ..., 0.1350, 0.0000, 0.2672],
        [0.0307, 0.0000, 0.1215,  ..., 0.1256, 0.0000, 0.2466],
        ...,
        [0.0665, 0.0805, 0.0029,  ..., 0.0557, 0.0996, 0.0234],
        [0.0665, 0.0804, 0.0029,  ..., 0.0557, 0.0996, 0.0234],
        [0.0665, 0.0804, 0.0029,  ..., 0.0557, 0.0995, 0.0234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549246.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8062.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(796.7654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(297.3725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(304.0180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2915],
        [ 0.3147],
        [ 0.3364],
        ...,
        [-1.7620],
        [-1.7574],
        [-1.7559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237538.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0167],
        [1.0223],
        ...,
        [1.0005],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369400.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0168],
        [1.0223],
        ...,
        [1.0005],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369409.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0143, -0.0037,  0.0087,  ...,  0.0088, -0.0046,  0.0287],
        [ 0.0116, -0.0030,  0.0070,  ...,  0.0075, -0.0038,  0.0237],
        [ 0.0141, -0.0036,  0.0086,  ...,  0.0087, -0.0045,  0.0283],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1524.1511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-12.6896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5763, device='cuda:0')



h[200].sum tensor(59.6880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0420, 0.0000, 0.0254,  ..., 0.0279, 0.0000, 0.0867],
        [0.0560, 0.0000, 0.0339,  ..., 0.0345, 0.0000, 0.1124],
        [0.0635, 0.0000, 0.0385,  ..., 0.0381, 0.0000, 0.1262],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53943.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0000, 0.1466,  ..., 0.1430, 0.0000, 0.2970],
        [0.0132, 0.0000, 0.1707,  ..., 0.1622, 0.0000, 0.3483],
        [0.0115, 0.0000, 0.1747,  ..., 0.1669, 0.0000, 0.3592],
        ...,
        [0.0669, 0.0810, 0.0026,  ..., 0.0559, 0.1001, 0.0237],
        [0.0669, 0.0810, 0.0026,  ..., 0.0559, 0.1001, 0.0237],
        [0.0668, 0.0809, 0.0026,  ..., 0.0559, 0.1001, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532458.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8215.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(784.2594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(272.2939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.7856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1641],
        [ 0.1562],
        [ 0.1557],
        ...,
        [-1.7777],
        [-1.7731],
        [-1.7714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231548.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0168],
        [1.0223],
        ...,
        [1.0005],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369409.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0168],
        [1.0224],
        ...,
        [1.0004],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369418.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045, -0.0013,  0.0027,  ...,  0.0042, -0.0017,  0.0106],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2481.4395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0283, device='cuda:0')



h[100].sum tensor(-22.8172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0462, device='cuda:0')



h[200].sum tensor(72.2179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0104, 0.0000, 0.0062,  ..., 0.0121, 0.0000, 0.0243],
        [0.0046, 0.0000, 0.0027,  ..., 0.0088, 0.0000, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77269.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0198, 0.0430,  ..., 0.0712, 0.0215, 0.0924],
        [0.0633, 0.0385, 0.0245,  ..., 0.0631, 0.0447, 0.0606],
        [0.0651, 0.0641, 0.0111,  ..., 0.0576, 0.0765, 0.0378],
        ...,
        [0.0672, 0.0817, 0.0022,  ..., 0.0561, 0.1012, 0.0237],
        [0.0666, 0.0763, 0.0043,  ..., 0.0575, 0.0941, 0.0278],
        [0.0646, 0.0582, 0.0116,  ..., 0.0619, 0.0705, 0.0416]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659360.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8043.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.0291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(425.9438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(398.7783, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0032],
        [-0.3677],
        [-0.7683],
        ...,
        [-1.7243],
        [-1.5731],
        [-1.3026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230756.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0168],
        [1.0224],
        ...,
        [1.0004],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369418.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0168],
        [1.0224],
        ...,
        [1.0004],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369427.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1525.4869, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-12.8624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5845, device='cuda:0')



h[200].sum tensor(59.2364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54246.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0805, 0.0016,  ..., 0.0547, 0.0990, 0.0231],
        [0.0654, 0.0807, 0.0016,  ..., 0.0548, 0.0992, 0.0231],
        [0.0657, 0.0808, 0.0017,  ..., 0.0549, 0.0995, 0.0232],
        ...,
        [0.0675, 0.0826, 0.0018,  ..., 0.0561, 0.1022, 0.0237],
        [0.0675, 0.0826, 0.0018,  ..., 0.0561, 0.1022, 0.0237],
        [0.0675, 0.0826, 0.0018,  ..., 0.0561, 0.1021, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538472.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8310.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(792.8460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(279.4258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.7299, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9932],
        [-2.0167],
        [-2.0275],
        ...,
        [-1.8239],
        [-1.8192],
        [-1.8176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262646.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0168],
        [1.0224],
        ...,
        [1.0004],
        [0.9996],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369427.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(491.5655, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0169],
        [1.0225],
        ...,
        [1.0004],
        [0.9995],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369436.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [ 0.0033, -0.0010,  0.0020,  ...,  0.0036, -0.0013,  0.0083],
        [ 0.0033, -0.0010,  0.0020,  ...,  0.0036, -0.0013,  0.0083],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1507.2922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-12.6690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5788, device='cuda:0')



h[200].sum tensor(59.1620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0000, 0.0035,  ..., 0.0100, 0.0000, 0.0158],
        [0.0060, 0.0000, 0.0035,  ..., 0.0101, 0.0000, 0.0158],
        [0.0060, 0.0000, 0.0035,  ..., 0.0101, 0.0000, 0.0158],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54704.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0644, 0.0335, 0.0279,  ..., 0.0639, 0.0334, 0.0673],
        [0.0662, 0.0427, 0.0253,  ..., 0.0614, 0.0448, 0.0615],
        [0.0666, 0.0452, 0.0243,  ..., 0.0609, 0.0482, 0.0596],
        ...,
        [0.0677, 0.0834, 0.0016,  ..., 0.0561, 0.1028, 0.0238],
        [0.0676, 0.0834, 0.0016,  ..., 0.0561, 0.1027, 0.0238],
        [0.0676, 0.0833, 0.0016,  ..., 0.0560, 0.1027, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547319.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8237.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(796.7266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(285.2115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(299.1812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5515],
        [-0.8572],
        [-1.1531],
        ...,
        [-1.8437],
        [-1.8386],
        [-1.8367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290217.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0169],
        [1.0225],
        ...,
        [1.0004],
        [0.9995],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369436.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0169],
        [1.0226],
        ...,
        [1.0003],
        [0.9995],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369446.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1881.3292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0201, device='cuda:0')



h[100].sum tensor(-16.4314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7440, device='cuda:0')



h[200].sum tensor(64.7104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62827.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0606, 0.0359, 0.0227,  ..., 0.0673, 0.0415, 0.0643],
        [0.0610, 0.0379, 0.0209,  ..., 0.0663, 0.0436, 0.0604],
        [0.0617, 0.0428, 0.0170,  ..., 0.0646, 0.0493, 0.0536],
        ...,
        [0.0676, 0.0836, 0.0016,  ..., 0.0561, 0.1024, 0.0240],
        [0.0676, 0.0836, 0.0016,  ..., 0.0561, 0.1024, 0.0240],
        [0.0675, 0.0835, 0.0016,  ..., 0.0561, 0.1024, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586147.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8091.2202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(829.1391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(336.5729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(340.2668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0188],
        [-0.0246],
        [-0.1085],
        ...,
        [-1.6208],
        [-1.6042],
        [-1.5352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272212.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0169],
        [1.0226],
        ...,
        [1.0003],
        [0.9995],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369446.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0170],
        [1.0226],
        ...,
        [1.0003],
        [0.9995],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369456.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031, -0.0010,  0.0018,  ...,  0.0035, -0.0012,  0.0080],
        [ 0.0051, -0.0015,  0.0031,  ...,  0.0045, -0.0018,  0.0117],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2191.6990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0236, device='cuda:0')



h[100].sum tensor(-19.4646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8748, device='cuda:0')



h[200].sum tensor(69.5183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0306, 0.0000, 0.0183,  ..., 0.0227, 0.0000, 0.0654],
        [0.0107, 0.0000, 0.0064,  ..., 0.0123, 0.0000, 0.0248],
        [0.0052, 0.0000, 0.0031,  ..., 0.0092, 0.0000, 0.0126],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68826.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0530, 0.0000, 0.0731,  ..., 0.0902, 0.0000, 0.1496],
        [0.0580, 0.0157, 0.0479,  ..., 0.0773, 0.0160, 0.1051],
        [0.0621, 0.0369, 0.0245,  ..., 0.0660, 0.0427, 0.0641],
        ...,
        [0.0674, 0.0794, 0.0042,  ..., 0.0570, 0.0961, 0.0280],
        [0.0674, 0.0774, 0.0054,  ..., 0.0574, 0.0932, 0.0299],
        [0.0674, 0.0794, 0.0042,  ..., 0.0570, 0.0960, 0.0280]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610526., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8052.1938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(855.2189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(370.2556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(368.2588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2489],
        [ 0.0126],
        [-0.3808],
        ...,
        [-1.5829],
        [-1.5398],
        [-1.5761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247722.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0170],
        [1.0226],
        ...,
        [1.0003],
        [0.9995],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369456.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0170],
        [1.0227],
        ...,
        [1.0002],
        [0.9994],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369465.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0066, -0.0018,  0.0040,  ...,  0.0052, -0.0023,  0.0145],
        [ 0.0125, -0.0032,  0.0076,  ...,  0.0079, -0.0040,  0.0253],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1721.8461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0177, device='cuda:0')



h[100].sum tensor(-14.3657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6556, device='cuda:0')



h[200].sum tensor(63.9006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8641, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0000, 0.0071,  ..., 0.0129, 0.0000, 0.0272],
        [0.0217, 0.0000, 0.0130,  ..., 0.0180, 0.0000, 0.0472],
        [0.0389, 0.0000, 0.0234,  ..., 0.0266, 0.0000, 0.0809],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57491.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0568, 0.0228, 0.0461,  ..., 0.0778, 0.0252, 0.1036],
        [0.0495, 0.0052, 0.0791,  ..., 0.0967, 0.0036, 0.1648],
        [0.0411, 0.0000, 0.1128,  ..., 0.1174, 0.0000, 0.2291],
        ...,
        [0.0675, 0.0835, 0.0021,  ..., 0.0564, 0.1009, 0.0246],
        [0.0675, 0.0835, 0.0021,  ..., 0.0564, 0.1008, 0.0246],
        [0.0674, 0.0835, 0.0021,  ..., 0.0564, 0.1008, 0.0246]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552415.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8094.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(799.8104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(296.2411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.4414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2293],
        [ 0.1151],
        [ 0.3025],
        ...,
        [-1.8290],
        [-1.8243],
        [-1.8225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269264.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0170],
        [1.0227],
        ...,
        [1.0002],
        [0.9994],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369465.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0171],
        [1.0228],
        ...,
        [1.0002],
        [0.9994],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369474.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099, -0.0026,  0.0060,  ...,  0.0067, -0.0032,  0.0206],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0075, -0.0020,  0.0045,  ...,  0.0056, -0.0025,  0.0162],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1373.4905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0129, device='cuda:0')



h[100].sum tensor(-10.5639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4783, device='cuda:0')



h[200].sum tensor(59.7723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0000, 0.0084,  ..., 0.0145, 0.0000, 0.0337],
        [0.0338, 0.0000, 0.0203,  ..., 0.0242, 0.0000, 0.0718],
        [0.0507, 0.0000, 0.0307,  ..., 0.0316, 0.0000, 0.1007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51958.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0505, 0.0000, 0.0892,  ..., 0.0978, 0.0000, 0.1781],
        [0.0361, 0.0000, 0.1281,  ..., 0.1281, 0.0000, 0.2603],
        [0.0175, 0.0000, 0.1704,  ..., 0.1652, 0.0000, 0.3554],
        ...,
        [0.0675, 0.0836, 0.0023,  ..., 0.0566, 0.1001, 0.0250],
        [0.0675, 0.0836, 0.0023,  ..., 0.0566, 0.1001, 0.0250],
        [0.0675, 0.0835, 0.0023,  ..., 0.0566, 0.1001, 0.0250]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531457.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8146.3286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(776.6124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(257.2489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(277.7090, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3227],
        [ 0.2826],
        [ 0.2361],
        ...,
        [-1.8197],
        [-1.8152],
        [-1.8136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251300.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0171],
        [1.0228],
        ...,
        [1.0002],
        [0.9994],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369474.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0171],
        [1.0229],
        ...,
        [1.0002],
        [0.9993],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369483.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0068, -0.0019,  0.0041,  ...,  0.0053, -0.0023,  0.0149],
        [ 0.0043, -0.0013,  0.0025,  ...,  0.0040, -0.0016,  0.0102],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1545.6433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-12.1787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5580, device='cuda:0')



h[200].sum tensor(62.8441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0000, 0.0041,  ..., 0.0099, 0.0000, 0.0160],
        [0.0098, 0.0000, 0.0058,  ..., 0.0118, 0.0000, 0.0234],
        [0.0430, 0.0000, 0.0259,  ..., 0.0285, 0.0000, 0.0886],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53044.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0603, 0.0284, 0.0328,  ..., 0.0698, 0.0318, 0.0783],
        [0.0560, 0.0134, 0.0574,  ..., 0.0822, 0.0141, 0.1217],
        [0.0460, 0.0000, 0.0974,  ..., 0.1067, 0.0000, 0.1974],
        ...,
        [0.0674, 0.0838, 0.0023,  ..., 0.0567, 0.0995, 0.0251],
        [0.0674, 0.0837, 0.0023,  ..., 0.0566, 0.0995, 0.0251],
        [0.0674, 0.0837, 0.0023,  ..., 0.0566, 0.0995, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531033.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8137.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(783.2358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(260.3570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.3024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1679],
        [ 0.2529],
        [ 0.3158],
        ...,
        [-1.8169],
        [-1.8123],
        [-1.8079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236739.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0171],
        [1.0229],
        ...,
        [1.0002],
        [0.9993],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369483.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0171],
        [1.0229],
        ...,
        [1.0002],
        [0.9993],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369483.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1521.7380, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0146, device='cuda:0')



h[100].sum tensor(-11.9322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5387, device='cuda:0')



h[200].sum tensor(62.5246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56379.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0652, 0.0816, 0.0021,  ..., 0.0552, 0.0965, 0.0244],
        [0.0651, 0.0793, 0.0031,  ..., 0.0559, 0.0934, 0.0265],
        [0.0636, 0.0607, 0.0110,  ..., 0.0603, 0.0695, 0.0410],
        ...,
        [0.0674, 0.0838, 0.0023,  ..., 0.0567, 0.0995, 0.0251],
        [0.0674, 0.0837, 0.0023,  ..., 0.0566, 0.0995, 0.0251],
        [0.0674, 0.0837, 0.0023,  ..., 0.0566, 0.0995, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556639.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8140.9653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(803.2577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(279.4464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.2467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0948],
        [-0.8849],
        [-0.5453],
        ...,
        [-1.8157],
        [-1.8118],
        [-1.8105]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212450.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0171],
        [1.0229],
        ...,
        [1.0002],
        [0.9993],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369483.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0157],
        [1.0172],
        [1.0229],
        ...,
        [1.0001],
        [0.9993],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369492.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1742.7130, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-14.0942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6531, device='cuda:0')



h[200].sum tensor(66.3072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58975.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0651, 0.0820, 0.0020,  ..., 0.0552, 0.0962, 0.0244],
        [0.0652, 0.0821, 0.0020,  ..., 0.0553, 0.0964, 0.0245],
        [0.0654, 0.0822, 0.0022,  ..., 0.0553, 0.0967, 0.0245],
        ...,
        [0.0673, 0.0841, 0.0023,  ..., 0.0566, 0.0992, 0.0251],
        [0.0673, 0.0841, 0.0023,  ..., 0.0566, 0.0992, 0.0251],
        [0.0673, 0.0841, 0.0023,  ..., 0.0565, 0.0992, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563609.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8026.6147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(814.1573, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(297.5868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.6899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8471],
        [-1.8181],
        [-1.7566],
        ...,
        [-1.8221],
        [-1.8176],
        [-1.8158]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219690.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0157],
        [1.0172],
        [1.0229],
        ...,
        [1.0001],
        [0.9993],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369492.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0173],
        [1.0230],
        ...,
        [1.0001],
        [0.9993],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369501.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1571.8455, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-12.2721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5596, device='cuda:0')



h[200].sum tensor(64.8862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55532.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0649, 0.0824, 0.0019,  ..., 0.0550, 0.0962, 0.0244],
        [0.0650, 0.0825, 0.0019,  ..., 0.0551, 0.0964, 0.0244],
        [0.0653, 0.0827, 0.0020,  ..., 0.0551, 0.0966, 0.0245],
        ...,
        [0.0671, 0.0846, 0.0022,  ..., 0.0564, 0.0992, 0.0251],
        [0.0671, 0.0846, 0.0022,  ..., 0.0564, 0.0992, 0.0251],
        [0.0671, 0.0845, 0.0022,  ..., 0.0563, 0.0991, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547865.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8075.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(809.7169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(274.1770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(310.7238, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7086],
        [-1.8153],
        [-1.9080],
        ...,
        [-1.8305],
        [-1.8257],
        [-1.8237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208184.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0173],
        [1.0230],
        ...,
        [1.0001],
        [0.9993],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369501.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0174],
        [1.0231],
        ...,
        [1.0001],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369510.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1654.9839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-13.0846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5993, device='cuda:0')



h[200].sum tensor(66.7202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8459, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57888.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0649, 0.0829, 0.0017,  ..., 0.0548, 0.0964, 0.0243],
        [0.0650, 0.0830, 0.0018,  ..., 0.0549, 0.0965, 0.0244],
        [0.0652, 0.0832, 0.0019,  ..., 0.0550, 0.0968, 0.0244],
        ...,
        [0.0671, 0.0851, 0.0020,  ..., 0.0562, 0.0994, 0.0250],
        [0.0670, 0.0851, 0.0020,  ..., 0.0562, 0.0993, 0.0250],
        [0.0670, 0.0851, 0.0020,  ..., 0.0562, 0.0993, 0.0250]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565756.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7966.9536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(819.5606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(291.7219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.1302, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7508],
        [-1.8804],
        [-1.9614],
        ...,
        [-1.8482],
        [-1.8435],
        [-1.8419]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225654.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0174],
        [1.0231],
        ...,
        [1.0001],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369510.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(427.1852, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0174],
        [1.0232],
        ...,
        [1.0001],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369519.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [ 0.0048, -0.0014,  0.0029,  ...,  0.0043, -0.0017,  0.0111],
        [ 0.0033, -0.0010,  0.0019,  ...,  0.0036, -0.0013,  0.0083],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1427.3066, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0132, device='cuda:0')



h[100].sum tensor(-10.8020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4891, device='cuda:0')



h[200].sum tensor(63.7095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0000, 0.0029,  ..., 0.0090, 0.0000, 0.0119],
        [0.0108, 0.0000, 0.0064,  ..., 0.0124, 0.0000, 0.0249],
        [0.0405, 0.0000, 0.0244,  ..., 0.0274, 0.0000, 0.0837],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54151.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0619, 0.0359, 0.0278,  ..., 0.0666, 0.0374, 0.0706],
        [0.0567, 0.0163, 0.0543,  ..., 0.0809, 0.0145, 0.1185],
        [0.0460, 0.0000, 0.0967,  ..., 0.1074, 0.0000, 0.1995],
        ...,
        [0.0672, 0.0857, 0.0018,  ..., 0.0563, 0.1001, 0.0251],
        [0.0672, 0.0856, 0.0018,  ..., 0.0563, 0.1001, 0.0251],
        [0.0672, 0.0856, 0.0018,  ..., 0.0563, 0.1000, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551502.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7935.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(801.9691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(273.8831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(313.5319, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0408],
        [ 0.1797],
        [ 0.2885],
        ...,
        [-1.8658],
        [-1.8610],
        [-1.8593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265704.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0174],
        [1.0232],
        ...,
        [1.0001],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369519.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0176],
        [1.0233],
        ...,
        [1.0000],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369527.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046, -0.0013,  0.0028,  ...,  0.0042, -0.0017,  0.0108],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2041.8950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0214, device='cuda:0')



h[100].sum tensor(-17.1098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7920, device='cuda:0')



h[200].sum tensor(71.8009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0153, 0.0000, 0.0092,  ..., 0.0145, 0.0000, 0.0331],
        [0.0280, 0.0000, 0.0169,  ..., 0.0210, 0.0000, 0.0586],
        [0.0238, 0.0000, 0.0144,  ..., 0.0186, 0.0000, 0.0488],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63565.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0000, 0.0739,  ..., 0.0922, 0.0000, 0.1554],
        [0.0468, 0.0000, 0.0900,  ..., 0.1055, 0.0000, 0.1897],
        [0.0447, 0.0000, 0.0918,  ..., 0.1092, 0.0000, 0.1961],
        ...,
        [0.0675, 0.0862, 0.0016,  ..., 0.0565, 0.1007, 0.0252],
        [0.0675, 0.0862, 0.0016,  ..., 0.0565, 0.1007, 0.0252],
        [0.0674, 0.0861, 0.0016,  ..., 0.0565, 0.1007, 0.0252]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591699.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7955.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(845.9929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(332.5743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(362.3034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3275],
        [ 0.3118],
        [ 0.2957],
        ...,
        [-1.8833],
        [-1.8785],
        [-1.8768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238472.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0176],
        [1.0233],
        ...,
        [1.0000],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369527.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0177],
        [1.0234],
        ...,
        [1.0000],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369536.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0156, -0.0038,  0.0095,  ...,  0.0094, -0.0048,  0.0310],
        [ 0.0227, -0.0054,  0.0138,  ...,  0.0128, -0.0068,  0.0441],
        [ 0.0160, -0.0039,  0.0097,  ...,  0.0096, -0.0049,  0.0317],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2074.8113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0216, device='cuda:0')



h[100].sum tensor(-17.4853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7999, device='cuda:0')



h[200].sum tensor(71.7298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0852, 0.0000, 0.0518,  ..., 0.0485, 0.0000, 0.1661],
        [0.0737, 0.0000, 0.0448,  ..., 0.0431, 0.0000, 0.1450],
        [0.0604, 0.0000, 0.0367,  ..., 0.0369, 0.0000, 0.1206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65693.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0127, 0.0000, 0.1950,  ..., 0.1854, 0.0000, 0.4121],
        [0.0172, 0.0000, 0.1842,  ..., 0.1761, 0.0000, 0.3875],
        [0.0240, 0.0000, 0.1683,  ..., 0.1621, 0.0000, 0.3512],
        ...,
        [0.0679, 0.0866, 0.0016,  ..., 0.0568, 0.1015, 0.0255],
        [0.0679, 0.0865, 0.0016,  ..., 0.0568, 0.1015, 0.0254],
        [0.0679, 0.0865, 0.0016,  ..., 0.0568, 0.1014, 0.0254]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597664.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8000.2153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(855.0577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(347.4340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(371.1200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1626],
        [ 0.1612],
        [ 0.1631],
        ...,
        [-1.7146],
        [-1.6562],
        [-1.6543]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237343.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0177],
        [1.0234],
        ...,
        [1.0000],
        [0.9992],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369536.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0178],
        [1.0234],
        ...,
        [1.0000],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369545.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1988.7480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0205, device='cuda:0')



h[100].sum tensor(-16.6314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7599, device='cuda:0')



h[200].sum tensor(69.9238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63066.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0618, 0.0286, 0.0295,  ..., 0.0703, 0.0300, 0.0769],
        [0.0613, 0.0273, 0.0311,  ..., 0.0716, 0.0284, 0.0801],
        [0.0607, 0.0229, 0.0345,  ..., 0.0737, 0.0227, 0.0865],
        ...,
        [0.0684, 0.0868, 0.0016,  ..., 0.0572, 0.1022, 0.0258],
        [0.0684, 0.0868, 0.0016,  ..., 0.0572, 0.1021, 0.0258],
        [0.0684, 0.0868, 0.0016,  ..., 0.0572, 0.1021, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581549.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8235.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(847.7344, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(328.8143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(353.9676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1133],
        [ 0.1252],
        [ 0.1457],
        ...,
        [-1.9087],
        [-1.9039],
        [-1.9022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208656.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0178],
        [1.0234],
        ...,
        [1.0000],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369545.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0178],
        [1.0234],
        ...,
        [1.0000],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369545.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        ...,
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002],
        [-0.0011,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1355.1260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0127, device='cuda:0')



h[100].sum tensor(-10.2086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4693, device='cuda:0')



h[200].sum tensor(61.5188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0010],
        [0.0057, 0.0000, 0.0034,  ..., 0.0095, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50421.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0825, 0.0019,  ..., 0.0562, 0.0962, 0.0268],
        [0.0654, 0.0698, 0.0080,  ..., 0.0590, 0.0796, 0.0373],
        [0.0634, 0.0373, 0.0253,  ..., 0.0672, 0.0405, 0.0681],
        ...,
        [0.0684, 0.0868, 0.0016,  ..., 0.0572, 0.1022, 0.0258],
        [0.0684, 0.0868, 0.0016,  ..., 0.0572, 0.1021, 0.0258],
        [0.0684, 0.0868, 0.0016,  ..., 0.0572, 0.1021, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529313., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8173.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(780.2375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(255.6752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.4403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4430],
        [-1.0438],
        [-0.5601],
        ...,
        [-1.9087],
        [-1.9039],
        [-1.9022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281725.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0178],
        [1.0234],
        ...,
        [1.0000],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369545.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0179],
        [1.0235],
        ...,
        [1.0000],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369554.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0149, -0.0037,  0.0091,  ...,  0.0091, -0.0046,  0.0299],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1685.5562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-13.5721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6301, device='cuda:0')



h[200].sum tensor(65.0934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0152, 0.0000, 0.0092,  ..., 0.0140, 0.0000, 0.0312],
        [0.0122, 0.0000, 0.0074,  ..., 0.0126, 0.0000, 0.0258],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56888.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0646, 0.0613, 0.0102,  ..., 0.0620, 0.0695, 0.0434],
        [0.0602, 0.0290, 0.0307,  ..., 0.0748, 0.0309, 0.0836],
        [0.0590, 0.0122, 0.0405,  ..., 0.0797, 0.0109, 0.1012],
        ...,
        [0.0690, 0.0870, 0.0017,  ..., 0.0576, 0.1027, 0.0262],
        [0.0690, 0.0870, 0.0017,  ..., 0.0576, 0.1027, 0.0262],
        [0.0690, 0.0870, 0.0017,  ..., 0.0576, 0.1027, 0.0262]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557382.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8302.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(808.7127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(295.2357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.7388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3173],
        [-0.8357],
        [-0.3769],
        ...,
        [-1.9116],
        [-1.9069],
        [-1.9058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251090.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0179],
        [1.0235],
        ...,
        [1.0000],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369554.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0180],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369563.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0043, -0.0012,  0.0026,  ...,  0.0041, -0.0016,  0.0104],
        [ 0.0038, -0.0011,  0.0023,  ...,  0.0039, -0.0014,  0.0094],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1530.8993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0150, device='cuda:0')



h[100].sum tensor(-11.9738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5565, device='cuda:0')



h[200].sum tensor(62.7570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0709, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0044, 0.0000, 0.0026,  ..., 0.0089, 0.0000, 0.0115],
        [0.0072, 0.0000, 0.0043,  ..., 0.0108, 0.0000, 0.0189],
        [0.0198, 0.0000, 0.0118,  ..., 0.0179, 0.0000, 0.0463],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55186.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0645, 0.0248, 0.0348,  ..., 0.0701, 0.0264, 0.0831],
        [0.0638, 0.0117, 0.0518,  ..., 0.0763, 0.0114, 0.1112],
        [0.0628, 0.0000, 0.0691,  ..., 0.0831, 0.0000, 0.1399],
        ...,
        [0.0695, 0.0873, 0.0017,  ..., 0.0579, 0.1031, 0.0265],
        [0.0695, 0.0872, 0.0017,  ..., 0.0579, 0.1031, 0.0265],
        [0.0695, 0.0872, 0.0017,  ..., 0.0578, 0.1031, 0.0264]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553725.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8359.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(797.9854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(286.4193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.5034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1137],
        [ 0.2766],
        [ 0.3715],
        ...,
        [-1.9250],
        [-1.9200],
        [-1.9183]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258864.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0180],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369563.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0181],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369572.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0212, -0.0051,  0.0129,  ...,  0.0121, -0.0064,  0.0415],
        [ 0.0291, -0.0068,  0.0177,  ...,  0.0158, -0.0086,  0.0559],
        [ 0.0221, -0.0053,  0.0135,  ...,  0.0125, -0.0066,  0.0431],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1908.5636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-15.7451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7274, device='cuda:0')



h[200].sum tensor(67.6199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1641, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0982, 0.0000, 0.0598,  ..., 0.0547, 0.0000, 0.1908],
        [0.1111, 0.0000, 0.0677,  ..., 0.0608, 0.0000, 0.2145],
        [0.1091, 0.0000, 0.0665,  ..., 0.0599, 0.0000, 0.2109],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62689.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0128, 0.0000, 0.2121,  ..., 0.2003, 0.0000, 0.4532],
        [0.0056, 0.0000, 0.2331,  ..., 0.2183, 0.0000, 0.5009],
        [0.0071, 0.0000, 0.2303,  ..., 0.2156, 0.0000, 0.4942],
        ...,
        [0.0699, 0.0876, 0.0017,  ..., 0.0580, 0.1036, 0.0266],
        [0.0698, 0.0876, 0.0017,  ..., 0.0580, 0.1036, 0.0266],
        [0.0698, 0.0876, 0.0017,  ..., 0.0579, 0.1035, 0.0266]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589612.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8346.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.9341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(334.9088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.8216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2295],
        [ 0.2245],
        [ 0.2175],
        ...,
        [-1.9365],
        [-1.9315],
        [-1.9296]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241940.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0181],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369572.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0181],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369572.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1595.2742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.5422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-12.5924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5880, device='cuda:0')



h[200].sum tensor(63.4771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6422, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0055, 0.0000, 0.0032,  ..., 0.0100, 0.0000, 0.0156],
        [0.0027, 0.0000, 0.0016,  ..., 0.0081, 0.0000, 0.0084],
        [0.0045, 0.0000, 0.0027,  ..., 0.0090, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55903.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0690, 0.0427, 0.0279,  ..., 0.0629, 0.0409, 0.0680],
        [0.0690, 0.0492, 0.0238,  ..., 0.0619, 0.0499, 0.0617],
        [0.0692, 0.0418, 0.0281,  ..., 0.0632, 0.0399, 0.0686],
        ...,
        [0.0699, 0.0876, 0.0017,  ..., 0.0580, 0.1036, 0.0266],
        [0.0698, 0.0876, 0.0017,  ..., 0.0580, 0.1036, 0.0266],
        [0.0698, 0.0876, 0.0017,  ..., 0.0579, 0.1035, 0.0266]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555295.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8447.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(804.3068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(290.3607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(305.1307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3618],
        [-0.2478],
        [-0.1167],
        ...,
        [-1.9351],
        [-1.9302],
        [-1.9286]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251164.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0181],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369572.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0182],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369581.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0085, -0.0022,  0.0052,  ...,  0.0061, -0.0028,  0.0182],
        [ 0.0056, -0.0015,  0.0034,  ...,  0.0047, -0.0019,  0.0128],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1726.3483, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-13.8540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6407, device='cuda:0')



h[200].sum tensor(65.2542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5958, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0341, 0.0000, 0.0206,  ..., 0.0245, 0.0000, 0.0727],
        [0.0131, 0.0000, 0.0079,  ..., 0.0136, 0.0000, 0.0298],
        [0.0057, 0.0000, 0.0035,  ..., 0.0096, 0.0000, 0.0140],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57414.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0560, 0.0000, 0.0776,  ..., 0.0961, 0.0000, 0.1650],
        [0.0615, 0.0072, 0.0553,  ..., 0.0819, 0.0057, 0.1219],
        [0.0653, 0.0229, 0.0373,  ..., 0.0716, 0.0238, 0.0882],
        ...,
        [0.0702, 0.0879, 0.0017,  ..., 0.0580, 0.1039, 0.0268],
        [0.0702, 0.0879, 0.0017,  ..., 0.0580, 0.1039, 0.0268],
        [0.0701, 0.0879, 0.0017,  ..., 0.0580, 0.1038, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558918., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8496.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(809.4667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(301.6127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(311.0229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3538],
        [ 0.2635],
        [ 0.0973],
        ...,
        [-1.9475],
        [-1.9425],
        [-1.9408]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245191.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0182],
        [1.0235],
        ...,
        [0.9999],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369581.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(383.5851, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0183],
        [1.0236],
        ...,
        [0.9999],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369590., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1564.0039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-12.1617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5591, device='cuda:0')



h[200].sum tensor(63.6712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54743.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0680, 0.0861, 0.0013,  ..., 0.0565, 0.1009, 0.0259],
        [0.0681, 0.0862, 0.0014,  ..., 0.0565, 0.1011, 0.0260],
        [0.0683, 0.0840, 0.0022,  ..., 0.0570, 0.0983, 0.0281],
        ...,
        [0.0703, 0.0884, 0.0016,  ..., 0.0579, 0.1041, 0.0267],
        [0.0703, 0.0883, 0.0016,  ..., 0.0579, 0.1041, 0.0267],
        [0.0702, 0.0883, 0.0016,  ..., 0.0579, 0.1041, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550836.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8472.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(799.5532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(286.9507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.7231, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9678],
        [-1.8217],
        [-1.5860],
        ...,
        [-1.7427],
        [-1.8906],
        [-1.9358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270094.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0183],
        [1.0236],
        ...,
        [0.9999],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369590., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0184],
        [1.0236],
        ...,
        [0.9999],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369598.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2089.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0218, device='cuda:0')



h[100].sum tensor(-17.3036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8084, device='cuda:0')



h[200].sum tensor(71.1981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.6300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64814.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0677, 0.0492, 0.0205,  ..., 0.0629, 0.0513, 0.0593],
        [0.0690, 0.0681, 0.0121,  ..., 0.0588, 0.0754, 0.0442],
        [0.0694, 0.0745, 0.0089,  ..., 0.0577, 0.0840, 0.0387],
        ...,
        [0.0703, 0.0886, 0.0016,  ..., 0.0578, 0.1041, 0.0268],
        [0.0703, 0.0886, 0.0016,  ..., 0.0578, 0.1041, 0.0268],
        [0.0703, 0.0886, 0.0016,  ..., 0.0577, 0.1041, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595762.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8431.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(849.5286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(348.5221, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(353.6385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1278],
        [-0.4102],
        [-0.6416],
        ...,
        [-1.9715],
        [-1.9604],
        [-1.9258]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233930.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0184],
        [1.0236],
        ...,
        [0.9999],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369598.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0173],
        [1.0185],
        [1.0237],
        ...,
        [0.9998],
        [0.9990],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369607.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0100, -0.0025,  0.0061,  ...,  0.0068, -0.0031,  0.0209],
        [ 0.0055, -0.0015,  0.0034,  ...,  0.0047, -0.0019,  0.0126],
        [ 0.0138, -0.0033,  0.0084,  ...,  0.0086, -0.0042,  0.0277],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2264.3408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0235, device='cuda:0')



h[100].sum tensor(-18.9712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8713, device='cuda:0')



h[200].sum tensor(73.6580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7680, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0228, 0.0000, 0.0138,  ..., 0.0192, 0.0000, 0.0516],
        [0.0333, 0.0000, 0.0203,  ..., 0.0236, 0.0000, 0.0689],
        [0.0169, 0.0000, 0.0103,  ..., 0.0153, 0.0000, 0.0365],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66953.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0607, 0.0000, 0.0766,  ..., 0.0898, 0.0000, 0.1577],
        [0.0570, 0.0000, 0.0810,  ..., 0.0969, 0.0000, 0.1707],
        [0.0589, 0.0000, 0.0683,  ..., 0.0908, 0.0000, 0.1481],
        ...,
        [0.0706, 0.0890, 0.0015,  ..., 0.0579, 0.1044, 0.0269],
        [0.0706, 0.0890, 0.0015,  ..., 0.0579, 0.1044, 0.0269],
        [0.0706, 0.0890, 0.0015,  ..., 0.0579, 0.1044, 0.0269]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603341.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8417.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(857.2294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(364.2458, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(365.4605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3534],
        [ 0.3842],
        [ 0.3840],
        ...,
        [-1.9651],
        [-1.9512],
        [-1.9455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260567.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0173],
        [1.0185],
        [1.0237],
        ...,
        [0.9998],
        [0.9990],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369607.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0186],
        [1.0238],
        ...,
        [0.9998],
        [0.9990],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369615.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0012,  0.0025,  ...,  0.0040, -0.0015,  0.0100],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1513.6045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-11.3723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5284, device='cuda:0')



h[200].sum tensor(63.9652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5631, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0158, 0.0000, 0.0096,  ..., 0.0148, 0.0000, 0.0345],
        [0.0042, 0.0000, 0.0025,  ..., 0.0088, 0.0000, 0.0110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54318.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0594, 0.0098, 0.0638,  ..., 0.0894, 0.0092, 0.1414],
        [0.0647, 0.0295, 0.0355,  ..., 0.0731, 0.0323, 0.0882],
        [0.0679, 0.0579, 0.0155,  ..., 0.0626, 0.0635, 0.0511],
        ...,
        [0.0709, 0.0893, 0.0015,  ..., 0.0580, 0.1044, 0.0270],
        [0.0709, 0.0892, 0.0015,  ..., 0.0580, 0.1043, 0.0270],
        [0.0708, 0.0892, 0.0015,  ..., 0.0580, 0.1043, 0.0270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549980.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8645.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(810.0854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.0967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.7959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2317],
        [ 0.0412],
        [-0.2692],
        ...,
        [-1.9914],
        [-1.9862],
        [-1.9844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240011.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0186],
        [1.0238],
        ...,
        [0.9998],
        [0.9990],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369615.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0187],
        [1.0238],
        ...,
        [0.9998],
        [0.9990],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369623.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0091, -0.0023,  0.0056,  ...,  0.0064, -0.0029,  0.0193],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1435.7468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0132, device='cuda:0')



h[100].sum tensor(-10.4615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4878, device='cuda:0')



h[200].sum tensor(63.0118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        [0.0124, 0.0000, 0.0075,  ..., 0.0132, 0.0000, 0.0284],
        [0.0103, 0.0000, 0.0062,  ..., 0.0128, 0.0000, 0.0267],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52575.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0666, 0.0402, 0.0216,  ..., 0.0668, 0.0407, 0.0645],
        [0.0661, 0.0222, 0.0370,  ..., 0.0725, 0.0229, 0.0897],
        [0.0663, 0.0123, 0.0499,  ..., 0.0766, 0.0120, 0.1099],
        ...,
        [0.0711, 0.0894, 0.0017,  ..., 0.0582, 0.1041, 0.0274],
        [0.0711, 0.0894, 0.0017,  ..., 0.0582, 0.1041, 0.0273],
        [0.0711, 0.0893, 0.0017,  ..., 0.0582, 0.1041, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(545176.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8561.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(790.2933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(274.8570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(289.8567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0380],
        [ 0.0894],
        [ 0.1999],
        ...,
        [-1.9765],
        [-1.9806],
        [-1.9802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281957.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0187],
        [1.0238],
        ...,
        [0.9998],
        [0.9990],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369623.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0188],
        [1.0239],
        ...,
        [0.9998],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369632.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1711.6869, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0166, device='cuda:0')



h[100].sum tensor(-13.0408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6141, device='cuda:0')



h[200].sum tensor(66.8016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56601.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0691, 0.0820, 0.0039,  ..., 0.0579, 0.0937, 0.0315],
        [0.0694, 0.0822, 0.0049,  ..., 0.0577, 0.0937, 0.0320],
        [0.0696, 0.0791, 0.0067,  ..., 0.0584, 0.0899, 0.0348],
        ...,
        [0.0711, 0.0856, 0.0031,  ..., 0.0593, 0.0989, 0.0306],
        [0.0714, 0.0895, 0.0018,  ..., 0.0584, 0.1038, 0.0277],
        [0.0714, 0.0894, 0.0018,  ..., 0.0584, 0.1037, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560567.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8541.8799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(802.7339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(300.4960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.2378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0350],
        [-1.0021],
        [-0.8260],
        ...,
        [-1.8022],
        [-1.9240],
        [-1.9714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274259.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0188],
        [1.0239],
        ...,
        [0.9998],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369632.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0188],
        [1.0239],
        ...,
        [0.9998],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369632.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1866.6633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0184, device='cuda:0')



h[100].sum tensor(-14.5741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6800, device='cuda:0')



h[200].sum tensor(68.8368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60049.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0694, 0.0846, 0.0035,  ..., 0.0571, 0.0968, 0.0298],
        [0.0692, 0.0864, 0.0022,  ..., 0.0571, 0.0995, 0.0278],
        [0.0694, 0.0874, 0.0017,  ..., 0.0571, 0.1011, 0.0270],
        ...,
        [0.0714, 0.0895, 0.0018,  ..., 0.0584, 0.1038, 0.0277],
        [0.0714, 0.0895, 0.0018,  ..., 0.0584, 0.1038, 0.0277],
        [0.0714, 0.0894, 0.0018,  ..., 0.0584, 0.1037, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572552.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8570.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(821.3079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(320.6128, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(325.1986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1625],
        [-1.4773],
        [-1.6915],
        ...,
        [-1.9922],
        [-1.9872],
        [-1.9855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255912.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0188],
        [1.0239],
        ...,
        [0.9998],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369632.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0178],
        [1.0190],
        [1.0240],
        ...,
        [0.9998],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369641.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1651.6633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-12.3090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5778, device='cuda:0')



h[200].sum tensor(66.1235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55556.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0693, 0.0874, 0.0017,  ..., 0.0572, 0.1005, 0.0271],
        [0.0693, 0.0874, 0.0017,  ..., 0.0572, 0.1006, 0.0271],
        [0.0696, 0.0876, 0.0018,  ..., 0.0573, 0.1009, 0.0272],
        ...,
        [0.0716, 0.0896, 0.0020,  ..., 0.0586, 0.1036, 0.0279],
        [0.0716, 0.0896, 0.0020,  ..., 0.0586, 0.1036, 0.0279],
        [0.0716, 0.0896, 0.0020,  ..., 0.0586, 0.1036, 0.0279]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555065.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8664.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(803.3146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(290.8778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(301.6601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0795],
        [-1.9967],
        [-1.8539],
        ...,
        [-1.9941],
        [-1.9892],
        [-1.9875]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247862.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0178],
        [1.0190],
        [1.0240],
        ...,
        [0.9998],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369641.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0191],
        [1.0241],
        ...,
        [0.9997],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369649.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0097, -0.0024,  0.0059,  ...,  0.0066, -0.0030,  0.0203],
        [ 0.0103, -0.0025,  0.0063,  ...,  0.0069, -0.0032,  0.0216],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1907.5627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0187, device='cuda:0')



h[100].sum tensor(-14.7490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6937, device='cuda:0')



h[200].sum tensor(69.5003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0098, 0.0000, 0.0060,  ..., 0.0114, 0.0000, 0.0216],
        [0.0183, 0.0000, 0.0112,  ..., 0.0160, 0.0000, 0.0396],
        [0.0571, 0.0000, 0.0348,  ..., 0.0353, 0.0000, 0.1153],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60554.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0646, 0.0341, 0.0301,  ..., 0.0742, 0.0366, 0.0824],
        [0.0599, 0.0114, 0.0561,  ..., 0.0900, 0.0097, 0.1324],
        [0.0506, 0.0000, 0.0993,  ..., 0.1188, 0.0000, 0.2190],
        ...,
        [0.0719, 0.0899, 0.0020,  ..., 0.0588, 0.1037, 0.0281],
        [0.0719, 0.0899, 0.0020,  ..., 0.0588, 0.1037, 0.0281],
        [0.0718, 0.0899, 0.0020,  ..., 0.0587, 0.1037, 0.0280]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581015.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8638.0068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(822.9945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(323.3543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(325.3248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6403],
        [-0.3553],
        [-0.0413],
        ...,
        [-2.0017],
        [-1.9968],
        [-1.9951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237892.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0191],
        [1.0241],
        ...,
        [0.9997],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369649.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0192],
        [1.0242],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369658.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2273.3071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0233, device='cuda:0')



h[100].sum tensor(-18.2950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8608, device='cuda:0')



h[200].sum tensor(74.3894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.5790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69081.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0697, 0.0880, 0.0017,  ..., 0.0574, 0.1009, 0.0273],
        [0.0699, 0.0862, 0.0024,  ..., 0.0578, 0.0984, 0.0291],
        [0.0704, 0.0781, 0.0074,  ..., 0.0591, 0.0877, 0.0368],
        ...,
        [0.0721, 0.0903, 0.0019,  ..., 0.0589, 0.1040, 0.0281],
        [0.0721, 0.0903, 0.0019,  ..., 0.0589, 0.1040, 0.0281],
        [0.0721, 0.0902, 0.0019,  ..., 0.0589, 0.1040, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618953.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8684.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.8665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(374.7214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(369.5927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2048],
        [-1.0698],
        [-0.7747],
        ...,
        [-2.0056],
        [-2.0049],
        [-2.0046]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216936.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0192],
        [1.0242],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369658.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(452.7640, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0183],
        [1.0194],
        [1.0243],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369666.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1829.0217, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0178, device='cuda:0')



h[100].sum tensor(-13.9493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6574, device='cuda:0')



h[200].sum tensor(68.9146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61759.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0698, 0.0886, 0.0014,  ..., 0.0574, 0.1016, 0.0269],
        [0.0699, 0.0887, 0.0014,  ..., 0.0575, 0.1017, 0.0270],
        [0.0701, 0.0888, 0.0015,  ..., 0.0576, 0.1020, 0.0270],
        ...,
        [0.0722, 0.0909, 0.0016,  ..., 0.0589, 0.1047, 0.0277],
        [0.0722, 0.0909, 0.0016,  ..., 0.0589, 0.1047, 0.0277],
        [0.0721, 0.0908, 0.0016,  ..., 0.0589, 0.1047, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599341.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8541.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(827.6029, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(337.0541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(335.7089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6462],
        [-1.7688],
        [-1.8477],
        ...,
        [-2.0370],
        [-2.0320],
        [-2.0303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287287.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0183],
        [1.0194],
        [1.0243],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369666.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0184],
        [1.0195],
        [1.0243],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369675.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1868.1372, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0181, device='cuda:0')



h[100].sum tensor(-14.3310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6689, device='cuda:0')



h[200].sum tensor(69.7375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59553.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0702, 0.0882, 0.0021,  ..., 0.0573, 0.1005, 0.0281],
        [0.0706, 0.0820, 0.0054,  ..., 0.0582, 0.0920, 0.0341],
        [0.0704, 0.0644, 0.0143,  ..., 0.0617, 0.0694, 0.0492],
        ...,
        [0.0723, 0.0914, 0.0014,  ..., 0.0589, 0.1053, 0.0275],
        [0.0723, 0.0914, 0.0014,  ..., 0.0589, 0.1053, 0.0275],
        [0.0722, 0.0914, 0.0014,  ..., 0.0588, 0.1052, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574046.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8782.3545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(840.1013, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(317.0266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.4556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4327],
        [-1.1047],
        [-0.7096],
        ...,
        [-2.0557],
        [-2.0505],
        [-2.0488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244376.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0184],
        [1.0195],
        [1.0243],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369675.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0196],
        [1.0243],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369683.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0104, -0.0025,  0.0064,  ...,  0.0070, -0.0032,  0.0216],
        [ 0.0202, -0.0046,  0.0123,  ...,  0.0116, -0.0059,  0.0395],
        [ 0.0246, -0.0056,  0.0150,  ...,  0.0137, -0.0071,  0.0478],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1692.4211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0160, device='cuda:0')



h[100].sum tensor(-12.5835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5921, device='cuda:0')



h[200].sum tensor(67.6167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0712, 0.0000, 0.0435,  ..., 0.0420, 0.0000, 0.1409],
        [0.0959, 0.0000, 0.0586,  ..., 0.0536, 0.0000, 0.1864],
        [0.0956, 0.0000, 0.0584,  ..., 0.0535, 0.0000, 0.1859],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57369.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0000, 0.1808,  ..., 0.1753, 0.0000, 0.3886],
        [0.0242, 0.0000, 0.2045,  ..., 0.1954, 0.0000, 0.4426],
        [0.0218, 0.0000, 0.2133,  ..., 0.2026, 0.0000, 0.4617],
        ...,
        [0.0725, 0.0918, 0.0013,  ..., 0.0590, 0.1057, 0.0275],
        [0.0725, 0.0918, 0.0013,  ..., 0.0590, 0.1057, 0.0275],
        [0.0724, 0.0918, 0.0013,  ..., 0.0590, 0.1056, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571435.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8736.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.3574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(306.9735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.5127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2828],
        [ 0.2703],
        [ 0.2692],
        ...,
        [-2.0684],
        [-2.0629],
        [-2.0609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263403.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0196],
        [1.0243],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369683.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0187],
        [1.0197],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369692.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089, -0.0022,  0.0054,  ...,  0.0063, -0.0028,  0.0188],
        [ 0.0070, -0.0018,  0.0043,  ...,  0.0054, -0.0022,  0.0153],
        [ 0.0174, -0.0040,  0.0106,  ...,  0.0103, -0.0051,  0.0345],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1252.8518, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0105, device='cuda:0')



h[100].sum tensor(-8.1991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.3892, device='cuda:0')



h[200].sum tensor(62.0693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0436, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0000, 0.0087,  ..., 0.0141, 0.0000, 0.0318],
        [0.0480, 0.0000, 0.0293,  ..., 0.0311, 0.0000, 0.0982],
        [0.0500, 0.0000, 0.0305,  ..., 0.0321, 0.0000, 0.1021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48356.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0631, 0.0179, 0.0473,  ..., 0.0839, 0.0175, 0.1154],
        [0.0556, 0.0000, 0.0844,  ..., 0.1079, 0.0000, 0.1890],
        [0.0528, 0.0000, 0.0989,  ..., 0.1173, 0.0000, 0.2175],
        ...,
        [0.0726, 0.0920, 0.0014,  ..., 0.0592, 0.1056, 0.0277],
        [0.0726, 0.0920, 0.0014,  ..., 0.0591, 0.1056, 0.0277],
        [0.0726, 0.0919, 0.0014,  ..., 0.0591, 0.1056, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531477.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8726.5762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(780.2311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(254.2358, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(274.9414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6193],
        [-0.1298],
        [ 0.1911],
        ...,
        [-2.0744],
        [-2.0692],
        [-2.0675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311642.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0187],
        [1.0197],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369692.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0189],
        [1.0199],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369701.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0011,  0.0025,  ...,  0.0040, -0.0014,  0.0100],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0041, -0.0011,  0.0025,  ...,  0.0040, -0.0014,  0.0100],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1621.4026, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0149, device='cuda:0')



h[100].sum tensor(-11.6654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5525, device='cuda:0')



h[200].sum tensor(66.8592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9989, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0000, 0.0019,  ..., 0.0083, 0.0000, 0.0092],
        [0.0146, 0.0000, 0.0089,  ..., 0.0154, 0.0000, 0.0371],
        [0.0032, 0.0000, 0.0019,  ..., 0.0083, 0.0000, 0.0093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54463.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0708, 0.0610, 0.0164,  ..., 0.0627, 0.0648, 0.0531],
        [0.0710, 0.0401, 0.0281,  ..., 0.0663, 0.0377, 0.0717],
        [0.0712, 0.0611, 0.0166,  ..., 0.0629, 0.0650, 0.0533],
        ...,
        [0.0729, 0.0920, 0.0016,  ..., 0.0594, 0.1055, 0.0280],
        [0.0729, 0.0920, 0.0016,  ..., 0.0594, 0.1055, 0.0280],
        [0.0728, 0.0920, 0.0016,  ..., 0.0594, 0.1055, 0.0280]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553127.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8756.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(806.1240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(290.4711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.3133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2926],
        [-1.3444],
        [-1.5328],
        ...,
        [-2.0755],
        [-2.0704],
        [-2.0687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287834.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0189],
        [1.0199],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369701.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0200],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369710.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0060, -0.0016,  0.0037,  ...,  0.0049, -0.0020,  0.0137],
        [ 0.0258, -0.0058,  0.0157,  ...,  0.0142, -0.0073,  0.0500],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3389.1470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0370, device='cuda:0')



h[100].sum tensor(-28.6184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.3695, device='cuda:0')



h[200].sum tensor(90.0907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7854, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0000, 0.0104,  ..., 0.0154, 0.0000, 0.0371],
        [0.0535, 0.0000, 0.0327,  ..., 0.0332, 0.0000, 0.1067],
        [0.0717, 0.0000, 0.0438,  ..., 0.0423, 0.0000, 0.1425],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89883.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0532, 0.0103, 0.0904,  ..., 0.1143, 0.0097, 0.2044],
        [0.0353, 0.0000, 0.1602,  ..., 0.1651, 0.0000, 0.3517],
        [0.0207, 0.0000, 0.2172,  ..., 0.2068, 0.0000, 0.4723],
        ...,
        [0.0731, 0.0919, 0.0018,  ..., 0.0596, 0.1050, 0.0285],
        [0.0731, 0.0919, 0.0018,  ..., 0.0596, 0.1050, 0.0285],
        [0.0731, 0.0919, 0.0018,  ..., 0.0596, 0.1050, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(724590., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8472.2822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(943.9593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(515.7842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(472.0140, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1314],
        [ 0.1652],
        [ 0.1486],
        ...,
        [-2.0717],
        [-2.0666],
        [-2.0650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251423.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0200],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369710.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0201],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369719.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1409.5220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0119, device='cuda:0')



h[100].sum tensor(-9.2937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4419, device='cuda:0')



h[200].sum tensor(64.5325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9973, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51051.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0709, 0.0895, 0.0018,  ..., 0.0583, 0.1014, 0.0281],
        [0.0710, 0.0896, 0.0018,  ..., 0.0583, 0.1015, 0.0281],
        [0.0710, 0.0857, 0.0031,  ..., 0.0593, 0.0969, 0.0312],
        ...,
        [0.0734, 0.0918, 0.0021,  ..., 0.0597, 0.1046, 0.0289],
        [0.0734, 0.0918, 0.0021,  ..., 0.0597, 0.1046, 0.0289],
        [0.0733, 0.0917, 0.0021,  ..., 0.0597, 0.1045, 0.0289]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541411.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8727.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(776.9586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(271.1541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(273.4236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0030],
        [-1.8276],
        [-1.5345],
        ...,
        [-2.0660],
        [-2.0610],
        [-2.0593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300008.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0201],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369719.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0202],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369728.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1799.6208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-12.9166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6182, device='cuda:0')



h[200].sum tensor(69.7790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57477.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0712, 0.0895, 0.0020,  ..., 0.0584, 0.1011, 0.0284],
        [0.0713, 0.0896, 0.0020,  ..., 0.0585, 0.1012, 0.0284],
        [0.0715, 0.0897, 0.0021,  ..., 0.0586, 0.1015, 0.0285],
        ...,
        [0.0736, 0.0918, 0.0023,  ..., 0.0599, 0.1042, 0.0292],
        [0.0736, 0.0918, 0.0023,  ..., 0.0599, 0.1042, 0.0292],
        [0.0735, 0.0917, 0.0023,  ..., 0.0599, 0.1041, 0.0292]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564517.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8835.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(811.3085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(305.6367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.5922, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9995],
        [-2.0433],
        [-2.0420],
        ...,
        [-2.0628],
        [-2.0579],
        [-2.0563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240582.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0202],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369728.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0202],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369728.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0269, -0.0060,  0.0164,  ...,  0.0148, -0.0076,  0.0522],
        [ 0.0174, -0.0040,  0.0106,  ...,  0.0103, -0.0050,  0.0346],
        [ 0.0085, -0.0021,  0.0052,  ...,  0.0061, -0.0026,  0.0182],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2489.9138, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0248, device='cuda:0')



h[100].sum tensor(-19.5685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9185, device='cuda:0')



h[200].sum tensor(78.7414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0799, 0.0000, 0.0488,  ..., 0.0462, 0.0000, 0.1580],
        [0.0571, 0.0000, 0.0348,  ..., 0.0354, 0.0000, 0.1159],
        [0.0310, 0.0000, 0.0189,  ..., 0.0226, 0.0000, 0.0655],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71540.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0354, 0.0000, 0.1741,  ..., 0.1684, 0.0000, 0.3717],
        [0.0424, 0.0000, 0.1515,  ..., 0.1505, 0.0000, 0.3223],
        [0.0511, 0.0000, 0.1226,  ..., 0.1281, 0.0000, 0.2593],
        ...,
        [0.0736, 0.0918, 0.0023,  ..., 0.0599, 0.1042, 0.0292],
        [0.0736, 0.0918, 0.0023,  ..., 0.0599, 0.1042, 0.0292],
        [0.0735, 0.0917, 0.0023,  ..., 0.0599, 0.1041, 0.0292]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633133.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8732.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(873.0187, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(392.9473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(373.1866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4019],
        [ 0.4191],
        [ 0.4399],
        ...,
        [-2.0628],
        [-2.0579],
        [-2.0563]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208814.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0202],
        [1.0244],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369728.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0203],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369737.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2204.5400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0216, device='cuda:0')



h[100].sum tensor(-16.7227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8001, device='cuda:0')



h[200].sum tensor(75.4784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0029,  ..., 0.0090, 0.0000, 0.0126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64922.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0703, 0.0427, 0.0253,  ..., 0.0669, 0.0448, 0.0686],
        [0.0715, 0.0743, 0.0100,  ..., 0.0608, 0.0811, 0.0420],
        [0.0718, 0.0868, 0.0038,  ..., 0.0588, 0.0972, 0.0316],
        ...,
        [0.0736, 0.0919, 0.0023,  ..., 0.0599, 0.1042, 0.0293],
        [0.0736, 0.0919, 0.0023,  ..., 0.0599, 0.1042, 0.0293],
        [0.0736, 0.0919, 0.0023,  ..., 0.0599, 0.1041, 0.0293]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599286.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8781.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.4276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.8354, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(339.8907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3080],
        [-0.7247],
        [-1.0909],
        ...,
        [-2.0697],
        [-2.0648],
        [-2.0632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230363.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0203],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369737.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 400.0 event: 2000 loss: tensor(433.1750, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.3762, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0203],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369745.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.3762, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2068.1157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-15.3614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7276, device='cuda:0')



h[200].sum tensor(74.3346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0000, 0.0028,  ..., 0.0090, 0.0000, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63575.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0700, 0.0448, 0.0237,  ..., 0.0665, 0.0479, 0.0658],
        [0.0710, 0.0768, 0.0082,  ..., 0.0607, 0.0847, 0.0393],
        [0.0714, 0.0882, 0.0027,  ..., 0.0588, 0.0993, 0.0300],
        ...,
        [0.0736, 0.0922, 0.0022,  ..., 0.0598, 0.1044, 0.0292],
        [0.0736, 0.0922, 0.0022,  ..., 0.0598, 0.1044, 0.0292],
        [0.0735, 0.0922, 0.0022,  ..., 0.0598, 0.1043, 0.0292]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598425.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8736.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.4012, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(344.8997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.7903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5696],
        [-1.0386],
        [-1.3775],
        ...,
        [-2.0840],
        [-2.0790],
        [-2.0774]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234461.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0203],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369745.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(280.8237, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0196],
        [1.0204],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369754.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(280.8237, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056, -0.0014,  0.0034,  ...,  0.0047, -0.0018,  0.0128],
        [ 0.0045, -0.0012,  0.0027,  ...,  0.0042, -0.0015,  0.0109],
        [ 0.0113, -0.0026,  0.0069,  ...,  0.0074, -0.0033,  0.0233],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2170.8486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0207, device='cuda:0')



h[100].sum tensor(-16.3308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7671, device='cuda:0')



h[200].sum tensor(76.2660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0137, 0.0000, 0.0083,  ..., 0.0138, 0.0000, 0.0312],
        [0.0358, 0.0000, 0.0218,  ..., 0.0254, 0.0000, 0.0766],
        [0.0277, 0.0000, 0.0169,  ..., 0.0210, 0.0000, 0.0592],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63551.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0639, 0.0081, 0.0588,  ..., 0.0853, 0.0068, 0.1305],
        [0.0583, 0.0000, 0.0886,  ..., 0.1025, 0.0000, 0.1865],
        [0.0568, 0.0000, 0.0938,  ..., 0.1065, 0.0000, 0.1974],
        ...,
        [0.0735, 0.0926, 0.0021,  ..., 0.0597, 0.1048, 0.0290],
        [0.0735, 0.0926, 0.0021,  ..., 0.0597, 0.1048, 0.0290],
        [0.0734, 0.0926, 0.0021,  ..., 0.0597, 0.1047, 0.0290]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591058.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8819.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(858.2977, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(341.5737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(345.0952, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3689],
        [ 0.3839],
        [ 0.3787],
        ...,
        [-2.1031],
        [-2.0981],
        [-2.0965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212950.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0196],
        [1.0204],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369754.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3367],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0197],
        [1.0204],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369762.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3367],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058, -0.0015,  0.0035,  ...,  0.0048, -0.0019,  0.0131],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0058, -0.0015,  0.0035,  ...,  0.0048, -0.0019,  0.0131],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2175.6919, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-16.3290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7742, device='cuda:0')



h[200].sum tensor(77.2078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0110, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0000, 0.0028,  ..., 0.0090, 0.0000, 0.0119],
        [0.0209, 0.0000, 0.0127,  ..., 0.0184, 0.0000, 0.0487],
        [0.0046, 0.0000, 0.0028,  ..., 0.0090, 0.0000, 0.0119],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65415.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0700, 0.0430, 0.0252,  ..., 0.0661, 0.0421, 0.0679],
        [0.0690, 0.0167, 0.0378,  ..., 0.0709, 0.0097, 0.0889],
        [0.0704, 0.0419, 0.0262,  ..., 0.0662, 0.0406, 0.0692],
        ...,
        [0.0733, 0.0931, 0.0018,  ..., 0.0595, 0.1052, 0.0286],
        [0.0732, 0.0931, 0.0018,  ..., 0.0595, 0.1052, 0.0286],
        [0.0732, 0.0931, 0.0018,  ..., 0.0595, 0.1051, 0.0286]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607734.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8580.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.3115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(360.0278, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(360.8174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0775],
        [-0.1948],
        [-0.3754],
        ...,
        [-2.1231],
        [-2.1176],
        [-2.1147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256727.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0197],
        [1.0204],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369762.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.9503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0205],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369770.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.9503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0184, -0.0041,  0.0112,  ...,  0.0108, -0.0052,  0.0364],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2054.6787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0196, device='cuda:0')



h[100].sum tensor(-15.1127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7237, device='cuda:0')



h[200].sum tensor(76.2341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0012],
        [0.0188, 0.0000, 0.0115,  ..., 0.0157, 0.0000, 0.0380],
        [0.0152, 0.0000, 0.0093,  ..., 0.0140, 0.0000, 0.0314],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62066.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0665, 0.0402, 0.0198,  ..., 0.0694, 0.0412, 0.0648],
        [0.0608, 0.0218, 0.0394,  ..., 0.0832, 0.0227, 0.1053],
        [0.0570, 0.0069, 0.0542,  ..., 0.0931, 0.0059, 0.1351],
        ...,
        [0.0731, 0.0934, 0.0016,  ..., 0.0594, 0.1054, 0.0284],
        [0.0731, 0.0934, 0.0016,  ..., 0.0594, 0.1054, 0.0284],
        [0.0731, 0.0934, 0.0016,  ..., 0.0594, 0.1054, 0.0284]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594617.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8489.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(849.0318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(341.3960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(348.1104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8068],
        [-0.7263],
        [-0.4967],
        ...,
        [-2.1374],
        [-2.1322],
        [-2.1306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286476.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0205],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369770.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3127],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.6909, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0199],
        [1.0206],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369778.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3127],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.6909, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [ 0.0053, -0.0014,  0.0032,  ...,  0.0046, -0.0017,  0.0122],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2770.9065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0282, device='cuda:0')



h[100].sum tensor(-21.8420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0454, device='cuda:0')



h[200].sum tensor(85.6994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9186, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0000, 0.0033,  ..., 0.0094, 0.0000, 0.0133],
        [0.0042, 0.0000, 0.0025,  ..., 0.0088, 0.0000, 0.0111],
        [0.0191, 0.0000, 0.0116,  ..., 0.0176, 0.0000, 0.0454],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75658.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0697, 0.0599, 0.0159,  ..., 0.0636, 0.0637, 0.0532],
        [0.0697, 0.0535, 0.0194,  ..., 0.0647, 0.0558, 0.0588],
        [0.0690, 0.0263, 0.0332,  ..., 0.0696, 0.0223, 0.0810],
        ...,
        [0.0732, 0.0936, 0.0016,  ..., 0.0596, 0.1055, 0.0285],
        [0.0732, 0.0936, 0.0016,  ..., 0.0596, 0.1055, 0.0285],
        [0.0731, 0.0935, 0.0016,  ..., 0.0595, 0.1055, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653932.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8357.0967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(907.5134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(426.9748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(416.9431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2866],
        [-1.3921],
        [-1.4804],
        ...,
        [-2.1326],
        [-2.1280],
        [-2.1281]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260776.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0199],
        [1.0206],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369778.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0200],
        [1.0206],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369786.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1683.4534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-11.4496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5451, device='cuda:0')



h[200].sum tensor(71.4005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56802.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0710, 0.0913, 0.0015,  ..., 0.0585, 0.1024, 0.0279],
        [0.0711, 0.0914, 0.0015,  ..., 0.0585, 0.1026, 0.0280],
        [0.0713, 0.0915, 0.0016,  ..., 0.0586, 0.1028, 0.0280],
        ...,
        [0.0734, 0.0936, 0.0018,  ..., 0.0599, 0.1056, 0.0288],
        [0.0734, 0.0936, 0.0018,  ..., 0.0599, 0.1056, 0.0288],
        [0.0734, 0.0936, 0.0018,  ..., 0.0599, 0.1056, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573922.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8584.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.2621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(307.6658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.6762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3120],
        [-2.3056],
        [-2.2777],
        ...,
        [-2.1446],
        [-2.1395],
        [-2.1378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261394.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0200],
        [1.0206],
        [1.0245],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369786.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7031],
        [0.2686],
        [0.3066],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0201],
        [1.0207],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369794.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7031],
        [0.2686],
        [0.3066],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0012,  0.0026,  ...,  0.0041, -0.0015,  0.0106],
        [ 0.0303, -0.0066,  0.0185,  ...,  0.0164, -0.0084,  0.0585],
        [ 0.0150, -0.0034,  0.0092,  ...,  0.0092, -0.0043,  0.0303],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1878.5125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-13.2486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6294, device='cuda:0')



h[200].sum tensor(73.2907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0796, 0.0000, 0.0486,  ..., 0.0461, 0.0000, 0.1573],
        [0.0528, 0.0000, 0.0322,  ..., 0.0334, 0.0000, 0.1079],
        [0.0578, 0.0000, 0.0353,  ..., 0.0358, 0.0000, 0.1173],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58645.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0402, 0.0000, 0.1264,  ..., 0.1356, 0.0000, 0.2720],
        [0.0441, 0.0000, 0.1181,  ..., 0.1281, 0.0000, 0.2527],
        [0.0459, 0.0000, 0.1121,  ..., 0.1243, 0.0000, 0.2408],
        ...,
        [0.0739, 0.0936, 0.0020,  ..., 0.0604, 0.1058, 0.0292],
        [0.0739, 0.0936, 0.0020,  ..., 0.0604, 0.1058, 0.0292],
        [0.0738, 0.0935, 0.0020,  ..., 0.0604, 0.1057, 0.0292]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576244.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8648.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.5743, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(320.1605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(321.3662, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3244],
        [ 0.2261],
        [-0.0280],
        ...,
        [-2.1515],
        [-2.1464],
        [-2.1448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270663.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0201],
        [1.0207],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369794.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.5658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0202],
        [1.0208],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369802.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.5658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1828.1915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-12.6973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6025, device='cuda:0')



h[200].sum tensor(72.0664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0017],
        [0.0051, 0.0000, 0.0031,  ..., 0.0093, 0.0000, 0.0135],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60296.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0723, 0.0871, 0.0045,  ..., 0.0596, 0.0969, 0.0330],
        [0.0718, 0.0739, 0.0108,  ..., 0.0620, 0.0808, 0.0435],
        [0.0692, 0.0385, 0.0296,  ..., 0.0708, 0.0406, 0.0769],
        ...,
        [0.0743, 0.0935, 0.0023,  ..., 0.0609, 0.1058, 0.0297],
        [0.0743, 0.0935, 0.0023,  ..., 0.0609, 0.1058, 0.0297],
        [0.0743, 0.0935, 0.0023,  ..., 0.0609, 0.1057, 0.0297]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595489.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8659.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(826.9161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(331.7721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(321.3954, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1944],
        [-0.7620],
        [-0.2949],
        ...,
        [-2.1496],
        [-2.1436],
        [-2.1400]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257609.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0202],
        [1.0208],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369802.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4327, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0208],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369810.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4327, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0108, -0.0025,  0.0066,  ...,  0.0072, -0.0032,  0.0226],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1567.7098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.9565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0132, device='cuda:0')



h[100].sum tensor(-10.1880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4901, device='cuda:0')



h[200].sum tensor(68.3009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0203, 0.0000, 0.0123,  ..., 0.0176, 0.0000, 0.0463],
        [0.0138, 0.0000, 0.0084,  ..., 0.0139, 0.0000, 0.0319],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53516.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0603, 0.0052, 0.0754,  ..., 0.0949, 0.0037, 0.1595],
        [0.0650, 0.0243, 0.0474,  ..., 0.0816, 0.0257, 0.1100],
        [0.0703, 0.0551, 0.0177,  ..., 0.0671, 0.0592, 0.0570],
        ...,
        [0.0747, 0.0935, 0.0025,  ..., 0.0614, 0.1060, 0.0301],
        [0.0747, 0.0935, 0.0025,  ..., 0.0614, 0.1059, 0.0301],
        [0.0747, 0.0935, 0.0025,  ..., 0.0613, 0.1059, 0.0301]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555202.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8820.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(794.2419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(288.8166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.4594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2042],
        [-0.0711],
        [-0.3844],
        ...,
        [-2.1521],
        [-2.1471],
        [-2.1455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259266.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0208],
        [1.0246],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369810.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.2086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0209],
        [1.0247],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369818.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.2086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2195.9233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-16.0447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7736, device='cuda:0')



h[200].sum tensor(76.2027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0006, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64434.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0725, 0.0914, 0.0022,  ..., 0.0602, 0.1030, 0.0293],
        [0.0726, 0.0915, 0.0022,  ..., 0.0603, 0.1031, 0.0294],
        [0.0728, 0.0916, 0.0023,  ..., 0.0603, 0.1033, 0.0294],
        ...,
        [0.0749, 0.0937, 0.0025,  ..., 0.0617, 0.1062, 0.0302],
        [0.0749, 0.0937, 0.0025,  ..., 0.0617, 0.1062, 0.0302],
        [0.0749, 0.0936, 0.0025,  ..., 0.0617, 0.1061, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599994.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8801.9521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.3911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(356.4318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(335.3907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9291],
        [-1.9439],
        [-1.8948],
        ...,
        [-2.1599],
        [-2.1548],
        [-2.1532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219678.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0209],
        [1.0247],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369818.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.3821, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0209],
        [1.0247],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369818.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.3821, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0112, -0.0026,  0.0068,  ...,  0.0074, -0.0033,  0.0234],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1789.2397, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-12.2274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5856, device='cuda:0')



h[200].sum tensor(70.9765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0555, 0.0000, 0.0339,  ..., 0.0342, 0.0000, 0.1114],
        [0.0404, 0.0000, 0.0246,  ..., 0.0270, 0.0000, 0.0834],
        [0.0124, 0.0000, 0.0075,  ..., 0.0127, 0.0000, 0.0271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56944.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0309, 0.0000, 0.1546,  ..., 0.1559, 0.0000, 0.3289],
        [0.0454, 0.0000, 0.1101,  ..., 0.1251, 0.0000, 0.2369],
        [0.0600, 0.0143, 0.0558,  ..., 0.0916, 0.0144, 0.1313],
        ...,
        [0.0749, 0.0937, 0.0025,  ..., 0.0617, 0.1062, 0.0302],
        [0.0749, 0.0937, 0.0025,  ..., 0.0617, 0.1062, 0.0302],
        [0.0749, 0.0936, 0.0025,  ..., 0.0617, 0.1061, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570014.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8837.1523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(809.0057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(310.4485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.4623, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2943],
        [ 0.2300],
        [-0.0081],
        ...,
        [-2.1599],
        [-2.1548],
        [-2.1532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250441.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0209],
        [1.0247],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369818.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.9158, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0210],
        [1.0248],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369826., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.9158, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2245.9888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0213, device='cuda:0')



h[100].sum tensor(-16.4437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7892, device='cuda:0')



h[200].sum tensor(77.3597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66175.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0708, 0.0711, 0.0107,  ..., 0.0644, 0.0786, 0.0443],
        [0.0711, 0.0738, 0.0097,  ..., 0.0639, 0.0819, 0.0425],
        [0.0719, 0.0803, 0.0073,  ..., 0.0626, 0.0896, 0.0380],
        ...,
        [0.0748, 0.0939, 0.0023,  ..., 0.0618, 0.1063, 0.0300],
        [0.0748, 0.0939, 0.0023,  ..., 0.0617, 0.1063, 0.0300],
        [0.0747, 0.0938, 0.0023,  ..., 0.0617, 0.1062, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612574.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8696.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(853.8615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(369.6623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(349.6632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6551],
        [-0.7726],
        [-0.9052],
        ...,
        [-2.1730],
        [-2.1681],
        [-2.1668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237129.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0210],
        [1.0248],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369826., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.1721, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0211],
        [1.0248],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369833.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.1721, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1894.0236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0171, device='cuda:0')



h[100].sum tensor(-13.0790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6315, device='cuda:0')



h[200].sum tensor(73.6988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59748.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0721, 0.0918, 0.0018,  ..., 0.0601, 0.1032, 0.0288],
        [0.0722, 0.0919, 0.0018,  ..., 0.0602, 0.1034, 0.0288],
        [0.0724, 0.0920, 0.0019,  ..., 0.0603, 0.1036, 0.0289],
        ...,
        [0.0745, 0.0941, 0.0021,  ..., 0.0617, 0.1064, 0.0297],
        [0.0745, 0.0941, 0.0021,  ..., 0.0616, 0.1064, 0.0296],
        [0.0745, 0.0940, 0.0021,  ..., 0.0616, 0.1064, 0.0296]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587658.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8688.3232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(835.2505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(328.5899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(325.7205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1689],
        [-2.2496],
        [-2.3018],
        ...,
        [-2.1896],
        [-2.1841],
        [-2.1819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241171.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0211],
        [1.0248],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369833.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.0181, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0212],
        [1.0249],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369841.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.0181, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0077, -0.0018,  0.0047,  ...,  0.0058, -0.0023,  0.0169],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [ 0.0077, -0.0018,  0.0047,  ...,  0.0058, -0.0023,  0.0169],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2245.9583, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0210, device='cuda:0')



h[100].sum tensor(-16.2953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7758, device='cuda:0')



h[200].sum tensor(78.8584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0000, 0.0037,  ..., 0.0098, 0.0000, 0.0153],
        [0.0280, 0.0000, 0.0170,  ..., 0.0219, 0.0000, 0.0627],
        [0.0062, 0.0000, 0.0037,  ..., 0.0098, 0.0000, 0.0154],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66293.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0683, 0.0380, 0.0251,  ..., 0.0709, 0.0398, 0.0705],
        [0.0656, 0.0000, 0.0426,  ..., 0.0789, 0.0000, 0.1008],
        [0.0686, 0.0380, 0.0254,  ..., 0.0710, 0.0399, 0.0707],
        ...,
        [0.0744, 0.0943, 0.0020,  ..., 0.0616, 0.1066, 0.0295],
        [0.0744, 0.0943, 0.0020,  ..., 0.0616, 0.1065, 0.0295],
        [0.0744, 0.0943, 0.0020,  ..., 0.0615, 0.1065, 0.0295]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619694.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8527.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(859.9011, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(374.3752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.8154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3168],
        [-1.2962],
        [-1.4857],
        ...,
        [-2.2015],
        [-2.1962],
        [-2.1945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270428.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0212],
        [1.0249],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369841.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2310, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0207],
        [1.0213],
        [1.0250],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369849.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2310, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [ 0.0047, -0.0012,  0.0029,  ...,  0.0044, -0.0016,  0.0114],
        ...,
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0012,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2427.8828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0233, device='cuda:0')



h[100].sum tensor(-17.8961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8638, device='cuda:0')



h[200].sum tensor(81.6299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0016],
        [0.0048, 0.0000, 0.0029,  ..., 0.0092, 0.0000, 0.0128],
        [0.0168, 0.0000, 0.0102,  ..., 0.0154, 0.0000, 0.0372],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70994.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0709, 0.0657, 0.0137,  ..., 0.0645, 0.0709, 0.0497],
        [0.0670, 0.0333, 0.0353,  ..., 0.0754, 0.0359, 0.0887],
        [0.0610, 0.0090, 0.0642,  ..., 0.0908, 0.0085, 0.1410],
        ...,
        [0.0745, 0.0945, 0.0019,  ..., 0.0616, 0.1066, 0.0295],
        [0.0745, 0.0945, 0.0019,  ..., 0.0616, 0.1066, 0.0295],
        [0.0744, 0.0944, 0.0019,  ..., 0.0616, 0.1065, 0.0295]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641833.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8483.9434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(888.6691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(401.5234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(390.4281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7939],
        [-0.2838],
        [ 0.0856],
        ...,
        [-2.2128],
        [-2.2055],
        [-2.1896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231579.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0207],
        [1.0213],
        [1.0250],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369849.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.6808, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0214],
        [1.0251],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369856.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.6808, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045, -0.0012,  0.0027,  ...,  0.0043, -0.0015,  0.0110],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1832.5352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-12.3011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5974, device='cuda:0')



h[200].sum tensor(73.8414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8107, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0103, 0.0000, 0.0062,  ..., 0.0124, 0.0000, 0.0253],
        [0.0046, 0.0000, 0.0028,  ..., 0.0091, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0016],
        ...,
        [0.0078, 0.0000, 0.0047,  ..., 0.0108, 0.0000, 0.0185],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57711.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0209, 0.0451,  ..., 0.0780, 0.0211, 0.1033],
        [0.0703, 0.0435, 0.0248,  ..., 0.0692, 0.0468, 0.0687],
        [0.0727, 0.0764, 0.0101,  ..., 0.0627, 0.0836, 0.0428],
        ...,
        [0.0660, 0.0164, 0.0450,  ..., 0.0848, 0.0169, 0.1090],
        [0.0709, 0.0420, 0.0236,  ..., 0.0727, 0.0446, 0.0686],
        [0.0730, 0.0707, 0.0116,  ..., 0.0668, 0.0785, 0.0474]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575371., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8683.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(831.8295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(318.0648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(322.8920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0121],
        [-0.4045],
        [-0.8660],
        ...,
        [-0.0313],
        [-0.5051],
        [-1.0618]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252447.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0214],
        [1.0251],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369856.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8413],
        [0.5146],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.7324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0215],
        [1.0252],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369864.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8413],
        [0.5146],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.7324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0296, -0.0062,  0.0181,  ...,  0.0161, -0.0080,  0.0574],
        [ 0.0327, -0.0069,  0.0200,  ...,  0.0175, -0.0088,  0.0631],
        [ 0.0218, -0.0046,  0.0133,  ...,  0.0124, -0.0059,  0.0429],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3140.0049, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0316, device='cuda:0')



h[100].sum tensor(-24.3270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1711, device='cuda:0')



h[200].sum tensor(90.1754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1574, 0.0000, 0.0961,  ..., 0.0828, 0.0000, 0.3014],
        [0.1244, 0.0000, 0.0760,  ..., 0.0673, 0.0000, 0.2406],
        [0.0914, 0.0000, 0.0558,  ..., 0.0517, 0.0000, 0.1797],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018],
        [0.0132, 0.0000, 0.0080,  ..., 0.0133, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78810.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.3271,  ..., 0.2918, 0.0000, 0.7079],
        [0.0000, 0.0000, 0.2966,  ..., 0.2669, 0.0000, 0.6390],
        [0.0017, 0.0000, 0.2675,  ..., 0.2428, 0.0000, 0.5726],
        ...,
        [0.0747, 0.0893, 0.0037,  ..., 0.0638, 0.1006, 0.0339],
        [0.0727, 0.0669, 0.0124,  ..., 0.0686, 0.0743, 0.0499],
        [0.0664, 0.0310, 0.0368,  ..., 0.0835, 0.0344, 0.0974]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663518.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8466.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(901.5072, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(456.9375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(420.6985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0088],
        [ 0.0395],
        [ 0.0713],
        ...,
        [-1.9424],
        [-1.5724],
        [-1.0607]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271670.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0215],
        [1.0252],
        ...,
        [0.9997],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369864.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(173.6267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0216],
        [1.0253],
        ...,
        [0.9996],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369872.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.6267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1574.8088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.6146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0128, device='cuda:0')



h[100].sum tensor(-9.7366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4743, device='cuda:0')



h[200].sum tensor(69.9115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5834, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53827.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0732, 0.0924, 0.0020,  ..., 0.0615, 0.1035, 0.0296],
        [0.0732, 0.0925, 0.0020,  ..., 0.0615, 0.1036, 0.0296],
        [0.0735, 0.0926, 0.0021,  ..., 0.0616, 0.1038, 0.0297],
        ...,
        [0.0756, 0.0947, 0.0023,  ..., 0.0630, 0.1067, 0.0305],
        [0.0756, 0.0947, 0.0023,  ..., 0.0630, 0.1066, 0.0305],
        [0.0756, 0.0947, 0.0023,  ..., 0.0630, 0.1066, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561288.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8835.6279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(802.5053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(294.9427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.8692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2904],
        [-2.3458],
        [-2.3936],
        ...,
        [-2.2228],
        [-2.2176],
        [-2.2161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262042.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0216],
        [1.0253],
        ...,
        [0.9996],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369872.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3909],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.0527, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0210],
        [1.0217],
        [1.0253],
        ...,
        [0.9996],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369880.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3909],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.0527, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [ 0.0068, -0.0016,  0.0041,  ...,  0.0053, -0.0021,  0.0154],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2137.2271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0195, device='cuda:0')



h[100].sum tensor(-14.8028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7213, device='cuda:0')



h[200].sum tensor(76.7811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0000, 0.0042,  ..., 0.0101, 0.0000, 0.0172],
        [0.0054, 0.0000, 0.0033,  ..., 0.0094, 0.0000, 0.0144],
        [0.0247, 0.0000, 0.0150,  ..., 0.0204, 0.0000, 0.0574],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63113.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0703, 0.0184, 0.0353,  ..., 0.0751, 0.0150, 0.0870],
        [0.0705, 0.0234, 0.0337,  ..., 0.0745, 0.0218, 0.0840],
        [0.0679, 0.0019, 0.0516,  ..., 0.0831, 0.0000, 0.1149],
        ...,
        [0.0761, 0.0948, 0.0026,  ..., 0.0635, 0.1064, 0.0311],
        [0.0761, 0.0948, 0.0026,  ..., 0.0635, 0.1064, 0.0310],
        [0.0761, 0.0947, 0.0026,  ..., 0.0635, 0.1063, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596150.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8844.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(836.5427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(351.5187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(335.2129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1101],
        [-0.0268],
        [-0.1221],
        ...,
        [-2.2196],
        [-2.2146],
        [-2.2132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235298.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0210],
        [1.0217],
        [1.0253],
        ...,
        [0.9996],
        [0.9989],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369880.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(267.1412, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0210],
        [1.0218],
        [1.0254],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369888.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(267.1412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030, -0.0009,  0.0018,  ...,  0.0035, -0.0011,  0.0084],
        [ 0.0299, -0.0062,  0.0182,  ...,  0.0162, -0.0080,  0.0580],
        [ 0.0222, -0.0047,  0.0135,  ...,  0.0126, -0.0060,  0.0438],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2174.7417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-15.0375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7297, device='cuda:0')



h[200].sum tensor(77.2939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1021, 0.0000, 0.0623,  ..., 0.0568, 0.0000, 0.2002],
        [0.0614, 0.0000, 0.0375,  ..., 0.0377, 0.0000, 0.1252],
        [0.0978, 0.0000, 0.0597,  ..., 0.0548, 0.0000, 0.1923],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63591.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0000, 0.2299,  ..., 0.2121, 0.0000, 0.4875],
        [0.0151, 0.0000, 0.2159,  ..., 0.2002, 0.0000, 0.4557],
        [0.0107, 0.0000, 0.2262,  ..., 0.2092, 0.0000, 0.4789],
        ...,
        [0.0764, 0.0949, 0.0027,  ..., 0.0638, 0.1063, 0.0313],
        [0.0764, 0.0949, 0.0027,  ..., 0.0637, 0.1063, 0.0313],
        [0.0764, 0.0949, 0.0027,  ..., 0.0637, 0.1063, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598062.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8934.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(839.0754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.8825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(336.3521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2988],
        [ 0.2985],
        [ 0.2972],
        ...,
        [-2.2231],
        [-2.2181],
        [-2.2167]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211180.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0210],
        [1.0218],
        [1.0254],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369888.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 420.0 event: 2100 loss: tensor(978.8334, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.2498, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0211],
        [1.0218],
        [1.0254],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369896.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.2498, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [ 0.0032, -0.0009,  0.0019,  ...,  0.0036, -0.0011,  0.0088],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1817.0869, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0154, device='cuda:0')



h[100].sum tensor(-11.6821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5716, device='cuda:0')



h[200].sum tensor(72.8725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0000, 0.0118,  ..., 0.0160, 0.0000, 0.0402],
        [0.0091, 0.0000, 0.0055,  ..., 0.0118, 0.0000, 0.0237],
        [0.0236, 0.0000, 0.0143,  ..., 0.0192, 0.0000, 0.0529],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58683.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0627, 0.0000, 0.0562,  ..., 0.0925, 0.0000, 0.1322],
        [0.0665, 0.0000, 0.0578,  ..., 0.0877, 0.0000, 0.1284],
        [0.0641, 0.0000, 0.0779,  ..., 0.0967, 0.0000, 0.1628],
        ...,
        [0.0766, 0.0952, 0.0027,  ..., 0.0639, 0.1065, 0.0313],
        [0.0766, 0.0952, 0.0027,  ..., 0.0639, 0.1065, 0.0313],
        [0.0765, 0.0951, 0.0027,  ..., 0.0639, 0.1065, 0.0312]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584197.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9026.8564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(823.4861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(320.9435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.4088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2721],
        [ 0.3465],
        [ 0.4111],
        ...,
        [-2.2339],
        [-2.2288],
        [-2.2274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208775.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0211],
        [1.0218],
        [1.0254],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369896.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.4161, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0219],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369904., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.4161, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1823.7008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0154, device='cuda:0')



h[100].sum tensor(-11.7248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5693, device='cuda:0')



h[200].sum tensor(73.2571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57931.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0704, 0.0399, 0.0227,  ..., 0.0731, 0.0419, 0.0687],
        [0.0701, 0.0316, 0.0265,  ..., 0.0747, 0.0321, 0.0750],
        [0.0708, 0.0372, 0.0244,  ..., 0.0736, 0.0386, 0.0713],
        ...,
        [0.0766, 0.0957, 0.0024,  ..., 0.0640, 0.1071, 0.0309],
        [0.0766, 0.0956, 0.0024,  ..., 0.0640, 0.1071, 0.0309],
        [0.0766, 0.0956, 0.0024,  ..., 0.0639, 0.1071, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579819.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8922.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(818.4218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(322.7648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.6552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1631],
        [-0.0193],
        [ 0.0330],
        ...,
        [-2.2162],
        [-2.2003],
        [-2.1806]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252378.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0219],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369904., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5986],
        [0.6084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.0029, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0219],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369911.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5986],
        [0.6084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.0029, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111, -0.0024,  0.0068,  ...,  0.0074, -0.0031,  0.0232],
        [ 0.0113, -0.0025,  0.0069,  ...,  0.0075, -0.0032,  0.0236],
        [ 0.0237, -0.0049,  0.0145,  ...,  0.0133, -0.0063,  0.0464],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2633.7239, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0252, device='cuda:0')



h[100].sum tensor(-19.1125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9315, device='cuda:0')



h[200].sum tensor(83.9195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.8578, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0205, 0.0000, 0.0125,  ..., 0.0172, 0.0000, 0.0444],
        [0.0640, 0.0000, 0.0390,  ..., 0.0389, 0.0000, 0.1294],
        [0.0762, 0.0000, 0.0465,  ..., 0.0446, 0.0000, 0.1519],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72495.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0580, 0.0089, 0.0732,  ..., 0.1041, 0.0086, 0.1665],
        [0.0405, 0.0000, 0.1349,  ..., 0.1449, 0.0000, 0.2905],
        [0.0285, 0.0000, 0.1748,  ..., 0.1725, 0.0000, 0.3719],
        ...,
        [0.0766, 0.0961, 0.0020,  ..., 0.0640, 0.1078, 0.0305],
        [0.0766, 0.0961, 0.0020,  ..., 0.0640, 0.1078, 0.0305],
        [0.0765, 0.0961, 0.0020,  ..., 0.0639, 0.1077, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649999.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8770.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.2278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(416.9825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(393.7881, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2536],
        [ 0.3237],
        [ 0.3292],
        ...,
        [-2.2777],
        [-2.2731],
        [-2.2722]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233268.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0219],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369911.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(162.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0219],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369919.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(162.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [ 0.0038, -0.0010,  0.0023,  ...,  0.0040, -0.0013,  0.0098],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1535.6843, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0120, device='cuda:0')



h[100].sum tensor(-9.0613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4427, device='cuda:0')



h[200].sum tensor(70.3384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0113, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0017],
        [0.0039, 0.0000, 0.0024,  ..., 0.0088, 0.0000, 0.0113],
        [0.0030, 0.0000, 0.0018,  ..., 0.0084, 0.0000, 0.0096],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52414.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0741, 0.0695, 0.0131,  ..., 0.0658, 0.0747, 0.0499],
        [0.0746, 0.0662, 0.0160,  ..., 0.0660, 0.0704, 0.0535],
        [0.0749, 0.0624, 0.0184,  ..., 0.0666, 0.0659, 0.0570],
        ...,
        [0.0767, 0.0966, 0.0018,  ..., 0.0639, 0.1082, 0.0303],
        [0.0766, 0.0965, 0.0018,  ..., 0.0639, 0.1082, 0.0303],
        [0.0766, 0.0965, 0.0018,  ..., 0.0639, 0.1082, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557398.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8864.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(802.0422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(295.3647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.2717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6475],
        [-0.9034],
        [-1.1164],
        ...,
        [-2.2575],
        [-2.2785],
        [-2.2858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306560.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0219],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369919.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9312],
        [0.3267],
        [0.3677],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(327.0529, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0220],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369927.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.9312],
        [0.3267],
        [0.3677],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(327.0529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055, -0.0013,  0.0033,  ...,  0.0047, -0.0017,  0.0128],
        [ 0.0256, -0.0053,  0.0156,  ...,  0.0142, -0.0068,  0.0499],
        [ 0.0216, -0.0045,  0.0132,  ...,  0.0123, -0.0058,  0.0425],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2525.7373, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0241, device='cuda:0')



h[100].sum tensor(-17.9778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8934, device='cuda:0')



h[200].sum tensor(83.3146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0795, 0.0000, 0.0485,  ..., 0.0462, 0.0000, 0.1577],
        [0.0710, 0.0000, 0.0434,  ..., 0.0422, 0.0000, 0.1421],
        [0.0883, 0.0000, 0.0540,  ..., 0.0504, 0.0000, 0.1741],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70789.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0343, 0.0000, 0.1544,  ..., 0.1592, 0.0000, 0.3319],
        [0.0263, 0.0000, 0.1834,  ..., 0.1781, 0.0000, 0.3897],
        [0.0207, 0.0000, 0.2004,  ..., 0.1908, 0.0000, 0.4255],
        ...,
        [0.0767, 0.0969, 0.0017,  ..., 0.0639, 0.1084, 0.0303],
        [0.0767, 0.0968, 0.0017,  ..., 0.0639, 0.1083, 0.0302],
        [0.0767, 0.0968, 0.0017,  ..., 0.0639, 0.1083, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639383.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8678.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.1326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.8863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(391.4507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2510],
        [ 0.2450],
        [ 0.2304],
        ...,
        [-2.3085],
        [-2.3030],
        [-2.3016]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286367., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0220],
        [1.0255],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369927.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.6455, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0220],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369936., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.6455, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2434.9277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0221, device='cuda:0')



h[100].sum tensor(-17.0319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8185, device='cuda:0')



h[200].sum tensor(82.4053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68286.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0743, 0.0946, 0.0015,  ..., 0.0623, 0.1050, 0.0295],
        [0.0744, 0.0945, 0.0017,  ..., 0.0623, 0.1047, 0.0298],
        [0.0748, 0.0938, 0.0026,  ..., 0.0623, 0.1038, 0.0308],
        ...,
        [0.0768, 0.0971, 0.0018,  ..., 0.0639, 0.1083, 0.0304],
        [0.0768, 0.0970, 0.0018,  ..., 0.0639, 0.1082, 0.0303],
        [0.0768, 0.0970, 0.0018,  ..., 0.0638, 0.1082, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627961.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8710.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(872.1308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(394.4403, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(379.0402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2808],
        [-2.1166],
        [-1.8670],
        ...,
        [-2.3132],
        [-2.3078],
        [-2.3064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282781.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0220],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369936., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.8868, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0220],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369936., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.8868, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1589.7710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0125, device='cuda:0')



h[100].sum tensor(-9.3756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4641, device='cuda:0')



h[200].sum tensor(71.7000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3985, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52885.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0742, 0.0925, 0.0019,  ..., 0.0626, 0.1024, 0.0312],
        [0.0743, 0.0947, 0.0015,  ..., 0.0623, 0.1051, 0.0295],
        [0.0746, 0.0948, 0.0016,  ..., 0.0624, 0.1053, 0.0296],
        ...,
        [0.0768, 0.0971, 0.0018,  ..., 0.0639, 0.1083, 0.0304],
        [0.0768, 0.0970, 0.0018,  ..., 0.0639, 0.1082, 0.0303],
        [0.0768, 0.0970, 0.0018,  ..., 0.0638, 0.1082, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559309.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8940.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(812.0495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(294.6704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.6133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1005],
        [-2.3529],
        [-2.4801],
        ...,
        [-2.3129],
        [-2.3076],
        [-2.3063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278441.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0220],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369936., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0220],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369944.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0004],
        [ 0.0047, -0.0012,  0.0028,  ...,  0.0043, -0.0015,  0.0114]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1794.6511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-11.0990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5443, device='cuda:0')



h[200].sum tensor(74.3852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0018],
        [0.0086, 0.0000, 0.0053,  ..., 0.0119, 0.0000, 0.0227],
        [0.0086, 0.0000, 0.0053,  ..., 0.0119, 0.0000, 0.0227]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56720.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0743, 0.0947, 0.0017,  ..., 0.0623, 0.1047, 0.0297],
        [0.0744, 0.0948, 0.0017,  ..., 0.0624, 0.1048, 0.0298],
        [0.0747, 0.0949, 0.0018,  ..., 0.0624, 0.1051, 0.0298],
        ...,
        [0.0765, 0.0792, 0.0106,  ..., 0.0667, 0.0868, 0.0448],
        [0.0758, 0.0434, 0.0283,  ..., 0.0722, 0.0445, 0.0733],
        [0.0756, 0.0321, 0.0340,  ..., 0.0740, 0.0310, 0.0823]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575334.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8833.1387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(821.0262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(320.1149, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.7115, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0731],
        [-2.1607],
        [-2.1852],
        ...,
        [-1.7877],
        [-1.4456],
        [-1.2309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279677.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0220],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369944.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(327.2301, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0215],
        [1.0221],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369952.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(327.2301, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0038, -0.0010,  0.0023,  ...,  0.0040, -0.0013,  0.0099],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2600.6313, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0242, device='cuda:0')



h[100].sum tensor(-18.2156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8939, device='cuda:0')



h[200].sum tensor(84.4926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1769, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0000, 0.0079,  ..., 0.0143, 0.0000, 0.0332],
        [0.0106, 0.0000, 0.0065,  ..., 0.0126, 0.0000, 0.0264],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71196.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0712, 0.0102, 0.0541,  ..., 0.0805, 0.0093, 0.1162],
        [0.0723, 0.0229, 0.0416,  ..., 0.0759, 0.0233, 0.0954],
        [0.0743, 0.0555, 0.0216,  ..., 0.0685, 0.0578, 0.0618],
        ...,
        [0.0771, 0.0972, 0.0023,  ..., 0.0641, 0.1076, 0.0310],
        [0.0770, 0.0971, 0.0023,  ..., 0.0641, 0.1075, 0.0310],
        [0.0770, 0.0971, 0.0023,  ..., 0.0640, 0.1075, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637655.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8785.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(882.6955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(406.3170, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(383.6779, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3173],
        [ 0.2355],
        [ 0.1561],
        ...,
        [-2.3060],
        [-2.3008],
        [-2.2994]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231064.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0215],
        [1.0221],
        [1.0256],
        ...,
        [0.9996],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369952.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2986],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1532, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0221],
        [1.0257],
        ...,
        [0.9996],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369960.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2986],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1532, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [ 0.0102, -0.0022,  0.0062,  ...,  0.0070, -0.0029,  0.0217],
        [ 0.0040, -0.0010,  0.0025,  ...,  0.0041, -0.0013,  0.0104],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2559.5146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0234, device='cuda:0')



h[100].sum tensor(-17.7012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8663, device='cuda:0')



h[200].sum tensor(83.9268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0135, 0.0000, 0.0082,  ..., 0.0139, 0.0000, 0.0319],
        [0.0144, 0.0000, 0.0087,  ..., 0.0149, 0.0000, 0.0359],
        [0.0341, 0.0000, 0.0208,  ..., 0.0249, 0.0000, 0.0748],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73671.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0702, 0.0254, 0.0439,  ..., 0.0792, 0.0260, 0.1021],
        [0.0689, 0.0086, 0.0603,  ..., 0.0852, 0.0077, 0.1286],
        [0.0655, 0.0000, 0.0766,  ..., 0.0944, 0.0000, 0.1586],
        ...,
        [0.0772, 0.0973, 0.0025,  ..., 0.0642, 0.1073, 0.0314],
        [0.0772, 0.0972, 0.0025,  ..., 0.0642, 0.1073, 0.0313],
        [0.0772, 0.0972, 0.0025,  ..., 0.0642, 0.1073, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662401.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8729.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.3372, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(422.2018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(389.5428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1600],
        [ 0.1461],
        [ 0.2464],
        ...,
        [-2.3040],
        [-2.2988],
        [-2.2975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215076.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0221],
        [1.0257],
        ...,
        [0.9996],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369960.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 430.0 event: 2150 loss: tensor(473.5521, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2617],
        [0.6826],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(306.5269, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0222],
        [1.0257],
        ...,
        [0.9995],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369968.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2617],
        [0.6826],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(306.5269, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0010,  0.0025,  ...,  0.0041, -0.0013,  0.0105],
        [ 0.0179, -0.0037,  0.0109,  ...,  0.0106, -0.0048,  0.0360],
        [ 0.0146, -0.0031,  0.0089,  ...,  0.0090, -0.0040,  0.0299],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2496.7434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0226, device='cuda:0')



h[100].sum tensor(-17.0625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8373, device='cuda:0')



h[200].sum tensor(82.9078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1534, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0295, 0.0000, 0.0180,  ..., 0.0214, 0.0000, 0.0615],
        [0.0492, 0.0000, 0.0300,  ..., 0.0319, 0.0000, 0.1026],
        [0.0827, 0.0000, 0.0505,  ..., 0.0477, 0.0000, 0.1645],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70551.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0566, 0.0061, 0.0871,  ..., 0.1088, 0.0052, 0.1889],
        [0.0467, 0.0000, 0.1299,  ..., 0.1331, 0.0000, 0.2694],
        [0.0350, 0.0000, 0.1676,  ..., 0.1589, 0.0000, 0.3460],
        ...,
        [0.0774, 0.0976, 0.0026,  ..., 0.0644, 0.1076, 0.0316],
        [0.0774, 0.0975, 0.0026,  ..., 0.0643, 0.1076, 0.0316],
        [0.0774, 0.0975, 0.0026,  ..., 0.0643, 0.1076, 0.0315]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643903., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8760.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(866.6907, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(405.4115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(369.6561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3965],
        [ 0.3859],
        [ 0.3671],
        ...,
        [-2.3120],
        [-2.3068],
        [-2.3055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243944.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0222],
        [1.0257],
        ...,
        [0.9995],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369968.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.9720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0223],
        [1.0258],
        ...,
        [0.9995],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369977.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.9720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0345, -0.0069,  0.0211,  ...,  0.0184, -0.0089,  0.0665],
        [ 0.0242, -0.0049,  0.0148,  ...,  0.0135, -0.0063,  0.0476],
        [ 0.0284, -0.0057,  0.0173,  ...,  0.0155, -0.0074,  0.0552],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2276.1838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0201, device='cuda:0')



h[100].sum tensor(-15.0031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7429, device='cuda:0')



h[200].sum tensor(80.2075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0553, 0.0000, 0.0337,  ..., 0.0341, 0.0000, 0.1114],
        [0.1156, 0.0000, 0.0706,  ..., 0.0632, 0.0000, 0.2252],
        [0.0793, 0.0000, 0.0484,  ..., 0.0461, 0.0000, 0.1583],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64780.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0352, 0.0000, 0.1491,  ..., 0.1515, 0.0000, 0.3162],
        [0.0209, 0.0000, 0.1901,  ..., 0.1805, 0.0000, 0.4014],
        [0.0301, 0.0000, 0.1688,  ..., 0.1636, 0.0000, 0.3541],
        ...,
        [0.0775, 0.0979, 0.0026,  ..., 0.0643, 0.1078, 0.0317],
        [0.0775, 0.0978, 0.0026,  ..., 0.0643, 0.1078, 0.0317],
        [0.0775, 0.0978, 0.0026,  ..., 0.0643, 0.1078, 0.0317]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609034.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8905.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(850.3510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(366.2425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(341.4335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3690],
        [ 0.3326],
        [ 0.2837],
        ...,
        [-2.3204],
        [-2.3152],
        [-2.3139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214233.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0223],
        [1.0258],
        ...,
        [0.9995],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369977.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.9929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0218],
        [1.0224],
        [1.0258],
        ...,
        [0.9995],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369985.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.9929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2147.3086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-13.8195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6856, device='cuda:0')



h[200].sum tensor(78.8161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4080, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63592.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0753, 0.0945, 0.0035,  ..., 0.0623, 0.1029, 0.0323],
        [0.0751, 0.0955, 0.0026,  ..., 0.0626, 0.1045, 0.0312],
        [0.0753, 0.0958, 0.0025,  ..., 0.0627, 0.1050, 0.0310],
        ...,
        [0.0775, 0.0983, 0.0025,  ..., 0.0643, 0.1084, 0.0315],
        [0.0775, 0.0983, 0.0025,  ..., 0.0643, 0.1084, 0.0315],
        [0.0775, 0.0983, 0.0024,  ..., 0.0642, 0.1083, 0.0315]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610309.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8809.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.8771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(362.6581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(338.0692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5391],
        [-1.7263],
        [-1.8080],
        ...,
        [-2.3388],
        [-2.3335],
        [-2.3322]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246825.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0218],
        [1.0224],
        [1.0258],
        ...,
        [0.9995],
        [0.9988],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369985.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.1515, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0219],
        [1.0224],
        [1.0259],
        ...,
        [0.9995],
        [0.9987],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369993., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.1515, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2192.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-14.2059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6970, device='cuda:0')



h[200].sum tensor(79.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62658.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0722, 0.0610, 0.0153,  ..., 0.0695, 0.0658, 0.0554],
        [0.0685, 0.0296, 0.0294,  ..., 0.0779, 0.0307, 0.0830],
        [0.0663, 0.0160, 0.0404,  ..., 0.0838, 0.0153, 0.1031],
        ...,
        [0.0775, 0.0988, 0.0022,  ..., 0.0643, 0.1091, 0.0314],
        [0.0775, 0.0988, 0.0022,  ..., 0.0642, 0.1091, 0.0314],
        [0.0775, 0.0987, 0.0022,  ..., 0.0642, 0.1090, 0.0314]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600379.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8726.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(839.2115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(362.0655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.0440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2727],
        [-0.0423],
        [ 0.1016],
        ...,
        [-2.3602],
        [-2.3548],
        [-2.3535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286213.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0219],
        [1.0224],
        [1.0259],
        ...,
        [0.9995],
        [0.9987],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369993., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0220],
        [1.0225],
        [1.0259],
        ...,
        [0.9994],
        [0.9987],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370000.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2340.1711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-15.4852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7716, device='cuda:0')



h[200].sum tensor(81.5065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9651, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0000, 0.0074,  ..., 0.0133, 0.0000, 0.0292],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66304.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0681, 0.0273, 0.0462,  ..., 0.0823, 0.0288, 0.1089],
        [0.0735, 0.0554, 0.0199,  ..., 0.0694, 0.0585, 0.0619],
        [0.0749, 0.0686, 0.0152,  ..., 0.0667, 0.0734, 0.0531],
        ...,
        [0.0776, 0.0992, 0.0021,  ..., 0.0643, 0.1097, 0.0313],
        [0.0775, 0.0992, 0.0021,  ..., 0.0643, 0.1096, 0.0313],
        [0.0775, 0.0992, 0.0021,  ..., 0.0643, 0.1096, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621521.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8694.6357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(860.4586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(385.7860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(359.3661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0688],
        [-0.1319],
        [-0.2414],
        ...,
        [-2.3784],
        [-2.3728],
        [-2.3693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276599.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0220],
        [1.0225],
        [1.0259],
        ...,
        [0.9994],
        [0.9987],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370000.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0221],
        [1.0226],
        [1.0260],
        ...,
        [0.9994],
        [0.9986],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370008.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [ 0.0194, -0.0039,  0.0118,  ...,  0.0113, -0.0051,  0.0385],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2059.7297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-12.9165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6407, device='cuda:0')



h[200].sum tensor(77.9680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5953, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0039, 0.0000, 0.0024,  ..., 0.0088, 0.0000, 0.0115],
        [0.0381, 0.0000, 0.0233,  ..., 0.0261, 0.0000, 0.0795],
        [0.0696, 0.0000, 0.0425,  ..., 0.0409, 0.0000, 0.1375],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59403.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0580, 0.0016, 0.0788,  ..., 0.1037, 0.0000, 0.1741],
        [0.0413, 0.0000, 0.1304,  ..., 0.1378, 0.0000, 0.2778],
        [0.0211, 0.0000, 0.1888,  ..., 0.1796, 0.0000, 0.3996],
        ...,
        [0.0777, 0.0995, 0.0019,  ..., 0.0645, 0.1099, 0.0314],
        [0.0777, 0.0995, 0.0019,  ..., 0.0644, 0.1099, 0.0314],
        [0.0776, 0.0995, 0.0019,  ..., 0.0644, 0.1099, 0.0314]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585698.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8815.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(834.0006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(342.2473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(325.9052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2093],
        [ 0.1614],
        [ 0.1239],
        ...,
        [-2.3902],
        [-2.3847],
        [-2.3834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284206.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0221],
        [1.0226],
        [1.0260],
        ...,
        [0.9994],
        [0.9986],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370008.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0222],
        [1.0227],
        [1.0261],
        ...,
        [0.9994],
        [0.9986],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370016.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1815.1575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.1111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-10.6552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5249, device='cuda:0')



h[200].sum tensor(74.7959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58941.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0753, 0.0972, 0.0017,  ..., 0.0630, 0.1066, 0.0307],
        [0.0754, 0.0973, 0.0017,  ..., 0.0631, 0.1067, 0.0308],
        [0.0753, 0.0929, 0.0029,  ..., 0.0640, 0.1019, 0.0339],
        ...,
        [0.0779, 0.0997, 0.0020,  ..., 0.0646, 0.1100, 0.0317],
        [0.0779, 0.0997, 0.0020,  ..., 0.0646, 0.1099, 0.0316],
        [0.0779, 0.0996, 0.0020,  ..., 0.0646, 0.1099, 0.0316]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604046.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8727.7568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(824.2080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(343.0839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.0112, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4152],
        [-2.2384],
        [-1.9369],
        ...,
        [-2.3947],
        [-2.3891],
        [-2.3878]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310385.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0222],
        [1.0227],
        [1.0261],
        ...,
        [0.9994],
        [0.9986],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370016.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.6471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0227],
        [1.0261],
        ...,
        [0.9994],
        [0.9986],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370024.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.6471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0182, -0.0037,  0.0111,  ...,  0.0107, -0.0047,  0.0364],
        [ 0.0183, -0.0037,  0.0111,  ...,  0.0107, -0.0048,  0.0365],
        [ 0.0190, -0.0038,  0.0116,  ...,  0.0111, -0.0050,  0.0380],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1870.3684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-11.0275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5481, device='cuda:0')



h[200].sum tensor(75.4937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0673, 0.0000, 0.0411,  ..., 0.0405, 0.0000, 0.1359],
        [0.0685, 0.0000, 0.0419,  ..., 0.0410, 0.0000, 0.1382],
        [0.0505, 0.0000, 0.0309,  ..., 0.0326, 0.0000, 0.1050],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58678.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0356, 0.0000, 0.1592,  ..., 0.1534, 0.0000, 0.3309],
        [0.0352, 0.0000, 0.1604,  ..., 0.1543, 0.0000, 0.3334],
        [0.0440, 0.0000, 0.1338,  ..., 0.1366, 0.0000, 0.2798],
        ...,
        [0.0781, 0.0998, 0.0021,  ..., 0.0647, 0.1097, 0.0320],
        [0.0781, 0.0997, 0.0021,  ..., 0.0647, 0.1097, 0.0320],
        [0.0781, 0.0997, 0.0021,  ..., 0.0647, 0.1097, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591474.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8825.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(824.4955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(339.0029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.0667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2814],
        [ 0.2506],
        [ 0.1050],
        ...,
        [-2.3941],
        [-2.3887],
        [-2.3874]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286858.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0227],
        [1.0261],
        ...,
        [0.9994],
        [0.9986],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370024.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0224],
        [1.0228],
        [1.0262],
        ...,
        [0.9993],
        [0.9986],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370031.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [ 0.0042, -0.0010,  0.0025,  ...,  0.0041, -0.0013,  0.0106],
        [ 0.0069, -0.0015,  0.0042,  ...,  0.0054, -0.0020,  0.0157],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1795.2311, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-10.1865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5090, device='cuda:0')



h[200].sum tensor(74.5906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0177, 0.0000, 0.0108,  ..., 0.0165, 0.0000, 0.0421],
        [0.0256, 0.0000, 0.0156,  ..., 0.0202, 0.0000, 0.0567],
        [0.0612, 0.0000, 0.0374,  ..., 0.0376, 0.0000, 0.1248],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54623.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0598, 0.0000, 0.0964,  ..., 0.1065, 0.0000, 0.1989],
        [0.0533, 0.0000, 0.1144,  ..., 0.1195, 0.0000, 0.2364],
        [0.0359, 0.0000, 0.1633,  ..., 0.1541, 0.0000, 0.3380],
        ...,
        [0.0782, 0.0997, 0.0022,  ..., 0.0650, 0.1094, 0.0323],
        [0.0782, 0.0997, 0.0022,  ..., 0.0650, 0.1094, 0.0323],
        [0.0782, 0.0996, 0.0022,  ..., 0.0650, 0.1093, 0.0323]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566877.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8916.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(805.9098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(311.9472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.6752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3971],
        [ 0.3726],
        [ 0.3312],
        ...,
        [-2.3916],
        [-2.3862],
        [-2.3849]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276359.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0224],
        [1.0228],
        [1.0262],
        ...,
        [0.9993],
        [0.9986],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370031.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2501, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0225],
        [1.0229],
        [1.0262],
        ...,
        [0.9993],
        [0.9985],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370039.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2501, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [ 0.0041, -0.0010,  0.0025,  ...,  0.0041, -0.0013,  0.0105],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1930.1152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-11.2314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5607, device='cuda:0')



h[200].sum tensor(76.4071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0023],
        [0.0042, 0.0000, 0.0026,  ..., 0.0089, 0.0000, 0.0124],
        [0.0156, 0.0000, 0.0096,  ..., 0.0149, 0.0000, 0.0360],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58840.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0751, 0.0725, 0.0132,  ..., 0.0670, 0.0770, 0.0508],
        [0.0732, 0.0382, 0.0309,  ..., 0.0740, 0.0399, 0.0811],
        [0.0696, 0.0126, 0.0553,  ..., 0.0847, 0.0117, 0.1232],
        ...,
        [0.0783, 0.0997, 0.0022,  ..., 0.0653, 0.1093, 0.0324],
        [0.0783, 0.0997, 0.0022,  ..., 0.0653, 0.1093, 0.0324],
        [0.0783, 0.0996, 0.0022,  ..., 0.0653, 0.1093, 0.0324]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592393.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8875.6025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.0984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(336.2397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.5906, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6799],
        [-0.2638],
        [ 0.0726],
        ...,
        [-2.3956],
        [-2.3903],
        [-2.3890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259078.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0225],
        [1.0229],
        [1.0262],
        ...,
        [0.9993],
        [0.9985],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370039.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 440.0 event: 2200 loss: tensor(491.5610, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4397],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0230],
        [1.0263],
        ...,
        [0.9993],
        [0.9985],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370046.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4397],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0197, -0.0039,  0.0121,  ...,  0.0114, -0.0051,  0.0394],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [ 0.0078, -0.0017,  0.0048,  ...,  0.0058, -0.0022,  0.0173],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [ 0.0040, -0.0010,  0.0025,  ...,  0.0041, -0.0013,  0.0104]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2102.1191, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-12.6751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6370, device='cuda:0')



h[200].sum tensor(78.0064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0341, 0.0000, 0.0209,  ..., 0.0243, 0.0000, 0.0726],
        [0.0459, 0.0000, 0.0281,  ..., 0.0304, 0.0000, 0.0967],
        [0.0062, 0.0000, 0.0038,  ..., 0.0099, 0.0000, 0.0163],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0023],
        [0.0043, 0.0000, 0.0026,  ..., 0.0092, 0.0000, 0.0127],
        [0.0172, 0.0000, 0.0105,  ..., 0.0159, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60475.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0535, 0.0000, 0.1101,  ..., 0.1226, 0.0000, 0.2346],
        [0.0561, 0.0000, 0.0987,  ..., 0.1164, 0.0000, 0.2134],
        [0.0670, 0.0169, 0.0534,  ..., 0.0906, 0.0173, 0.1275],
        ...,
        [0.0775, 0.0766, 0.0114,  ..., 0.0701, 0.0833, 0.0498],
        [0.0750, 0.0394, 0.0318,  ..., 0.0785, 0.0428, 0.0854],
        [0.0696, 0.0108, 0.0626,  ..., 0.0935, 0.0107, 0.1408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592615.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8954.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(834.5651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(346.2248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(321.2216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3441],
        [ 0.3222],
        [ 0.2691],
        ...,
        [-1.5315],
        [-0.8548],
        [-0.2209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255221.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0230],
        [1.0263],
        ...,
        [0.9993],
        [0.9985],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370046.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4294],
        [0.3438],
        [0.3335],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(252.4642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0227],
        [1.0231],
        [1.0263],
        ...,
        [0.9993],
        [0.9985],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370054., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4294],
        [0.3438],
        [0.3335],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(252.4642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0182, -0.0036,  0.0111,  ...,  0.0107, -0.0047,  0.0365],
        [ 0.0144, -0.0029,  0.0088,  ...,  0.0090, -0.0038,  0.0296],
        [ 0.0058, -0.0013,  0.0036,  ...,  0.0049, -0.0017,  0.0137],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2230.9509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0186, device='cuda:0')



h[100].sum tensor(-13.7187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6896, device='cuda:0')



h[200].sum tensor(79.1558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4808, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0508, 0.0000, 0.0311,  ..., 0.0328, 0.0000, 0.1059],
        [0.0468, 0.0000, 0.0287,  ..., 0.0309, 0.0000, 0.0986],
        [0.0352, 0.0000, 0.0216,  ..., 0.0255, 0.0000, 0.0773],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62225.8633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0559, 0.0000, 0.1004,  ..., 0.1199, 0.0000, 0.2204],
        [0.0580, 0.0000, 0.0954,  ..., 0.1159, 0.0000, 0.2094],
        [0.0638, 0.0000, 0.0752,  ..., 0.1026, 0.0000, 0.1689],
        ...,
        [0.0791, 0.1000, 0.0018,  ..., 0.0666, 0.1100, 0.0329],
        [0.0790, 0.1000, 0.0018,  ..., 0.0666, 0.1099, 0.0329],
        [0.0790, 0.1000, 0.0018,  ..., 0.0665, 0.1099, 0.0328]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601654.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8945.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(836.1933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(359.2735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.2386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2092],
        [-0.0782],
        [-0.5270],
        ...,
        [-2.4139],
        [-2.4086],
        [-2.4074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277483.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0227],
        [1.0231],
        [1.0263],
        ...,
        [0.9993],
        [0.9985],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370054., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.3571, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0228],
        [1.0231],
        [1.0264],
        ...,
        [0.9992],
        [0.9985],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370061.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.3571, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2231.8193, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0186, device='cuda:0')



h[100].sum tensor(-13.6540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6866, device='cuda:0')



h[200].sum tensor(78.7444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0023],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62677.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0767, 0.0977, 0.0013,  ..., 0.0653, 0.1069, 0.0321],
        [0.0768, 0.0978, 0.0013,  ..., 0.0654, 0.1071, 0.0321],
        [0.0771, 0.0977, 0.0016,  ..., 0.0654, 0.1069, 0.0325],
        ...,
        [0.0794, 0.1002, 0.0016,  ..., 0.0671, 0.1103, 0.0331],
        [0.0793, 0.1002, 0.0016,  ..., 0.0671, 0.1103, 0.0331],
        [0.0793, 0.1001, 0.0016,  ..., 0.0670, 0.1102, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606342.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8968.4990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(837.9159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(362.2560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.4542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4749],
        [-2.3881],
        [-2.2474],
        ...,
        [-2.4193],
        [-2.4145],
        [-2.4138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277546.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0228],
        [1.0231],
        [1.0264],
        ...,
        [0.9992],
        [0.9985],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370061.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.5404, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0232],
        [1.0264],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370068.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.5404, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0228, -0.0045,  0.0139,  ...,  0.0129, -0.0058,  0.0450],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [ 0.0082, -0.0018,  0.0050,  ...,  0.0061, -0.0023,  0.0182],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1907.0518, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-10.7237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5423, device='cuda:0')



h[200].sum tensor(74.6674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8150, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0322, 0.0000, 0.0198,  ..., 0.0228, 0.0000, 0.0668],
        [0.0570, 0.0000, 0.0350,  ..., 0.0358, 0.0000, 0.1176],
        [0.0166, 0.0000, 0.0102,  ..., 0.0155, 0.0000, 0.0381],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56306.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0588, 0.0000, 0.0842,  ..., 0.1146, 0.0000, 0.1951],
        [0.0569, 0.0000, 0.0988,  ..., 0.1214, 0.0000, 0.2215],
        [0.0657, 0.0000, 0.0689,  ..., 0.1011, 0.0000, 0.1602],
        ...,
        [0.0795, 0.1003, 0.0015,  ..., 0.0674, 0.1104, 0.0332],
        [0.0794, 0.1002, 0.0015,  ..., 0.0674, 0.1103, 0.0332],
        [0.0794, 0.1002, 0.0015,  ..., 0.0674, 0.1103, 0.0332]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577447.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9003.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(807.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(323.5397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(295.7350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0647],
        [ 0.2667],
        [ 0.3751],
        ...,
        [-2.4284],
        [-2.4234],
        [-2.4225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305298.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0232],
        [1.0264],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370068.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3113],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.0870, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0232],
        [1.0264],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370076.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3113],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.0870, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031, -0.0008,  0.0019,  ...,  0.0036, -0.0011,  0.0088],
        [ 0.0095, -0.0020,  0.0058,  ...,  0.0067, -0.0026,  0.0206],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2005.7260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-11.4415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5739, device='cuda:0')



h[200].sum tensor(76.1170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0344, 0.0000, 0.0211,  ..., 0.0251, 0.0000, 0.0759],
        [0.0134, 0.0000, 0.0083,  ..., 0.0146, 0.0000, 0.0347],
        [0.0121, 0.0000, 0.0074,  ..., 0.0134, 0.0000, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58118.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0650, 0.0000, 0.0912,  ..., 0.1080, 0.0000, 0.1958],
        [0.0709, 0.0055, 0.0674,  ..., 0.0932, 0.0043, 0.1496],
        [0.0736, 0.0080, 0.0512,  ..., 0.0853, 0.0069, 0.1204],
        ...,
        [0.0795, 0.1002, 0.0016,  ..., 0.0676, 0.1102, 0.0334],
        [0.0795, 0.1002, 0.0016,  ..., 0.0675, 0.1101, 0.0334],
        [0.0795, 0.1002, 0.0016,  ..., 0.0675, 0.1101, 0.0334]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583349.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8965.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(816.2574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.3047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(304.7616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5075],
        [ 0.4923],
        [ 0.4778],
        ...,
        [-2.4280],
        [-2.4224],
        [-2.4202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293662.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0232],
        [1.0264],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370076.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.5724, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0233],
        [1.0265],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370083.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.5724, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [ 0.0117, -0.0024,  0.0072,  ...,  0.0077, -0.0031,  0.0247],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1873.1241, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.4431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-10.1435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5124, device='cuda:0')



h[200].sum tensor(74.9011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0024],
        [0.0120, 0.0000, 0.0074,  ..., 0.0127, 0.0000, 0.0270],
        [0.0423, 0.0000, 0.0259,  ..., 0.0276, 0.0000, 0.0854],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56200.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0735, 0.0600, 0.0145,  ..., 0.0743, 0.0661, 0.0600],
        [0.0647, 0.0312, 0.0506,  ..., 0.0969, 0.0338, 0.1332],
        [0.0457, 0.0000, 0.1204,  ..., 0.1424, 0.0000, 0.2760],
        ...,
        [0.0795, 0.1002, 0.0016,  ..., 0.0676, 0.1099, 0.0336],
        [0.0795, 0.1001, 0.0016,  ..., 0.0675, 0.1098, 0.0335],
        [0.0795, 0.1001, 0.0016,  ..., 0.0675, 0.1098, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578733.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8960.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(809.8057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(319.5364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.2261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9852],
        [-0.3584],
        [ 0.0739],
        ...,
        [-2.4302],
        [-2.4251],
        [-2.4241]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294264.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0233],
        [1.0265],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370083.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.3330, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0233],
        [1.0265],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370091.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.3330, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2143.2043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-12.3485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6264, device='cuda:0')



h[200].sum tensor(78.6800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3373, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62876.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0768, 0.0977, 0.0013,  ..., 0.0658, 0.1063, 0.0326],
        [0.0770, 0.0978, 0.0013,  ..., 0.0659, 0.1065, 0.0327],
        [0.0772, 0.0979, 0.0015,  ..., 0.0660, 0.1067, 0.0327],
        ...,
        [0.0795, 0.1002, 0.0016,  ..., 0.0676, 0.1097, 0.0336],
        [0.0795, 0.1001, 0.0016,  ..., 0.0675, 0.1097, 0.0336],
        [0.0795, 0.1001, 0.0016,  ..., 0.0675, 0.1097, 0.0336]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615811., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8897.6973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(842.1118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(358.6335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.6297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4733],
        [-2.5255],
        [-2.5629],
        ...,
        [-2.4327],
        [-2.4276],
        [-2.4266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262904.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0233],
        [1.0265],
        ...,
        [0.9992],
        [0.9984],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370091.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4595],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.4579, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0233],
        [1.0265],
        ...,
        [0.9991],
        [0.9984],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370098.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4595],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.4579, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0138, -0.0028,  0.0084,  ...,  0.0086, -0.0036,  0.0284],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [ 0.0082, -0.0017,  0.0050,  ...,  0.0060, -0.0022,  0.0181],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1983.1855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-10.8504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5476, device='cuda:0')



h[200].sum tensor(77.0752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9098, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0000, 0.0061,  ..., 0.0123, 0.0000, 0.0256],
        [0.0356, 0.0000, 0.0218,  ..., 0.0256, 0.0000, 0.0780],
        [0.0066, 0.0000, 0.0040,  ..., 0.0101, 0.0000, 0.0170],
        ...,
        [0.0028, 0.0000, 0.0017,  ..., 0.0085, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56786.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0715, 0.0024, 0.0490,  ..., 0.0862, 0.0012, 0.1179],
        [0.0687, 0.0000, 0.0637,  ..., 0.0942, 0.0000, 0.1454],
        [0.0707, 0.0024, 0.0524,  ..., 0.0887, 0.0012, 0.1247],
        ...,
        [0.0796, 0.0499, 0.0260,  ..., 0.0742, 0.0516, 0.0742],
        [0.0794, 0.0762, 0.0131,  ..., 0.0708, 0.0820, 0.0529],
        [0.0796, 0.0935, 0.0047,  ..., 0.0683, 0.1017, 0.0393]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575972., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8950.4043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(817.9999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(319.7618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.2253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4257],
        [ 0.4406],
        [ 0.4538],
        ...,
        [-1.1130],
        [-1.5946],
        [-2.0142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278916.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0233],
        [1.0265],
        ...,
        [0.9991],
        [0.9984],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370098.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.6042, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0234],
        [1.0265],
        ...,
        [0.9991],
        [0.9984],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370105.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.6042, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1991.9729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-10.8097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5452, device='cuda:0')



h[200].sum tensor(77.4861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0024],
        [0.0021, 0.0000, 0.0013,  ..., 0.0079, 0.0000, 0.0087],
        [0.0041, 0.0000, 0.0026,  ..., 0.0096, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58581.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0784, 0.0811, 0.0112,  ..., 0.0666, 0.0853, 0.0485],
        [0.0788, 0.0649, 0.0192,  ..., 0.0687, 0.0667, 0.0621],
        [0.0791, 0.0522, 0.0260,  ..., 0.0705, 0.0522, 0.0726],
        ...,
        [0.0796, 0.1003, 0.0017,  ..., 0.0676, 0.1095, 0.0339],
        [0.0796, 0.1003, 0.0017,  ..., 0.0676, 0.1094, 0.0339],
        [0.0796, 0.1002, 0.0017,  ..., 0.0676, 0.1094, 0.0339]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587605.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9001.5498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(831.8173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(326.5419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.6997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0262],
        [-0.7551],
        [-0.6124],
        ...,
        [-2.4389],
        [-2.4339],
        [-2.4330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240255.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0234],
        [1.0265],
        ...,
        [0.9991],
        [0.9984],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370105.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2126, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0234],
        [1.0266],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370113.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2126, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2221.3042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-12.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6452, device='cuda:0')



h[200].sum tensor(80.4677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0000, 0.0103,  ..., 0.0161, 0.0000, 0.0407],
        [0.0036, 0.0000, 0.0022,  ..., 0.0086, 0.0000, 0.0115],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62729.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0695, 0.0092, 0.0657,  ..., 0.0933, 0.0087, 0.1470],
        [0.0746, 0.0333, 0.0352,  ..., 0.0786, 0.0351, 0.0921],
        [0.0771, 0.0668, 0.0161,  ..., 0.0704, 0.0707, 0.0577],
        ...,
        [0.0797, 0.1005, 0.0017,  ..., 0.0678, 0.1096, 0.0340],
        [0.0797, 0.1004, 0.0017,  ..., 0.0677, 0.1096, 0.0340],
        [0.0797, 0.1004, 0.0017,  ..., 0.0677, 0.1096, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607210.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8913.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.4688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(354.2053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.5823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1688],
        [-0.1023],
        [-0.5378],
        ...,
        [-2.4453],
        [-2.4391],
        [-2.4365]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253548.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0234],
        [1.0266],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370113.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 450.0 event: 2250 loss: tensor(480.9717, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.0785, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0234],
        [1.0266],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370120.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.0785, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0010,  0.0025,  ...,  0.0041, -0.0013,  0.0106],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2253.7734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0177, device='cuda:0')



h[100].sum tensor(-12.8905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6558, device='cuda:0')



h[200].sum tensor(81.1420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0000, 0.0072,  ..., 0.0131, 0.0000, 0.0291],
        [0.0042, 0.0000, 0.0026,  ..., 0.0089, 0.0000, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61723.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0724, 0.0204, 0.0514,  ..., 0.0855, 0.0206, 0.1210],
        [0.0758, 0.0453, 0.0273,  ..., 0.0748, 0.0477, 0.0785],
        [0.0778, 0.0797, 0.0106,  ..., 0.0679, 0.0846, 0.0486],
        ...,
        [0.0798, 0.1006, 0.0018,  ..., 0.0678, 0.1096, 0.0341],
        [0.0798, 0.1006, 0.0018,  ..., 0.0678, 0.1096, 0.0341],
        [0.0798, 0.1005, 0.0018,  ..., 0.0677, 0.1096, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599038.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8917.3799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(840.7884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(348.1412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.4853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0104],
        [-0.5045],
        [-1.0489],
        ...,
        [-2.4537],
        [-2.4488],
        [-2.4480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264297.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0234],
        [1.0266],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370120.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.8153, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0267],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370128.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.8153, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0097, -0.0020,  0.0059,  ...,  0.0067, -0.0026,  0.0209],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2477.9683, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0203, device='cuda:0')



h[100].sum tensor(-14.7637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7507, device='cuda:0')



h[200].sum tensor(83.9324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0228, 0.0000, 0.0140,  ..., 0.0183, 0.0000, 0.0496],
        [0.0099, 0.0000, 0.0061,  ..., 0.0116, 0.0000, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66314.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0597, 0.0069, 0.0798,  ..., 0.1104, 0.0067, 0.1845],
        [0.0692, 0.0344, 0.0388,  ..., 0.0873, 0.0372, 0.1060],
        [0.0754, 0.0696, 0.0114,  ..., 0.0719, 0.0755, 0.0530],
        ...,
        [0.0799, 0.1008, 0.0018,  ..., 0.0679, 0.1100, 0.0341],
        [0.0799, 0.1008, 0.0018,  ..., 0.0679, 0.1100, 0.0341],
        [0.0799, 0.1008, 0.0018,  ..., 0.0679, 0.1100, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623804.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8943.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(866.5014, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(374.1098, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(353.6482, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0859],
        [-0.2576],
        [-0.7955],
        ...,
        [-2.4636],
        [-2.4546],
        [-2.4295]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231845.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0267],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370128.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2620],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.5635, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0267],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370135.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2620],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.5635, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0038, -0.0009,  0.0024,  ...,  0.0039, -0.0012,  0.0101],
        [ 0.0041, -0.0010,  0.0025,  ...,  0.0041, -0.0013,  0.0106],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2811.2778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0243, device='cuda:0')



h[100].sum tensor(-17.6050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9002, device='cuda:0')



h[200].sum tensor(87.8430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0316, 0.0000, 0.0194,  ..., 0.0236, 0.0000, 0.0707],
        [0.0151, 0.0000, 0.0093,  ..., 0.0146, 0.0000, 0.0353],
        [0.0170, 0.0000, 0.0104,  ..., 0.0155, 0.0000, 0.0388],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71562.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0628, 0.0000, 0.0935,  ..., 0.1097, 0.0000, 0.2011],
        [0.0688, 0.0000, 0.0630,  ..., 0.0942, 0.0000, 0.1448],
        [0.0699, 0.0000, 0.0534,  ..., 0.0907, 0.0000, 0.1282],
        ...,
        [0.0802, 0.1012, 0.0017,  ..., 0.0681, 0.1107, 0.0342],
        [0.0802, 0.1011, 0.0017,  ..., 0.0681, 0.1107, 0.0342],
        [0.0801, 0.1011, 0.0017,  ..., 0.0681, 0.1106, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645773.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8873.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(883.7858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(410.0643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(378.8892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3468],
        [ 0.2945],
        [ 0.1830],
        ...,
        [-2.4728],
        [-2.4685],
        [-2.4689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250092.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0267],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370135.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7593],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(176.2734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0268],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370143.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7593],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(176.2734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0293, -0.0055,  0.0179,  ...,  0.0159, -0.0071,  0.0571],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [ 0.0262, -0.0049,  0.0160,  ...,  0.0144, -0.0064,  0.0513],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1851.1157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.8440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0130, device='cuda:0')



h[100].sum tensor(-9.4391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4815, device='cuda:0')



h[200].sum tensor(75.5674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7142, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0228, 0.0000, 0.0140,  ..., 0.0182, 0.0000, 0.0496],
        [0.0901, 0.0000, 0.0551,  ..., 0.0511, 0.0000, 0.1787],
        [0.0324, 0.0000, 0.0198,  ..., 0.0227, 0.0000, 0.0672],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54672.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0490, 0.0000, 0.1112,  ..., 0.1328, 0.0000, 0.2510],
        [0.0394, 0.0000, 0.1440,  ..., 0.1539, 0.0000, 0.3173],
        [0.0517, 0.0000, 0.1080,  ..., 0.1292, 0.0000, 0.2420],
        ...,
        [0.0806, 0.1015, 0.0017,  ..., 0.0683, 0.1114, 0.0344],
        [0.0805, 0.1014, 0.0017,  ..., 0.0683, 0.1113, 0.0344],
        [0.0805, 0.1014, 0.0017,  ..., 0.0683, 0.1113, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572534.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9057.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(809.7330, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(307.0442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.1386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2872],
        [ 0.2657],
        [ 0.2258],
        ...,
        [-2.4907],
        [-2.4856],
        [-2.4847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289421., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0268],
        ...,
        [0.9991],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370143.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.7620, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0268],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370150.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.7620, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2247.8142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-12.7532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6522, device='cuda:0')



h[200].sum tensor(80.1863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0000, 0.0038,  ..., 0.0098, 0.0000, 0.0165],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61710.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0766, 0.0610, 0.0168,  ..., 0.0728, 0.0659, 0.0615],
        [0.0779, 0.0874, 0.0061,  ..., 0.0687, 0.0951, 0.0425],
        [0.0784, 0.0965, 0.0021,  ..., 0.0674, 0.1054, 0.0358],
        ...,
        [0.0810, 0.1017, 0.0019,  ..., 0.0686, 0.1115, 0.0348],
        [0.0809, 0.1016, 0.0019,  ..., 0.0686, 0.1115, 0.0348],
        [0.0809, 0.1016, 0.0019,  ..., 0.0685, 0.1115, 0.0348]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600404.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9087.8408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(836.6857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(349.5363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.7721, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6864],
        [-1.9942],
        [-2.1519],
        ...,
        [-2.5023],
        [-2.4971],
        [-2.4963]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265073.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0235],
        [1.0268],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370150.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3300, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370158.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3300, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084, -0.0017,  0.0051,  ...,  0.0061, -0.0022,  0.0185],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [ 0.0093, -0.0019,  0.0057,  ...,  0.0065, -0.0024,  0.0202],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2551.8530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0210, device='cuda:0')



h[100].sum tensor(-15.2772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7767, device='cuda:0')



h[200].sum tensor(83.7724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0561, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0145, 0.0000, 0.0089,  ..., 0.0142, 0.0000, 0.0343],
        [0.0508, 0.0000, 0.0311,  ..., 0.0326, 0.0000, 0.1064],
        [0.0563, 0.0000, 0.0344,  ..., 0.0345, 0.0000, 0.1139],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67534.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0575, 0.0000, 0.1060,  ..., 0.1201, 0.0000, 0.2288],
        [0.0386, 0.0000, 0.1637,  ..., 0.1590, 0.0000, 0.3471],
        [0.0251, 0.0000, 0.1979,  ..., 0.1851, 0.0000, 0.4213],
        ...,
        [0.0813, 0.1018, 0.0020,  ..., 0.0687, 0.1118, 0.0351],
        [0.0812, 0.1017, 0.0020,  ..., 0.0687, 0.1118, 0.0351],
        [0.0812, 0.1017, 0.0020,  ..., 0.0687, 0.1117, 0.0351]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630780.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9135.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(863.8275, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(382.8701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(351.4114, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2081],
        [ 0.1503],
        [ 0.0923],
        ...,
        [-2.5092],
        [-2.5041],
        [-2.5032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238147.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370158.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.1138, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370165.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.1138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0011,  0.0031,  ...,  0.0045, -0.0015,  0.0124],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2369.6914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0190, device='cuda:0')



h[100].sum tensor(-13.7233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7023, device='cuda:0')



h[200].sum tensor(81.1209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7106, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0111, 0.0000, 0.0068,  ..., 0.0127, 0.0000, 0.0282],
        [0.0052, 0.0000, 0.0032,  ..., 0.0092, 0.0000, 0.0147],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64487.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0744, 0.0215, 0.0463,  ..., 0.0837, 0.0229, 0.1115],
        [0.0770, 0.0461, 0.0255,  ..., 0.0756, 0.0502, 0.0762],
        [0.0792, 0.0815, 0.0102,  ..., 0.0692, 0.0882, 0.0488],
        ...,
        [0.0817, 0.1020, 0.0020,  ..., 0.0689, 0.1123, 0.0354],
        [0.0816, 0.1019, 0.0020,  ..., 0.0689, 0.1123, 0.0353],
        [0.0816, 0.1019, 0.0020,  ..., 0.0689, 0.1123, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625823.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9083.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(840.2327, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(369.1497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.5204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0059],
        [-0.4467],
        [-0.9616],
        ...,
        [-2.5204],
        [-2.5153],
        [-2.5144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289423.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370165.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4617],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370165.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4617],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [ 0.0082, -0.0017,  0.0050,  ...,  0.0060, -0.0022,  0.0183],
        [ 0.0079, -0.0016,  0.0048,  ...,  0.0058, -0.0021,  0.0176],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2611.0300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0218, device='cuda:0')



h[100].sum tensor(-15.7612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8075, device='cuda:0')



h[200].sum tensor(84.0995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.6144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0084, 0.0000, 0.0051,  ..., 0.0107, 0.0000, 0.0207],
        [0.0147, 0.0000, 0.0090,  ..., 0.0143, 0.0000, 0.0348],
        [0.0666, 0.0000, 0.0407,  ..., 0.0400, 0.0000, 0.1354],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67256.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0740, 0.0343, 0.0380,  ..., 0.0826, 0.0367, 0.1009],
        [0.0677, 0.0122, 0.0679,  ..., 0.0983, 0.0122, 0.1566],
        [0.0510, 0.0000, 0.1275,  ..., 0.1348, 0.0000, 0.2742],
        ...,
        [0.0817, 0.1020, 0.0020,  ..., 0.0689, 0.1123, 0.0354],
        [0.0816, 0.1019, 0.0020,  ..., 0.0689, 0.1123, 0.0353],
        [0.0816, 0.1019, 0.0020,  ..., 0.0689, 0.1123, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627077., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9194.2949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(858.5834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(382.7818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.8838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2271],
        [ 0.1484],
        [ 0.3473],
        ...,
        [-2.5204],
        [-2.5153],
        [-2.5144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254823.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370165.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2605, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370172.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [ 0.0233, -0.0043,  0.0142,  ...,  0.0131, -0.0057,  0.0460],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2108.6201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-11.5005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5880, device='cuda:0')



h[200].sum tensor(77.7812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0000, 0.0025,  ..., 0.0087, 0.0000, 0.0127],
        [0.0382, 0.0000, 0.0234,  ..., 0.0254, 0.0000, 0.0781],
        [0.0421, 0.0000, 0.0258,  ..., 0.0279, 0.0000, 0.0878],
        ...,
        [0.0048, 0.0000, 0.0030,  ..., 0.0093, 0.0000, 0.0143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59974.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0000, 0.0672,  ..., 0.0982, 0.0000, 0.1544],
        [0.0548, 0.0000, 0.1042,  ..., 0.1240, 0.0000, 0.2309],
        [0.0483, 0.0000, 0.1275,  ..., 0.1385, 0.0000, 0.2764],
        ...,
        [0.0767, 0.0357, 0.0367,  ..., 0.0841, 0.0398, 0.0975],
        [0.0800, 0.0726, 0.0132,  ..., 0.0741, 0.0810, 0.0560],
        [0.0816, 0.0997, 0.0024,  ..., 0.0693, 0.1104, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599846.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9215.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(829.8511, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(339.6756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.2969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4305],
        [ 0.3993],
        [ 0.3696],
        ...,
        [-0.6087],
        [-1.3503],
        [-1.9686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276282.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370172.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.4385],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.0035, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370179.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.4385],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.0035, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0156, -0.0030,  0.0096,  ...,  0.0095, -0.0039,  0.0319],
        [ 0.0131, -0.0025,  0.0080,  ...,  0.0083, -0.0033,  0.0272],
        [ 0.0079, -0.0016,  0.0048,  ...,  0.0058, -0.0021,  0.0176],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [ 0.0180, -0.0034,  0.0110,  ...,  0.0106, -0.0044,  0.0362]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2482.6216, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0204, device='cuda:0')



h[100].sum tensor(-14.5416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7539, device='cuda:0')



h[200].sum tensor(82.9010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0626, 0.0000, 0.0383,  ..., 0.0381, 0.0000, 0.1280],
        [0.0570, 0.0000, 0.0348,  ..., 0.0354, 0.0000, 0.1175],
        [0.0503, 0.0000, 0.0308,  ..., 0.0323, 0.0000, 0.1053],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0190, 0.0000, 0.0116,  ..., 0.0159, 0.0000, 0.0402],
        [0.0152, 0.0000, 0.0093,  ..., 0.0142, 0.0000, 0.0334]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64870.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0355, 0.0000, 0.1738,  ..., 0.1641, 0.0000, 0.3644],
        [0.0453, 0.0000, 0.1478,  ..., 0.1458, 0.0000, 0.3106],
        [0.0551, 0.0000, 0.1199,  ..., 0.1267, 0.0000, 0.2534],
        ...,
        [0.0783, 0.0668, 0.0134,  ..., 0.0760, 0.0756, 0.0585],
        [0.0702, 0.0263, 0.0421,  ..., 0.0933, 0.0306, 0.1155],
        [0.0647, 0.0075, 0.0625,  ..., 0.1052, 0.0086, 0.1550]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617271.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9073.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(850.3526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(372.5382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(339.6311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3304],
        [ 0.3581],
        [ 0.3870],
        ...,
        [-1.7658],
        [-1.1616],
        [-0.5634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278013.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0269],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370179.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 460.0 event: 2300 loss: tensor(432.5245, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2952],
        [0.4929],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.1295, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0270],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370186.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2952],
        [0.4929],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.1295, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048, -0.0011,  0.0029,  ...,  0.0044, -0.0014,  0.0119],
        [ 0.0089, -0.0018,  0.0054,  ...,  0.0063, -0.0023,  0.0194],
        [ 0.0048, -0.0011,  0.0029,  ...,  0.0044, -0.0014,  0.0119],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2947.1577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0253, device='cuda:0')



h[100].sum tensor(-18.3084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9346, device='cuda:0')



h[200].sum tensor(89.1767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.9134, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0173, 0.0000, 0.0106,  ..., 0.0156, 0.0000, 0.0394],
        [0.0349, 0.0000, 0.0214,  ..., 0.0251, 0.0000, 0.0767],
        [0.0557, 0.0000, 0.0341,  ..., 0.0349, 0.0000, 0.1152],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76829.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0691, 0.0122, 0.0619,  ..., 0.0935, 0.0128, 0.1425],
        [0.0637, 0.0000, 0.0910,  ..., 0.1074, 0.0000, 0.1949],
        [0.0571, 0.0000, 0.1128,  ..., 0.1214, 0.0000, 0.2382],
        ...,
        [0.0814, 0.1019, 0.0018,  ..., 0.0688, 0.1130, 0.0351],
        [0.0814, 0.1019, 0.0018,  ..., 0.0688, 0.1130, 0.0350],
        [0.0813, 0.1018, 0.0018,  ..., 0.0688, 0.1130, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692376.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8805.8926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(894.5718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(451.8111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(400.3613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1380],
        [ 0.3269],
        [ 0.3895],
        ...,
        [-2.5545],
        [-2.5494],
        [-2.5486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293975.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0236],
        [1.0270],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370186.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.9282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0234],
        [1.0236],
        [1.0271],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370194.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.9282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [ 0.0154, -0.0029,  0.0094,  ...,  0.0093, -0.0038,  0.0313],
        [ 0.0073, -0.0015,  0.0045,  ...,  0.0056, -0.0020,  0.0166],
        ...,
        [ 0.0081, -0.0016,  0.0049,  ...,  0.0059, -0.0021,  0.0179],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2164.9595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-11.6342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5953, device='cuda:0')



h[200].sum tensor(79.8926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0405, 0.0000, 0.0248,  ..., 0.0277, 0.0000, 0.0870],
        [0.0201, 0.0000, 0.0123,  ..., 0.0169, 0.0000, 0.0446],
        [0.0216, 0.0000, 0.0132,  ..., 0.0176, 0.0000, 0.0474],
        ...,
        [0.0067, 0.0000, 0.0041,  ..., 0.0102, 0.0000, 0.0176],
        [0.0085, 0.0000, 0.0052,  ..., 0.0110, 0.0000, 0.0209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62271.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0000, 0.0692,  ..., 0.0996, 0.0000, 0.1580],
        [0.0675, 0.0000, 0.0629,  ..., 0.0957, 0.0000, 0.1458],
        [0.0684, 0.0000, 0.0664,  ..., 0.0956, 0.0000, 0.1505],
        ...,
        [0.0772, 0.0387, 0.0281,  ..., 0.0803, 0.0432, 0.0825],
        [0.0779, 0.0518, 0.0218,  ..., 0.0779, 0.0591, 0.0719],
        [0.0803, 0.0853, 0.0079,  ..., 0.0716, 0.0952, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623393.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8948.7588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.1984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(357.0042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.0731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3464],
        [ 0.3709],
        [ 0.4044],
        ...,
        [-1.5221],
        [-1.8083],
        [-2.1398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285476.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0234],
        [1.0236],
        [1.0271],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370194.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2800],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.9011, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0234],
        [1.0236],
        [1.0271],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370201.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2800],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.9011, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [ 0.0045, -0.0010,  0.0028,  ...,  0.0042, -0.0013,  0.0113],
        [ 0.0032, -0.0008,  0.0020,  ...,  0.0036, -0.0010,  0.0089],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2275.4414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-12.4173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6417, device='cuda:0')



h[200].sum tensor(81.5888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0000, 0.0028,  ..., 0.0090, 0.0000, 0.0136],
        [0.0152, 0.0000, 0.0093,  ..., 0.0146, 0.0000, 0.0357],
        [0.0453, 0.0000, 0.0277,  ..., 0.0299, 0.0000, 0.0959],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62529.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0743, 0.0371, 0.0335,  ..., 0.0796, 0.0406, 0.0911],
        [0.0679, 0.0116, 0.0645,  ..., 0.0951, 0.0122, 0.1474],
        [0.0574, 0.0000, 0.1083,  ..., 0.1185, 0.0000, 0.2287],
        ...,
        [0.0813, 0.1016, 0.0020,  ..., 0.0688, 0.1126, 0.0354],
        [0.0813, 0.1016, 0.0020,  ..., 0.0687, 0.1125, 0.0353],
        [0.0813, 0.1015, 0.0020,  ..., 0.0687, 0.1125, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612022.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8972.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.8577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(356.6519, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.1049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3407],
        [ 0.0728],
        [ 0.3093],
        ...,
        [-2.5531],
        [-2.5470],
        [-2.5411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275105.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0234],
        [1.0236],
        [1.0271],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370201.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9297],
        [0.8750],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.0247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0234],
        [1.0236],
        [1.0272],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370209.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.9297],
        [0.8750],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.0247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0168, -0.0031,  0.0103,  ...,  0.0100, -0.0041,  0.0340],
        [ 0.0179, -0.0033,  0.0110,  ...,  0.0105, -0.0044,  0.0361],
        [ 0.0168, -0.0031,  0.0103,  ...,  0.0100, -0.0041,  0.0340],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2180.2490, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-11.4652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5874, device='cuda:0')



h[200].sum tensor(80.6963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0811, 0.0000, 0.0496,  ..., 0.0467, 0.0000, 0.1620],
        [0.0780, 0.0000, 0.0477,  ..., 0.0453, 0.0000, 0.1564],
        [0.0368, 0.0000, 0.0225,  ..., 0.0253, 0.0000, 0.0779],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59450.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0424, 0.0000, 0.1272,  ..., 0.1404, 0.0000, 0.2797],
        [0.0410, 0.0000, 0.1377,  ..., 0.1447, 0.0000, 0.2975],
        [0.0508, 0.0000, 0.1178,  ..., 0.1284, 0.0000, 0.2525],
        ...,
        [0.0813, 0.1014, 0.0022,  ..., 0.0688, 0.1120, 0.0356],
        [0.0813, 0.1013, 0.0022,  ..., 0.0688, 0.1120, 0.0356],
        [0.0813, 0.1013, 0.0022,  ..., 0.0688, 0.1120, 0.0356]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595520.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8959.0820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(830.0415, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(337.3709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.7038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2535],
        [ 0.3475],
        [ 0.3746],
        ...,
        [-2.5529],
        [-2.5477],
        [-2.5468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275665.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0234],
        [1.0236],
        [1.0272],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370209.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2700],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(306.2042, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0235],
        [1.0237],
        [1.0273],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370216.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2700],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(306.2042, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0010,  0.0026,  ...,  0.0041, -0.0013,  0.0110],
        [ 0.0037, -0.0009,  0.0023,  ...,  0.0039, -0.0011,  0.0099],
        [ 0.0093, -0.0018,  0.0057,  ...,  0.0065, -0.0024,  0.0202],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2744.6602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0226, device='cuda:0')



h[100].sum tensor(-15.9953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8364, device='cuda:0')



h[200].sum tensor(87.8079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1375, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0000, 0.0094,  ..., 0.0146, 0.0000, 0.0359],
        [0.0327, 0.0000, 0.0201,  ..., 0.0241, 0.0000, 0.0730],
        [0.0140, 0.0000, 0.0086,  ..., 0.0146, 0.0000, 0.0360],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68937.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0680, 0.0000, 0.0676,  ..., 0.0949, 0.0000, 0.1506],
        [0.0671, 0.0000, 0.0766,  ..., 0.0983, 0.0000, 0.1658],
        [0.0713, 0.0039, 0.0608,  ..., 0.0899, 0.0039, 0.1365],
        ...,
        [0.0814, 0.1011, 0.0023,  ..., 0.0690, 0.1115, 0.0360],
        [0.0814, 0.1011, 0.0023,  ..., 0.0689, 0.1114, 0.0360],
        [0.0813, 0.1011, 0.0023,  ..., 0.0689, 0.1114, 0.0360]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635596.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8888.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.7979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(395.3706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(360.2972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2537],
        [ 0.2923],
        [ 0.2543],
        ...,
        [-2.5450],
        [-2.5404],
        [-2.5399]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254796.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0235],
        [1.0237],
        [1.0273],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370216.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4056, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0236],
        [1.0237],
        [1.0273],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370224.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4056, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2501.4834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-13.8856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7277, device='cuda:0')



h[200].sum tensor(84.5942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0131, 0.0000, 0.0081,  ..., 0.0136, 0.0000, 0.0321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64711.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0731, 0.0261, 0.0431,  ..., 0.0836, 0.0290, 0.1073],
        [0.0772, 0.0651, 0.0157,  ..., 0.0727, 0.0719, 0.0593],
        [0.0788, 0.0908, 0.0051,  ..., 0.0687, 0.0997, 0.0412],
        ...,
        [0.0816, 0.1011, 0.0024,  ..., 0.0692, 0.1114, 0.0363],
        [0.0816, 0.1010, 0.0024,  ..., 0.0692, 0.1114, 0.0363],
        [0.0816, 0.1010, 0.0024,  ..., 0.0692, 0.1114, 0.0363]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616790.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8931.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(848.3331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(367.8575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.0007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3710],
        [-0.9922],
        [-1.5610],
        ...,
        [-2.5471],
        [-2.5421],
        [-2.5413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253248.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0236],
        [1.0237],
        [1.0273],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370224.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.2611, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0236],
        [1.0238],
        [1.0274],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370230.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.2611, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0086, -0.0017,  0.0053,  ...,  0.0062, -0.0022,  0.0191],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2241.4722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-11.6784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6099, device='cuda:0')



h[200].sum tensor(81.0514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0371, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0327, 0.0000, 0.0201,  ..., 0.0228, 0.0000, 0.0684],
        [0.0089, 0.0000, 0.0054,  ..., 0.0110, 0.0000, 0.0218],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60894.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0603, 0.0032, 0.0768,  ..., 0.1088, 0.0037, 0.1786],
        [0.0707, 0.0355, 0.0373,  ..., 0.0869, 0.0394, 0.1035],
        [0.0767, 0.0666, 0.0129,  ..., 0.0740, 0.0746, 0.0570],
        ...,
        [0.0819, 0.1012, 0.0023,  ..., 0.0696, 0.1115, 0.0367],
        [0.0819, 0.1011, 0.0023,  ..., 0.0695, 0.1115, 0.0367],
        [0.0819, 0.1011, 0.0023,  ..., 0.0695, 0.1115, 0.0367]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601737.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9023.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(830.4156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(343.8785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.0211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4409],
        [-1.1149],
        [-1.7878],
        ...,
        [-2.5533],
        [-2.5483],
        [-2.5474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247686.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0236],
        [1.0238],
        [1.0274],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370230.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.6360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0238],
        [1.0275],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370237.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.6360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2211.4741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-11.4281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5863, device='cuda:0')



h[200].sum tensor(80.2630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6107, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0030],
        [0.0264, 0.0000, 0.0162,  ..., 0.0199, 0.0000, 0.0567],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60637.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0733, 0.0238, 0.0317,  ..., 0.0830, 0.0255, 0.0923],
        [0.0721, 0.0196, 0.0367,  ..., 0.0862, 0.0212, 0.1021],
        [0.0647, 0.0077, 0.0679,  ..., 0.1034, 0.0085, 0.1608],
        ...,
        [0.0822, 0.1014, 0.0022,  ..., 0.0700, 0.1120, 0.0369],
        [0.0822, 0.1013, 0.0022,  ..., 0.0699, 0.1120, 0.0369],
        [0.0822, 0.1013, 0.0022,  ..., 0.0699, 0.1120, 0.0369]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600943.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9063.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.7731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(344.3194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(313.6453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2568],
        [ 0.3107],
        [ 0.3554],
        ...,
        [-2.5682],
        [-2.5630],
        [-2.5620]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263661.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0238],
        [1.0275],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370237.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4326],
        [0.4409],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(312.9434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0239],
        [1.0276],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370243.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4326],
        [0.4409],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(312.9434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0285, -0.0051,  0.0175,  ...,  0.0155, -0.0067,  0.0557],
        [ 0.0145, -0.0027,  0.0089,  ...,  0.0090, -0.0035,  0.0300],
        [ 0.0076, -0.0015,  0.0047,  ...,  0.0057, -0.0020,  0.0172],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2814.2791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0231, device='cuda:0')



h[100].sum tensor(-16.3942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8548, device='cuda:0')



h[200].sum tensor(87.4999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.4706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0427, 0.0000, 0.0262,  ..., 0.0289, 0.0000, 0.0916],
        [0.0682, 0.0000, 0.0418,  ..., 0.0409, 0.0000, 0.1388],
        [0.0599, 0.0000, 0.0368,  ..., 0.0370, 0.0000, 0.1235],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71387.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0491, 0.0000, 0.1405,  ..., 0.1408, 0.0000, 0.2971],
        [0.0423, 0.0000, 0.1582,  ..., 0.1542, 0.0000, 0.3354],
        [0.0400, 0.0000, 0.1653,  ..., 0.1594, 0.0000, 0.3499],
        ...,
        [0.0825, 0.1015, 0.0020,  ..., 0.0703, 0.1126, 0.0369],
        [0.0824, 0.1015, 0.0020,  ..., 0.0703, 0.1125, 0.0369],
        [0.0824, 0.1014, 0.0020,  ..., 0.0703, 0.1125, 0.0369]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648655.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9061.4551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(882.5936, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(408.9606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(371.3703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3821],
        [ 0.3642],
        [ 0.3373],
        ...,
        [-2.5853],
        [-2.5799],
        [-2.5789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222213.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0239],
        [1.0276],
        ...,
        [0.9990],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370243.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.1638, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0239],
        [1.0277],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370250.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.1638, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2403.2412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-12.9724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6779, device='cuda:0')



h[200].sum tensor(82.5563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65078.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0796, 0.0968, 0.0017,  ..., 0.0690, 0.1068, 0.0374],
        [0.0799, 0.0993, 0.0015,  ..., 0.0688, 0.1095, 0.0359],
        [0.0801, 0.0993, 0.0016,  ..., 0.0689, 0.1097, 0.0359],
        ...,
        [0.0826, 0.1016, 0.0019,  ..., 0.0706, 0.1129, 0.0370],
        [0.0826, 0.1016, 0.0019,  ..., 0.0706, 0.1128, 0.0370],
        [0.0826, 0.1016, 0.0019,  ..., 0.0706, 0.1128, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629534.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9026.2939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(852.7678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(373.1043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(339.9180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8839],
        [-2.2471],
        [-2.4912],
        ...,
        [-2.5983],
        [-2.5929],
        [-2.5918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268445.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0239],
        [1.0277],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370250.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 470.0 event: 2350 loss: tensor(487.6861, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6108],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2024, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0239],
        [1.0277],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370257.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6108],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2024, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [ 0.0113, -0.0021,  0.0070,  ...,  0.0075, -0.0028,  0.0240],
        [ 0.0177, -0.0032,  0.0108,  ...,  0.0105, -0.0042,  0.0357],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2943.9065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0247, device='cuda:0')



h[100].sum tensor(-17.3572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9156, device='cuda:0')



h[200].sum tensor(89.3999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.5710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0000, 0.0098,  ..., 0.0151, 0.0000, 0.0371],
        [0.0343, 0.0000, 0.0211,  ..., 0.0237, 0.0000, 0.0712],
        [0.0936, 0.0000, 0.0574,  ..., 0.0529, 0.0000, 0.1855],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73419.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0587, 0.0000, 0.0905,  ..., 0.1173, 0.0000, 0.2074],
        [0.0442, 0.0000, 0.1291,  ..., 0.1454, 0.0000, 0.2900],
        [0.0202, 0.0000, 0.2065,  ..., 0.1976, 0.0000, 0.4514],
        ...,
        [0.0823, 0.0935, 0.0045,  ..., 0.0721, 0.1043, 0.0428],
        [0.0814, 0.0768, 0.0114,  ..., 0.0748, 0.0866, 0.0545],
        [0.0804, 0.0602, 0.0173,  ..., 0.0776, 0.0689, 0.0661]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661347.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9048.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(897.4754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(421.6268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(385.3852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2437],
        [ 0.2265],
        [ 0.2061],
        ...,
        [-2.0494],
        [-1.5657],
        [-1.0400]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226161.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0239],
        [1.0277],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370257.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0239],
        [1.0278],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370264.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101, -0.0019,  0.0062,  ...,  0.0069, -0.0025,  0.0218],
        [ 0.0096, -0.0018,  0.0059,  ...,  0.0067, -0.0024,  0.0208],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2703.4697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0217, device='cuda:0')



h[100].sum tensor(-15.3826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8049, device='cuda:0')



h[200].sum tensor(86.2628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0449, 0.0000, 0.0276,  ..., 0.0299, 0.0000, 0.0956],
        [0.0204, 0.0000, 0.0126,  ..., 0.0178, 0.0000, 0.0479],
        [0.0125, 0.0000, 0.0077,  ..., 0.0135, 0.0000, 0.0309],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70785.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0630, 0.0000, 0.0960,  ..., 0.1134, 0.0000, 0.2095],
        [0.0695, 0.0067, 0.0688,  ..., 0.0990, 0.0074, 0.1585],
        [0.0735, 0.0229, 0.0447,  ..., 0.0886, 0.0267, 0.1157],
        ...,
        [0.0830, 0.1020, 0.0017,  ..., 0.0710, 0.1136, 0.0371],
        [0.0830, 0.1020, 0.0017,  ..., 0.0710, 0.1136, 0.0371],
        [0.0829, 0.1019, 0.0017,  ..., 0.0710, 0.1136, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657014.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8975.1523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.6215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.8191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(368.7224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2631],
        [-0.1169],
        [-0.6330],
        ...,
        [-2.6249],
        [-2.6194],
        [-2.6183]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287137.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0239],
        [1.0278],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370264.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6143],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(262.4556, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0239],
        [1.0279],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370271.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6143],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(262.4556, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0190, -0.0034,  0.0116,  ...,  0.0111, -0.0045,  0.0382],
        [ 0.0226, -0.0040,  0.0138,  ...,  0.0128, -0.0053,  0.0448],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2489.8926, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0194, device='cuda:0')



h[100].sum tensor(-13.6283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7169, device='cuda:0')



h[200].sum tensor(83.4264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9747, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1309, 0.0000, 0.0802,  ..., 0.0703, 0.0000, 0.2543],
        [0.0526, 0.0000, 0.0322,  ..., 0.0329, 0.0000, 0.1073],
        [0.0312, 0.0000, 0.0191,  ..., 0.0223, 0.0000, 0.0654],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64940.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0112, 0.0000, 0.2254,  ..., 0.2092, 0.0000, 0.4897],
        [0.0368, 0.0000, 0.1493,  ..., 0.1592, 0.0000, 0.3323],
        [0.0547, 0.0094, 0.0926,  ..., 0.1238, 0.0107, 0.2181],
        ...,
        [0.0832, 0.1023, 0.0017,  ..., 0.0712, 0.1141, 0.0372],
        [0.0832, 0.1022, 0.0017,  ..., 0.0712, 0.1140, 0.0372],
        [0.0832, 0.1022, 0.0017,  ..., 0.0711, 0.1140, 0.0372]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623411.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9100.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(852.1652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(375.1301, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(336.7459, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3172],
        [ 0.3204],
        [ 0.3147],
        ...,
        [-2.6369],
        [-2.6313],
        [-2.6301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278352.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0239],
        [1.0279],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370271.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(351.4818, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0240],
        [1.0280],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370278.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(351.4818, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3105.0913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0259, device='cuda:0')



h[100].sum tensor(-18.6165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9601, device='cuda:0')



h[200].sum tensor(90.7939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.3758, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76982.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0805, 0.0959, 0.0028,  ..., 0.0697, 0.1064, 0.0392],
        [0.0802, 0.0772, 0.0115,  ..., 0.0723, 0.0862, 0.0535],
        [0.0790, 0.0481, 0.0237,  ..., 0.0768, 0.0551, 0.0748],
        ...,
        [0.0834, 0.1024, 0.0017,  ..., 0.0713, 0.1145, 0.0373],
        [0.0834, 0.1024, 0.0017,  ..., 0.0713, 0.1144, 0.0372],
        [0.0833, 0.1024, 0.0017,  ..., 0.0712, 0.1144, 0.0372]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684268.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9052.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(904.6467, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(449.5673, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(396.3730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3743],
        [-0.8706],
        [-0.3407],
        ...,
        [-2.6467],
        [-2.6410],
        [-2.6398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252826.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0240],
        [1.0280],
        ...,
        [0.9989],
        [0.9983],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370278.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9938, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0240],
        [1.0281],
        ...,
        [0.9989],
        [0.9983],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370285.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9938, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2182.1267, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-11.0726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5736, device='cuda:0')



h[200].sum tensor(79.5510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3812, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60771.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0711, 0.0158, 0.0476,  ..., 0.0917, 0.0176, 0.1225],
        [0.0717, 0.0223, 0.0421,  ..., 0.0902, 0.0245, 0.1138],
        [0.0724, 0.0321, 0.0373,  ..., 0.0887, 0.0350, 0.1055],
        ...,
        [0.0835, 0.1026, 0.0018,  ..., 0.0713, 0.1147, 0.0374],
        [0.0835, 0.1026, 0.0018,  ..., 0.0713, 0.1146, 0.0373],
        [0.0835, 0.1026, 0.0018,  ..., 0.0713, 0.1146, 0.0373]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611832.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9124.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(831.3797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.0661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.7747, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2769],
        [ 0.2140],
        [ 0.1330],
        ...,
        [-2.6537],
        [-2.6480],
        [-2.6469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304338.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0240],
        [1.0281],
        ...,
        [0.9989],
        [0.9983],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370285.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.4773, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0241],
        [1.0282],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370292.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.4773, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2180.1465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-10.9941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5777, device='cuda:0')



h[200].sum tensor(79.6787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4545, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59973.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0806, 0.1001, 0.0015,  ..., 0.0693, 0.1110, 0.0363],
        [0.0805, 0.0964, 0.0022,  ..., 0.0701, 0.1071, 0.0390],
        [0.0804, 0.0913, 0.0043,  ..., 0.0711, 0.1020, 0.0426],
        ...,
        [0.0836, 0.1027, 0.0018,  ..., 0.0713, 0.1147, 0.0375],
        [0.0836, 0.1027, 0.0018,  ..., 0.0713, 0.1147, 0.0375],
        [0.0835, 0.1026, 0.0018,  ..., 0.0712, 0.1146, 0.0375]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606372.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9161.3877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(827.1200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(346.4056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.3918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6111],
        [-2.6128],
        [-2.5542],
        ...,
        [-2.6581],
        [-2.6525],
        [-2.6512]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296282.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0241],
        [1.0282],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370292.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.0955, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0239],
        [1.0241],
        [1.0282],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370300.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.0955, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2314.1472, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-11.9966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6367, device='cuda:0')



h[200].sum tensor(81.6213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0186, 0.0000, 0.0114,  ..., 0.0163, 0.0000, 0.0423],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60912.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0649, 0.0128, 0.0669,  ..., 0.1025, 0.0141, 0.1587],
        [0.0746, 0.0383, 0.0305,  ..., 0.0831, 0.0438, 0.0898],
        [0.0791, 0.0675, 0.0147,  ..., 0.0745, 0.0766, 0.0598],
        ...,
        [0.0836, 0.1027, 0.0020,  ..., 0.0712, 0.1145, 0.0377],
        [0.0836, 0.1027, 0.0020,  ..., 0.0712, 0.1145, 0.0377],
        [0.0835, 0.1027, 0.0020,  ..., 0.0711, 0.1145, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604640.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9190.6611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(835.4683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(349.1891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.6593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1789],
        [ 0.0293],
        [-0.1727],
        ...,
        [-2.6590],
        [-2.6534],
        [-2.6521]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262912.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0239],
        [1.0241],
        [1.0282],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370300.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.3152]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8314, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0239],
        [1.0242],
        [1.0283],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370308.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.3152]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8314, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0089, -0.0017,  0.0055,  ...,  0.0063, -0.0022,  0.0196],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0052, -0.0011,  0.0032,  ...,  0.0046, -0.0014,  0.0128],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2258.6877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-11.4790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6005, device='cuda:0')



h[200].sum tensor(81.3061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8675, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0031],
        [0.0091, 0.0000, 0.0056,  ..., 0.0112, 0.0000, 0.0224],
        [0.0240, 0.0000, 0.0147,  ..., 0.0189, 0.0000, 0.0523],
        ...,
        [0.0055, 0.0000, 0.0034,  ..., 0.0097, 0.0000, 0.0159],
        [0.0042, 0.0000, 0.0026,  ..., 0.0092, 0.0000, 0.0136],
        [0.0195, 0.0000, 0.0120,  ..., 0.0182, 0.0000, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62956.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0785, 0.0755, 0.0100,  ..., 0.0735, 0.0853, 0.0530],
        [0.0713, 0.0379, 0.0350,  ..., 0.0878, 0.0428, 0.1020],
        [0.0591, 0.0094, 0.0757,  ..., 0.1110, 0.0106, 0.1804],
        ...,
        [0.0817, 0.0662, 0.0170,  ..., 0.0763, 0.0760, 0.0640],
        [0.0813, 0.0561, 0.0219,  ..., 0.0774, 0.0652, 0.0715],
        [0.0796, 0.0225, 0.0367,  ..., 0.0822, 0.0297, 0.0956]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622552.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9118.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(845.0896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(362.0775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.5101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7033],
        [-0.9603],
        [-0.2733],
        ...,
        [-2.1127],
        [-1.8969],
        [-1.7701]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255260.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0239],
        [1.0242],
        [1.0283],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370308.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0239],
        [1.0242],
        [1.0284],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370315.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0085, -0.0016,  0.0052,  ...,  0.0061, -0.0021,  0.0188],
        [ 0.0096, -0.0018,  0.0059,  ...,  0.0067, -0.0024,  0.0209],
        [ 0.0044, -0.0009,  0.0027,  ...,  0.0042, -0.0012,  0.0112],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2361.9414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-12.2258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6352, device='cuda:0')



h[200].sum tensor(83.0520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0390, 0.0000, 0.0239,  ..., 0.0271, 0.0000, 0.0847],
        [0.0414, 0.0000, 0.0254,  ..., 0.0283, 0.0000, 0.0893],
        [0.0306, 0.0000, 0.0187,  ..., 0.0226, 0.0000, 0.0668],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61404.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0532, 0.0000, 0.1194,  ..., 0.1254, 0.0000, 0.2513],
        [0.0528, 0.0000, 0.1199,  ..., 0.1264, 0.0000, 0.2529],
        [0.0599, 0.0000, 0.0957,  ..., 0.1133, 0.0000, 0.2075],
        ...,
        [0.0835, 0.1029, 0.0020,  ..., 0.0711, 0.1146, 0.0378],
        [0.0834, 0.1028, 0.0020,  ..., 0.0711, 0.1146, 0.0377],
        [0.0834, 0.1028, 0.0020,  ..., 0.0711, 0.1146, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609279.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9059.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(832.7645, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(355.8695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.9593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4491],
        [ 0.4120],
        [ 0.2366],
        ...,
        [-2.6720],
        [-2.6663],
        [-2.6650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300042.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0239],
        [1.0242],
        [1.0284],
        ...,
        [0.9989],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370315.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.8492, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0285],
        ...,
        [0.9988],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370323.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.8492, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2184.4695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0150, device='cuda:0')



h[100].sum tensor(-10.6974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5568, device='cuda:0')



h[200].sum tensor(81.6187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0030],
        [0.0047, 0.0000, 0.0029,  ..., 0.0092, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61164.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0802, 0.0980, 0.0019,  ..., 0.0693, 0.1085, 0.0381],
        [0.0796, 0.0848, 0.0076,  ..., 0.0714, 0.0946, 0.0477],
        [0.0773, 0.0467, 0.0240,  ..., 0.0775, 0.0540, 0.0763],
        ...,
        [0.0833, 0.1029, 0.0020,  ..., 0.0710, 0.1145, 0.0378],
        [0.0833, 0.1028, 0.0020,  ..., 0.0709, 0.1145, 0.0378],
        [0.0832, 0.1028, 0.0020,  ..., 0.0709, 0.1144, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615313.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8987.3564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(839.7806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(351.4276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(317.8212, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0156],
        [-1.5504],
        [-0.9526],
        ...,
        [-2.6766],
        [-2.6708],
        [-2.6694]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278974.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0285],
        ...,
        [0.9988],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370323.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 480.0 event: 2400 loss: tensor(495.4333, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.5248, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0285],
        ...,
        [0.9988],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370331.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.5248, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0087, -0.0016,  0.0053,  ...,  0.0062, -0.0021,  0.0191],
        [ 0.0123, -0.0022,  0.0075,  ...,  0.0079, -0.0029,  0.0258],
        [ 0.0153, -0.0027,  0.0094,  ...,  0.0093, -0.0036,  0.0313],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2759.1592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0215, device='cuda:0')



h[100].sum tensor(-15.2093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7963, device='cuda:0')



h[200].sum tensor(89.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4118, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0000, 0.0175,  ..., 0.0222, 0.0000, 0.0653],
        [0.0372, 0.0000, 0.0228,  ..., 0.0262, 0.0000, 0.0813],
        [0.0283, 0.0000, 0.0174,  ..., 0.0215, 0.0000, 0.0626],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69124.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0643, 0.0000, 0.0835,  ..., 0.1025, 0.0000, 0.1803],
        [0.0598, 0.0000, 0.0970,  ..., 0.1108, 0.0000, 0.2065],
        [0.0614, 0.0000, 0.0853,  ..., 0.1072, 0.0000, 0.1873],
        ...,
        [0.0832, 0.1029, 0.0021,  ..., 0.0708, 0.1144, 0.0379],
        [0.0832, 0.1028, 0.0020,  ..., 0.0708, 0.1144, 0.0379],
        [0.0832, 0.1028, 0.0020,  ..., 0.0708, 0.1144, 0.0379]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644794.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8909.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.6536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(398.0979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(359.4373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3545],
        [-0.1005],
        [-0.1025],
        ...,
        [-2.6809],
        [-2.6751],
        [-2.6737]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252663.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0285],
        ...,
        [0.9988],
        [0.9982],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370331.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(173.1658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0286],
        ...,
        [0.9988],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370339.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1979.0383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.9150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0128, device='cuda:0')



h[100].sum tensor(-8.8906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4730, device='cuda:0')



h[200].sum tensor(79.8308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0031],
        [0.0067, 0.0000, 0.0041,  ..., 0.0101, 0.0000, 0.0179],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55565.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0801, 0.0973, 0.0023,  ..., 0.0693, 0.1075, 0.0390],
        [0.0797, 0.0873, 0.0066,  ..., 0.0709, 0.0972, 0.0461],
        [0.0778, 0.0572, 0.0186,  ..., 0.0755, 0.0660, 0.0671],
        ...,
        [0.0833, 0.1030, 0.0022,  ..., 0.0708, 0.1144, 0.0381],
        [0.0833, 0.1029, 0.0022,  ..., 0.0707, 0.1143, 0.0381],
        [0.0832, 0.1029, 0.0021,  ..., 0.0707, 0.1143, 0.0381]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585886.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8997.9131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(814.0382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(316.2357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.3660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5097],
        [-2.2076],
        [-1.7795],
        ...,
        [-2.6831],
        [-2.6774],
        [-2.6761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295175.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0286],
        ...,
        [0.9988],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370339.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2483],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7183, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0287],
        ...,
        [0.9988],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370346.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2483],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7183, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0085, -0.0016,  0.0052,  ...,  0.0061, -0.0021,  0.0189],
        [ 0.0128, -0.0023,  0.0079,  ...,  0.0082, -0.0030,  0.0268],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2509.2502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0186, device='cuda:0')



h[100].sum tensor(-13.0095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6876, device='cuda:0')



h[200].sum tensor(86.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4439, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0000, 0.0069,  ..., 0.0128, 0.0000, 0.0288],
        [0.0223, 0.0000, 0.0136,  ..., 0.0186, 0.0000, 0.0515],
        [0.0529, 0.0000, 0.0323,  ..., 0.0336, 0.0000, 0.1104],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64496.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0716, 0.0311, 0.0460,  ..., 0.0865, 0.0349, 0.1156],
        [0.0603, 0.0079, 0.0848,  ..., 0.1068, 0.0088, 0.1874],
        [0.0450, 0.0000, 0.1318,  ..., 0.1331, 0.0000, 0.2764],
        ...,
        [0.0834, 0.1031, 0.0023,  ..., 0.0707, 0.1140, 0.0384],
        [0.0833, 0.1030, 0.0023,  ..., 0.0707, 0.1140, 0.0384],
        [0.0833, 0.1030, 0.0023,  ..., 0.0707, 0.1140, 0.0384]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621703.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8912.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(853.0047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(369.2182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.1028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0537],
        [ 0.2398],
        [ 0.3437],
        ...,
        [-2.6811],
        [-2.6755],
        [-2.6742]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261411.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0287],
        ...,
        [0.9988],
        [0.9982],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370346.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.3489, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0287],
        ...,
        [0.9988],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370354.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.3489, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0045, -0.0009,  0.0028,  ...,  0.0042, -0.0012,  0.0115],
        [ 0.0032, -0.0007,  0.0020,  ...,  0.0036, -0.0010,  0.0091],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2394.0811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-11.9932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6292, device='cuda:0')



h[200].sum tensor(85.4201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3875, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0000, 0.0105,  ..., 0.0156, 0.0000, 0.0398],
        [0.0150, 0.0000, 0.0092,  ..., 0.0145, 0.0000, 0.0357],
        [0.0525, 0.0000, 0.0321,  ..., 0.0334, 0.0000, 0.1098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63225.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0664, 0.0000, 0.0632,  ..., 0.0953, 0.0000, 0.1469],
        [0.0617, 0.0000, 0.0802,  ..., 0.1041, 0.0000, 0.1782],
        [0.0478, 0.0000, 0.1243,  ..., 0.1280, 0.0000, 0.2608],
        ...,
        [0.0835, 0.1033, 0.0024,  ..., 0.0708, 0.1138, 0.0388],
        [0.0835, 0.1032, 0.0024,  ..., 0.0707, 0.1138, 0.0387],
        [0.0835, 0.1032, 0.0024,  ..., 0.0707, 0.1137, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620864.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8978.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.9880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(359.1100, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.8271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4055],
        [ 0.4412],
        [ 0.4409],
        ...,
        [-2.6811],
        [-2.6757],
        [-2.6745]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247700.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0242],
        [1.0287],
        ...,
        [0.9988],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370354.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4448],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0310, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0241],
        [1.0242],
        [1.0288],
        ...,
        [0.9988],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370361.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4448],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0310, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0079, -0.0015,  0.0048,  ...,  0.0058, -0.0020,  0.0177],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2443.3098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-12.3345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6529, device='cuda:0')



h[200].sum tensor(86.0881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0289, 0.0000, 0.0177,  ..., 0.0222, 0.0000, 0.0661],
        [0.0064, 0.0000, 0.0039,  ..., 0.0098, 0.0000, 0.0174],
        [0.0081, 0.0000, 0.0050,  ..., 0.0107, 0.0000, 0.0205],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63769.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0718, 0.0000, 0.0448,  ..., 0.0863, 0.0000, 0.1124],
        [0.0758, 0.0396, 0.0267,  ..., 0.0791, 0.0449, 0.0815],
        [0.0770, 0.0520, 0.0210,  ..., 0.0772, 0.0596, 0.0720],
        ...,
        [0.0837, 0.1035, 0.0024,  ..., 0.0709, 0.1140, 0.0389],
        [0.0836, 0.1034, 0.0024,  ..., 0.0709, 0.1140, 0.0389],
        [0.0836, 0.1034, 0.0024,  ..., 0.0708, 0.1140, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622961.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8936.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.5462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(364.3160, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(326.6540, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2246],
        [-1.4376],
        [-1.7788],
        ...,
        [-2.6889],
        [-2.6833],
        [-2.6819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259232.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0241],
        [1.0242],
        [1.0288],
        ...,
        [0.9988],
        [0.9981],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370361.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(378.9468, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0241],
        [1.0243],
        [1.0288],
        ...,
        [0.9988],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370368.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(378.9468, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0216, -0.0037,  0.0132,  ...,  0.0123, -0.0049,  0.0430],
        [ 0.0213, -0.0036,  0.0130,  ...,  0.0121, -0.0048,  0.0425],
        [ 0.0226, -0.0038,  0.0138,  ...,  0.0127, -0.0051,  0.0448],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3315.9717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0280, device='cuda:0')



h[100].sum tensor(-19.2354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0351, device='cuda:0')



h[200].sum tensor(96.4184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0968, 0.0000, 0.0591,  ..., 0.0541, 0.0000, 0.1913],
        [0.0927, 0.0000, 0.0566,  ..., 0.0522, 0.0000, 0.1838],
        [0.0724, 0.0000, 0.0443,  ..., 0.0427, 0.0000, 0.1465],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79167.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0000, 0.2332,  ..., 0.2021, 0.0000, 0.4871],
        [0.0067, 0.0000, 0.2229,  ..., 0.1956, 0.0000, 0.4661],
        [0.0144, 0.0000, 0.1987,  ..., 0.1799, 0.0000, 0.4163],
        ...,
        [0.0839, 0.1038, 0.0023,  ..., 0.0711, 0.1145, 0.0391],
        [0.0839, 0.1038, 0.0023,  ..., 0.0711, 0.1145, 0.0391],
        [0.0839, 0.1037, 0.0023,  ..., 0.0711, 0.1145, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701128.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8820.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(911.5383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(460.0666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(402.9238, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0892],
        [ 0.0929],
        [ 0.0981],
        ...,
        [-2.6947],
        [-2.6878],
        [-2.6834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239638.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0241],
        [1.0243],
        [1.0288],
        ...,
        [0.9988],
        [0.9981],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370368.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4187],
        [0.6455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.6072, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0242],
        [1.0243],
        [1.0288],
        ...,
        [0.9988],
        [0.9980],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370375.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4187],
        [0.6455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.6072, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0215, -0.0036,  0.0132,  ...,  0.0122, -0.0048,  0.0429],
        [ 0.0074, -0.0014,  0.0045,  ...,  0.0056, -0.0018,  0.0167],
        [ 0.0180, -0.0031,  0.0110,  ...,  0.0106, -0.0041,  0.0364],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2098.2825, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-9.5941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5097, device='cuda:0')



h[200].sum tensor(81.5703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0557, 0.0000, 0.0341,  ..., 0.0349, 0.0000, 0.1156],
        [0.0675, 0.0000, 0.0413,  ..., 0.0404, 0.0000, 0.1373],
        [0.0299, 0.0000, 0.0183,  ..., 0.0228, 0.0000, 0.0680],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57648.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0267, 0.0000, 0.1748,  ..., 0.1612, 0.0000, 0.3638],
        [0.0310, 0.0000, 0.1662,  ..., 0.1551, 0.0000, 0.3456],
        [0.0445, 0.0000, 0.1388,  ..., 0.1349, 0.0000, 0.2871],
        ...,
        [0.0841, 0.1041, 0.0022,  ..., 0.0714, 0.1151, 0.0392],
        [0.0841, 0.1041, 0.0022,  ..., 0.0713, 0.1150, 0.0391],
        [0.0840, 0.1040, 0.0022,  ..., 0.0713, 0.1150, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597142.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9012.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(818.1077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(329.5866, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.8159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1446],
        [ 0.1505],
        [ 0.1569],
        ...,
        [-2.7212],
        [-2.7153],
        [-2.7140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307367.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0242],
        [1.0243],
        [1.0288],
        ...,
        [0.9988],
        [0.9980],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370375.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2979],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(353.5647, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0242],
        [1.0243],
        [1.0288],
        ...,
        [0.9987],
        [0.9980],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370382.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2979],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(353.5647, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0010,  0.0030,  ...,  0.0044, -0.0013,  0.0121],
        [ 0.0042, -0.0009,  0.0026,  ...,  0.0041, -0.0012,  0.0109],
        [ 0.0151, -0.0026,  0.0092,  ...,  0.0092, -0.0035,  0.0310],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3159.1704, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0261, device='cuda:0')



h[100].sum tensor(-17.9202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9658, device='cuda:0')



h[200].sum tensor(94.5619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.4788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0000, 0.0078,  ..., 0.0135, 0.0000, 0.0313],
        [0.0367, 0.0000, 0.0225,  ..., 0.0260, 0.0000, 0.0805],
        [0.0334, 0.0000, 0.0205,  ..., 0.0244, 0.0000, 0.0744],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74045.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.8576e-02, 5.4887e-05, 6.2857e-02,  ..., 9.3646e-02, 9.7715e-04,
         1.4533e-01],
        [5.7593e-02, 0.0000e+00, 9.7193e-02,  ..., 1.1250e-01, 0.0000e+00,
         2.0984e-01],
        [4.7670e-02, 0.0000e+00, 1.2634e-01,  ..., 1.2920e-01, 0.0000e+00,
         2.6534e-01],
        ...,
        [8.4160e-02, 1.0434e-01, 2.0244e-03,  ..., 7.1486e-02, 1.1531e-01,
         3.9242e-02],
        [8.4121e-02, 1.0429e-01, 2.0234e-03,  ..., 7.1453e-02, 1.1526e-01,
         3.9224e-02],
        [8.4099e-02, 1.0427e-01, 2.0221e-03,  ..., 7.1435e-02, 1.1523e-01,
         3.9214e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664637.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8821.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(889.7479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(430.6967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(377.3240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4571],
        [ 0.4298],
        [ 0.3925],
        ...,
        [-2.7344],
        [-2.7285],
        [-2.7271]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276938.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0242],
        [1.0243],
        [1.0288],
        ...,
        [0.9987],
        [0.9980],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370382.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2778],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1986, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0243],
        [1.0244],
        [1.0289],
        ...,
        [0.9987],
        [0.9979],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370389.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2778],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1986, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0092, -0.0017,  0.0056,  ...,  0.0065, -0.0022,  0.0201],
        [ 0.0035, -0.0008,  0.0021,  ...,  0.0038, -0.0010,  0.0095],
        [ 0.0045, -0.0009,  0.0027,  ...,  0.0042, -0.0012,  0.0113],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2130.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-9.7872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5277, device='cuda:0')



h[200].sum tensor(82.1177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0139, 0.0000, 0.0086,  ..., 0.0146, 0.0000, 0.0360],
        [0.0334, 0.0000, 0.0205,  ..., 0.0244, 0.0000, 0.0745],
        [0.0152, 0.0000, 0.0094,  ..., 0.0147, 0.0000, 0.0361],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58267.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0690, 0.0090, 0.0639,  ..., 0.0937, 0.0101, 0.1463],
        [0.0644, 0.0000, 0.0797,  ..., 0.1020, 0.0000, 0.1754],
        [0.0698, 0.0123, 0.0590,  ..., 0.0927, 0.0135, 0.1387],
        ...,
        [0.0843, 0.1046, 0.0019,  ..., 0.0717, 0.1156, 0.0394],
        [0.0843, 0.1045, 0.0019,  ..., 0.0716, 0.1155, 0.0394],
        [0.0843, 0.1045, 0.0019,  ..., 0.0716, 0.1155, 0.0394]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602686.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8987.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(824.9733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.3685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.0094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2883],
        [ 0.3281],
        [ 0.1561],
        ...,
        [-2.7451],
        [-2.7391],
        [-2.7377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311034.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0243],
        [1.0244],
        [1.0289],
        ...,
        [0.9987],
        [0.9979],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370389.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2487],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.4779, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0244],
        [1.0289],
        ...,
        [0.9987],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370396.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2487],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.4779, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0038, -0.0008,  0.0024,  ...,  0.0039, -0.0011,  0.0102],
        [ 0.0041, -0.0009,  0.0026,  ...,  0.0041, -0.0011,  0.0108],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2195.3667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0150, device='cuda:0')



h[100].sum tensor(-10.2801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5558, device='cuda:0')



h[200].sum tensor(82.7421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0591, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0039, 0.0000, 0.0024,  ..., 0.0087, 0.0000, 0.0128],
        [0.0153, 0.0000, 0.0094,  ..., 0.0147, 0.0000, 0.0362],
        [0.0311, 0.0000, 0.0191,  ..., 0.0234, 0.0000, 0.0703],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60691.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0751, 0.0191, 0.0365,  ..., 0.0825, 0.0251, 0.0999],
        [0.0715, 0.0097, 0.0546,  ..., 0.0899, 0.0131, 0.1307],
        [0.0677, 0.0000, 0.0733,  ..., 0.0976, 0.0000, 0.1626],
        ...,
        [0.0845, 0.1047, 0.0019,  ..., 0.0719, 0.1159, 0.0396],
        [0.0845, 0.1047, 0.0019,  ..., 0.0719, 0.1158, 0.0396],
        [0.0844, 0.1047, 0.0019,  ..., 0.0718, 0.1158, 0.0396]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618216.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9025.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(842.1863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(345.2234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(310.6850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1127],
        [ 0.2544],
        [ 0.3007],
        ...,
        [-2.7548],
        [-2.7488],
        [-2.7473]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275667.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0244],
        [1.0289],
        ...,
        [0.9987],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370396.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 490.0 event: 2450 loss: tensor(489.5559, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.6091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0245],
        [1.0245],
        [1.0290],
        ...,
        [0.9986],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370402.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.6091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0181, -0.0031,  0.0111,  ...,  0.0107, -0.0041,  0.0366],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0174, -0.0029,  0.0107,  ...,  0.0103, -0.0039,  0.0353],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2152.6543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0144, device='cuda:0')



h[100].sum tensor(-9.9673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5343, device='cuda:0')



h[200].sum tensor(81.6951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0136, 0.0000, 0.0083,  ..., 0.0139, 0.0000, 0.0330],
        [0.0435, 0.0000, 0.0267,  ..., 0.0293, 0.0000, 0.0932],
        [0.0252, 0.0000, 0.0155,  ..., 0.0200, 0.0000, 0.0569],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60321.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0652, 0.0000, 0.0730,  ..., 0.1013, 0.0000, 0.1680],
        [0.0581, 0.0000, 0.0959,  ..., 0.1136, 0.0000, 0.2117],
        [0.0594, 0.0000, 0.0924,  ..., 0.1119, 0.0000, 0.2048],
        ...,
        [0.0848, 0.1050, 0.0017,  ..., 0.0722, 0.1165, 0.0398],
        [0.0848, 0.1050, 0.0017,  ..., 0.0722, 0.1165, 0.0398],
        [0.0848, 0.1049, 0.0017,  ..., 0.0722, 0.1164, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617571.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9039.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(832.9293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(347.9275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(304.4516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5054],
        [ 0.5020],
        [ 0.4803],
        ...,
        [-2.7705],
        [-2.7644],
        [-2.7629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314278.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0245],
        [1.0245],
        [1.0290],
        ...,
        [0.9986],
        [0.9979],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370402.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.3582],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0245],
        [1.0290],
        ...,
        [0.9986],
        [0.9978],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370409.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.3582],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084, -0.0015,  0.0051,  ...,  0.0061, -0.0020,  0.0186],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0084, -0.0015,  0.0051,  ...,  0.0061, -0.0020,  0.0186],
        ...,
        [ 0.0061, -0.0012,  0.0037,  ...,  0.0050, -0.0015,  0.0144],
        [ 0.0029, -0.0007,  0.0018,  ...,  0.0035, -0.0009,  0.0086],
        [ 0.0103, -0.0018,  0.0063,  ...,  0.0070, -0.0024,  0.0223]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2824.1975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0223, device='cuda:0')



h[100].sum tensor(-15.2072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8244, device='cuda:0')



h[200].sum tensor(89.3118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.9203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0000, 0.0041,  ..., 0.0101, 0.0000, 0.0180],
        [0.0306, 0.0000, 0.0188,  ..., 0.0232, 0.0000, 0.0695],
        [0.0067, 0.0000, 0.0041,  ..., 0.0102, 0.0000, 0.0181],
        ...,
        [0.0118, 0.0000, 0.0073,  ..., 0.0134, 0.0000, 0.0301],
        [0.0361, 0.0000, 0.0222,  ..., 0.0261, 0.0000, 0.0800],
        [0.0205, 0.0000, 0.0126,  ..., 0.0181, 0.0000, 0.0487]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69596.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0763, 0.0391, 0.0262,  ..., 0.0815, 0.0451, 0.0848],
        [0.0721, 0.0000, 0.0454,  ..., 0.0898, 0.0000, 0.1183],
        [0.0766, 0.0390, 0.0265,  ..., 0.0818, 0.0453, 0.0852],
        ...,
        [0.0731, 0.0163, 0.0554,  ..., 0.0953, 0.0212, 0.1358],
        [0.0654, 0.0000, 0.0858,  ..., 0.1093, 0.0000, 0.1900],
        [0.0676, 0.0007, 0.0787,  ..., 0.1055, 0.0021, 0.1768]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654278.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8964.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(871.5976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(405.7819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(348.3768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4854],
        [-1.4044],
        [-1.5603],
        ...,
        [-0.4410],
        [ 0.0804],
        [ 0.1028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309347.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0245],
        [1.0290],
        ...,
        [0.9986],
        [0.9978],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370409.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.8994, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0247],
        [1.0246],
        [1.0291],
        ...,
        [0.9986],
        [0.9978],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370416.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.8994, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0008],
        [ 0.0098, -0.0017,  0.0060,  ...,  0.0068, -0.0023,  0.0214],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0016,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2605.0840, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0196, device='cuda:0')



h[100].sum tensor(-13.4348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7263, device='cuda:0')



h[200].sum tensor(86.6404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1449, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0133, 0.0000, 0.0082,  ..., 0.0139, 0.0000, 0.0326],
        [0.0143, 0.0000, 0.0088,  ..., 0.0150, 0.0000, 0.0370],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66145.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0795, 0.0676, 0.0147,  ..., 0.0758, 0.0770, 0.0631],
        [0.0740, 0.0274, 0.0424,  ..., 0.0870, 0.0319, 0.1121],
        [0.0722, 0.0090, 0.0594,  ..., 0.0919, 0.0102, 0.1392],
        ...,
        [0.0852, 0.1053, 0.0017,  ..., 0.0725, 0.1170, 0.0402],
        [0.0851, 0.1053, 0.0017,  ..., 0.0725, 0.1169, 0.0401],
        [0.0851, 0.1052, 0.0017,  ..., 0.0725, 0.1169, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639010.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9071.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.4817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(379.9296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(332.1250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2728],
        [-0.5766],
        [-0.0490],
        ...,
        [-2.7849],
        [-2.7787],
        [-2.7772]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260792.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0247],
        [1.0246],
        [1.0291],
        ...,
        [0.9986],
        [0.9978],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370416.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.7242, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0247],
        [1.0292],
        ...,
        [0.9986],
        [0.9978],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370423.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.7242, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2766.6113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0212, device='cuda:0')



h[100].sum tensor(-14.5807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7860, device='cuda:0')



h[200].sum tensor(88.9225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70223.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0820, 0.1026, 0.0014,  ..., 0.0704, 0.1128, 0.0389],
        [0.0818, 0.0977, 0.0028,  ..., 0.0712, 0.1077, 0.0426],
        [0.0815, 0.0888, 0.0069,  ..., 0.0725, 0.0985, 0.0492],
        ...,
        [0.0850, 0.1053, 0.0018,  ..., 0.0724, 0.1166, 0.0403],
        [0.0850, 0.1053, 0.0018,  ..., 0.0724, 0.1166, 0.0403],
        [0.0850, 0.1052, 0.0018,  ..., 0.0724, 0.1166, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661211.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8921.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(873.5783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(409.0416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(350.0464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2283],
        [-1.9372],
        [-1.4985],
        ...,
        [-2.7831],
        [-2.7771],
        [-2.7756]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282246.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0247],
        [1.0292],
        ...,
        [0.9986],
        [0.9978],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370423.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.7826, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0248],
        [1.0293],
        ...,
        [0.9985],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370430.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.7826, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3277.5107, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0262, device='cuda:0')



h[100].sum tensor(-18.3865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9691, device='cuda:0')



h[200].sum tensor(95.8254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.5390, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81705.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0810, 0.0850, 0.0093,  ..., 0.0717, 0.0930, 0.0523],
        [0.0821, 0.0983, 0.0036,  ..., 0.0701, 0.1070, 0.0430],
        [0.0823, 0.1017, 0.0026,  ..., 0.0702, 0.1111, 0.0403],
        ...,
        [0.0847, 0.1053, 0.0020,  ..., 0.0722, 0.1160, 0.0403],
        [0.0846, 0.1052, 0.0020,  ..., 0.0722, 0.1159, 0.0403],
        [0.0846, 0.1052, 0.0020,  ..., 0.0721, 0.1159, 0.0403]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(733640., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8633.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(914.2004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(482.0154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(406.8358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9547],
        [-1.2556],
        [-1.3700],
        ...,
        [-2.7765],
        [-2.7707],
        [-2.7693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287889.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0248],
        [1.0293],
        ...,
        [0.9985],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370430.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.8209, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0250],
        [1.0248],
        [1.0294],
        ...,
        [0.9985],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370437.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.8209, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2193.3667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0144, device='cuda:0')



h[100].sum tensor(-9.8556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5322, device='cuda:0')



h[200].sum tensor(83.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60239.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0814, 0.1026, 0.0017,  ..., 0.0699, 0.1117, 0.0389],
        [0.0816, 0.1028, 0.0017,  ..., 0.0701, 0.1120, 0.0390],
        [0.0817, 0.1008, 0.0021,  ..., 0.0704, 0.1101, 0.0405],
        ...,
        [0.0844, 0.1052, 0.0021,  ..., 0.0720, 0.1155, 0.0403],
        [0.0844, 0.1052, 0.0021,  ..., 0.0719, 0.1154, 0.0402],
        [0.0844, 0.1052, 0.0021,  ..., 0.0719, 0.1154, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613945.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8921.8379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(841.3193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(340.1198, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(305.8102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6734],
        [-2.5242],
        [-2.2422],
        ...,
        [-2.7737],
        [-2.7681],
        [-2.7667]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254245.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0250],
        [1.0248],
        [1.0294],
        ...,
        [0.9985],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370437.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5205],
        [0.4160],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.6918, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0251],
        [1.0249],
        [1.0296],
        ...,
        [0.9985],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370444.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5205],
        [0.4160],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.6918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0143, -0.0024,  0.0088,  ...,  0.0089, -0.0032,  0.0296],
        [ 0.0162, -0.0027,  0.0099,  ...,  0.0098, -0.0036,  0.0331],
        [ 0.0140, -0.0024,  0.0086,  ...,  0.0087, -0.0031,  0.0291],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2351.5913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-11.0066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5892, device='cuda:0')



h[200].sum tensor(85.8703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0822, 0.0000, 0.0503,  ..., 0.0474, 0.0000, 0.1649],
        [0.0763, 0.0000, 0.0467,  ..., 0.0447, 0.0000, 0.1540],
        [0.0318, 0.0000, 0.0195,  ..., 0.0231, 0.0000, 0.0693],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62018.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0181, 0.0000, 0.1836,  ..., 0.1686, 0.0000, 0.3838],
        [0.0227, 0.0000, 0.1703,  ..., 0.1619, 0.0000, 0.3593],
        [0.0437, 0.0000, 0.1163,  ..., 0.1294, 0.0000, 0.2531],
        ...,
        [0.0843, 0.1053, 0.0021,  ..., 0.0719, 0.1154, 0.0402],
        [0.0843, 0.1053, 0.0021,  ..., 0.0719, 0.1153, 0.0402],
        [0.0842, 0.1052, 0.0021,  ..., 0.0719, 0.1153, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619987.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8790.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.8438, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(353.4667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.7824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3545],
        [ 0.3444],
        [ 0.2822],
        ...,
        [-2.7781],
        [-2.7725],
        [-2.7711]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285178., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0251],
        [1.0249],
        [1.0296],
        ...,
        [0.9985],
        [0.9977],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370444.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0252],
        [1.0250],
        [1.0297],
        ...,
        [0.9985],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370450.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2339.1677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-10.8478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5886, device='cuda:0')



h[200].sum tensor(86.1554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61807.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0798, 0.0864, 0.0079,  ..., 0.0722, 0.0949, 0.0501],
        [0.0812, 0.1006, 0.0021,  ..., 0.0703, 0.1094, 0.0406],
        [0.0816, 0.1030, 0.0019,  ..., 0.0700, 0.1121, 0.0390],
        ...,
        [0.0842, 0.1054, 0.0022,  ..., 0.0718, 0.1153, 0.0402],
        [0.0842, 0.1053, 0.0022,  ..., 0.0718, 0.1153, 0.0402],
        [0.0841, 0.1053, 0.0022,  ..., 0.0718, 0.1153, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622634.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8717.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(844.3851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.3218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.8536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5989],
        [-1.9972],
        [-2.1478],
        ...,
        [-2.7843],
        [-2.7788],
        [-2.7773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280159.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0252],
        [1.0250],
        [1.0297],
        ...,
        [0.9985],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370450.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(258.8282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0253],
        [1.0251],
        [1.0297],
        ...,
        [0.9985],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370457.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(258.8282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0175, -0.0029,  0.0107,  ...,  0.0104, -0.0038,  0.0355],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2621.4927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0191, device='cuda:0')



h[100].sum tensor(-12.9696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7070, device='cuda:0')



h[200].sum tensor(89.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7954, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0143, 0.0000, 0.0088,  ..., 0.0143, 0.0000, 0.0346],
        [0.0224, 0.0000, 0.0137,  ..., 0.0181, 0.0000, 0.0494],
        [0.0145, 0.0000, 0.0088,  ..., 0.0137, 0.0000, 0.0324],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67485.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0590, 0.0000, 0.0808,  ..., 0.1049, 0.0000, 0.1794],
        [0.0586, 0.0000, 0.0778,  ..., 0.1054, 0.0000, 0.1763],
        [0.0607, 0.0000, 0.0673,  ..., 0.1021, 0.0000, 0.1595],
        ...,
        [0.0842, 0.1055, 0.0022,  ..., 0.0718, 0.1154, 0.0402],
        [0.0841, 0.1055, 0.0022,  ..., 0.0717, 0.1153, 0.0402],
        [0.0841, 0.1054, 0.0022,  ..., 0.0717, 0.1153, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653383.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8606.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.4308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.6461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(345.9711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4208],
        [ 0.4423],
        [ 0.4536],
        ...,
        [-2.7920],
        [-2.7862],
        [-2.7845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271034.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0253],
        [1.0251],
        [1.0297],
        ...,
        [0.9985],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370457.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0254],
        [1.0251],
        [1.0298],
        ...,
        [0.9984],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370463.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2723.1960, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0202, device='cuda:0')



h[100].sum tensor(-13.7139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7487, device='cuda:0')



h[200].sum tensor(91.0253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5489, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0130, 0.0000, 0.0079,  ..., 0.0131, 0.0000, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68851.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0798, 0.0816, 0.0101,  ..., 0.0725, 0.0898, 0.0544],
        [0.0776, 0.0669, 0.0141,  ..., 0.0761, 0.0755, 0.0633],
        [0.0705, 0.0341, 0.0341,  ..., 0.0871, 0.0393, 0.1004],
        ...,
        [0.0842, 0.1057, 0.0021,  ..., 0.0719, 0.1156, 0.0403],
        [0.0842, 0.1056, 0.0021,  ..., 0.0718, 0.1155, 0.0403],
        [0.0842, 0.1056, 0.0021,  ..., 0.0718, 0.1155, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653096.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8682.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(888.1249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(388.9245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(355.1760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6873],
        [-1.6721],
        [-1.4366],
        ...,
        [-2.8013],
        [-2.7958],
        [-2.7944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225235.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0254],
        [1.0251],
        [1.0298],
        ...,
        [0.9984],
        [0.9976],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370463.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 500.0 event: 2500 loss: tensor(479.4332, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.5120, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0255],
        [1.0252],
        [1.0299],
        ...,
        [0.9984],
        [0.9975],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370470.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.5120, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2347.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-10.8225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5860, device='cuda:0')



h[200].sum tensor(86.5109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0268, 0.0000, 0.0164,  ..., 0.0202, 0.0000, 0.0578],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61533.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0790, 0.0835, 0.0080,  ..., 0.0735, 0.0924, 0.0517],
        [0.0721, 0.0482, 0.0273,  ..., 0.0839, 0.0532, 0.0876],
        [0.0518, 0.0105, 0.0806,  ..., 0.1134, 0.0124, 0.1890],
        ...,
        [0.0843, 0.1059, 0.0021,  ..., 0.0719, 0.1159, 0.0403],
        [0.0843, 0.1059, 0.0021,  ..., 0.0719, 0.1158, 0.0403],
        [0.0843, 0.1058, 0.0021,  ..., 0.0719, 0.1158, 0.0403]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619685.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8737.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(849.3038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(348.4196, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.7997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6990],
        [-0.9131],
        [-0.2056],
        ...,
        [-2.8113],
        [-2.8057],
        [-2.8043]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276324.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0255],
        [1.0252],
        [1.0299],
        ...,
        [0.9984],
        [0.9975],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370470.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.0281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0256],
        [1.0252],
        [1.0299],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370477., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.0281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2835.3025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0216, device='cuda:0')



h[100].sum tensor(-14.5214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7977, device='cuda:0')



h[200].sum tensor(92.2298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0000, 0.0031,  ..., 0.0093, 0.0000, 0.0151],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70441.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0776, 0.0508, 0.0241,  ..., 0.0765, 0.0578, 0.0769],
        [0.0805, 0.0847, 0.0099,  ..., 0.0722, 0.0926, 0.0531],
        [0.0819, 0.1000, 0.0032,  ..., 0.0704, 0.1087, 0.0423],
        ...,
        [0.0845, 0.1062, 0.0022,  ..., 0.0720, 0.1162, 0.0404],
        [0.0844, 0.1061, 0.0022,  ..., 0.0720, 0.1161, 0.0404],
        [0.0844, 0.1061, 0.0022,  ..., 0.0720, 0.1161, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659945.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8646.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.4966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(402.8838, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(358.1150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8109],
        [-1.3723],
        [-1.8442],
        ...,
        [-2.8214],
        [-2.8158],
        [-2.8144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254686.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0256],
        [1.0252],
        [1.0299],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370477., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.5049, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0253],
        [1.0300],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370483.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.5049, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040, -0.0008,  0.0024,  ...,  0.0040, -0.0011,  0.0105],
        [ 0.0040, -0.0008,  0.0024,  ...,  0.0040, -0.0011,  0.0105],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2331.4587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-10.6419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5750, device='cuda:0')



h[200].sum tensor(86.3315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4065, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0095, 0.0000, 0.0058,  ..., 0.0126, 0.0000, 0.0282],
        [0.0133, 0.0000, 0.0081,  ..., 0.0143, 0.0000, 0.0351],
        [0.0109, 0.0000, 0.0067,  ..., 0.0126, 0.0000, 0.0283],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62371.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0740, 0.0146, 0.0475,  ..., 0.0827, 0.0156, 0.1131],
        [0.0727, 0.0107, 0.0527,  ..., 0.0850, 0.0116, 0.1221],
        [0.0749, 0.0268, 0.0419,  ..., 0.0818, 0.0304, 0.1049],
        ...,
        [0.0845, 0.1064, 0.0022,  ..., 0.0720, 0.1163, 0.0405],
        [0.0845, 0.1064, 0.0022,  ..., 0.0720, 0.1162, 0.0405],
        [0.0839, 0.1017, 0.0035,  ..., 0.0728, 0.1115, 0.0436]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629466.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8678.3213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(848.8126, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(355.5387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.5984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0658],
        [-0.0283],
        [-0.2542],
        ...,
        [-2.7999],
        [-2.7093],
        [-2.4499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289631.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0253],
        [1.0300],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370483.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6567, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0259],
        [1.0253],
        [1.0301],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370490.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6567, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039, -0.0008,  0.0024,  ...,  0.0040, -0.0010,  0.0104],
        [ 0.0148, -0.0024,  0.0090,  ...,  0.0091, -0.0032,  0.0305],
        [ 0.0148, -0.0024,  0.0090,  ...,  0.0091, -0.0032,  0.0305],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2434.2090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-11.4098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6246, device='cuda:0')



h[200].sum tensor(87.5013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0426, 0.0000, 0.0260,  ..., 0.0287, 0.0000, 0.0917],
        [0.0314, 0.0000, 0.0192,  ..., 0.0228, 0.0000, 0.0686],
        [0.0334, 0.0000, 0.0204,  ..., 0.0238, 0.0000, 0.0724],
        ...,
        [0.0051, 0.0000, 0.0032,  ..., 0.0101, 0.0000, 0.0179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63229.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0458, 0.0000, 0.1195,  ..., 0.1218, 0.0000, 0.2460],
        [0.0489, 0.0000, 0.1091,  ..., 0.1177, 0.0000, 0.2285],
        [0.0504, 0.0000, 0.1042,  ..., 0.1159, 0.0000, 0.2203],
        ...,
        [0.0836, 0.0673, 0.0226,  ..., 0.0749, 0.0749, 0.0720],
        [0.0843, 0.0936, 0.0089,  ..., 0.0729, 0.1026, 0.0511],
        [0.0845, 0.1019, 0.0044,  ..., 0.0723, 0.1114, 0.0444]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628301., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8708.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(855.2321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(359.8302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.7149, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5119],
        [ 0.5126],
        [ 0.5117],
        ...,
        [-2.1736],
        [-2.4345],
        [-2.6361]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279258.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0259],
        [1.0253],
        [1.0301],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370490.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9001, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0254],
        [1.0301],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370497.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9001, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2664.1943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0192, device='cuda:0')



h[100].sum tensor(-13.1167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7099, device='cuda:0')



h[200].sum tensor(90.0086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8484, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67831.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0724, 0.0485, 0.0275,  ..., 0.0834, 0.0527, 0.0870],
        [0.0796, 0.0841, 0.0086,  ..., 0.0739, 0.0928, 0.0527],
        [0.0816, 0.0982, 0.0038,  ..., 0.0715, 0.1070, 0.0437],
        ...,
        [0.0850, 0.1071, 0.0024,  ..., 0.0721, 0.1167, 0.0408],
        [0.0850, 0.1070, 0.0024,  ..., 0.0721, 0.1166, 0.0408],
        [0.0849, 0.1070, 0.0024,  ..., 0.0721, 0.1166, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655115.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8688.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(874.6083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.7680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(340.6335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0299],
        [-1.5281],
        [-1.6420],
        ...,
        [-2.8432],
        [-2.8375],
        [-2.8360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265803.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0254],
        [1.0301],
        ...,
        [0.9984],
        [0.9975],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370497.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0262],
        [1.0254],
        [1.0302],
        ...,
        [0.9984],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370504.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0116, -0.0019,  0.0071,  ...,  0.0076, -0.0026,  0.0246],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2786.4712, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-14.0208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7744, device='cuda:0')



h[200].sum tensor(91.1288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0145, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        [0.0119, 0.0000, 0.0073,  ..., 0.0124, 0.0000, 0.0278],
        [0.0150, 0.0000, 0.0092,  ..., 0.0145, 0.0000, 0.0360],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66994.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0794, 0.0789, 0.0112,  ..., 0.0745, 0.0873, 0.0567],
        [0.0704, 0.0363, 0.0371,  ..., 0.0875, 0.0433, 0.1048],
        [0.0623, 0.0126, 0.0654,  ..., 0.0992, 0.0166, 0.1536],
        ...,
        [0.0853, 0.1074, 0.0024,  ..., 0.0723, 0.1170, 0.0410],
        [0.0853, 0.1074, 0.0024,  ..., 0.0723, 0.1170, 0.0410],
        [0.0853, 0.1073, 0.0024,  ..., 0.0722, 0.1169, 0.0410]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(638686.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8785.9561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(871.0999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(381.9785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(335.0546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5765],
        [-0.8169],
        [-0.1273],
        ...,
        [-2.8543],
        [-2.8485],
        [-2.8470]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261334.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0262],
        [1.0254],
        [1.0302],
        ...,
        [0.9984],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370504.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.0105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0255],
        [1.0302],
        ...,
        [0.9984],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370510.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.0105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2109.1084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.3566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0131, device='cuda:0')



h[100].sum tensor(-8.9181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4863, device='cuda:0')



h[200].sum tensor(82.7896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0108, 0.0000, 0.0066,  ..., 0.0119, 0.0000, 0.0258],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58256.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0803, 0.0750, 0.0138,  ..., 0.0741, 0.0827, 0.0612],
        [0.0768, 0.0485, 0.0236,  ..., 0.0794, 0.0566, 0.0780],
        [0.0707, 0.0211, 0.0417,  ..., 0.0884, 0.0267, 0.1109],
        ...,
        [0.0856, 0.1078, 0.0023,  ..., 0.0724, 0.1177, 0.0411],
        [0.0856, 0.1078, 0.0023,  ..., 0.0724, 0.1176, 0.0411],
        [0.0856, 0.1078, 0.0023,  ..., 0.0724, 0.1176, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610429.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8907.3330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(830.9577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(331.9515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(289.6204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4727],
        [-0.3487],
        [-0.2075],
        ...,
        [-2.8723],
        [-2.8665],
        [-2.8650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314044.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0255],
        [1.0302],
        ...,
        [0.9984],
        [0.9974],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370510.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.4646, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0265],
        [1.0255],
        [1.0303],
        ...,
        [0.9984],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370517.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.4646, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0181, -0.0029,  0.0111,  ...,  0.0106, -0.0038,  0.0366],
        [ 0.0110, -0.0018,  0.0067,  ...,  0.0073, -0.0024,  0.0234],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2162.5610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0139, device='cuda:0')



h[100].sum tensor(-9.3124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5148, device='cuda:0')



h[200].sum tensor(83.3513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0644, 0.0000, 0.0394,  ..., 0.0389, 0.0000, 0.1320],
        [0.0343, 0.0000, 0.0210,  ..., 0.0241, 0.0000, 0.0739],
        [0.0153, 0.0000, 0.0094,  ..., 0.0146, 0.0000, 0.0364],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58602.2852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0208, 0.0000, 0.1683,  ..., 0.1571, 0.0000, 0.3478],
        [0.0425, 0.0000, 0.1161,  ..., 0.1274, 0.0004, 0.2488],
        [0.0633, 0.0218, 0.0650,  ..., 0.0992, 0.0260, 0.1531],
        ...,
        [0.0855, 0.0920, 0.0098,  ..., 0.0737, 0.1010, 0.0540],
        [0.0855, 0.0960, 0.0082,  ..., 0.0734, 0.1052, 0.0508],
        [0.0858, 0.1041, 0.0038,  ..., 0.0728, 0.1137, 0.0444]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612066., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9017.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(837.8113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(332.4275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.0989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2556],
        [ 0.1887],
        [-0.0064],
        ...,
        [-2.3569],
        [-2.4680],
        [-2.6347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293420.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0265],
        [1.0255],
        [1.0303],
        ...,
        [0.9984],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370517.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.1918, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0256],
        [1.0303],
        ...,
        [0.9983],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370523.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.1918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051, -0.0009,  0.0031,  ...,  0.0045, -0.0013,  0.0126],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0051, -0.0009,  0.0031,  ...,  0.0045, -0.0013,  0.0126],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2485.0840, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-11.7009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6397, device='cuda:0')



h[200].sum tensor(87.3533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0000, 0.0025,  ..., 0.0087, 0.0000, 0.0131],
        [0.0186, 0.0000, 0.0114,  ..., 0.0174, 0.0000, 0.0473],
        [0.0041, 0.0000, 0.0025,  ..., 0.0087, 0.0000, 0.0131],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65476.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0785, 0.0363, 0.0361,  ..., 0.0785, 0.0407, 0.0948],
        [0.0770, 0.0109, 0.0482,  ..., 0.0815, 0.0145, 0.1138],
        [0.0807, 0.0477, 0.0295,  ..., 0.0761, 0.0540, 0.0842],
        ...,
        [0.0862, 0.1086, 0.0021,  ..., 0.0725, 0.1182, 0.0412],
        [0.0861, 0.1085, 0.0021,  ..., 0.0725, 0.1182, 0.0412],
        [0.0861, 0.1085, 0.0021,  ..., 0.0725, 0.1181, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645758.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8891.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(866.7872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(377.3346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.3551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0191],
        [-0.0716],
        [-0.3739],
        ...,
        [-2.9001],
        [-2.8942],
        [-2.8926]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290479.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0256],
        [1.0303],
        ...,
        [0.9983],
        [0.9974],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370523.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4915, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0256],
        [1.0304],
        ...,
        [0.9983],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370530.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4915, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0137, -0.0022,  0.0084,  ...,  0.0085, -0.0029,  0.0284],
        [ 0.0029, -0.0006,  0.0018,  ...,  0.0035, -0.0008,  0.0085],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2863.8105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0217, device='cuda:0')



h[100].sum tensor(-14.4643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8044, device='cuda:0')



h[200].sum tensor(92.0947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5584, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0211, 0.0000, 0.0130,  ..., 0.0185, 0.0000, 0.0518],
        [0.0186, 0.0000, 0.0114,  ..., 0.0167, 0.0000, 0.0447],
        [0.0176, 0.0000, 0.0108,  ..., 0.0169, 0.0000, 0.0453],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68349.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0631, 0.0000, 0.0850,  ..., 0.1005, 0.0000, 0.1808],
        [0.0650, 0.0000, 0.0767,  ..., 0.0980, 0.0000, 0.1680],
        [0.0631, 0.0000, 0.0828,  ..., 0.1008, 0.0000, 0.1789],
        ...,
        [0.0863, 0.1088, 0.0021,  ..., 0.0724, 0.1181, 0.0412],
        [0.0862, 0.1087, 0.0021,  ..., 0.0724, 0.1181, 0.0412],
        [0.0862, 0.1087, 0.0021,  ..., 0.0724, 0.1181, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649442.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8843.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.3033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(396.8692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(341.9717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4790],
        [ 0.4787],
        [ 0.4678],
        ...,
        [-2.9055],
        [-2.8996],
        [-2.8980]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299540.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0256],
        [1.0304],
        ...,
        [0.9983],
        [0.9973],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370530.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 510.0 event: 2550 loss: tensor(476.7822, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.4774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0256],
        [1.0304],
        ...,
        [0.9983],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370537.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.4774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2109.8132, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0130, device='cuda:0')



h[100].sum tensor(-8.7110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4793, device='cuda:0')



h[200].sum tensor(83.4762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0000, 0.0038,  ..., 0.0102, 0.0000, 0.0193],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58108.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0814, 0.0632, 0.0225,  ..., 0.0741, 0.0698, 0.0728],
        [0.0828, 0.0921, 0.0085,  ..., 0.0718, 0.0996, 0.0510],
        [0.0834, 0.1011, 0.0040,  ..., 0.0710, 0.1090, 0.0441],
        ...,
        [0.0863, 0.1089, 0.0023,  ..., 0.0724, 0.1178, 0.0413],
        [0.0863, 0.1088, 0.0023,  ..., 0.0724, 0.1177, 0.0413],
        [0.0862, 0.1088, 0.0022,  ..., 0.0723, 0.1177, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611484.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9014.4619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(839.1586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(331.2079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.0982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2797],
        [-2.5545],
        [-2.7638],
        ...,
        [-2.9053],
        [-2.8995],
        [-2.8979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303793.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0256],
        [1.0304],
        ...,
        [0.9983],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370537.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.1833, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0270],
        [1.0256],
        [1.0304],
        ...,
        [0.9983],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370545., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.1833, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0100, -0.0016,  0.0061,  ...,  0.0068, -0.0022,  0.0216],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2363.9353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0154, device='cuda:0')



h[100].sum tensor(-10.4539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5714, device='cuda:0')



h[200].sum tensor(87.0451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3411, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0144, 0.0000, 0.0089,  ..., 0.0147, 0.0000, 0.0370],
        [0.0133, 0.0000, 0.0082,  ..., 0.0136, 0.0000, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62806.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0694, 0.0096, 0.0627,  ..., 0.0907, 0.0106, 0.1410],
        [0.0701, 0.0053, 0.0539,  ..., 0.0896, 0.0089, 0.1288],
        [0.0727, 0.0134, 0.0391,  ..., 0.0856, 0.0169, 0.1059],
        ...,
        [0.0862, 0.1089, 0.0024,  ..., 0.0722, 0.1171, 0.0414],
        [0.0862, 0.1089, 0.0024,  ..., 0.0722, 0.1171, 0.0414],
        [0.0861, 0.1088, 0.0024,  ..., 0.0722, 0.1170, 0.0414]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635820.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8844.3213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(857.4638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(362.0069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.9797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3522],
        [ 0.3786],
        [ 0.3639],
        ...,
        [-2.8859],
        [-2.8797],
        [-2.8778]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303867.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0270],
        [1.0256],
        [1.0304],
        ...,
        [0.9983],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370545., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3137],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.3694, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0271],
        [1.0256],
        [1.0305],
        ...,
        [0.9982],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370551.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3137],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.3694, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0052, -0.0009,  0.0032,  ...,  0.0045, -0.0013,  0.0128],
        [ 0.0031, -0.0006,  0.0019,  ...,  0.0035, -0.0008,  0.0088],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2526.9177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-11.5465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6347, device='cuda:0')



h[200].sum tensor(89.2209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0000, 0.0073,  ..., 0.0134, 0.0000, 0.0321],
        [0.0142, 0.0000, 0.0087,  ..., 0.0145, 0.0000, 0.0366],
        [0.0310, 0.0000, 0.0190,  ..., 0.0231, 0.0000, 0.0701],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63936.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0634, 0.0084, 0.0782,  ..., 0.0986, 0.0092, 0.1688],
        [0.0603, 0.0000, 0.0899,  ..., 0.1035, 0.0007, 0.1886],
        [0.0573, 0.0000, 0.1000,  ..., 0.1080, 0.0000, 0.2058],
        ...,
        [0.0862, 0.1090, 0.0025,  ..., 0.0722, 0.1167, 0.0415],
        [0.0862, 0.1090, 0.0025,  ..., 0.0722, 0.1167, 0.0415],
        [0.0862, 0.1089, 0.0025,  ..., 0.0722, 0.1167, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635355., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8784.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(854.4876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(372.8651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.5802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2234],
        [ 0.2693],
        [ 0.2811],
        ...,
        [-2.9010],
        [-2.8954],
        [-2.8939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328743.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0271],
        [1.0256],
        [1.0305],
        ...,
        [0.9982],
        [0.9973],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370551.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2496],
        [0.5571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.6084, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0257],
        [1.0305],
        ...,
        [0.9982],
        [0.9972],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370558.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2496],
        [0.5571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.6084, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0252, -0.0038,  0.0154,  ...,  0.0139, -0.0052,  0.0497],
        [ 0.0214, -0.0033,  0.0131,  ...,  0.0121, -0.0044,  0.0426],
        [ 0.0286, -0.0043,  0.0175,  ...,  0.0155, -0.0058,  0.0560],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2421.5281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0158, device='cuda:0')



h[100].sum tensor(-10.6451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5862, device='cuda:0')



h[200].sum tensor(88.1726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0956, 0.0000, 0.0584,  ..., 0.0533, 0.0000, 0.1893],
        [0.1021, 0.0000, 0.0624,  ..., 0.0564, 0.0000, 0.2013],
        [0.1174, 0.0000, 0.0717,  ..., 0.0636, 0.0000, 0.2296],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63477.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.2810,  ..., 0.2256, 0.0000, 0.5699],
        [0.0000, 0.0000, 0.2828,  ..., 0.2268, 0.0000, 0.5738],
        [0.0000, 0.0000, 0.2799,  ..., 0.2248, 0.0000, 0.5680],
        ...,
        [0.0862, 0.1091, 0.0026,  ..., 0.0723, 0.1163, 0.0417],
        [0.0862, 0.1091, 0.0026,  ..., 0.0722, 0.1163, 0.0417],
        [0.0862, 0.1090, 0.0026,  ..., 0.0722, 0.1163, 0.0417]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637047., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8814.8584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(857.4389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(366.6002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.2844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1681],
        [ 0.1656],
        [ 0.1697],
        ...,
        [-2.8992],
        [-2.8937],
        [-2.8922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300742.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0257],
        [1.0305],
        ...,
        [0.9982],
        [0.9972],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370558.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.3571, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0257],
        [1.0305],
        ...,
        [0.9982],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370565.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.3571, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2522.0374, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-11.2826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6265, device='cuda:0')



h[200].sum tensor(89.5351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3384, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65218.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0833, 0.1065, 0.0022,  ..., 0.0703, 0.1123, 0.0404],
        [0.0835, 0.1068, 0.0022,  ..., 0.0705, 0.1126, 0.0405],
        [0.0837, 0.1067, 0.0024,  ..., 0.0705, 0.1127, 0.0406],
        ...,
        [0.0863, 0.1093, 0.0026,  ..., 0.0723, 0.1160, 0.0419],
        [0.0863, 0.1092, 0.0026,  ..., 0.0723, 0.1160, 0.0419],
        [0.0862, 0.1092, 0.0026,  ..., 0.0723, 0.1159, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646587.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8765.3213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.2484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(376.5774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.9218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9872],
        [-2.9892],
        [-2.9737],
        ...,
        [-2.8990],
        [-2.8936],
        [-2.8923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281186.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0257],
        [1.0305],
        ...,
        [0.9982],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370565.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.3033, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0258],
        [1.0306],
        ...,
        [0.9982],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370571.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.3033, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0046, -0.0008,  0.0028,  ...,  0.0042, -0.0011,  0.0117],
        [ 0.0101, -0.0016,  0.0062,  ...,  0.0068, -0.0022,  0.0217],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2309.3008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0146, device='cuda:0')



h[100].sum tensor(-9.6200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5390, device='cuda:0')



h[200].sum tensor(87.1205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0029,  ..., 0.0088, 0.0000, 0.0144],
        [0.0182, 0.0000, 0.0111,  ..., 0.0157, 0.0000, 0.0417],
        [0.0348, 0.0000, 0.0213,  ..., 0.0247, 0.0000, 0.0771],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59478.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0763, 0.0458, 0.0292,  ..., 0.0804, 0.0506, 0.0870],
        [0.0672, 0.0153, 0.0608,  ..., 0.0935, 0.0181, 0.1408],
        [0.0601, 0.0000, 0.0881,  ..., 0.1038, 0.0000, 0.1863],
        ...,
        [0.0864, 0.1095, 0.0027,  ..., 0.0724, 0.1158, 0.0421],
        [0.0864, 0.1094, 0.0027,  ..., 0.0724, 0.1158, 0.0420],
        [0.0864, 0.1094, 0.0027,  ..., 0.0723, 0.1158, 0.0420]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613804.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8898.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(839.6638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(341.1511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.2803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9080],
        [-0.2096],
        [ 0.2146],
        ...,
        [-2.9022],
        [-2.8967],
        [-2.8953]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295056.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0258],
        [1.0306],
        ...,
        [0.9982],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370571.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.8446, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0258],
        [1.0307],
        ...,
        [0.9981],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370578.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.8446, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0131, -0.0020,  0.0080,  ...,  0.0082, -0.0028,  0.0273],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2902.6228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-13.9214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7726, device='cuda:0')



h[200].sum tensor(94.2048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0737, 0.0000, 0.0450,  ..., 0.0424, 0.0000, 0.1465],
        [0.0328, 0.0000, 0.0201,  ..., 0.0226, 0.0000, 0.0686],
        [0.0091, 0.0000, 0.0056,  ..., 0.0115, 0.0000, 0.0249],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69899.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0202, 0.0000, 0.1670,  ..., 0.1551, 0.0000, 0.3448],
        [0.0428, 0.0000, 0.1134,  ..., 0.1244, 0.0000, 0.2422],
        [0.0600, 0.0000, 0.0802,  ..., 0.1030, 0.0000, 0.1760],
        ...,
        [0.0865, 0.1096, 0.0026,  ..., 0.0724, 0.1158, 0.0422],
        [0.0865, 0.1096, 0.0027,  ..., 0.0724, 0.1158, 0.0422],
        [0.0865, 0.1096, 0.0026,  ..., 0.0724, 0.1157, 0.0422]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659605.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8783.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(888.9767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(403.0785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(352.0785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3656],
        [ 0.4022],
        [ 0.4351],
        ...,
        [-2.8991],
        [-2.8731],
        [-2.7922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249837., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0258],
        [1.0307],
        ...,
        [0.9981],
        [0.9972],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370578.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.5667, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0259],
        [1.0307],
        ...,
        [0.9981],
        [0.9971],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370584.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.5667, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2248.4663, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0137, device='cuda:0')



h[100].sum tensor(-9.0455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5069, device='cuda:0')



h[200].sum tensor(86.4386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59967.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0837, 0.1072, 0.0022,  ..., 0.0706, 0.1123, 0.0409],
        [0.0840, 0.1074, 0.0022,  ..., 0.0707, 0.1126, 0.0410],
        [0.0841, 0.1074, 0.0023,  ..., 0.0708, 0.1127, 0.0411],
        ...,
        [0.0868, 0.1099, 0.0026,  ..., 0.0726, 0.1160, 0.0424],
        [0.0867, 0.1099, 0.0026,  ..., 0.0726, 0.1159, 0.0423],
        [0.0867, 0.1099, 0.0026,  ..., 0.0725, 0.1159, 0.0423]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620394.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8953.9463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(848.6406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(341.2144, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.6714, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9996],
        [-2.9776],
        [-2.9224],
        ...,
        [-2.9190],
        [-2.9135],
        [-2.9121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274263.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0259],
        [1.0307],
        ...,
        [0.9981],
        [0.9971],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370584.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.4700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0260],
        [1.0308],
        ...,
        [0.9981],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370590.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.4700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0050, -0.0009,  0.0031,  ...,  0.0044, -0.0012,  0.0123],
        [ 0.0033, -0.0006,  0.0020,  ...,  0.0036, -0.0009,  0.0092],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2928.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0213, device='cuda:0')



h[100].sum tensor(-14.0349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7880, device='cuda:0')



h[200].sum tensor(94.0334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2607, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0000, 0.0032,  ..., 0.0090, 0.0000, 0.0150],
        [0.0111, 0.0000, 0.0069,  ..., 0.0124, 0.0000, 0.0286],
        [0.0311, 0.0000, 0.0191,  ..., 0.0230, 0.0000, 0.0702],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72473.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0788, 0.0482, 0.0277,  ..., 0.0789, 0.0523, 0.0845],
        [0.0734, 0.0222, 0.0507,  ..., 0.0874, 0.0247, 0.1224],
        [0.0667, 0.0000, 0.0759,  ..., 0.0972, 0.0000, 0.1642],
        ...,
        [0.0872, 0.1104, 0.0024,  ..., 0.0729, 0.1165, 0.0425],
        [0.0872, 0.1104, 0.0024,  ..., 0.0728, 0.1164, 0.0425],
        [0.0871, 0.1103, 0.0024,  ..., 0.0728, 0.1164, 0.0425]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687191.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8894.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(907.8864, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(415.4949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(366.6423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2979],
        [ 0.0553],
        [ 0.2401],
        ...,
        [-2.8802],
        [-2.8696],
        [-2.8615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230443.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0260],
        [1.0308],
        ...,
        [0.9981],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370590.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(268.2554, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0260],
        [1.0308],
        ...,
        [0.9981],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370597.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(268.2554, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107, -0.0017,  0.0066,  ...,  0.0071, -0.0023,  0.0229],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2794.5068, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0198, device='cuda:0')



h[100].sum tensor(-13.0733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7328, device='cuda:0')



h[200].sum tensor(91.7577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0087, 0.0000, 0.0054,  ..., 0.0107, 0.0000, 0.0217],
        [0.0110, 0.0000, 0.0068,  ..., 0.0118, 0.0000, 0.0259],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66897.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0662, 0.0227, 0.0573,  ..., 0.0971, 0.0257, 0.1419],
        [0.0753, 0.0449, 0.0313,  ..., 0.0851, 0.0502, 0.0955],
        [0.0827, 0.0867, 0.0095,  ..., 0.0748, 0.0926, 0.0555],
        ...,
        [0.0878, 0.1109, 0.0023,  ..., 0.0732, 0.1173, 0.0427],
        [0.0878, 0.1109, 0.0023,  ..., 0.0732, 0.1173, 0.0427],
        [0.0877, 0.1109, 0.0023,  ..., 0.0732, 0.1172, 0.0427]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645010., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9077.3564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.4479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(384.8171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(336.2685, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0327],
        [-0.2648],
        [-0.5809],
        ...,
        [-2.9616],
        [-2.9558],
        [-2.9543]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272131.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0260],
        [1.0308],
        ...,
        [0.9981],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370597.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 520.0 event: 2600 loss: tensor(474.1622, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1527, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0261],
        [1.0309],
        ...,
        [0.9981],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370603.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1527, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0008,  0.0025,  ...,  0.0040, -0.0010,  0.0106],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2244.9065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-9.0804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5112, device='cuda:0')



h[200].sum tensor(84.4347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2520, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0209, 0.0000, 0.0128,  ..., 0.0170, 0.0000, 0.0465],
        [0.0042, 0.0000, 0.0026,  ..., 0.0086, 0.0000, 0.0133],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59272.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0707, 0.0161, 0.0584,  ..., 0.0938, 0.0191, 0.1403],
        [0.0798, 0.0478, 0.0277,  ..., 0.0808, 0.0527, 0.0869],
        [0.0840, 0.0887, 0.0097,  ..., 0.0744, 0.0944, 0.0556],
        ...,
        [0.0884, 0.1115, 0.0021,  ..., 0.0736, 0.1181, 0.0429],
        [0.0884, 0.1115, 0.0021,  ..., 0.0736, 0.1181, 0.0429],
        [0.0883, 0.1114, 0.0021,  ..., 0.0736, 0.1180, 0.0429]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618010.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9280.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(845.2180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(339.9890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(295.8631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2292],
        [-0.9891],
        [-1.7596],
        ...,
        [-2.9820],
        [-2.9763],
        [-2.9749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303132.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0261],
        [1.0309],
        ...,
        [0.9981],
        [0.9971],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370603.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0139, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0261],
        [1.0309],
        ...,
        [0.9981],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370609.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0139, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089, -0.0014,  0.0055,  ...,  0.0063, -0.0019,  0.0196],
        [ 0.0089, -0.0014,  0.0055,  ...,  0.0063, -0.0019,  0.0196],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2334.2957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-9.7375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5464, device='cuda:0')



h[200].sum tensor(84.7168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8878, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0250, 0.0000, 0.0154,  ..., 0.0190, 0.0000, 0.0542],
        [0.0229, 0.0000, 0.0141,  ..., 0.0180, 0.0000, 0.0502],
        [0.0280, 0.0000, 0.0173,  ..., 0.0210, 0.0000, 0.0621],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61349.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0000, 0.1051,  ..., 0.1206, 0.0000, 0.2287],
        [0.0585, 0.0000, 0.0938,  ..., 0.1137, 0.0000, 0.2068],
        [0.0620, 0.0000, 0.0905,  ..., 0.1094, 0.0000, 0.1976],
        ...,
        [0.0889, 0.1120, 0.0020,  ..., 0.0740, 0.1187, 0.0433],
        [0.0889, 0.1119, 0.0020,  ..., 0.0740, 0.1186, 0.0432],
        [0.0889, 0.1119, 0.0020,  ..., 0.0739, 0.1186, 0.0432]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629461.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9369.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(853.9560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.7401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(304.5636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4097],
        [ 0.4298],
        [ 0.4531],
        ...,
        [-2.9992],
        [-2.9932],
        [-2.9917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298738.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0261],
        [1.0309],
        ...,
        [0.9981],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370609.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.8723, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0309],
        ...,
        [0.9981],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370615.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.8723, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2452.3403, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-10.5737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5951, device='cuda:0')



h[200].sum tensor(85.5521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7707, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62840.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0854, 0.0969, 0.0064,  ..., 0.0737, 0.1027, 0.0509],
        [0.0862, 0.1073, 0.0018,  ..., 0.0727, 0.1132, 0.0438],
        [0.0866, 0.1097, 0.0017,  ..., 0.0724, 0.1157, 0.0422],
        ...,
        [0.0893, 0.1123, 0.0020,  ..., 0.0743, 0.1191, 0.0435],
        [0.0893, 0.1123, 0.0020,  ..., 0.0743, 0.1191, 0.0435],
        [0.0893, 0.1122, 0.0020,  ..., 0.0742, 0.1190, 0.0435]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637163.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9378.2354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(855.8740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(364.8541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(309.8980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0403],
        [-2.4455],
        [-2.6804],
        ...,
        [-2.9989],
        [-2.9902],
        [-2.9708]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310268.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0309],
        ...,
        [0.9981],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370615.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4536],
        [0.4971],
        [0.4702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.4404, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370621.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4536],
        [0.4971],
        [0.4702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.4404, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0292, -0.0043,  0.0179,  ...,  0.0158, -0.0058,  0.0571],
        [ 0.0178, -0.0027,  0.0110,  ...,  0.0104, -0.0036,  0.0361],
        [ 0.0275, -0.0040,  0.0168,  ...,  0.0150, -0.0054,  0.0539],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2332.4858, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-9.6580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5448, device='cuda:0')



h[200].sum tensor(83.6934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1275, 0.0000, 0.0781,  ..., 0.0683, 0.0000, 0.2483],
        [0.1302, 0.0000, 0.0798,  ..., 0.0696, 0.0000, 0.2533],
        [0.1258, 0.0000, 0.0771,  ..., 0.0676, 0.0000, 0.2451],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60986.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.3590,  ..., 0.2817, 0.0000, 0.7423],
        [0.0000, 0.0000, 0.3520,  ..., 0.2772, 0.0000, 0.7276],
        [0.0000, 0.0000, 0.3398,  ..., 0.2688, 0.0000, 0.7015],
        ...,
        [0.0897, 0.1125, 0.0019,  ..., 0.0746, 0.1194, 0.0438],
        [0.0896, 0.1125, 0.0019,  ..., 0.0745, 0.1194, 0.0438],
        [0.0896, 0.1125, 0.0019,  ..., 0.0745, 0.1194, 0.0438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627377.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9455.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.3607, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(355.5214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.6024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1276],
        [ 0.1223],
        [ 0.1223],
        ...,
        [-3.0068],
        [-3.0121],
        [-3.0127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329379.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9971],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370621.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.4639, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370628.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.4639, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2586.8931, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-11.4221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6377, device='cuda:0')



h[200].sum tensor(86.7403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64324.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0866, 0.1098, 0.0015,  ..., 0.0726, 0.1156, 0.0425],
        [0.0867, 0.1074, 0.0020,  ..., 0.0730, 0.1131, 0.0446],
        [0.0865, 0.0970, 0.0072,  ..., 0.0739, 0.1026, 0.0524],
        ...,
        [0.0898, 0.1126, 0.0019,  ..., 0.0747, 0.1195, 0.0440],
        [0.0898, 0.1126, 0.0019,  ..., 0.0747, 0.1194, 0.0440],
        [0.0898, 0.1126, 0.0019,  ..., 0.0746, 0.1194, 0.0440]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641198., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9476.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(860.6694, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(373.4236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.5414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7047],
        [-2.3326],
        [-1.7855],
        ...,
        [-3.0265],
        [-3.0204],
        [-3.0171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308807.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370628.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(319.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370635.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(319.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3158.3364, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0236, device='cuda:0')



h[100].sum tensor(-15.4575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8723, device='cuda:0')



h[200].sum tensor(93.5220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7868, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72102.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0867, 0.1098, 0.0016,  ..., 0.0726, 0.1154, 0.0427],
        [0.0870, 0.1101, 0.0016,  ..., 0.0728, 0.1157, 0.0428],
        [0.0871, 0.1100, 0.0017,  ..., 0.0728, 0.1159, 0.0429],
        ...,
        [0.0899, 0.1127, 0.0020,  ..., 0.0747, 0.1193, 0.0442],
        [0.0899, 0.1127, 0.0020,  ..., 0.0747, 0.1193, 0.0442],
        [0.0899, 0.1126, 0.0020,  ..., 0.0747, 0.1193, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670211.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9357.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(885.9888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(423.5374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(349.9337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9519],
        [-3.0594],
        [-3.1415],
        ...,
        [-3.0275],
        [-3.0215],
        [-3.0201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306289.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370635.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370642.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2373.6072, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-9.6594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5465, device='cuda:0')



h[200].sum tensor(84.9370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61382.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0867, 0.1098, 0.0016,  ..., 0.0725, 0.1151, 0.0427],
        [0.0869, 0.1100, 0.0016,  ..., 0.0727, 0.1154, 0.0428],
        [0.0871, 0.1100, 0.0018,  ..., 0.0728, 0.1156, 0.0429],
        ...,
        [0.0899, 0.1126, 0.0020,  ..., 0.0747, 0.1190, 0.0442],
        [0.0899, 0.1126, 0.0020,  ..., 0.0746, 0.1190, 0.0442],
        [0.0898, 0.1126, 0.0020,  ..., 0.0746, 0.1189, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629919.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9554.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(854.0813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(350.5510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(301.0006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1219],
        [-3.0682],
        [-2.9650],
        ...,
        [-3.0277],
        [-3.0218],
        [-3.0204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275958.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370642.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.3467, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370649.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.3467, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0044, -0.0008,  0.0028,  ...,  0.0041, -0.0011,  0.0113],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2336.1912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-9.2777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5281, device='cuda:0')



h[200].sum tensor(85.3562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0028,  ..., 0.0088, 0.0000, 0.0139],
        [0.0070, 0.0000, 0.0043,  ..., 0.0100, 0.0000, 0.0184],
        [0.0230, 0.0000, 0.0144,  ..., 0.0193, 0.0000, 0.0552],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59742.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0839, 0.0588, 0.0232,  ..., 0.0778, 0.0637, 0.0793],
        [0.0819, 0.0393, 0.0351,  ..., 0.0822, 0.0425, 0.0990],
        [0.0787, 0.0053, 0.0540,  ..., 0.0882, 0.0069, 0.1292],
        ...,
        [0.0898, 0.1126, 0.0019,  ..., 0.0745, 0.1188, 0.0440],
        [0.0897, 0.1125, 0.0019,  ..., 0.0745, 0.1188, 0.0440],
        [0.0897, 0.1125, 0.0019,  ..., 0.0744, 0.1188, 0.0440]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619263.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9530.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(851.7734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(339.4149, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.7005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9159],
        [-0.4853],
        [-0.2495],
        ...,
        [-3.0344],
        [-3.0285],
        [-3.0271]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288953.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0261],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370649.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.5800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0260],
        [1.0311],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370657.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.5800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [ 0.0137, -0.0020,  0.0085,  ...,  0.0085, -0.0028,  0.0284],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2565.5400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-10.8141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6107, device='cuda:0')



h[200].sum tensor(88.8436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0529, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0031],
        [0.0165, 0.0000, 0.0103,  ..., 0.0151, 0.0000, 0.0383],
        [0.0266, 0.0000, 0.0165,  ..., 0.0204, 0.0000, 0.0593],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64588.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0824, 0.0643, 0.0183,  ..., 0.0789, 0.0707, 0.0725],
        [0.0729, 0.0268, 0.0533,  ..., 0.0943, 0.0301, 0.1357],
        [0.0645, 0.0035, 0.0851,  ..., 0.1079, 0.0047, 0.1911],
        ...,
        [0.0896, 0.1125, 0.0019,  ..., 0.0744, 0.1188, 0.0438],
        [0.0896, 0.1124, 0.0019,  ..., 0.0744, 0.1188, 0.0437],
        [0.0896, 0.1124, 0.0019,  ..., 0.0743, 0.1188, 0.0437]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644185.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9417.3467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(880.6008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(367.9493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.8807, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5003],
        [-0.6771],
        [-0.0171],
        ...,
        [-3.0385],
        [-3.0335],
        [-3.0326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268201.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0260],
        [1.0311],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370657.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(275.6162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0260],
        [1.0311],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370664.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(275.6162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028, -0.0006,  0.0018,  ...,  0.0034, -0.0007,  0.0082],
        [ 0.0028, -0.0006,  0.0018,  ...,  0.0034, -0.0007,  0.0082],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2932.2253, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0203, device='cuda:0')



h[100].sum tensor(-13.3154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7529, device='cuda:0')



h[200].sum tensor(93.6912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0000, 0.0121,  ..., 0.0170, 0.0000, 0.0457],
        [0.0050, 0.0000, 0.0033,  ..., 0.0097, 0.0000, 0.0170],
        [0.0050, 0.0000, 0.0033,  ..., 0.0097, 0.0000, 0.0170],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69762.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0751, 0.0065, 0.0655,  ..., 0.0926, 0.0072, 0.1497],
        [0.0827, 0.0329, 0.0386,  ..., 0.0811, 0.0351, 0.1033],
        [0.0855, 0.0617, 0.0237,  ..., 0.0763, 0.0667, 0.0789],
        ...,
        [0.0895, 0.1124, 0.0018,  ..., 0.0742, 0.1186, 0.0437],
        [0.0895, 0.1124, 0.0018,  ..., 0.0742, 0.1186, 0.0437],
        [0.0895, 0.1124, 0.0018,  ..., 0.0742, 0.1186, 0.0437]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665186.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9231.1504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(899.2136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(404.5321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(355.2436, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0871],
        [-0.4280],
        [-1.0443],
        ...,
        [-3.0447],
        [-3.0388],
        [-3.0375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300384.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0260],
        [1.0311],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370664.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 530.0 event: 2650 loss: tensor(498.4842, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.7788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0311],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370671.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.7788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2483.8311, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0152, device='cuda:0')



h[100].sum tensor(-9.9685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5621, device='cuda:0')



h[200].sum tensor(88.9354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61767.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0854, 0.0972, 0.0059,  ..., 0.0737, 0.1024, 0.0505],
        [0.0863, 0.1069, 0.0018,  ..., 0.0726, 0.1118, 0.0442],
        [0.0867, 0.1097, 0.0017,  ..., 0.0723, 0.1147, 0.0424],
        ...,
        [0.0894, 0.1123, 0.0020,  ..., 0.0741, 0.1181, 0.0437],
        [0.0894, 0.1123, 0.0020,  ..., 0.0741, 0.1180, 0.0437],
        [0.0894, 0.1122, 0.0020,  ..., 0.0741, 0.1180, 0.0437]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629054.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9363.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.3617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.1444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.8390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7423],
        [-2.9211],
        [-2.9531],
        ...,
        [-3.0291],
        [-3.0243],
        [-3.0248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290699.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0311],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370671.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2678],
        [0.2859],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(449.4545, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370678.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2678],
        [0.2859],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(449.4545, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094, -0.0014,  0.0058,  ...,  0.0065, -0.0019,  0.0204],
        [ 0.0137, -0.0020,  0.0085,  ...,  0.0085, -0.0027,  0.0284],
        [ 0.0094, -0.0014,  0.0058,  ...,  0.0065, -0.0020,  0.0204],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4150.2754, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0332, device='cuda:0')



h[100].sum tensor(-21.7299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.2277, device='cuda:0')



h[200].sum tensor(108.3615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0652, 0.0000, 0.0402,  ..., 0.0391, 0.0000, 0.1329],
        [0.0556, 0.0000, 0.0343,  ..., 0.0347, 0.0000, 0.1152],
        [0.0305, 0.0000, 0.0189,  ..., 0.0223, 0.0000, 0.0665],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89240.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0335, 0.0000, 0.1650,  ..., 0.1516, 0.0000, 0.3393],
        [0.0410, 0.0000, 0.1450,  ..., 0.1411, 0.0000, 0.3023],
        [0.0575, 0.0012, 0.1000,  ..., 0.1171, 0.0027, 0.2191],
        ...,
        [0.0895, 0.1121, 0.0022,  ..., 0.0743, 0.1174, 0.0439],
        [0.0895, 0.1121, 0.0022,  ..., 0.0742, 0.1173, 0.0439],
        [0.0895, 0.1121, 0.0022,  ..., 0.0742, 0.1173, 0.0438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(756710., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8908.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(973.1359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(526.3402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(450.3475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4170],
        [ 0.3757],
        [ 0.2043],
        ...,
        [-3.0287],
        [-3.0231],
        [-3.0219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270057.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9980],
        [0.9970],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370678.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.1211, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370684.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.1211, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2395.4590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0140, device='cuda:0')



h[100].sum tensor(-9.1205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5166, device='cuda:0')



h[200].sum tensor(87.5881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3494, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61721.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0867, 0.1072, 0.0028,  ..., 0.0724, 0.1108, 0.0444],
        [0.0874, 0.1032, 0.0052,  ..., 0.0724, 0.1064, 0.0484],
        [0.0877, 0.1010, 0.0064,  ..., 0.0723, 0.1040, 0.0505],
        ...,
        [0.0897, 0.1120, 0.0023,  ..., 0.0746, 0.1168, 0.0442],
        [0.0897, 0.1120, 0.0023,  ..., 0.0746, 0.1167, 0.0442],
        [0.0897, 0.1119, 0.0023,  ..., 0.0746, 0.1167, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632169.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9352.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(858.2061, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(350.6404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(308.3754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5147],
        [-2.3512],
        [-2.2373],
        ...,
        [-3.0150],
        [-3.0095],
        [-3.0083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266824.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370684.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(252.9172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370690.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(252.9172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2831.3574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0187, device='cuda:0')



h[100].sum tensor(-12.1759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6909, device='cuda:0')



h[200].sum tensor(91.9758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67885.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0872, 0.1062, 0.0040,  ..., 0.0728, 0.1095, 0.0457],
        [0.0875, 0.1056, 0.0046,  ..., 0.0729, 0.1088, 0.0467],
        [0.0877, 0.1049, 0.0052,  ..., 0.0729, 0.1081, 0.0474],
        ...,
        [0.0901, 0.1120, 0.0025,  ..., 0.0751, 0.1166, 0.0446],
        [0.0901, 0.1120, 0.0025,  ..., 0.0751, 0.1166, 0.0445],
        [0.0900, 0.1119, 0.0025,  ..., 0.0751, 0.1165, 0.0445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656041.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9360.9092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.4431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.1794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.2809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0158],
        [-2.0811],
        [-2.0750],
        ...,
        [-3.0097],
        [-3.0042],
        [-3.0031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249225.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370690.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6962, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370696.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6962, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0152, -0.0022,  0.0094,  ...,  0.0092, -0.0030,  0.0313],
        [ 0.0152, -0.0022,  0.0093,  ...,  0.0092, -0.0030,  0.0313],
        [ 0.0310, -0.0043,  0.0190,  ...,  0.0167, -0.0059,  0.0605],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2929.5891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0199, device='cuda:0')



h[100].sum tensor(-12.8445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7367, device='cuda:0')



h[200].sum tensor(92.7805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0573, 0.0000, 0.0353,  ..., 0.0350, 0.0000, 0.1168],
        [0.0859, 0.0000, 0.0528,  ..., 0.0490, 0.0000, 0.1719],
        [0.0552, 0.0000, 0.0340,  ..., 0.0346, 0.0000, 0.1153],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69460.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0359, 0.0000, 0.1493,  ..., 0.1501, 0.0000, 0.3174],
        [0.0222, 0.0000, 0.1866,  ..., 0.1705, 0.0000, 0.3880],
        [0.0334, 0.0000, 0.1608,  ..., 0.1552, 0.0000, 0.3370],
        ...,
        [0.0903, 0.1121, 0.0025,  ..., 0.0755, 0.1167, 0.0447],
        [0.0902, 0.1121, 0.0025,  ..., 0.0754, 0.1167, 0.0447],
        [0.0902, 0.1120, 0.0025,  ..., 0.0754, 0.1166, 0.0447]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665987.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9370.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(878.9627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(397.7004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(338.1921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4328],
        [ 0.4646],
        [ 0.4842],
        ...,
        [-3.0139],
        [-3.0085],
        [-3.0074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252832.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370696.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(173.0450, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0309],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370702.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.0450, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2276.5718, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.9001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0128, device='cuda:0')



h[100].sum tensor(-8.2431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4727, device='cuda:0')



h[200].sum tensor(84.8280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0020, 0.0000, 0.0013,  ..., 0.0079, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59501.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0871, 0.0939, 0.0090,  ..., 0.0745, 0.0975, 0.0553],
        [0.0873, 0.0864, 0.0137,  ..., 0.0752, 0.0899, 0.0615],
        [0.0873, 0.0707, 0.0213,  ..., 0.0763, 0.0744, 0.0737],
        ...,
        [0.0902, 0.1097, 0.0031,  ..., 0.0762, 0.1148, 0.0465],
        [0.0904, 0.1122, 0.0025,  ..., 0.0758, 0.1172, 0.0448],
        [0.0903, 0.1122, 0.0025,  ..., 0.0758, 0.1172, 0.0447]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624065.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9473.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(837.6217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(336.7201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.2615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5381],
        [-0.5773],
        [-0.5559],
        ...,
        [-2.2729],
        [-2.7442],
        [-2.9445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276999.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0309],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370702.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(167.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370707.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(167.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2231.9600, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.3922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0124, device='cuda:0')



h[100].sum tensor(-7.9842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4578, device='cuda:0')



h[200].sum tensor(83.9742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2843, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57628.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0873, 0.1096, 0.0019,  ..., 0.0739, 0.1143, 0.0432],
        [0.0876, 0.1099, 0.0019,  ..., 0.0741, 0.1146, 0.0433],
        [0.0877, 0.1099, 0.0020,  ..., 0.0742, 0.1147, 0.0434],
        ...,
        [0.0906, 0.1125, 0.0023,  ..., 0.0761, 0.1181, 0.0448],
        [0.0906, 0.1124, 0.0023,  ..., 0.0761, 0.1181, 0.0448],
        [0.0905, 0.1124, 0.0023,  ..., 0.0761, 0.1180, 0.0447]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613032.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9513.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(830.2781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(328.4917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(279.8884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0284],
        [-2.9209],
        [-2.6990],
        ...,
        [-3.0493],
        [-3.0437],
        [-3.0425]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294997.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370707.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370713.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2735.6375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0180, device='cuda:0')



h[100].sum tensor(-11.5768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6659, device='cuda:0')



h[200].sum tensor(89.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        ...,
        [0.0040, 0.0000, 0.0025,  ..., 0.0091, 0.0000, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66421.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0870, 0.1011, 0.0051,  ..., 0.0750, 0.1066, 0.0494],
        [0.0876, 0.1081, 0.0019,  ..., 0.0745, 0.1135, 0.0447],
        [0.0879, 0.1101, 0.0019,  ..., 0.0743, 0.1156, 0.0433],
        ...,
        [0.0884, 0.0736, 0.0185,  ..., 0.0810, 0.0811, 0.0722],
        [0.0901, 0.1014, 0.0068,  ..., 0.0775, 0.1080, 0.0528],
        [0.0905, 0.1105, 0.0025,  ..., 0.0765, 0.1169, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652007.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9411.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(873.5128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(383.5263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.8653, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0005],
        [-2.3912],
        [-2.5480],
        ...,
        [-2.2801],
        [-2.6658],
        [-2.8991]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269540.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0260],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370713.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(176.8605, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0261],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370719.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(176.8605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044, -0.0007,  0.0028,  ...,  0.0042, -0.0010,  0.0114],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2275.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0131, device='cuda:0')



h[100].sum tensor(-8.3645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4831, device='cuda:0')



h[200].sum tensor(84.2965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7432, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0101, 0.0000, 0.0064,  ..., 0.0124, 0.0000, 0.0271],
        [0.0045, 0.0000, 0.0029,  ..., 0.0092, 0.0000, 0.0144],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58396.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0709, 0.0236, 0.0615,  ..., 0.1006, 0.0278, 0.1504],
        [0.0804, 0.0520, 0.0290,  ..., 0.0863, 0.0586, 0.0931],
        [0.0867, 0.0951, 0.0073,  ..., 0.0765, 0.1017, 0.0535],
        ...,
        [0.0907, 0.1130, 0.0020,  ..., 0.0764, 0.1198, 0.0445],
        [0.0907, 0.1129, 0.0020,  ..., 0.0763, 0.1197, 0.0445],
        [0.0906, 0.1129, 0.0020,  ..., 0.0763, 0.1197, 0.0445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617983.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9454.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(838.4601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(338.6841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.8175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1895],
        [-0.9645],
        [-1.8004],
        ...,
        [-3.0938],
        [-3.0880],
        [-3.0868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329733.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0261],
        [1.0310],
        ...,
        [0.9979],
        [0.9970],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370719.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.7156, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0261],
        [1.0310],
        ...,
        [0.9979],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370726.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.7156, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3038.2822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0214, device='cuda:0')



h[100].sum tensor(-13.6806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7914, device='cuda:0')



h[200].sum tensor(93.5663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0030,  ..., 0.0093, 0.0000, 0.0147],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70636.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0836, 0.0636, 0.0195,  ..., 0.0806, 0.0712, 0.0744],
        [0.0868, 0.0979, 0.0064,  ..., 0.0758, 0.1041, 0.0518],
        [0.0872, 0.1041, 0.0027,  ..., 0.0752, 0.1102, 0.0473],
        ...,
        [0.0906, 0.1132, 0.0020,  ..., 0.0763, 0.1198, 0.0444],
        [0.0906, 0.1131, 0.0020,  ..., 0.0762, 0.1198, 0.0444],
        [0.0905, 0.1131, 0.0020,  ..., 0.0762, 0.1197, 0.0444]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675395.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9284.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(898.5760, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.3285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(354.6058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8544],
        [-2.2073],
        [-2.1965],
        ...,
        [-3.1008],
        [-3.0949],
        [-3.0937]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267836.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0261],
        [1.0310],
        ...,
        [0.9979],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370726.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 540.0 event: 2700 loss: tensor(500.3299, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.9058, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0311],
        ...,
        [0.9979],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370732.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.9058, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2931.5710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0199, device='cuda:0')



h[100].sum tensor(-12.8423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7373, device='cuda:0')



h[200].sum tensor(93.1340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68951.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0871, 0.1104, 0.0018,  ..., 0.0738, 0.1154, 0.0428],
        [0.0874, 0.1107, 0.0018,  ..., 0.0741, 0.1158, 0.0430],
        [0.0875, 0.1106, 0.0019,  ..., 0.0741, 0.1159, 0.0430],
        ...,
        [0.0903, 0.1133, 0.0022,  ..., 0.0760, 0.1193, 0.0444],
        [0.0903, 0.1132, 0.0022,  ..., 0.0760, 0.1193, 0.0444],
        [0.0903, 0.1132, 0.0022,  ..., 0.0760, 0.1192, 0.0444]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664982.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9211.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.9268, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(403.1244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.0092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0976],
        [-3.1610],
        [-3.1489],
        ...,
        [-3.0953],
        [-3.0895],
        [-3.0883]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295042.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0261],
        [1.0311],
        ...,
        [0.9979],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370732.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2742]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0262],
        [1.0311],
        ...,
        [0.9979],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370739.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2742]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0016,  0.0000,  0.0009],
        [ 0.0105, -0.0015,  0.0065,  ...,  0.0071, -0.0021,  0.0226],
        [ 0.0048, -0.0008,  0.0031,  ...,  0.0044, -0.0011,  0.0122]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2514.9238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0153, device='cuda:0')



h[100].sum tensor(-9.8329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5677, device='cuda:0')



h[200].sum tensor(89.0608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        ...,
        [0.0151, 0.0000, 0.0094,  ..., 0.0149, 0.0000, 0.0366],
        [0.0172, 0.0000, 0.0108,  ..., 0.0165, 0.0000, 0.0429],
        [0.0373, 0.0000, 0.0232,  ..., 0.0266, 0.0000, 0.0826]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62561.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0870, 0.1105, 0.0020,  ..., 0.0737, 0.1148, 0.0429],
        [0.0872, 0.1108, 0.0020,  ..., 0.0739, 0.1151, 0.0430],
        [0.0874, 0.1107, 0.0022,  ..., 0.0739, 0.1152, 0.0431],
        ...,
        [0.0756, 0.0233, 0.0571,  ..., 0.0982, 0.0277, 0.1382],
        [0.0711, 0.0065, 0.0785,  ..., 0.1054, 0.0080, 0.1729],
        [0.0655, 0.0000, 0.0952,  ..., 0.1135, 0.0000, 0.2025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637343.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9242., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(861.6552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(361.0757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.7205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6511],
        [-2.7707],
        [-2.7974],
        ...,
        [-0.5390],
        [ 0.1373],
        [ 0.4902]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291869.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0262],
        [1.0311],
        ...,
        [0.9979],
        [0.9969],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370739.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.9797, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0262],
        [1.0311],
        ...,
        [0.9979],
        [0.9969],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370745.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.9797, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0079, -0.0012,  0.0049,  ...,  0.0059, -0.0016,  0.0179],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0079, -0.0012,  0.0049,  ...,  0.0059, -0.0016,  0.0179],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3083.2397, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0213, device='cuda:0')



h[100].sum tensor(-13.6853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7894, device='cuda:0')



h[200].sum tensor(96.0643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2859, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0000, 0.0040,  ..., 0.0099, 0.0000, 0.0180],
        [0.0341, 0.0000, 0.0212,  ..., 0.0247, 0.0000, 0.0762],
        [0.0148, 0.0000, 0.0092,  ..., 0.0145, 0.0000, 0.0359],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72286.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0778, 0.0306, 0.0394,  ..., 0.0875, 0.0336, 0.1062],
        [0.0685, 0.0000, 0.0748,  ..., 0.1021, 0.0000, 0.1659],
        [0.0678, 0.0000, 0.0774,  ..., 0.1031, 0.0000, 0.1702],
        ...,
        [0.0901, 0.1135, 0.0026,  ..., 0.0757, 0.1179, 0.0447],
        [0.0900, 0.1135, 0.0026,  ..., 0.0757, 0.1179, 0.0447],
        [0.0900, 0.1134, 0.0026,  ..., 0.0757, 0.1178, 0.0447]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680192.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8991.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(893.8010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(422.6600, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.4884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0335],
        [ 0.2805],
        [ 0.3964],
        ...,
        [-3.0736],
        [-3.0680],
        [-3.0668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279613.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0262],
        [1.0311],
        ...,
        [0.9979],
        [0.9969],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370745.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.2200, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0262],
        [1.0312],
        ...,
        [0.9978],
        [0.9969],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370752.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.2200, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0050, -0.0008,  0.0031,  ...,  0.0044, -0.0011,  0.0125],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2278.3169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.9048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0127, device='cuda:0')



h[100].sum tensor(-8.0139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4704, device='cuda:0')



h[200].sum tensor(87.1350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0000, 0.0032,  ..., 0.0092, 0.0000, 0.0156],
        [0.0077, 0.0000, 0.0048,  ..., 0.0104, 0.0000, 0.0204],
        [0.0256, 0.0000, 0.0160,  ..., 0.0207, 0.0000, 0.0607],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58797.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0809, 0.0248, 0.0403,  ..., 0.0836, 0.0292, 0.1044],
        [0.0801, 0.0274, 0.0441,  ..., 0.0854, 0.0305, 0.1106],
        [0.0761, 0.0008, 0.0612,  ..., 0.0916, 0.0018, 0.1380],
        ...,
        [0.0901, 0.1137, 0.0028,  ..., 0.0756, 0.1174, 0.0449],
        [0.0900, 0.1137, 0.0028,  ..., 0.0756, 0.1174, 0.0449],
        [0.0900, 0.1137, 0.0028,  ..., 0.0756, 0.1174, 0.0449]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620739.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9249.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(842.0499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.2695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(294.3657, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3392],
        [ 0.2687],
        [ 0.1714],
        ...,
        [-3.0660],
        [-3.0605],
        [-3.0594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259576.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0262],
        [1.0312],
        ...,
        [0.9978],
        [0.9969],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370752.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5479],
        [0.3860],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.3955, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0263],
        [1.0312],
        ...,
        [0.9978],
        [0.9968],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370758.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5479],
        [0.3860],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.3955, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0392, -0.0052,  0.0240,  ...,  0.0205, -0.0071,  0.0756],
        [ 0.0360, -0.0048,  0.0220,  ...,  0.0190, -0.0066,  0.0697],
        [ 0.0265, -0.0036,  0.0162,  ...,  0.0145, -0.0049,  0.0522],
        ...,
        [ 0.0088, -0.0013,  0.0055,  ...,  0.0062, -0.0018,  0.0195],
        [ 0.0088, -0.0013,  0.0055,  ...,  0.0062, -0.0018,  0.0195],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2659.1797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0166, device='cuda:0')



h[100].sum tensor(-10.6324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6157, device='cuda:0')



h[200].sum tensor(91.4950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1426, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1538, 0.0000, 0.0942,  ..., 0.0808, 0.0000, 0.2976],
        [0.1333, 0.0000, 0.0817,  ..., 0.0712, 0.0000, 0.2597],
        [0.1088, 0.0000, 0.0667,  ..., 0.0597, 0.0000, 0.2144],
        ...,
        [0.0167, 0.0000, 0.0104,  ..., 0.0155, 0.0000, 0.0398],
        [0.0167, 0.0000, 0.0104,  ..., 0.0155, 0.0000, 0.0397],
        [0.0167, 0.0000, 0.0104,  ..., 0.0155, 0.0000, 0.0397]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64896.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.3570,  ..., 0.2697, 0.0000, 0.7200],
        [0.0000, 0.0000, 0.3342,  ..., 0.2554, 0.0000, 0.6735],
        [0.0000, 0.0000, 0.3006,  ..., 0.2339, 0.0000, 0.6043],
        ...,
        [0.0766, 0.0116, 0.0551,  ..., 0.0959, 0.0126, 0.1328],
        [0.0765, 0.0116, 0.0551,  ..., 0.0959, 0.0126, 0.1327],
        [0.0789, 0.0288, 0.0458,  ..., 0.0923, 0.0320, 0.1174]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647039.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9168.2236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(864.4748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(371.6091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.3964, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2016],
        [ 0.2111],
        [ 0.2258],
        ...,
        [-1.0176],
        [-1.0118],
        [-1.4091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261252.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0263],
        [1.0312],
        ...,
        [0.9978],
        [0.9968],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370758.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(167.6388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0263],
        [1.0313],
        ...,
        [0.9977],
        [0.9968],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370764.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(167.6388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0033, -0.0006,  0.0021,  ...,  0.0036, -0.0008,  0.0093],
        [ 0.0126, -0.0018,  0.0078,  ...,  0.0080, -0.0024,  0.0265],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2265.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0124, device='cuda:0')



h[100].sum tensor(-7.9078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4579, device='cuda:0')



h[200].sum tensor(86.8206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0000, 0.0100,  ..., 0.0155, 0.0000, 0.0404],
        [0.0250, 0.0000, 0.0155,  ..., 0.0197, 0.0000, 0.0571],
        [0.0201, 0.0000, 0.0126,  ..., 0.0181, 0.0000, 0.0506],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58574.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0743, 0.0096, 0.0670,  ..., 0.0938, 0.0102, 0.1485],
        [0.0680, 0.0002, 0.0852,  ..., 0.1032, 0.0012, 0.1812],
        [0.0651, 0.0000, 0.0969,  ..., 0.1075, 0.0000, 0.2007],
        ...,
        [0.0905, 0.1146, 0.0028,  ..., 0.0758, 0.1179, 0.0453],
        [0.0904, 0.1145, 0.0028,  ..., 0.0757, 0.1179, 0.0453],
        [0.0904, 0.1145, 0.0028,  ..., 0.0757, 0.1178, 0.0453]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621409.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9258.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(836.3375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.4664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.6476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3979],
        [ 0.4782],
        [ 0.5089],
        ...,
        [-3.0852],
        [-3.0796],
        [-3.0784]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293370.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0263],
        [1.0313],
        ...,
        [0.9977],
        [0.9968],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370764.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5459],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.5153, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0314],
        ...,
        [0.9977],
        [0.9968],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370769.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5459],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.5153, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072, -0.0011,  0.0045,  ...,  0.0055, -0.0015,  0.0165],
        [ 0.0192, -0.0026,  0.0118,  ...,  0.0111, -0.0036,  0.0387],
        [ 0.0159, -0.0022,  0.0098,  ...,  0.0095, -0.0030,  0.0325],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2910.2722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0195, device='cuda:0')



h[100].sum tensor(-12.3575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7226, device='cuda:0')



h[200].sum tensor(94.1714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0765, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0768, 0.0000, 0.0472,  ..., 0.0446, 0.0000, 0.1551],
        [0.0584, 0.0000, 0.0360,  ..., 0.0360, 0.0000, 0.1212],
        [0.0723, 0.0000, 0.0445,  ..., 0.0426, 0.0000, 0.1469],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67687.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0150, 0.0000, 0.2041,  ..., 0.1743, 0.0000, 0.4115],
        [0.0111, 0.0000, 0.2188,  ..., 0.1829, 0.0000, 0.4400],
        [0.0039, 0.0000, 0.2506,  ..., 0.2022, 0.0000, 0.5032],
        ...,
        [0.0907, 0.1150, 0.0027,  ..., 0.0758, 0.1185, 0.0454],
        [0.0907, 0.1150, 0.0027,  ..., 0.0758, 0.1184, 0.0454],
        [0.0906, 0.1149, 0.0027,  ..., 0.0758, 0.1184, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655946.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9174.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.0062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(388.6332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(341.1891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2173],
        [ 0.2018],
        [ 0.1892],
        ...,
        [-3.1018],
        [-3.0915],
        [-3.0638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270686.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0314],
        ...,
        [0.9977],
        [0.9968],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370769.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.9354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9977],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370775.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.9354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2633.6072, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-10.4803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6117, device='cuda:0')



h[200].sum tensor(90.4125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0037],
        [0.0188, 0.0000, 0.0117,  ..., 0.0162, 0.0000, 0.0433],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64741.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0866, 0.0997, 0.0065,  ..., 0.0758, 0.1032, 0.0524],
        [0.0819, 0.0571, 0.0231,  ..., 0.0831, 0.0614, 0.0812],
        [0.0700, 0.0205, 0.0640,  ..., 0.1007, 0.0238, 0.1527],
        ...,
        [0.0880, 0.0830, 0.0148,  ..., 0.0809, 0.0884, 0.0668],
        [0.0900, 0.1046, 0.0063,  ..., 0.0777, 0.1089, 0.0527],
        [0.0911, 0.1154, 0.0026,  ..., 0.0760, 0.1191, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648549.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9334.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.9155, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(367.6257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.1508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8844],
        [-1.0886],
        [-0.2808],
        ...,
        [-1.9267],
        [-2.4380],
        [-2.8156]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255725.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9977],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370775.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.6985, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9977],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370781.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.6985, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3089.3389, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0214, device='cuda:0')



h[100].sum tensor(-13.6709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7913, device='cuda:0')



h[200].sum tensor(94.8226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72626.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0878, 0.0882, 0.0142,  ..., 0.0759, 0.0915, 0.0634],
        [0.0884, 0.0995, 0.0079,  ..., 0.0751, 0.1026, 0.0553],
        [0.0888, 0.1044, 0.0064,  ..., 0.0747, 0.1074, 0.0518],
        ...,
        [0.0916, 0.1160, 0.0024,  ..., 0.0765, 0.1203, 0.0460],
        [0.0916, 0.1160, 0.0024,  ..., 0.0764, 0.1203, 0.0459],
        [0.0916, 0.1159, 0.0024,  ..., 0.0764, 0.1202, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689245.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9263.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(898.3354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(420.2459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(364.9156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2553],
        [-1.5307],
        [-1.7728],
        ...,
        [-3.1473],
        [-3.1415],
        [-3.1403]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282144.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9977],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370781.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.8843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9977],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370781.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.8843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2708.0400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-11.0540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6443, device='cuda:0')



h[200].sum tensor(90.4467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        [0.0090, 0.0000, 0.0056,  ..., 0.0110, 0.0000, 0.0226],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67385.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0819, 0.0415, 0.0294,  ..., 0.0843, 0.0484, 0.0916],
        [0.0806, 0.0360, 0.0314,  ..., 0.0866, 0.0406, 0.0964],
        [0.0763, 0.0258, 0.0452,  ..., 0.0932, 0.0280, 0.1208],
        ...,
        [0.0916, 0.1160, 0.0024,  ..., 0.0765, 0.1203, 0.0460],
        [0.0916, 0.1160, 0.0024,  ..., 0.0764, 0.1203, 0.0459],
        [0.0916, 0.1159, 0.0024,  ..., 0.0764, 0.1202, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665697.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9340.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.5125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.0774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(339.5712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2532],
        [ 0.2450],
        [ 0.2362],
        ...,
        [-3.1473],
        [-3.1415],
        [-3.1403]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276999.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9977],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370781.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 550.0 event: 2750 loss: tensor(450.7340, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2600],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370786.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2600],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0041, -0.0007,  0.0026,  ...,  0.0040, -0.0009,  0.0108],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2523.5916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0156, device='cuda:0')



h[100].sum tensor(-9.8340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5756, device='cuda:0')



h[200].sum tensor(87.6316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0027,  ..., 0.0088, 0.0000, 0.0137],
        [0.0032, 0.0000, 0.0021,  ..., 0.0083, 0.0000, 0.0119],
        [0.0148, 0.0000, 0.0095,  ..., 0.0156, 0.0000, 0.0407],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63372.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0872, 0.0834, 0.0146,  ..., 0.0775, 0.0879, 0.0662],
        [0.0870, 0.0761, 0.0183,  ..., 0.0785, 0.0810, 0.0719],
        [0.0855, 0.0493, 0.0309,  ..., 0.0815, 0.0554, 0.0908],
        ...,
        [0.0921, 0.1165, 0.0022,  ..., 0.0767, 0.1213, 0.0462],
        [0.0921, 0.1165, 0.0022,  ..., 0.0767, 0.1212, 0.0462],
        [0.0920, 0.1165, 0.0022,  ..., 0.0767, 0.1212, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644482.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9542.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.4578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(359.3965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.2012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1988],
        [-2.2700],
        [-2.3586],
        ...,
        [-3.1713],
        [-3.1653],
        [-3.1641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275604.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9967],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370786.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3646, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9967],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370792.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3646, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2557.5085, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-10.1102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5992, device='cuda:0')



h[200].sum tensor(87.2401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0000, 0.0039,  ..., 0.0102, 0.0000, 0.0195],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0035],
        [0.0109, 0.0000, 0.0069,  ..., 0.0126, 0.0000, 0.0286],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63218.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0825, 0.0286, 0.0425,  ..., 0.0865, 0.0313, 0.1127],
        [0.0856, 0.0480, 0.0302,  ..., 0.0822, 0.0541, 0.0920],
        [0.0828, 0.0234, 0.0463,  ..., 0.0872, 0.0276, 0.1171],
        ...,
        [0.0926, 0.1171, 0.0021,  ..., 0.0770, 0.1222, 0.0464],
        [0.0925, 0.1170, 0.0021,  ..., 0.0770, 0.1221, 0.0464],
        [0.0925, 0.1170, 0.0021,  ..., 0.0769, 0.1221, 0.0464]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643321.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9628.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.8644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(359.9696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.5433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2477],
        [ 0.1525],
        [ 0.1492],
        ...,
        [-3.1899],
        [-3.1832],
        [-3.1812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299802.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9967],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370792.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4058],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(154.3708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9967],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370798.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4058],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(154.3708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0071, -0.0011,  0.0044,  ...,  0.0054, -0.0014,  0.0164],
        [ 0.0067, -0.0010,  0.0042,  ...,  0.0052, -0.0014,  0.0156],
        [ 0.0136, -0.0019,  0.0084,  ...,  0.0085, -0.0026,  0.0283],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2139.7808, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.1671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0114, device='cuda:0')



h[100].sum tensor(-7.2291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4217, device='cuda:0')



h[200].sum tensor(82.0533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0125, 0.0000, 0.0079,  ..., 0.0133, 0.0000, 0.0316],
        [0.0394, 0.0000, 0.0244,  ..., 0.0271, 0.0000, 0.0862],
        [0.0370, 0.0000, 0.0230,  ..., 0.0260, 0.0000, 0.0817],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56634.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0782, 0.0283, 0.0462,  ..., 0.0927, 0.0317, 0.1225],
        [0.0692, 0.0000, 0.0783,  ..., 0.1071, 0.0000, 0.1786],
        [0.0682, 0.0000, 0.0837,  ..., 0.1091, 0.0000, 0.1873],
        ...,
        [0.0927, 0.1173, 0.0021,  ..., 0.0771, 0.1222, 0.0466],
        [0.0927, 0.1173, 0.0021,  ..., 0.0770, 0.1222, 0.0465],
        [0.0927, 0.1173, 0.0021,  ..., 0.0770, 0.1221, 0.0465]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616651.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9695.3350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(828.2039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(322.3780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(280.4101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8336],
        [-0.4359],
        [-0.2185],
        ...,
        [-3.1999],
        [-3.1937],
        [-3.1924]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334699.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9967],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370798.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3411],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.0217, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370805.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3411],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.0217, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0058, -0.0009,  0.0036,  ...,  0.0048, -0.0012,  0.0139],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2690.8198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-10.9179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6365, device='cuda:0')



h[200].sum tensor(88.0174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0000, 0.0132,  ..., 0.0184, 0.0000, 0.0522],
        [0.0046, 0.0000, 0.0029,  ..., 0.0089, 0.0000, 0.0146],
        [0.0059, 0.0000, 0.0037,  ..., 0.0095, 0.0000, 0.0171],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64152.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0831, 0.0278, 0.0376,  ..., 0.0858, 0.0352, 0.1040],
        [0.0861, 0.0642, 0.0222,  ..., 0.0814, 0.0699, 0.0797],
        [0.0869, 0.0750, 0.0173,  ..., 0.0802, 0.0806, 0.0722],
        ...,
        [0.0928, 0.1175, 0.0022,  ..., 0.0771, 0.1220, 0.0468],
        [0.0928, 0.1175, 0.0022,  ..., 0.0771, 0.1220, 0.0468],
        [0.0927, 0.1174, 0.0022,  ..., 0.0771, 0.1220, 0.0468]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642442.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9611.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(855.9826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(369.2382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.2301, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2891],
        [-2.4096],
        [-2.5350],
        ...,
        [-3.1984],
        [-3.1924],
        [-3.1915]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309372.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370805.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5459],
        [0.4412],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.3665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370811.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5459],
        [0.4412],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.3665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0280, -0.0036,  0.0171,  ...,  0.0152, -0.0050,  0.0549],
        [ 0.0078, -0.0011,  0.0049,  ...,  0.0058, -0.0016,  0.0178],
        [ 0.0193, -0.0026,  0.0119,  ...,  0.0112, -0.0035,  0.0390],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2679.8938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-10.7505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6375, device='cuda:0')



h[200].sum tensor(87.8209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0364, 0.0000, 0.0226,  ..., 0.0257, 0.0000, 0.0808],
        [0.0790, 0.0000, 0.0486,  ..., 0.0457, 0.0000, 0.1596],
        [0.0562, 0.0000, 0.0346,  ..., 0.0350, 0.0000, 0.1174],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65140.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0000, 0.1331,  ..., 0.1303, 0.0000, 0.2740],
        [0.0394, 0.0000, 0.1660,  ..., 0.1500, 0.0000, 0.3383],
        [0.0456, 0.0000, 0.1495,  ..., 0.1416, 0.0000, 0.3074],
        ...,
        [0.0928, 0.1176, 0.0024,  ..., 0.0771, 0.1217, 0.0469],
        [0.0927, 0.1176, 0.0024,  ..., 0.0771, 0.1217, 0.0469],
        [0.0927, 0.1175, 0.0024,  ..., 0.0771, 0.1216, 0.0469]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653205.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9532.7480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(856.8787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(376.0756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.9702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5256],
        [ 0.5282],
        [ 0.5300],
        ...,
        [-3.1983],
        [-3.1923],
        [-3.1912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299747.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370811.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(167.7452, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370811.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(167.7452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2235.5684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0124, device='cuda:0')



h[100].sum tensor(-7.7351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4582, device='cuda:0')



h[200].sum tensor(82.7397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57607.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0891, 0.1078, 0.0051,  ..., 0.0755, 0.1111, 0.0501],
        [0.0896, 0.1127, 0.0030,  ..., 0.0753, 0.1159, 0.0469],
        [0.0896, 0.1117, 0.0026,  ..., 0.0755, 0.1152, 0.0475],
        ...,
        [0.0928, 0.1176, 0.0024,  ..., 0.0771, 0.1217, 0.0469],
        [0.0927, 0.1176, 0.0024,  ..., 0.0771, 0.1217, 0.0469],
        [0.0927, 0.1175, 0.0024,  ..., 0.0771, 0.1216, 0.0469]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618524.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9681.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(827.5317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(327.9940, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(282.4076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7648],
        [-2.1559],
        [-2.4554],
        ...,
        [-3.1983],
        [-3.1923],
        [-3.1912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312094.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9976],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370811.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5156, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370818.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5156, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2654.9260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0166, device='cuda:0')



h[100].sum tensor(-10.4822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6160, device='cuda:0')



h[200].sum tensor(87.7915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65495.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0883, 0.0994, 0.0078,  ..., 0.0767, 0.1028, 0.0555],
        [0.0872, 0.0913, 0.0092,  ..., 0.0788, 0.0955, 0.0603],
        [0.0843, 0.0636, 0.0197,  ..., 0.0834, 0.0697, 0.0776],
        ...,
        [0.0928, 0.1178, 0.0024,  ..., 0.0771, 0.1214, 0.0469],
        [0.0927, 0.1177, 0.0024,  ..., 0.0770, 0.1214, 0.0469],
        [0.0927, 0.1177, 0.0024,  ..., 0.0770, 0.1214, 0.0469]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657742.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9538.0225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.5509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(376.8637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.0963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5334],
        [-1.5698],
        [-1.2752],
        ...,
        [-3.1996],
        [-3.1936],
        [-3.1925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286023.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370818.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.9648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370824.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.9648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0088, -0.0012,  0.0055,  ...,  0.0062, -0.0017,  0.0195],
        [ 0.0104, -0.0014,  0.0064,  ...,  0.0069, -0.0020,  0.0224],
        [ 0.0099, -0.0014,  0.0061,  ...,  0.0067, -0.0019,  0.0215],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2766.7949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-11.1367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6528, device='cuda:0')



h[200].sum tensor(89.6521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8134, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0341, 0.0000, 0.0211,  ..., 0.0245, 0.0000, 0.0764],
        [0.0356, 0.0000, 0.0220,  ..., 0.0252, 0.0000, 0.0791],
        [0.0213, 0.0000, 0.0132,  ..., 0.0179, 0.0000, 0.0503],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66795.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0719, 0.0000, 0.0782,  ..., 0.1024, 0.0000, 0.1728],
        [0.0709, 0.0000, 0.0845,  ..., 0.1046, 0.0000, 0.1830],
        [0.0751, 0.0018, 0.0711,  ..., 0.0985, 0.0030, 0.1596],
        ...,
        [0.0928, 0.1179, 0.0025,  ..., 0.0769, 0.1212, 0.0467],
        [0.0928, 0.1179, 0.0025,  ..., 0.0769, 0.1211, 0.0467],
        [0.0928, 0.1179, 0.0025,  ..., 0.0769, 0.1211, 0.0467]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659835.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9527.1553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(869.2490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(385.7342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(332.3478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2028],
        [ 0.1606],
        [ 0.3404],
        ...,
        [-3.2029],
        [-3.1970],
        [-3.1959]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290627.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0264],
        [1.0314],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370824.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.7649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0314],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370831.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.7649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0171, -0.0023,  0.0105,  ...,  0.0101, -0.0031,  0.0347],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2494.8308, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-9.2085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5457, device='cuda:0')



h[200].sum tensor(87.1561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0141, 0.0000, 0.0087,  ..., 0.0133, 0.0000, 0.0321],
        [0.0175, 0.0000, 0.0108,  ..., 0.0149, 0.0000, 0.0385],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62394.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0690, 0.0142, 0.0620,  ..., 0.1039, 0.0182, 0.1550],
        [0.0762, 0.0362, 0.0407,  ..., 0.0941, 0.0421, 0.1171],
        [0.0861, 0.0811, 0.0133,  ..., 0.0804, 0.0858, 0.0663],
        ...,
        [0.0929, 0.1180, 0.0025,  ..., 0.0768, 0.1211, 0.0466],
        [0.0928, 0.1180, 0.0025,  ..., 0.0767, 0.1211, 0.0466],
        [0.0928, 0.1180, 0.0025,  ..., 0.0767, 0.1211, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642461.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9577.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(857.1622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(357.7625, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(313.3621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8815],
        [-1.5037],
        [-2.0443],
        ...,
        [-3.2086],
        [-3.2027],
        [-3.2017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286400.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0314],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370831.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2686],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.2859, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370837.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2686],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.2859, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0204, -0.0027,  0.0125,  ...,  0.0116, -0.0037,  0.0409],
        [ 0.0039, -0.0006,  0.0025,  ...,  0.0039, -0.0009,  0.0104],
        [ 0.0043, -0.0007,  0.0027,  ...,  0.0041, -0.0009,  0.0112],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2643.8911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-10.1024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5990, device='cuda:0')



h[200].sum tensor(89.6026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0341, 0.0000, 0.0211,  ..., 0.0244, 0.0000, 0.0760],
        [0.0441, 0.0000, 0.0272,  ..., 0.0292, 0.0000, 0.0946],
        [0.0155, 0.0000, 0.0096,  ..., 0.0146, 0.0000, 0.0371],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66881.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0557, 0.0000, 0.1233,  ..., 0.1240, 0.0000, 0.2539],
        [0.0611, 0.0000, 0.1096,  ..., 0.1171, 0.0000, 0.2290],
        [0.0753, 0.0186, 0.0656,  ..., 0.0970, 0.0216, 0.1513],
        ...,
        [0.0929, 0.1180, 0.0025,  ..., 0.0766, 0.1209, 0.0464],
        [0.0929, 0.1180, 0.0025,  ..., 0.0765, 0.1209, 0.0464],
        [0.0928, 0.1180, 0.0025,  ..., 0.0765, 0.1208, 0.0464]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677849., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9492.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.8181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(385.5227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.9967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4393],
        [ 0.2897],
        [-0.1136],
        ...,
        [-3.1131],
        [-3.1810],
        [-3.1996]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285520.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370837.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 560.0 event: 2800 loss: tensor(448.8360, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.9606, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370844.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.9606, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0044, -0.0007,  0.0028,  ...,  0.0041, -0.0010,  0.0114],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2339.1558, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.0085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0127, device='cuda:0')



h[100].sum tensor(-7.9347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4697, device='cuda:0')



h[200].sum tensor(86.8910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0037],
        [0.0081, 0.0000, 0.0051,  ..., 0.0110, 0.0000, 0.0233],
        [0.0249, 0.0000, 0.0154,  ..., 0.0195, 0.0000, 0.0567],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59146.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0863, 0.0712, 0.0199,  ..., 0.0793, 0.0750, 0.0744],
        [0.0827, 0.0402, 0.0395,  ..., 0.0855, 0.0418, 0.1057],
        [0.0747, 0.0116, 0.0695,  ..., 0.0974, 0.0121, 0.1564],
        ...,
        [0.0929, 0.1180, 0.0026,  ..., 0.0764, 0.1205, 0.0464],
        [0.0928, 0.1180, 0.0026,  ..., 0.0764, 0.1204, 0.0464],
        [0.0928, 0.1180, 0.0026,  ..., 0.0764, 0.1204, 0.0464]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629291.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9584.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(850.3911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(337.5928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.4159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7201],
        [-0.6440],
        [-0.4666],
        ...,
        [-3.2085],
        [-3.2028],
        [-3.2018]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304054.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370844.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(163.9147, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370850.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(163.9147, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2306.3247, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0121, device='cuda:0')



h[100].sum tensor(-7.6451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4478, device='cuda:0')



h[200].sum tensor(86.6943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58692.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0896, 0.1151, 0.0021,  ..., 0.0742, 0.1164, 0.0449],
        [0.0899, 0.1154, 0.0021,  ..., 0.0744, 0.1167, 0.0450],
        [0.0901, 0.1154, 0.0023,  ..., 0.0744, 0.1169, 0.0451],
        ...,
        [0.0930, 0.1182, 0.0026,  ..., 0.0764, 0.1204, 0.0465],
        [0.0930, 0.1181, 0.0026,  ..., 0.0764, 0.1203, 0.0465],
        [0.0930, 0.1181, 0.0026,  ..., 0.0764, 0.1203, 0.0465]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627465.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9577.2842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(847.7514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(335.7988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.8772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2689],
        [-3.2295],
        [-3.1545],
        ...,
        [-3.2132],
        [-3.2075],
        [-3.2066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324813.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370850.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3296],
        [0.3193],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370856.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3296],
        [0.3193],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056, -0.0008,  0.0035,  ...,  0.0047, -0.0011,  0.0135],
        [ 0.0096, -0.0013,  0.0059,  ...,  0.0066, -0.0018,  0.0210],
        [ 0.0099, -0.0013,  0.0061,  ...,  0.0067, -0.0019,  0.0214],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2815.6396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-10.9825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6608, device='cuda:0')



h[200].sum tensor(92.4101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0179, 0.0000, 0.0110,  ..., 0.0156, 0.0000, 0.0414],
        [0.0382, 0.0000, 0.0235,  ..., 0.0263, 0.0000, 0.0836],
        [0.0418, 0.0000, 0.0258,  ..., 0.0280, 0.0000, 0.0903],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67570.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0730, 0.0046, 0.0684,  ..., 0.0996, 0.0070, 0.1575],
        [0.0670, 0.0000, 0.0940,  ..., 0.1092, 0.0000, 0.2001],
        [0.0657, 0.0000, 0.0999,  ..., 0.1114, 0.0000, 0.2099],
        ...,
        [0.0932, 0.1183, 0.0026,  ..., 0.0764, 0.1202, 0.0468],
        [0.0931, 0.1182, 0.0026,  ..., 0.0764, 0.1201, 0.0468],
        [0.0931, 0.1182, 0.0026,  ..., 0.0764, 0.1201, 0.0468]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668968.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9446.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(883.2312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(390.9273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(341.6721, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4355],
        [ 0.4435],
        [ 0.3672],
        ...,
        [-3.2107],
        [-3.2043],
        [-3.2027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288669.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9966],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370856.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.8344, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370863., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.8344, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2670.0830, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-9.9283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5950, device='cuda:0')



h[200].sum tensor(90.6898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64283.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0898, 0.1057, 0.0068,  ..., 0.0744, 0.1060, 0.0528],
        [0.0903, 0.1139, 0.0034,  ..., 0.0742, 0.1142, 0.0471],
        [0.0903, 0.1152, 0.0026,  ..., 0.0744, 0.1158, 0.0459],
        ...,
        [0.0932, 0.1183, 0.0027,  ..., 0.0765, 0.1197, 0.0471],
        [0.0932, 0.1183, 0.0027,  ..., 0.0764, 0.1197, 0.0471],
        [0.0932, 0.1183, 0.0027,  ..., 0.0764, 0.1197, 0.0471]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649062., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9562.1650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(872.2604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(367.2091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.3608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6665],
        [-2.1428],
        [-2.4141],
        ...,
        [-3.2064],
        [-3.1999],
        [-3.1986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268948.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0263],
        [1.0313],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370863., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6011],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5073, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0313],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370868.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6011],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5073, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0209, -0.0027,  0.0128,  ...,  0.0118, -0.0037,  0.0418],
        [ 0.0112, -0.0015,  0.0069,  ...,  0.0073, -0.0021,  0.0239],
        [ 0.0043, -0.0007,  0.0027,  ...,  0.0041, -0.0009,  0.0112],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2833.4912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0175, device='cuda:0')



h[100].sum tensor(-10.9534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6488, device='cuda:0')



h[200].sum tensor(92.2538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1215, 0.0000, 0.0743,  ..., 0.0653, 0.0000, 0.2376],
        [0.0763, 0.0000, 0.0468,  ..., 0.0442, 0.0000, 0.1542],
        [0.0496, 0.0000, 0.0305,  ..., 0.0317, 0.0000, 0.1049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67998.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.2961,  ..., 0.2220, 0.0000, 0.5893],
        [0.0156, 0.0000, 0.2405,  ..., 0.1889, 0.0000, 0.4789],
        [0.0345, 0.0000, 0.1829,  ..., 0.1546, 0.0000, 0.3650],
        ...,
        [0.0934, 0.1185, 0.0027,  ..., 0.0766, 0.1196, 0.0474],
        [0.0934, 0.1184, 0.0027,  ..., 0.0766, 0.1195, 0.0474],
        [0.0934, 0.1184, 0.0027,  ..., 0.0766, 0.1195, 0.0474]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671673.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9500.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(881.7241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(392.7324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(339.6926, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1683],
        [ 0.2318],
        [ 0.2974],
        ...,
        [-3.2154],
        [-3.2098],
        [-3.2091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279606.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0313],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370868.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3208],
        [0.6973],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(566.5446, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0313],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370868.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3208],
        [0.6973],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(566.5446, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054, -0.0008,  0.0033,  ...,  0.0046, -0.0011,  0.0132],
        [ 0.0132, -0.0017,  0.0081,  ...,  0.0082, -0.0024,  0.0275],
        [ 0.0054, -0.0008,  0.0033,  ...,  0.0046, -0.0011,  0.0132],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5186.3105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.5805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0418, device='cuda:0')



h[100].sum tensor(-26.6000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.5476, device='cuda:0')



h[200].sum tensor(119.0208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0076, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0178, 0.0000, 0.0109,  ..., 0.0155, 0.0000, 0.0413],
        [0.0318, 0.0000, 0.0196,  ..., 0.0233, 0.0000, 0.0720],
        [0.0544, 0.0000, 0.0334,  ..., 0.0339, 0.0000, 0.1138],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(109206.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0770, 0.0217, 0.0529,  ..., 0.0940, 0.0242, 0.1324],
        [0.0716, 0.0000, 0.0748,  ..., 0.1027, 0.0000, 0.1693],
        [0.0659, 0.0000, 0.0882,  ..., 0.1110, 0.0000, 0.1952],
        ...,
        [0.0934, 0.1185, 0.0027,  ..., 0.0766, 0.1196, 0.0474],
        [0.0934, 0.1184, 0.0027,  ..., 0.0766, 0.1195, 0.0474],
        [0.0934, 0.1184, 0.0027,  ..., 0.0766, 0.1195, 0.0474]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(877853.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8986.5947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1043.8237, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(652.3097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(544.4803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6568],
        [-0.6037],
        [-0.7622],
        ...,
        [-3.2154],
        [-3.2098],
        [-3.2091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250685.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0313],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370868.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.4489, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0313],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370873.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.4489, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2950.3040, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-11.6527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6951, device='cuda:0')



h[200].sum tensor(93.3284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5789, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0039],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67995.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0883, 0.0959, 0.0091,  ..., 0.0774, 0.0970, 0.0585],
        [0.0902, 0.1131, 0.0035,  ..., 0.0751, 0.1130, 0.0481],
        [0.0898, 0.1046, 0.0072,  ..., 0.0761, 0.1051, 0.0538],
        ...,
        [0.0935, 0.1186, 0.0027,  ..., 0.0768, 0.1192, 0.0479],
        [0.0935, 0.1186, 0.0027,  ..., 0.0768, 0.1192, 0.0479],
        [0.0935, 0.1186, 0.0027,  ..., 0.0768, 0.1192, 0.0479]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664099.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9535.0029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.2391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(391.7296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.7289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8068],
        [-0.9616],
        [-0.8119],
        ...,
        [-3.2151],
        [-3.2096],
        [-3.2089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263633.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0264],
        [1.0313],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370873.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(258.0659, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0277],
        [1.0264],
        [1.0313],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370878.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(258.0659, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2960.3804, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0190, device='cuda:0')



h[100].sum tensor(-11.6201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7049, device='cuda:0')



h[200].sum tensor(93.1384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0033,  ..., 0.0091, 0.0000, 0.0161],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0040],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68229.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0867, 0.0615, 0.0244,  ..., 0.0809, 0.0639, 0.0827],
        [0.0893, 0.0982, 0.0092,  ..., 0.0770, 0.0988, 0.0583],
        [0.0906, 0.1136, 0.0030,  ..., 0.0751, 0.1135, 0.0483],
        ...,
        [0.0938, 0.1191, 0.0028,  ..., 0.0768, 0.1195, 0.0481],
        [0.0938, 0.1191, 0.0028,  ..., 0.0768, 0.1194, 0.0480],
        [0.0938, 0.1191, 0.0028,  ..., 0.0768, 0.1194, 0.0480]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664953.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9625.0537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(878.7125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(391.1142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.1480, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1716],
        [-1.9219],
        [-2.5071],
        ...,
        [-3.2153],
        [-3.2116],
        [-3.2122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250960.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0277],
        [1.0264],
        [1.0313],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370878.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(158.6543, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0277],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370883.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(158.6543, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2312.1333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.9153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0117, device='cuda:0')



h[100].sum tensor(-7.2834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4334, device='cuda:0')



h[200].sum tensor(85.5125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8432, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0039],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58839.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0908, 0.1166, 0.0022,  ..., 0.0746, 0.1162, 0.0462],
        [0.0910, 0.1169, 0.0022,  ..., 0.0748, 0.1166, 0.0464],
        [0.0912, 0.1169, 0.0024,  ..., 0.0748, 0.1167, 0.0465],
        ...,
        [0.0943, 0.1197, 0.0027,  ..., 0.0768, 0.1202, 0.0480],
        [0.0942, 0.1197, 0.0027,  ..., 0.0768, 0.1202, 0.0480],
        [0.0942, 0.1197, 0.0027,  ..., 0.0768, 0.1202, 0.0480]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625914.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9780.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(837.9549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.9957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.3571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7833],
        [-2.9145],
        [-2.9919],
        ...,
        [-3.2371],
        [-3.2316],
        [-3.2309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296019.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0277],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370883.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.3402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0278],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370888.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.3402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0194, -0.0024,  0.0119,  ...,  0.0112, -0.0034,  0.0391],
        [ 0.0129, -0.0017,  0.0080,  ...,  0.0081, -0.0023,  0.0271],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3168.3169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0211, device='cuda:0')



h[100].sum tensor(-12.8703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7794, device='cuda:0')



h[200].sum tensor(95.3954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0775, 0.0000, 0.0476,  ..., 0.0448, 0.0000, 0.1564],
        [0.0294, 0.0000, 0.0181,  ..., 0.0211, 0.0000, 0.0628],
        [0.0306, 0.0000, 0.0188,  ..., 0.0216, 0.0000, 0.0650],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73067.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0399, 0.0000, 0.1645,  ..., 0.1514, 0.0000, 0.3399],
        [0.0595, 0.0000, 0.1088,  ..., 0.1238, 0.0000, 0.2371],
        [0.0663, 0.0034, 0.0863,  ..., 0.1137, 0.0046, 0.1970],
        ...,
        [0.0948, 0.1204, 0.0026,  ..., 0.0768, 0.1212, 0.0477],
        [0.0947, 0.1204, 0.0026,  ..., 0.0768, 0.1212, 0.0477],
        [0.0948, 0.1204, 0.0026,  ..., 0.0768, 0.1212, 0.0477]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692872.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9739.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(905.6180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(418.3694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(363.6269, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4308],
        [ 0.3356],
        [-0.0771],
        ...,
        [-3.2519],
        [-3.2454],
        [-3.2436]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253447.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0278],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370888.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 570.0 event: 2850 loss: tensor(433.0901, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.3839, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0279],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370892.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.3839, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3081.8923, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0200, device='cuda:0')



h[100].sum tensor(-12.2684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7386, device='cuda:0')



h[200].sum tensor(94.5154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71566.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0918, 0.1179, 0.0020,  ..., 0.0746, 0.1184, 0.0456],
        [0.0922, 0.1175, 0.0025,  ..., 0.0748, 0.1179, 0.0464],
        [0.0924, 0.1135, 0.0042,  ..., 0.0749, 0.1139, 0.0497],
        ...,
        [0.0954, 0.1212, 0.0025,  ..., 0.0769, 0.1225, 0.0473],
        [0.0954, 0.1211, 0.0025,  ..., 0.0769, 0.1225, 0.0473],
        [0.0954, 0.1211, 0.0025,  ..., 0.0769, 0.1225, 0.0473]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692814.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9799.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(901.5053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(410.6635, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(357.9761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9906],
        [-2.7264],
        [-2.3176],
        ...,
        [-3.2861],
        [-3.2801],
        [-3.2793]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299116.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0279],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370892.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(383.7083, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0279],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370897.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(383.7083, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0151, -0.0019,  0.0093,  ...,  0.0092, -0.0027,  0.0310],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3857.9121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0283, device='cuda:0')



h[100].sum tensor(-17.3826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0481, device='cuda:0')



h[200].sum tensor(103.1023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9689, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0036],
        [0.0231, 0.0000, 0.0143,  ..., 0.0182, 0.0000, 0.0510],
        [0.0310, 0.0000, 0.0192,  ..., 0.0226, 0.0000, 0.0679],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81574.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0836, 0.0382, 0.0407,  ..., 0.0895, 0.0397, 0.1131],
        [0.0667, 0.0082, 0.0936,  ..., 0.1156, 0.0091, 0.2091],
        [0.0491, 0.0000, 0.1442,  ..., 0.1417, 0.0000, 0.3030],
        ...,
        [0.0957, 0.1215, 0.0023,  ..., 0.0772, 0.1234, 0.0473],
        [0.0957, 0.1215, 0.0023,  ..., 0.0772, 0.1234, 0.0473],
        [0.0957, 0.1215, 0.0023,  ..., 0.0772, 0.1234, 0.0473]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(726782.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9713.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(938.9024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(475.5533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(406.0397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0068],
        [ 0.2548],
        [ 0.3551],
        ...,
        [-3.3075],
        [-3.3014],
        [-3.3006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317275.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0279],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370897.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3274],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.1976, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370901.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3274],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.1976, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0126, -0.0016,  0.0078,  ...,  0.0080, -0.0023,  0.0266],
        [ 0.0055, -0.0008,  0.0035,  ...,  0.0047, -0.0011,  0.0134],
        [ 0.0057, -0.0008,  0.0036,  ...,  0.0048, -0.0011,  0.0138],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2354.6965, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0123, device='cuda:0')



h[100].sum tensor(-7.5304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4540, device='cuda:0')



h[200].sum tensor(85.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2161, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0387, 0.0000, 0.0241,  ..., 0.0268, 0.0000, 0.0848],
        [0.0338, 0.0000, 0.0210,  ..., 0.0245, 0.0000, 0.0756],
        [0.0102, 0.0000, 0.0065,  ..., 0.0122, 0.0000, 0.0273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59405.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0644, 0.0000, 0.1043,  ..., 0.1194, 0.0000, 0.2267],
        [0.0724, 0.0000, 0.0810,  ..., 0.1079, 0.0000, 0.1842],
        [0.0831, 0.0215, 0.0495,  ..., 0.0918, 0.0241, 0.1265],
        ...,
        [0.0957, 0.1214, 0.0021,  ..., 0.0776, 0.1235, 0.0477],
        [0.0957, 0.1214, 0.0021,  ..., 0.0776, 0.1234, 0.0477],
        [0.0957, 0.1214, 0.0021,  ..., 0.0776, 0.1234, 0.0477]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631336.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10024.8301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(853.1828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(334.2783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.8506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5080],
        [ 0.4324],
        [ 0.3420],
        ...,
        [-3.3142],
        [-3.3081],
        [-3.3073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325230.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370901.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5347, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370906.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5347, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2442.0210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0132, device='cuda:0')



h[100].sum tensor(-8.0728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4877, device='cuda:0')



h[200].sum tensor(86.6950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59718.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0920, 0.1160, 0.0029,  ..., 0.0753, 0.1165, 0.0482],
        [0.0922, 0.1176, 0.0021,  ..., 0.0757, 0.1184, 0.0471],
        [0.0923, 0.1182, 0.0018,  ..., 0.0758, 0.1193, 0.0466],
        ...,
        [0.0954, 0.1211, 0.0021,  ..., 0.0779, 0.1230, 0.0481],
        [0.0954, 0.1211, 0.0021,  ..., 0.0778, 0.1229, 0.0481],
        [0.0954, 0.1211, 0.0021,  ..., 0.0779, 0.1229, 0.0481]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630101.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9917.2041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(847.6016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(338.1493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.2028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6088],
        [-2.9168],
        [-3.1463],
        ...,
        [-3.3098],
        [-3.3037],
        [-3.3029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326648.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370906.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2576],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(338.0618, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370912.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2576],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(338.0618, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [ 0.0040, -0.0006,  0.0026,  ...,  0.0040, -0.0009,  0.0108],
        [ 0.0036, -0.0006,  0.0023,  ...,  0.0038, -0.0008,  0.0100],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3567.6545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0250, device='cuda:0')



h[100].sum tensor(-15.3440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9235, device='cuda:0')



h[200].sum tensor(99.3918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7124, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0000, 0.0026,  ..., 0.0087, 0.0000, 0.0139],
        [0.0149, 0.0000, 0.0093,  ..., 0.0144, 0.0000, 0.0363],
        [0.0316, 0.0000, 0.0197,  ..., 0.0235, 0.0000, 0.0721],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75703.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0816, 0.0201, 0.0482,  ..., 0.0916, 0.0227, 0.1246],
        [0.0773, 0.0080, 0.0688,  ..., 0.0991, 0.0088, 0.1588],
        [0.0749, 0.0000, 0.0816,  ..., 0.1034, 0.0000, 0.1795],
        ...,
        [0.0949, 0.1207, 0.0022,  ..., 0.0780, 0.1222, 0.0485],
        [0.0949, 0.1206, 0.0022,  ..., 0.0780, 0.1222, 0.0485],
        [0.0949, 0.1206, 0.0022,  ..., 0.0780, 0.1222, 0.0485]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690245.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9600.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(911.3342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(435.1078, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(369.1396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4762],
        [ 0.4976],
        [ 0.4582],
        ...,
        [-3.2985],
        [-3.2926],
        [-3.2918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273066., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370912.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.1720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370917.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.1720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2934.1392, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-11.1021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6752, device='cuda:0')



h[200].sum tensor(92.3170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0000, 0.0026,  ..., 0.0087, 0.0000, 0.0140],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67272.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0841, 0.0553, 0.0306,  ..., 0.0859, 0.0581, 0.0967],
        [0.0900, 0.1011, 0.0083,  ..., 0.0778, 0.1023, 0.0582],
        [0.0912, 0.1153, 0.0023,  ..., 0.0762, 0.1159, 0.0487],
        ...,
        [0.0944, 0.1203, 0.0024,  ..., 0.0780, 0.1215, 0.0488],
        [0.0944, 0.1203, 0.0024,  ..., 0.0780, 0.1214, 0.0488],
        [0.0944, 0.1203, 0.0024,  ..., 0.0780, 0.1214, 0.0488]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658685.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9598.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(874.6915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(380.6824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.7796, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6620],
        [-1.4068],
        [-1.9789],
        ...,
        [-3.2757],
        [-3.2728],
        [-3.2742]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251653.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0265],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370917.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(290.0931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0266],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370922.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(290.0931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3237.3154, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0214, device='cuda:0')



h[100].sum tensor(-13.0178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7924, device='cuda:0')



h[200].sum tensor(95.6786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74868.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0907, 0.1171, 0.0020,  ..., 0.0758, 0.1173, 0.0471],
        [0.0909, 0.1174, 0.0020,  ..., 0.0760, 0.1176, 0.0472],
        [0.0908, 0.1138, 0.0028,  ..., 0.0765, 0.1145, 0.0496],
        ...,
        [0.0942, 0.1202, 0.0025,  ..., 0.0781, 0.1213, 0.0489],
        [0.0942, 0.1202, 0.0025,  ..., 0.0781, 0.1213, 0.0489],
        [0.0942, 0.1202, 0.0025,  ..., 0.0781, 0.1213, 0.0489]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700220., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9377.0791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(903.3499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(427.4055, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(359.5883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1851],
        [-3.0178],
        [-2.6958],
        ...,
        [-3.2839],
        [-3.2783],
        [-3.2776]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239053.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0266],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370922.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.4862, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0282],
        [1.0266],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370928.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.4862, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3110.3950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0199, device='cuda:0')



h[100].sum tensor(-12.1399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7361, device='cuda:0')



h[200].sum tensor(94.4457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0000, 0.0027,  ..., 0.0087, 0.0000, 0.0145],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68752.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0888, 0.0830, 0.0171,  ..., 0.0786, 0.0851, 0.0711],
        [0.0904, 0.1076, 0.0061,  ..., 0.0768, 0.1085, 0.0540],
        [0.0910, 0.1153, 0.0025,  ..., 0.0762, 0.1160, 0.0486],
        ...,
        [0.0942, 0.1204, 0.0026,  ..., 0.0780, 0.1217, 0.0487],
        [0.0942, 0.1204, 0.0026,  ..., 0.0780, 0.1217, 0.0487],
        [0.0942, 0.1204, 0.0026,  ..., 0.0780, 0.1217, 0.0487]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661597., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9468.6807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(875.4319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(392.3564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.2811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1448],
        [-1.5593],
        [-1.8734],
        ...,
        [-3.2932],
        [-3.2876],
        [-3.2869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270833.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0282],
        [1.0266],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370928.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7583],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.1796, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0267],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370933.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7583],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.1796, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [ 0.0144, -0.0018,  0.0088,  ...,  0.0088, -0.0025,  0.0300],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2693.0195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-9.3995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5741, device='cuda:0')



h[200].sum tensor(89.9370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0340, 0.0000, 0.0208,  ..., 0.0232, 0.0000, 0.0717],
        [0.0119, 0.0000, 0.0073,  ..., 0.0123, 0.0000, 0.0285],
        [0.0736, 0.0000, 0.0451,  ..., 0.0431, 0.0000, 0.1498],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63492.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0618, 0.0000, 0.0838,  ..., 0.1129, 0.0000, 0.1927],
        [0.0658, 0.0000, 0.0795,  ..., 0.1085, 0.0000, 0.1821],
        [0.0481, 0.0000, 0.1317,  ..., 0.1315, 0.0000, 0.2735],
        ...,
        [0.0943, 0.1207, 0.0027,  ..., 0.0778, 0.1222, 0.0484],
        [0.0942, 0.1207, 0.0027,  ..., 0.0778, 0.1222, 0.0484],
        [0.0942, 0.1207, 0.0027,  ..., 0.0778, 0.1222, 0.0484]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644059.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9540.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(855.9621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(359.6168, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.2467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4750],
        [ 0.5392],
        [ 0.5689],
        ...,
        [-3.3058],
        [-3.3002],
        [-3.2995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277033.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0267],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370933.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6484],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3500, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0267],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370938.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6484],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3500, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [ 0.0122, -0.0015,  0.0075,  ...,  0.0078, -0.0021,  0.0258],
        [ 0.0103, -0.0013,  0.0063,  ...,  0.0069, -0.0018,  0.0223],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3175.3816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0204, device='cuda:0')



h[100].sum tensor(-12.5121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7549, device='cuda:0')



h[200].sum tensor(95.3570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0191, 0.0000, 0.0118,  ..., 0.0162, 0.0000, 0.0442],
        [0.0205, 0.0000, 0.0126,  ..., 0.0169, 0.0000, 0.0468],
        [0.0548, 0.0000, 0.0336,  ..., 0.0342, 0.0000, 0.1148],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71924.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0728, 0.0000, 0.0616,  ..., 0.0990, 0.0029, 0.1480],
        [0.0693, 0.0014, 0.0710,  ..., 0.1039, 0.0032, 0.1650],
        [0.0590, 0.0000, 0.0983,  ..., 0.1171, 0.0000, 0.2138],
        ...,
        [0.0945, 0.1211, 0.0027,  ..., 0.0777, 0.1231, 0.0482],
        [0.0944, 0.1211, 0.0027,  ..., 0.0777, 0.1230, 0.0482],
        [0.0945, 0.1211, 0.0027,  ..., 0.0777, 0.1231, 0.0482]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686304.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9431.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(893.5738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(412.7838, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(342.1855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4608],
        [-0.4574],
        [-0.5018],
        ...,
        [-3.3196],
        [-3.3127],
        [-3.3113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275094.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0267],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370938.8438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 580.0 event: 2900 loss: tensor(495.8686, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.5269],
        [0.5840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(262.7570, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0268],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370943.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.5269],
        [0.5840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(262.7570, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0309, -0.0036,  0.0189,  ...,  0.0165, -0.0051,  0.0603],
        [ 0.0334, -0.0039,  0.0204,  ...,  0.0177, -0.0055,  0.0649],
        [ 0.0224, -0.0027,  0.0137,  ...,  0.0126, -0.0037,  0.0447],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3058.1318, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0194, device='cuda:0')



h[100].sum tensor(-11.7969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7178, device='cuda:0')



h[200].sum tensor(93.7693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0961, 0.0000, 0.0588,  ..., 0.0535, 0.0000, 0.1911],
        [0.1076, 0.0000, 0.0658,  ..., 0.0589, 0.0000, 0.2123],
        [0.1057, 0.0000, 0.0646,  ..., 0.0580, 0.0000, 0.2087],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70728.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0000, 0.2062,  ..., 0.1736, 0.0000, 0.4167],
        [0.0013, 0.0000, 0.2314,  ..., 0.1875, 0.0000, 0.4648],
        [0.0055, 0.0000, 0.2245,  ..., 0.1846, 0.0000, 0.4524],
        ...,
        [0.0948, 0.1215, 0.0025,  ..., 0.0779, 0.1241, 0.0481],
        [0.0948, 0.1215, 0.0025,  ..., 0.0778, 0.1241, 0.0481],
        [0.0948, 0.1215, 0.0025,  ..., 0.0778, 0.1241, 0.0481]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682910., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9524.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(893.2835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(405.9642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(336.0373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1684],
        [ 0.2860],
        [ 0.3257],
        ...,
        [-3.3531],
        [-3.3470],
        [-3.3461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274161.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0268],
        [1.0314],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370943.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.1459, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0268],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370948.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.1459, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0202, -0.0024,  0.0124,  ...,  0.0115, -0.0034,  0.0406],
        [ 0.0147, -0.0018,  0.0090,  ...,  0.0090, -0.0025,  0.0305],
        [ 0.0051, -0.0007,  0.0032,  ...,  0.0045, -0.0010,  0.0128],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3462.9136, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0235, device='cuda:0')



h[100].sum tensor(-14.4484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8691, device='cuda:0')



h[200].sum tensor(97.6647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0853, 0.0000, 0.0522,  ..., 0.0485, 0.0000, 0.1711],
        [0.0617, 0.0000, 0.0378,  ..., 0.0374, 0.0000, 0.1275],
        [0.0276, 0.0000, 0.0169,  ..., 0.0202, 0.0000, 0.0598],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77534.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0203, 0.0000, 0.1942,  ..., 0.1628, 0.0000, 0.3880],
        [0.0382, 0.0000, 0.1546,  ..., 0.1421, 0.0000, 0.3140],
        [0.0602, 0.0096, 0.0955,  ..., 0.1155, 0.0111, 0.2097],
        ...,
        [0.0952, 0.1218, 0.0024,  ..., 0.0782, 0.1249, 0.0482],
        [0.0951, 0.1218, 0.0024,  ..., 0.0781, 0.1249, 0.0482],
        [0.0951, 0.1218, 0.0024,  ..., 0.0782, 0.1249, 0.0482]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718364.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9459.1904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(911.0627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(454.8245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(364.5291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4019],
        [ 0.3167],
        [-0.0142],
        ...,
        [-3.3728],
        [-3.3668],
        [-3.3661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309636.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0268],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370948.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.2150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0269],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370953., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.2150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2894.2061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0178, device='cuda:0')



h[100].sum tensor(-10.8128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6589, device='cuda:0')



h[200].sum tensor(90.5401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69316.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0918, 0.1187, 0.0018,  ..., 0.0762, 0.1207, 0.0467],
        [0.0921, 0.1190, 0.0018,  ..., 0.0764, 0.1211, 0.0469],
        [0.0923, 0.1190, 0.0020,  ..., 0.0764, 0.1212, 0.0469],
        ...,
        [0.0954, 0.1220, 0.0023,  ..., 0.0785, 0.1250, 0.0485],
        [0.0953, 0.1219, 0.0023,  ..., 0.0785, 0.1250, 0.0485],
        [0.0950, 0.1183, 0.0031,  ..., 0.0790, 0.1216, 0.0508]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682856.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9643.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(882.6478, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(400.8658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(322.9246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4954],
        [-3.5041],
        [-3.5016],
        ...,
        [-3.3516],
        [-3.2394],
        [-2.9111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273585.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0269],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370953., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2996],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.3251, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0270],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370958.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2996],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.3251, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034, -0.0005,  0.0021,  ...,  0.0036, -0.0007,  0.0096],
        [ 0.0096, -0.0012,  0.0059,  ...,  0.0065, -0.0017,  0.0210],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3320.4277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0225, device='cuda:0')



h[100].sum tensor(-13.5150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8340, device='cuda:0')



h[200].sum tensor(94.7960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0341, 0.0000, 0.0209,  ..., 0.0243, 0.0000, 0.0766],
        [0.0138, 0.0000, 0.0085,  ..., 0.0143, 0.0000, 0.0368],
        [0.0125, 0.0000, 0.0077,  ..., 0.0131, 0.0000, 0.0320],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74679.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0728, 0.0000, 0.0811,  ..., 0.1011, 0.0000, 0.1754],
        [0.0788, 0.0120, 0.0641,  ..., 0.0940, 0.0132, 0.1466],
        [0.0821, 0.0323, 0.0476,  ..., 0.0898, 0.0354, 0.1215],
        ...,
        [0.0955, 0.1220, 0.0023,  ..., 0.0787, 0.1249, 0.0489],
        [0.0955, 0.1220, 0.0023,  ..., 0.0786, 0.1248, 0.0488],
        [0.0955, 0.1220, 0.0023,  ..., 0.0787, 0.1249, 0.0489]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703672.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9531.9189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(899.5125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(436.6067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(347.0118, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5284e-01],
        [ 2.7359e-03],
        [-4.9687e-01],
        ...,
        [-3.3901e+00],
        [-3.3838e+00],
        [-3.3828e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283832.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0270],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370958.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4276, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0270],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370958.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4276, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2893.3462, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0177, device='cuda:0')



h[100].sum tensor(-10.7867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6540, device='cuda:0')



h[200].sum tensor(89.9976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8363, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68765.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0896, 0.0890, 0.0141,  ..., 0.0794, 0.0933, 0.0668],
        [0.0915, 0.1077, 0.0064,  ..., 0.0775, 0.1104, 0.0551],
        [0.0923, 0.1155, 0.0039,  ..., 0.0767, 0.1177, 0.0499],
        ...,
        [0.0955, 0.1220, 0.0023,  ..., 0.0787, 0.1249, 0.0489],
        [0.0955, 0.1220, 0.0023,  ..., 0.0786, 0.1248, 0.0488],
        [0.0955, 0.1220, 0.0023,  ..., 0.0787, 0.1249, 0.0489]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676218.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9665.6973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.5716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(398.7760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(317.8808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1918],
        [-1.8403],
        [-2.3690],
        ...,
        [-3.3874],
        [-3.3813],
        [-3.3805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276598.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0270],
        [1.0315],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370958.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.7318, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0270],
        [1.0316],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370964.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.7318, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0080, -0.0010,  0.0049,  ...,  0.0058, -0.0014,  0.0181],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [ 0.0139, -0.0017,  0.0085,  ...,  0.0085, -0.0024,  0.0290],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2615.7915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-8.9737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5483, device='cuda:0')



h[200].sum tensor(86.6061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0000, 0.0040,  ..., 0.0096, 0.0000, 0.0186],
        [0.0355, 0.0000, 0.0218,  ..., 0.0250, 0.0000, 0.0793],
        [0.0170, 0.0000, 0.0105,  ..., 0.0157, 0.0000, 0.0428],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63766.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0836, 0.0403, 0.0345,  ..., 0.0874, 0.0452, 0.1018],
        [0.0740, 0.0000, 0.0718,  ..., 0.1002, 0.0007, 0.1624],
        [0.0746, 0.0023, 0.0772,  ..., 0.0997, 0.0044, 0.1687],
        ...,
        [0.0956, 0.1221, 0.0024,  ..., 0.0788, 0.1248, 0.0491],
        [0.0956, 0.1221, 0.0024,  ..., 0.0787, 0.1247, 0.0491],
        [0.0956, 0.1221, 0.0024,  ..., 0.0788, 0.1248, 0.0491]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652601.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9733.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(847.3954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(372.2992, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.4547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0033],
        [-0.2218],
        [ 0.2399],
        ...,
        [-3.3923],
        [-3.3862],
        [-3.3854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317722.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0270],
        [1.0316],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370964.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0271],
        [1.0316],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370969.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2612.1929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-8.8886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5459, device='cuda:0')



h[200].sum tensor(86.4887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8789, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63451.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0799, 0.0385, 0.0440,  ..., 0.0919, 0.0410, 0.1192],
        [0.0872, 0.0597, 0.0250,  ..., 0.0836, 0.0665, 0.0859],
        [0.0899, 0.0837, 0.0161,  ..., 0.0804, 0.0884, 0.0711],
        ...,
        [0.0958, 0.1222, 0.0024,  ..., 0.0789, 0.1246, 0.0493],
        [0.0957, 0.1221, 0.0024,  ..., 0.0788, 0.1246, 0.0493],
        [0.0958, 0.1221, 0.0024,  ..., 0.0789, 0.1246, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652039.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9710.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(841.8871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(373.2065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(285.1184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1045],
        [-0.1689],
        [-0.4792],
        ...,
        [-3.3971],
        [-3.3907],
        [-3.3897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334289.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0271],
        [1.0316],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370969.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0287],
        [1.0272],
        [1.0317],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370975.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0350, -0.0040,  0.0214,  ...,  0.0184, -0.0056,  0.0680],
        [ 0.0331, -0.0038,  0.0202,  ...,  0.0175, -0.0053,  0.0644],
        [ 0.0226, -0.0026,  0.0138,  ...,  0.0126, -0.0037,  0.0450],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3036.7039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0190, device='cuda:0')



h[100].sum tensor(-11.5078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7017, device='cuda:0')



h[200].sum tensor(91.3517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1409, 0.0000, 0.0860,  ..., 0.0742, 0.0000, 0.2738],
        [0.1240, 0.0000, 0.0757,  ..., 0.0663, 0.0000, 0.2426],
        [0.0769, 0.0000, 0.0470,  ..., 0.0443, 0.0000, 0.1558],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70111.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.3333,  ..., 0.2365, 0.0000, 0.6511],
        [0.0021, 0.0000, 0.2948,  ..., 0.2149, 0.0000, 0.5767],
        [0.0180, 0.0000, 0.2304,  ..., 0.1784, 0.0000, 0.4527],
        ...,
        [0.0959, 0.1222, 0.0024,  ..., 0.0789, 0.1246, 0.0493],
        [0.0958, 0.1221, 0.0024,  ..., 0.0789, 0.1245, 0.0493],
        [0.0958, 0.1222, 0.0024,  ..., 0.0789, 0.1245, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679160.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9646.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(874.1936, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(412.7150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.7082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1671],
        [ 0.1891],
        [ 0.2345],
        ...,
        [-3.4022],
        [-3.3958],
        [-3.3948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306106.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0287],
        [1.0272],
        [1.0317],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370975.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.0618, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0273],
        [1.0317],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370981.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.0618, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2957.9746, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-10.9220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6776, device='cuda:0')



h[200].sum tensor(90.6276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2631, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66995.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0913, 0.1060, 0.0068,  ..., 0.0779, 0.1086, 0.0558],
        [0.0924, 0.1163, 0.0025,  ..., 0.0771, 0.1179, 0.0495],
        [0.0928, 0.1193, 0.0022,  ..., 0.0768, 0.1208, 0.0477],
        ...,
        [0.0960, 0.1223, 0.0025,  ..., 0.0789, 0.1246, 0.0493],
        [0.0959, 0.1222, 0.0025,  ..., 0.0789, 0.1245, 0.0493],
        [0.0959, 0.1222, 0.0025,  ..., 0.0789, 0.1245, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661912., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9723.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.1272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(393.7799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(304.1942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4269],
        [-2.9462],
        [-3.2362],
        ...,
        [-3.4073],
        [-3.4009],
        [-3.3999]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307298.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0273],
        [1.0317],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370981.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8306],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.3763, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0273],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370987.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8306],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.3763, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [ 0.0160, -0.0019,  0.0098,  ...,  0.0094, -0.0026,  0.0328],
        [ 0.0161, -0.0019,  0.0099,  ...,  0.0095, -0.0027,  0.0331],
        ...,
        [ 0.0101, -0.0012,  0.0062,  ...,  0.0067, -0.0017,  0.0219],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2848.2773, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.3362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-10.1675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6266, device='cuda:0')



h[200].sum tensor(89.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0591, 0.0000, 0.0361,  ..., 0.0358, 0.0000, 0.1228],
        [0.0393, 0.0000, 0.0241,  ..., 0.0259, 0.0000, 0.0840],
        [0.0681, 0.0000, 0.0416,  ..., 0.0400, 0.0000, 0.1394],
        ...,
        [0.0162, 0.0000, 0.0099,  ..., 0.0154, 0.0000, 0.0416],
        [0.0136, 0.0000, 0.0083,  ..., 0.0135, 0.0000, 0.0345],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65584.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0513, 0.0000, 0.1225,  ..., 0.1259, 0.0000, 0.2539],
        [0.0457, 0.0000, 0.1382,  ..., 0.1329, 0.0000, 0.2815],
        [0.0307, 0.0000, 0.1766,  ..., 0.1504, 0.0000, 0.3494],
        ...,
        [0.0768, 0.0086, 0.0803,  ..., 0.1027, 0.0105, 0.1737],
        [0.0816, 0.0285, 0.0576,  ..., 0.0965, 0.0328, 0.1386],
        [0.0912, 0.0737, 0.0211,  ..., 0.0846, 0.0807, 0.0795]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655292.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9787.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(860.0085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(382.3836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.7348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3709],
        [ 0.3863],
        [ 0.3758],
        ...,
        [-0.0392],
        [-0.7512],
        [-1.7413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287596.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0273],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370987.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 590.0 event: 2950 loss: tensor(438.6412, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.9597, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370993.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.9597, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3165.3057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0201, device='cuda:0')



h[100].sum tensor(-12.1036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7456, device='cuda:0')



h[200].sum tensor(93.2468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72482.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0927, 0.1156, 0.0046,  ..., 0.0758, 0.1161, 0.0505],
        [0.0930, 0.1158, 0.0047,  ..., 0.0761, 0.1164, 0.0507],
        [0.0932, 0.1155, 0.0050,  ..., 0.0760, 0.1162, 0.0510],
        ...,
        [0.0960, 0.1221, 0.0028,  ..., 0.0787, 0.1241, 0.0494],
        [0.0959, 0.1220, 0.0028,  ..., 0.0787, 0.1240, 0.0493],
        [0.0959, 0.1220, 0.0028,  ..., 0.0787, 0.1240, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691995., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9656.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(890.6949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(424.5693, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.0105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2457],
        [-2.3922],
        [-2.5507],
        ...,
        [-3.4010],
        [-3.3918],
        [-3.3842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261612.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370993.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370999.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0082, -0.0010,  0.0051,  ...,  0.0058, -0.0014,  0.0185],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [ 0.0099, -0.0012,  0.0061,  ...,  0.0066, -0.0017,  0.0216],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2903.1536, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-10.4049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6507, device='cuda:0')



h[200].sum tensor(90.4004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7770, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0142, 0.0000, 0.0087,  ..., 0.0135, 0.0000, 0.0353],
        [0.0187, 0.0000, 0.0114,  ..., 0.0156, 0.0000, 0.0435],
        [0.0179, 0.0000, 0.0109,  ..., 0.0153, 0.0000, 0.0421],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65205.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0756, 0.0019, 0.0654,  ..., 0.0968, 0.0044, 0.1498],
        [0.0749, 0.0000, 0.0655,  ..., 0.0980, 0.0000, 0.1515],
        [0.0730, 0.0019, 0.0718,  ..., 0.1004, 0.0044, 0.1621],
        ...,
        [0.0960, 0.1221, 0.0028,  ..., 0.0788, 0.1241, 0.0493],
        [0.0960, 0.1220, 0.0028,  ..., 0.0788, 0.1241, 0.0493],
        [0.0960, 0.1220, 0.0028,  ..., 0.0788, 0.1241, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(651450.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9668.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(852.8463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(385.3120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(294.3905, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4597],
        [ 0.4658],
        [ 0.4481],
        ...,
        [-3.4103],
        [-3.4037],
        [-3.4024]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327288.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370999.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4099],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.6831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370999.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4099],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.6831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [ 0.0073, -0.0009,  0.0045,  ...,  0.0054, -0.0013,  0.0168],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2745.3579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0157, device='cuda:0')



h[100].sum tensor(-9.4127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5810, device='cuda:0')



h[200].sum tensor(88.6339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5142, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0267, 0.0000, 0.0164,  ..., 0.0205, 0.0000, 0.0629],
        [0.0059, 0.0000, 0.0036,  ..., 0.0091, 0.0000, 0.0177],
        [0.0125, 0.0000, 0.0077,  ..., 0.0128, 0.0000, 0.0322],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65398.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0822, 0.0036, 0.0482,  ..., 0.0888, 0.0162, 0.1200],
        [0.0855, 0.0340, 0.0366,  ..., 0.0853, 0.0432, 0.1019],
        [0.0828, 0.0093, 0.0494,  ..., 0.0888, 0.0170, 0.1215],
        ...,
        [0.0960, 0.1221, 0.0028,  ..., 0.0788, 0.1241, 0.0493],
        [0.0960, 0.1220, 0.0028,  ..., 0.0788, 0.1241, 0.0493],
        [0.0960, 0.1220, 0.0028,  ..., 0.0788, 0.1241, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659070.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9725.4443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(860.8187, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(382.3474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.3727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9709],
        [-0.5303],
        [-0.1252],
        ...,
        [-3.4106],
        [-3.4041],
        [-3.4030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289075.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370999.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.5317, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371005.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.5317, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0095, -0.0012,  0.0058,  ...,  0.0064, -0.0016,  0.0209],
        [ 0.0034, -0.0005,  0.0021,  ...,  0.0036, -0.0007,  0.0097],
        [ 0.0049, -0.0007,  0.0030,  ...,  0.0042, -0.0009,  0.0124],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3252.3787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-12.5329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7718, device='cuda:0')



h[200].sum tensor(94.3030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9672, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0000, 0.0094,  ..., 0.0151, 0.0000, 0.0416],
        [0.0265, 0.0000, 0.0163,  ..., 0.0205, 0.0000, 0.0626],
        [0.0113, 0.0000, 0.0070,  ..., 0.0122, 0.0000, 0.0300],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72588.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0804, 0.0000, 0.0716,  ..., 0.0911, 0.0010, 0.1527],
        [0.0805, 0.0033, 0.0680,  ..., 0.0914, 0.0055, 0.1482],
        [0.0855, 0.0312, 0.0451,  ..., 0.0856, 0.0355, 0.1131],
        ...,
        [0.0960, 0.1221, 0.0028,  ..., 0.0789, 0.1240, 0.0494],
        [0.0959, 0.1220, 0.0028,  ..., 0.0788, 0.1239, 0.0494],
        [0.0960, 0.1220, 0.0028,  ..., 0.0788, 0.1240, 0.0494]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688076.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9615.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(889.2315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(427.6654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(332.9653, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2297],
        [-0.1297],
        [-0.6652],
        ...,
        [-3.4084],
        [-3.3992],
        [-3.3954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288130.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0274],
        [1.0318],
        ...,
        [0.9975],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371005.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6846],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3340, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0274],
        [1.0318],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371010.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6846],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3340, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [ 0.0130, -0.0015,  0.0079,  ...,  0.0080, -0.0022,  0.0273],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2625.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-8.5597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5308, device='cuda:0')



h[200].sum tensor(87.0832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6071, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0561, 0.0000, 0.0344,  ..., 0.0343, 0.0000, 0.1172],
        [0.0107, 0.0000, 0.0066,  ..., 0.0113, 0.0000, 0.0267],
        [0.0133, 0.0000, 0.0082,  ..., 0.0126, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64136.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0600, 0.0000, 0.1042,  ..., 0.1143, 0.0000, 0.2179],
        [0.0773, 0.0215, 0.0532,  ..., 0.0948, 0.0261, 0.1330],
        [0.0821, 0.0432, 0.0369,  ..., 0.0894, 0.0496, 0.1067],
        ...,
        [0.0960, 0.1221, 0.0027,  ..., 0.0791, 0.1240, 0.0495],
        [0.0960, 0.1220, 0.0027,  ..., 0.0790, 0.1239, 0.0495],
        [0.0960, 0.1220, 0.0027,  ..., 0.0790, 0.1239, 0.0495]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655906.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9724.5723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(856.6888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(375.7122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.1345, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2551],
        [-0.1381],
        [-0.7904],
        ...,
        [-3.4188],
        [-3.4122],
        [-3.4110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285777.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0274],
        [1.0318],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371010.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.8817, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0274],
        [1.0318],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371010.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.8817, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0091, -0.0011,  0.0056,  ...,  0.0062, -0.0016,  0.0202],
        [ 0.0183, -0.0021,  0.0112,  ...,  0.0105, -0.0030,  0.0371],
        [ 0.0169, -0.0020,  0.0103,  ...,  0.0099, -0.0028,  0.0345],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3233.0671, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0207, device='cuda:0')



h[100].sum tensor(-12.3648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7645, device='cuda:0')



h[200].sum tensor(93.8797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8362, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0327, 0.0000, 0.0200,  ..., 0.0227, 0.0000, 0.0717],
        [0.0483, 0.0000, 0.0296,  ..., 0.0306, 0.0000, 0.1028],
        [0.0654, 0.0000, 0.0400,  ..., 0.0387, 0.0000, 0.1344],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74114.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0570, 0.0031, 0.1120,  ..., 0.1178, 0.0056, 0.2308],
        [0.0413, 0.0000, 0.1553,  ..., 0.1364, 0.0000, 0.3051],
        [0.0257, 0.0000, 0.1921,  ..., 0.1541, 0.0000, 0.3710],
        ...,
        [0.0960, 0.1221, 0.0027,  ..., 0.0791, 0.1240, 0.0495],
        [0.0960, 0.1220, 0.0027,  ..., 0.0790, 0.1239, 0.0495],
        [0.0960, 0.1220, 0.0027,  ..., 0.0790, 0.1239, 0.0495]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701615.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9534.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(894.3239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(438.6437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(339.9375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2247],
        [ 0.2463],
        [ 0.2135],
        ...,
        [-3.4188],
        [-3.4122],
        [-3.4109]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279835.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0274],
        [1.0318],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371010.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.2545, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0275],
        [1.0319],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371015.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.2545, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040, -0.0006,  0.0025,  ...,  0.0038, -0.0008,  0.0107],
        [ 0.0040, -0.0006,  0.0025,  ...,  0.0038, -0.0008,  0.0107],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2712.4297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0152, device='cuda:0')



h[100].sum tensor(-9.1066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5634, device='cuda:0')



h[200].sum tensor(87.7000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1964, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0000, 0.0061,  ..., 0.0120, 0.0000, 0.0293],
        [0.0135, 0.0000, 0.0084,  ..., 0.0138, 0.0000, 0.0363],
        [0.0110, 0.0000, 0.0068,  ..., 0.0121, 0.0000, 0.0294],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65441.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0850, 0.0178, 0.0499,  ..., 0.0861, 0.0234, 0.1201],
        [0.0836, 0.0136, 0.0550,  ..., 0.0882, 0.0173, 0.1283],
        [0.0857, 0.0330, 0.0433,  ..., 0.0859, 0.0389, 0.1111],
        ...,
        [0.0962, 0.1221, 0.0025,  ..., 0.0794, 0.1243, 0.0496],
        [0.0961, 0.1221, 0.0025,  ..., 0.0794, 0.1242, 0.0496],
        [0.0961, 0.1221, 0.0025,  ..., 0.0794, 0.1243, 0.0496]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662862.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9671.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(859.6948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.3502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.0282, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4106],
        [-0.3713],
        [-0.7351],
        ...,
        [-3.4328],
        [-3.4260],
        [-3.4248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306595.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0275],
        [1.0319],
        ...,
        [0.9974],
        [0.9965],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371015.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4031],
        [0.0000],
        [0.2898],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0275],
        [1.0319],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371020.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4031],
        [0.0000],
        [0.2898],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0206, -0.0023,  0.0126,  ...,  0.0116, -0.0033,  0.0413],
        [ 0.0242, -0.0027,  0.0148,  ...,  0.0133, -0.0038,  0.0479],
        [ 0.0035, -0.0005,  0.0022,  ...,  0.0036, -0.0007,  0.0097],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2810.2383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-9.7159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6026, device='cuda:0')



h[200].sum tensor(88.3627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0998, 0.0000, 0.0611,  ..., 0.0548, 0.0000, 0.1979],
        [0.0577, 0.0000, 0.0354,  ..., 0.0351, 0.0000, 0.1203],
        [0.0542, 0.0000, 0.0333,  ..., 0.0335, 0.0000, 0.1138],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67286.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0000, 0.2432,  ..., 0.1834, 0.0000, 0.4714],
        [0.0235, 0.0000, 0.1905,  ..., 0.1556, 0.0000, 0.3733],
        [0.0436, 0.0000, 0.1458,  ..., 0.1335, 0.0000, 0.2922],
        ...,
        [0.0963, 0.1222, 0.0024,  ..., 0.0798, 0.1246, 0.0498],
        [0.0962, 0.1221, 0.0024,  ..., 0.0797, 0.1245, 0.0497],
        [0.0962, 0.1221, 0.0024,  ..., 0.0797, 0.1245, 0.0497]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672494.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9655.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.7946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(396.1479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(305.1351, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1485],
        [ 0.1956],
        [ 0.1998],
        ...,
        [-3.4447],
        [-3.4377],
        [-3.4364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288900.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0275],
        [1.0319],
        ...,
        [0.9974],
        [0.9964],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371020.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6040],
        [0.6099],
        [0.3452],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7043, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0291],
        [1.0275],
        [1.0319],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371025.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6040],
        [0.6099],
        [0.3452],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7043, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0218, -0.0025,  0.0133,  ...,  0.0122, -0.0035,  0.0436],
        [ 0.0396, -0.0044,  0.0242,  ...,  0.0205, -0.0061,  0.0765],
        [ 0.0409, -0.0045,  0.0250,  ...,  0.0211, -0.0063,  0.0788],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3000.5034, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-10.8548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6739, device='cuda:0')



h[200].sum tensor(90.3417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1960, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1599, 0.0000, 0.0977,  ..., 0.0829, 0.0000, 0.3088],
        [0.1470, 0.0000, 0.0899,  ..., 0.0769, 0.0000, 0.2850],
        [0.1305, 0.0000, 0.0798,  ..., 0.0692, 0.0000, 0.2547],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69793.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.4116,  ..., 0.2752, 0.0000, 0.7954],
        [0.0000, 0.0000, 0.3889,  ..., 0.2630, 0.0000, 0.7517],
        [0.0000, 0.0000, 0.3466,  ..., 0.2397, 0.0000, 0.6698],
        ...,
        [0.0962, 0.1222, 0.0023,  ..., 0.0800, 0.1244, 0.0499],
        [0.0961, 0.1221, 0.0023,  ..., 0.0799, 0.1244, 0.0499],
        [0.0962, 0.1221, 0.0023,  ..., 0.0799, 0.1244, 0.0499]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682270.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9611.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(875.7154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(413.1564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(315.2932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0937],
        [-0.0643],
        [-0.0320],
        ...,
        [-3.4468],
        [-3.4400],
        [-3.4386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304051.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0291],
        [1.0275],
        [1.0319],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371025.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.4116, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0276],
        [1.0320],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371031.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.4116, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3111.4956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0191, device='cuda:0')



h[100].sum tensor(-11.4621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7086, device='cuda:0')



h[200].sum tensor(91.5050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0048],
        [0.0057, 0.0000, 0.0035,  ..., 0.0091, 0.0000, 0.0177],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73556.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0921, 0.1160, 0.0022,  ..., 0.0780, 0.1171, 0.0500],
        [0.0884, 0.0817, 0.0147,  ..., 0.0828, 0.0872, 0.0707],
        [0.0754, 0.0389, 0.0508,  ..., 0.0973, 0.0417, 0.1328],
        ...,
        [0.0960, 0.1221, 0.0024,  ..., 0.0801, 0.1239, 0.0501],
        [0.0959, 0.1220, 0.0024,  ..., 0.0801, 0.1238, 0.0501],
        [0.0959, 0.1220, 0.0024,  ..., 0.0801, 0.1238, 0.0501]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703431.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9392.4561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.9870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(436.9447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.2446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4527],
        [-0.8188],
        [-0.2970],
        ...,
        [-3.4382],
        [-3.4314],
        [-3.4301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297601.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0276],
        [1.0320],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371031.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 600.0 event: 3000 loss: tensor(388.7501, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0277],
        [1.0320],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371036.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0121, -0.0014,  0.0074,  ...,  0.0076, -0.0020,  0.0257],
        [ 0.0036, -0.0005,  0.0023,  ...,  0.0037, -0.0007,  0.0100],
        [ 0.0145, -0.0017,  0.0089,  ...,  0.0088, -0.0024,  0.0302],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3069.6650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0189, device='cuda:0')



h[100].sum tensor(-11.1409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7004, device='cuda:0')



h[200].sum tensor(90.8069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0325, 0.0000, 0.0201,  ..., 0.0228, 0.0000, 0.0718],
        [0.0499, 0.0000, 0.0307,  ..., 0.0315, 0.0000, 0.1061],
        [0.0502, 0.0000, 0.0309,  ..., 0.0317, 0.0000, 0.1067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69796.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0521, 0.0000, 0.1225,  ..., 0.1232, 0.0000, 0.2495],
        [0.0415, 0.0000, 0.1514,  ..., 0.1356, 0.0000, 0.2990],
        [0.0376, 0.0000, 0.1610,  ..., 0.1399, 0.0000, 0.3162],
        ...,
        [0.0959, 0.1220, 0.0024,  ..., 0.0803, 0.1234, 0.0504],
        [0.0958, 0.1219, 0.0024,  ..., 0.0802, 0.1233, 0.0503],
        [0.0958, 0.1220, 0.0024,  ..., 0.0802, 0.1234, 0.0503]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677022.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9452.0400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(872.5366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.6225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(313.4033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4585],
        [ 0.4591],
        [ 0.4409],
        ...,
        [-3.4324],
        [-3.4257],
        [-3.4243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274278.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0277],
        [1.0320],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371036.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.1937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0277],
        [1.0321],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371041.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.1937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0007,  0.0031,  ...,  0.0043, -0.0009,  0.0126],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [ 0.0087, -0.0010,  0.0054,  ...,  0.0061, -0.0015,  0.0195],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2727.6638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-9.0103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5578, device='cuda:0')



h[200].sum tensor(86.5725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0945, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0039, 0.0000, 0.0025,  ..., 0.0083, 0.0000, 0.0145],
        [0.0162, 0.0000, 0.0101,  ..., 0.0152, 0.0000, 0.0418],
        [0.0088, 0.0000, 0.0056,  ..., 0.0117, 0.0000, 0.0281],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64421.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0856, 0.0273, 0.0422,  ..., 0.0858, 0.0349, 0.1102],
        [0.0830, 0.0049, 0.0528,  ..., 0.0895, 0.0146, 0.1272],
        [0.0854, 0.0171, 0.0476,  ..., 0.0868, 0.0254, 0.1180],
        ...,
        [0.0959, 0.1221, 0.0023,  ..., 0.0806, 0.1233, 0.0507],
        [0.0958, 0.1220, 0.0023,  ..., 0.0806, 0.1232, 0.0506],
        [0.0959, 0.1220, 0.0023,  ..., 0.0806, 0.1232, 0.0506]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(651711.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9558.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.3109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(379.9228, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.7929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1221],
        [-0.2569],
        [-0.5882],
        ...,
        [-3.4360],
        [-3.4292],
        [-3.4278]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296160., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0277],
        [1.0321],
        ...,
        [0.9974],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371041.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.3013, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0295],
        [1.0278],
        [1.0321],
        ...,
        [0.9973],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371046.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.3013, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0006,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0014,  0.0000,  0.0012],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0014,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2979.2349, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0180, device='cuda:0')



h[100].sum tensor(-10.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6646, device='cuda:0')



h[200].sum tensor(88.9925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0050],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68759.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0886, 0.0745, 0.0190,  ..., 0.0829, 0.0792, 0.0773],
        [0.0919, 0.1043, 0.0066,  ..., 0.0797, 0.1058, 0.0593],
        [0.0920, 0.1070, 0.0061,  ..., 0.0799, 0.1087, 0.0571],
        ...,
        [0.0961, 0.1222, 0.0020,  ..., 0.0811, 0.1236, 0.0508],
        [0.0960, 0.1221, 0.0020,  ..., 0.0810, 0.1235, 0.0508],
        [0.0960, 0.1221, 0.0020,  ..., 0.0810, 0.1235, 0.0508]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673088.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9549.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(872.6829, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(402.1048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.5146, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7470],
        [-1.4082],
        [-1.8770],
        ...,
        [-3.4496],
        [-3.4428],
        [-3.4416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253573.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0295],
        [1.0278],
        [1.0321],
        ...,
        [0.9973],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371046.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2634],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.9613, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0279],
        [1.0322],
        ...,
        [0.9973],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371051.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2634],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.9613, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0157, -0.0018,  0.0097,  ...,  0.0094, -0.0025,  0.0323],
        [ 0.0228, -0.0025,  0.0140,  ...,  0.0127, -0.0036,  0.0455],
        [ 0.0118, -0.0014,  0.0073,  ...,  0.0076, -0.0019,  0.0253],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3053.7905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-11.0753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6965, device='cuda:0')



h[200].sum tensor(89.4259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0997, 0.0000, 0.0613,  ..., 0.0551, 0.0000, 0.1982],
        [0.0590, 0.0000, 0.0364,  ..., 0.0360, 0.0000, 0.1230],
        [0.0366, 0.0000, 0.0226,  ..., 0.0249, 0.0000, 0.0793],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70964.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0109, 0.0000, 0.2122,  ..., 0.1714, 0.0000, 0.4207],
        [0.0296, 0.0000, 0.1709,  ..., 0.1513, 0.0000, 0.3452],
        [0.0501, 0.0000, 0.1188,  ..., 0.1283, 0.0029, 0.2536],
        ...,
        [0.0963, 0.1223, 0.0017,  ..., 0.0815, 0.1245, 0.0508],
        [0.0963, 0.1222, 0.0017,  ..., 0.0815, 0.1244, 0.0508],
        [0.0963, 0.1222, 0.0017,  ..., 0.0815, 0.1244, 0.0508]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688384.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9425.6504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(875.2516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(422.7861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.8715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4048],
        [ 0.3482],
        [ 0.0308],
        ...,
        [-3.4756],
        [-3.4684],
        [-3.4669]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317533.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0279],
        [1.0322],
        ...,
        [0.9973],
        [0.9964],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371051.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0297],
        [1.0280],
        [1.0323],
        ...,
        [0.9973],
        [0.9963],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371056.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2778.5840, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-9.4317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5942, device='cuda:0')



h[200].sum tensor(85.8257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0151, 0.0000, 0.0095,  ..., 0.0144, 0.0000, 0.0374],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66700.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0746, 0.0247, 0.0583,  ..., 0.1014, 0.0298, 0.1477],
        [0.0876, 0.0644, 0.0209,  ..., 0.0864, 0.0725, 0.0831],
        [0.0919, 0.0987, 0.0077,  ..., 0.0814, 0.1023, 0.0629],
        ...,
        [0.0966, 0.1224, 0.0014,  ..., 0.0820, 0.1251, 0.0509],
        [0.0965, 0.1223, 0.0014,  ..., 0.0819, 0.1250, 0.0509],
        [0.0965, 0.1224, 0.0014,  ..., 0.0820, 0.1250, 0.0509]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671000.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9507.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(858.4695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(398.8802, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(295.0551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0336],
        [-0.3692],
        [-0.6883],
        ...,
        [-3.4946],
        [-3.4878],
        [-3.4868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345519.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0297],
        [1.0280],
        [1.0323],
        ...,
        [0.9973],
        [0.9963],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371056.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(176.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0281],
        [1.0323],
        ...,
        [0.9973],
        [0.9963],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371061.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(176.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2482.6506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0130, device='cuda:0')



h[100].sum tensor(-7.6196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4814, device='cuda:0')



h[200].sum tensor(82.2804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0049],
        [0.0063, 0.0000, 0.0040,  ..., 0.0097, 0.0000, 0.0189],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61675.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0927, 0.1162, 0.0008,  ..., 0.0800, 0.1184, 0.0511],
        [0.0919, 0.1037, 0.0063,  ..., 0.0816, 0.1074, 0.0595],
        [0.0889, 0.0696, 0.0192,  ..., 0.0854, 0.0774, 0.0810],
        ...,
        [0.0966, 0.1225, 0.0013,  ..., 0.0822, 0.1254, 0.0510],
        [0.0966, 0.1224, 0.0013,  ..., 0.0821, 0.1253, 0.0510],
        [0.0966, 0.1225, 0.0013,  ..., 0.0821, 0.1253, 0.0510]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646243.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9685.3037, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.6995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(361.8801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(271.4403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3648],
        [-3.0802],
        [-2.6364],
        ...,
        [-3.5017],
        [-3.4956],
        [-3.4952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318897.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0281],
        [1.0323],
        ...,
        [0.9973],
        [0.9963],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371061.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5918],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.3479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0282],
        [1.0325],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371067.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5918],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.3479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [ 0.0110, -0.0013,  0.0068,  ...,  0.0072, -0.0018,  0.0238],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2897.2007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-10.0985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6401, device='cuda:0')



h[200].sum tensor(87.0232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5852, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0765, 0.0000, 0.0472,  ..., 0.0444, 0.0000, 0.1554],
        [0.0091, 0.0000, 0.0057,  ..., 0.0110, 0.0000, 0.0240],
        [0.0113, 0.0000, 0.0070,  ..., 0.0121, 0.0000, 0.0282],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67096.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0271, 0.0000, 0.1732,  ..., 0.1577, 0.0000, 0.3577],
        [0.0631, 0.0130, 0.0807,  ..., 0.1146, 0.0181, 0.1916],
        [0.0777, 0.0306, 0.0432,  ..., 0.0982, 0.0388, 0.1256],
        ...,
        [0.0966, 0.1226, 0.0014,  ..., 0.0822, 0.1252, 0.0512],
        [0.0965, 0.1225, 0.0014,  ..., 0.0821, 0.1251, 0.0511],
        [0.0965, 0.1226, 0.0014,  ..., 0.0821, 0.1251, 0.0511]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664664.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9563.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(865.8736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(394.1975, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.6946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3634],
        [ 0.2564],
        [ 0.0690],
        ...,
        [-3.5024],
        [-3.4950],
        [-3.4933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304310.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0282],
        [1.0325],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371067.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3215],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(332.2396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0300],
        [1.0282],
        [1.0326],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371073.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3215],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(332.2396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0191, -0.0021,  0.0118,  ...,  0.0110, -0.0030,  0.0387],
        [ 0.0040, -0.0005,  0.0026,  ...,  0.0040, -0.0008,  0.0109],
        [ 0.0054, -0.0007,  0.0034,  ...,  0.0046, -0.0010,  0.0135],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3604.1074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0245, device='cuda:0')



h[100].sum tensor(-14.3479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9075, device='cuda:0')



h[200].sum tensor(94.8531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.4245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.0000, 0.0163,  ..., 0.0207, 0.0000, 0.0622],
        [0.0421, 0.0000, 0.0262,  ..., 0.0283, 0.0000, 0.0922],
        [0.0326, 0.0000, 0.0204,  ..., 0.0238, 0.0000, 0.0746],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76712.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0577, 0.0000, 0.1206,  ..., 0.1208, 0.0000, 0.2465],
        [0.0550, 0.0000, 0.1287,  ..., 0.1246, 0.0000, 0.2605],
        [0.0607, 0.0000, 0.1175,  ..., 0.1183, 0.0000, 0.2392],
        ...,
        [0.0965, 0.1228, 0.0015,  ..., 0.0820, 0.1249, 0.0514],
        [0.0964, 0.1227, 0.0015,  ..., 0.0820, 0.1248, 0.0513],
        [0.0964, 0.1227, 0.0015,  ..., 0.0820, 0.1248, 0.0513]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702686.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9335.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(899.6628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(454.6940, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.1576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4741],
        [ 0.4921],
        [ 0.4987],
        ...,
        [-3.4966],
        [-3.4893],
        [-3.4876]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299736.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0300],
        [1.0282],
        [1.0326],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371073.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.2830, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0282],
        [1.0327],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371078.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.2830, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [ 0.0046, -0.0006,  0.0029,  ...,  0.0042, -0.0008,  0.0119],
        [ 0.0033, -0.0005,  0.0021,  ...,  0.0036, -0.0007,  0.0095],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3370.7224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0220, device='cuda:0')



h[100].sum tensor(-12.8438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8148, device='cuda:0')



h[200].sum tensor(92.2592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7459, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0264, 0.0000, 0.0165,  ..., 0.0203, 0.0000, 0.0608],
        [0.0150, 0.0000, 0.0094,  ..., 0.0144, 0.0000, 0.0375],
        [0.0336, 0.0000, 0.0210,  ..., 0.0243, 0.0000, 0.0765],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74784., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0657, 0.0000, 0.0943,  ..., 0.1109, 0.0000, 0.2032],
        [0.0735, 0.0000, 0.0769,  ..., 0.1025, 0.0000, 0.1716],
        [0.0712, 0.0000, 0.0867,  ..., 0.1056, 0.0000, 0.1866],
        ...,
        [0.0965, 0.1229, 0.0017,  ..., 0.0819, 0.1247, 0.0515],
        [0.0964, 0.1228, 0.0017,  ..., 0.0818, 0.1246, 0.0515],
        [0.0964, 0.1228, 0.0017,  ..., 0.0818, 0.1246, 0.0515]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704330., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9385.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(892.0507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(439.4182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(336.6251, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6171],
        [ 0.6198],
        [ 0.6085],
        ...,
        [-3.4900],
        [-3.4827],
        [-3.4810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278678.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0282],
        [1.0327],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371078.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0282],
        [1.0328],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371084.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045, -0.0006,  0.0028,  ...,  0.0042, -0.0008,  0.0118],
        [ 0.0040, -0.0005,  0.0025,  ...,  0.0039, -0.0008,  0.0109],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2634.5542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-8.3138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5274, device='cuda:0')



h[200].sum tensor(84.1326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5446, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0254, 0.0000, 0.0160,  ..., 0.0204, 0.0000, 0.0615],
        [0.0077, 0.0000, 0.0049,  ..., 0.0110, 0.0000, 0.0241],
        [0.0041, 0.0000, 0.0026,  ..., 0.0087, 0.0000, 0.0150],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64225.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0785, 0.0057, 0.0603,  ..., 0.0961, 0.0108, 0.1434],
        [0.0856, 0.0405, 0.0354,  ..., 0.0884, 0.0459, 0.1042],
        [0.0895, 0.0759, 0.0186,  ..., 0.0839, 0.0815, 0.0783],
        ...,
        [0.0966, 0.1231, 0.0018,  ..., 0.0817, 0.1246, 0.0516],
        [0.0965, 0.1230, 0.0018,  ..., 0.0817, 0.1245, 0.0515],
        [0.0965, 0.1230, 0.0018,  ..., 0.0817, 0.1245, 0.0515]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654628.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9529.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(848.4288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(373.8451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.1904, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4185],
        [-1.9765],
        [-2.5608],
        ...,
        [-3.4884],
        [-3.4811],
        [-3.4794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293028.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0282],
        [1.0328],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371084.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 610.0 event: 3050 loss: tensor(474.2155, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.5000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0283],
        [1.0329],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371090.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.5000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099, -0.0011,  0.0062,  ...,  0.0067, -0.0016,  0.0219],
        [ 0.0198, -0.0021,  0.0122,  ...,  0.0114, -0.0030,  0.0402],
        [ 0.0087, -0.0010,  0.0054,  ...,  0.0061, -0.0014,  0.0195],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3293.8457, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-12.2485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7717, device='cuda:0')



h[200].sum tensor(91.6048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0666, 0.0000, 0.0411,  ..., 0.0397, 0.0000, 0.1377],
        [0.0420, 0.0000, 0.0261,  ..., 0.0282, 0.0000, 0.0922],
        [0.0429, 0.0000, 0.0266,  ..., 0.0286, 0.0000, 0.0938],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73665.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0228, 0.0000, 0.1937,  ..., 0.1587, 0.0000, 0.3801],
        [0.0371, 0.0000, 0.1633,  ..., 0.1435, 0.0000, 0.3242],
        [0.0442, 0.0000, 0.1465,  ..., 0.1355, 0.0000, 0.2943],
        ...,
        [0.0967, 0.1233, 0.0020,  ..., 0.0815, 0.1246, 0.0515],
        [0.0966, 0.1232, 0.0020,  ..., 0.0815, 0.1245, 0.0515],
        [0.0966, 0.1233, 0.0020,  ..., 0.0815, 0.1245, 0.0515]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696509.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9401.9121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(885.2796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(430.8589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.6475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3841],
        [ 0.4244],
        [ 0.4685],
        ...,
        [-3.4893],
        [-3.4820],
        [-3.4803]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270603.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0283],
        [1.0329],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371090.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.3682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0283],
        [1.0330],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371096.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.3682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3354.1826, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0215, device='cuda:0')



h[100].sum tensor(-12.5520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7959, device='cuda:0')



h[200].sum tensor(92.5327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4040, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75981.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0932, 0.1204, 0.0016,  ..., 0.0789, 0.1205, 0.0495],
        [0.0931, 0.1153, 0.0034,  ..., 0.0796, 0.1159, 0.0533],
        [0.0923, 0.1028, 0.0092,  ..., 0.0805, 0.1048, 0.0616],
        ...,
        [0.0968, 0.1236, 0.0021,  ..., 0.0814, 0.1247, 0.0514],
        [0.0967, 0.1235, 0.0021,  ..., 0.0813, 0.1246, 0.0514],
        [0.0967, 0.1235, 0.0021,  ..., 0.0813, 0.1246, 0.0514]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711224.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9394.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(896.5004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(443.6976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(345.2635, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3583],
        [-1.9579],
        [-1.4703],
        ...,
        [-3.4914],
        [-3.4842],
        [-3.4826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270975.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0283],
        [1.0330],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371096.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.5737, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0283],
        [1.0331],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371101.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.5737, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [ 0.0111, -0.0012,  0.0069,  ...,  0.0073, -0.0018,  0.0241],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2839.2102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-9.3880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5998, device='cuda:0')



h[200].sum tensor(87.2379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8548, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0050],
        [0.0152, 0.0000, 0.0095,  ..., 0.0144, 0.0000, 0.0379],
        [0.0211, 0.0000, 0.0132,  ..., 0.0178, 0.0000, 0.0512],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66598.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0885, 0.0693, 0.0223,  ..., 0.0841, 0.0750, 0.0818],
        [0.0782, 0.0286, 0.0568,  ..., 0.0964, 0.0322, 0.1392],
        [0.0698, 0.0098, 0.0852,  ..., 0.1057, 0.0113, 0.1848],
        ...,
        [0.0970, 0.1239, 0.0021,  ..., 0.0813, 0.1248, 0.0513],
        [0.0969, 0.1238, 0.0021,  ..., 0.0813, 0.1247, 0.0512],
        [0.0969, 0.1238, 0.0021,  ..., 0.0813, 0.1247, 0.0513]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662761.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9553.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(858.6619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(385.0172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(299.8379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8156],
        [-0.1596],
        [ 0.2583],
        ...,
        [-3.4990],
        [-3.4917],
        [-3.4900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291965.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0283],
        [1.0331],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371101.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3999],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3744, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0283],
        [1.0332],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371107.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3999],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3744, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070, -0.0008,  0.0044,  ...,  0.0053, -0.0012,  0.0165],
        [ 0.0060, -0.0007,  0.0037,  ...,  0.0049, -0.0010,  0.0146],
        [ 0.0216, -0.0023,  0.0133,  ...,  0.0122, -0.0033,  0.0435],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2609.6938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-7.9893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5118, device='cuda:0')



h[200].sum tensor(84.9025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0178, 0.0000, 0.0111,  ..., 0.0156, 0.0000, 0.0427],
        [0.0532, 0.0000, 0.0329,  ..., 0.0334, 0.0000, 0.1127],
        [0.0340, 0.0000, 0.0212,  ..., 0.0245, 0.0000, 0.0774],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62943.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0728, 0.0157, 0.0690,  ..., 0.1024, 0.0211, 0.1609],
        [0.0550, 0.0000, 0.1183,  ..., 0.1230, 0.0000, 0.2450],
        [0.0553, 0.0000, 0.1249,  ..., 0.1229, 0.0000, 0.2524],
        ...,
        [0.0973, 0.1242, 0.0021,  ..., 0.0813, 0.1252, 0.0511],
        [0.0972, 0.1241, 0.0021,  ..., 0.0813, 0.1251, 0.0511],
        [0.0972, 0.1242, 0.0021,  ..., 0.0813, 0.1251, 0.0511]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649754.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9620.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(843.0209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(364.8831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.6335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1252],
        [ 0.4011],
        [ 0.5039],
        ...,
        [-3.5122],
        [-3.5047],
        [-3.5028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322571.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0283],
        [1.0332],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371107.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4607],
        [0.4805],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.4531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0284],
        [1.0333],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371112.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4607],
        [0.4805],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.4531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0228, -0.0024,  0.0140,  ...,  0.0128, -0.0034,  0.0457],
        [ 0.0163, -0.0018,  0.0100,  ...,  0.0097, -0.0025,  0.0335],
        [ 0.0083, -0.0010,  0.0051,  ...,  0.0059, -0.0014,  0.0188],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2900.8064, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0166, device='cuda:0')



h[100].sum tensor(-9.7347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6159, device='cuda:0')



h[200].sum tensor(88.2085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0638, 0.0000, 0.0393,  ..., 0.0384, 0.0000, 0.1323],
        [0.0662, 0.0000, 0.0408,  ..., 0.0395, 0.0000, 0.1367],
        [0.0542, 0.0000, 0.0335,  ..., 0.0339, 0.0000, 0.1147],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68598.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0351, 0.0000, 0.1661,  ..., 0.1448, 0.0000, 0.3297],
        [0.0389, 0.0000, 0.1548,  ..., 0.1410, 0.0000, 0.3116],
        [0.0485, 0.0000, 0.1335,  ..., 0.1306, 0.0000, 0.2729],
        ...,
        [0.0976, 0.1246, 0.0020,  ..., 0.0814, 0.1255, 0.0511],
        [0.0975, 0.1244, 0.0020,  ..., 0.0813, 0.1254, 0.0511],
        [0.0975, 0.1245, 0.0020,  ..., 0.0813, 0.1254, 0.0511]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678553.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9569.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(869.4603, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(399.7981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(311.1682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4815],
        [ 0.4670],
        [ 0.4492],
        ...,
        [-3.5252],
        [-3.5178],
        [-3.5161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313321., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0284],
        [1.0333],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371112.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3542],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.8549, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0285],
        [1.0333],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371117.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3542],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.8549, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [ 0.0103, -0.0012,  0.0064,  ...,  0.0069, -0.0016,  0.0225],
        [ 0.0030, -0.0004,  0.0019,  ...,  0.0035, -0.0006,  0.0090],
        [ 0.0061, -0.0007,  0.0038,  ...,  0.0049, -0.0010,  0.0147]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3394.7764, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.9762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0218, device='cuda:0')



h[100].sum tensor(-12.6754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8054, device='cuda:0')



h[200].sum tensor(93.8066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0049],
        ...,
        [0.0211, 0.0000, 0.0132,  ..., 0.0181, 0.0000, 0.0516],
        [0.0364, 0.0000, 0.0227,  ..., 0.0259, 0.0000, 0.0823],
        [0.0119, 0.0000, 0.0075,  ..., 0.0131, 0.0000, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76698.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0873, 0.0619, 0.0215,  ..., 0.0867, 0.0695, 0.0849],
        [0.0907, 0.0831, 0.0152,  ..., 0.0834, 0.0876, 0.0737],
        [0.0926, 0.0957, 0.0117,  ..., 0.0813, 0.0986, 0.0667],
        ...,
        [0.0743, 0.0029, 0.0871,  ..., 0.1081, 0.0057, 0.1876],
        [0.0718, 0.0000, 0.0943,  ..., 0.1108, 0.0000, 0.1995],
        [0.0819, 0.0230, 0.0603,  ..., 0.0992, 0.0282, 0.1450]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716796.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9559.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(910.4997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(445.6683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(354.5084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4068],
        [-0.6119],
        [-0.5973],
        ...,
        [ 0.1356],
        [ 0.1861],
        [-0.3923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272084.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0285],
        [1.0333],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371117.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.1078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0285],
        [1.0334],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371122.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.1078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2753.5132, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-8.8027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5603, device='cuda:0')



h[200].sum tensor(86.6393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0049],
        [0.0121, 0.0000, 0.0076,  ..., 0.0130, 0.0000, 0.0322],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65140.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0920, 0.0929, 0.0129,  ..., 0.0816, 0.0956, 0.0680],
        [0.0857, 0.0485, 0.0340,  ..., 0.0895, 0.0497, 0.1023],
        [0.0699, 0.0194, 0.0801,  ..., 0.1073, 0.0198, 0.1808],
        ...,
        [0.0981, 0.1251, 0.0019,  ..., 0.0814, 0.1257, 0.0513],
        [0.0980, 0.1250, 0.0019,  ..., 0.0814, 0.1256, 0.0513],
        [0.0980, 0.1250, 0.0019,  ..., 0.0814, 0.1256, 0.0513]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657632.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9791.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(861.9922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(375.8242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.1715, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6634],
        [-0.1835],
        [ 0.1264],
        ...,
        [-3.5395],
        [-3.5320],
        [-3.5301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294953.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0285],
        [1.0334],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371122.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.7710, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0286],
        [1.0335],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371128.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.7710, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0213, -0.0022,  0.0131,  ...,  0.0120, -0.0032,  0.0428],
        [ 0.0203, -0.0021,  0.0125,  ...,  0.0116, -0.0031,  0.0411],
        [ 0.0265, -0.0027,  0.0162,  ...,  0.0144, -0.0039,  0.0524],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2547.9790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.1066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0131, device='cuda:0')



h[100].sum tensor(-7.5455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4856, device='cuda:0')



h[200].sum tensor(84.2057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0577, 0.0000, 0.0356,  ..., 0.0355, 0.0000, 0.1211],
        [0.0867, 0.0000, 0.0533,  ..., 0.0491, 0.0000, 0.1748],
        [0.0747, 0.0000, 0.0460,  ..., 0.0435, 0.0000, 0.1526],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62837.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0428, 0.0000, 0.1499,  ..., 0.1368, 0.0000, 0.3026],
        [0.0258, 0.0000, 0.1917,  ..., 0.1561, 0.0000, 0.3774],
        [0.0258, 0.0000, 0.1948,  ..., 0.1565, 0.0000, 0.3811],
        ...,
        [0.0984, 0.1253, 0.0019,  ..., 0.0815, 0.1257, 0.0516],
        [0.0983, 0.1252, 0.0019,  ..., 0.0814, 0.1256, 0.0515],
        [0.0983, 0.1252, 0.0019,  ..., 0.0814, 0.1256, 0.0515]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654183.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9838.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(849.7375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(363.7546, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.0340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3517],
        [ 0.4378],
        [ 0.4575],
        ...,
        [-3.5350],
        [-3.5099],
        [-3.4248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313981.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0286],
        [1.0335],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371128.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2605],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.7390, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0286],
        [1.0335],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371128.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2605],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.7390, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [ 0.0041, -0.0005,  0.0026,  ...,  0.0040, -0.0008,  0.0111],
        [ 0.0035, -0.0005,  0.0023,  ...,  0.0037, -0.0007,  0.0101],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3369.6382, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0217, device='cuda:0')



h[100].sum tensor(-12.4449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8024, device='cuda:0')



h[200].sum tensor(93.2574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0000, 0.0027,  ..., 0.0086, 0.0000, 0.0151],
        [0.0149, 0.0000, 0.0093,  ..., 0.0142, 0.0000, 0.0374],
        [0.0320, 0.0000, 0.0200,  ..., 0.0235, 0.0000, 0.0738],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76276.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0787, 0.0000, 0.0614,  ..., 0.0968, 0.0000, 0.1459],
        [0.0766, 0.0109, 0.0738,  ..., 0.0999, 0.0122, 0.1643],
        [0.0659, 0.0000, 0.1078,  ..., 0.1122, 0.0000, 0.2198],
        ...,
        [0.0984, 0.1253, 0.0019,  ..., 0.0815, 0.1257, 0.0516],
        [0.0983, 0.1252, 0.0019,  ..., 0.0814, 0.1256, 0.0515],
        [0.0983, 0.1252, 0.0019,  ..., 0.0814, 0.1256, 0.0515]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(725856.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9666.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(901.7765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.2577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(349.2692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4848],
        [ 0.4964],
        [ 0.5111],
        ...,
        [-3.5436],
        [-3.5361],
        [-3.5342]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291333.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0286],
        [1.0335],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371128.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(168.0956, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0287],
        [1.0336],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371133.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(168.0956, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2468.8643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.3237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0124, device='cuda:0')



h[100].sum tensor(-7.0781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4592, device='cuda:0')



h[200].sum tensor(82.8447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0050],
        [0.0046, 0.0000, 0.0029,  ..., 0.0088, 0.0000, 0.0159],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60613.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0941, 0.0983, 0.0119,  ..., 0.0799, 0.0991, 0.0672],
        [0.0942, 0.1041, 0.0084,  ..., 0.0805, 0.1050, 0.0628],
        [0.0928, 0.0881, 0.0152,  ..., 0.0822, 0.0913, 0.0726],
        ...,
        [0.0987, 0.1255, 0.0019,  ..., 0.0816, 0.1257, 0.0520],
        [0.0986, 0.1254, 0.0019,  ..., 0.0816, 0.1255, 0.0520],
        [0.0986, 0.1254, 0.0019,  ..., 0.0816, 0.1256, 0.0520]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639928.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9940.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(836.4227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(351.4187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(268.1608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6854],
        [-1.9852],
        [-2.1901],
        ...,
        [-3.5482],
        [-3.5406],
        [-3.5386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319959.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0287],
        [1.0336],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371133.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 620.0 event: 3100 loss: tensor(428.4744, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2710],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(258.9158, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0288],
        [1.0336],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371137.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2710],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(258.9158, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0005,  0.0027,  ...,  0.0041, -0.0008,  0.0116],
        [ 0.0040, -0.0005,  0.0026,  ...,  0.0039, -0.0007,  0.0110],
        [ 0.0096, -0.0011,  0.0060,  ...,  0.0066, -0.0015,  0.0214],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3105.9922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0191, device='cuda:0')



h[100].sum tensor(-10.8921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7073, device='cuda:0')



h[200].sum tensor(89.2034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0000, 0.0099,  ..., 0.0147, 0.0000, 0.0393],
        [0.0336, 0.0000, 0.0209,  ..., 0.0243, 0.0000, 0.0769],
        [0.0146, 0.0000, 0.0092,  ..., 0.0148, 0.0000, 0.0395],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71106.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0827, 0.0200, 0.0566,  ..., 0.0937, 0.0243, 0.1379],
        [0.0774, 0.0000, 0.0779,  ..., 0.1007, 0.0000, 0.1715],
        [0.0828, 0.0128, 0.0603,  ..., 0.0945, 0.0139, 0.1426],
        ...,
        [0.0991, 0.1257, 0.0018,  ..., 0.0819, 0.1258, 0.0525],
        [0.0990, 0.1256, 0.0018,  ..., 0.0818, 0.1257, 0.0525],
        [0.0990, 0.1256, 0.0018,  ..., 0.0818, 0.1257, 0.0525]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687781.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9913.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.2663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(412.6332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.2503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2051],
        [-0.0809],
        [-0.4202],
        ...,
        [-3.5585],
        [-3.5510],
        [-3.5491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264678.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0288],
        [1.0336],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371137.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.5771],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(267.5365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0289],
        [1.0337],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371142.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.5771],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(267.5365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [ 0.0257, -0.0026,  0.0158,  ...,  0.0141, -0.0038,  0.0512],
        [ 0.0181, -0.0019,  0.0112,  ...,  0.0106, -0.0027,  0.0371],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3168.6177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-11.2729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7308, device='cuda:0')



h[200].sum tensor(89.7003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0611, 0.0000, 0.0377,  ..., 0.0371, 0.0000, 0.1278],
        [0.0653, 0.0000, 0.0402,  ..., 0.0385, 0.0000, 0.1332],
        [0.1288, 0.0000, 0.0790,  ..., 0.0689, 0.0000, 0.2529],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72324.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0207, 0.0000, 0.2078,  ..., 0.1661, 0.0000, 0.4115],
        [0.0120, 0.0000, 0.2512,  ..., 0.1900, 0.0000, 0.4952],
        [0.0000, 0.0000, 0.3307,  ..., 0.2292, 0.0000, 0.6428],
        ...,
        [0.0994, 0.1260, 0.0017,  ..., 0.0820, 0.1261, 0.0528],
        [0.0993, 0.1259, 0.0017,  ..., 0.0819, 0.1260, 0.0527],
        [0.0993, 0.1259, 0.0017,  ..., 0.0820, 0.1260, 0.0527]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693137.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9920.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.5553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(424.2778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.0593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2769],
        [ 0.2719],
        [ 0.2683],
        ...,
        [-3.5714],
        [-3.5637],
        [-3.5616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283338.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0289],
        [1.0337],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371142.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.6106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0290],
        [1.0338],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371148.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.6106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048, -0.0006,  0.0030,  ...,  0.0043, -0.0008,  0.0124],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [ 0.0065, -0.0008,  0.0041,  ...,  0.0051, -0.0011,  0.0156],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3806.7058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0257, device='cuda:0')



h[100].sum tensor(-15.0079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9523, device='cuda:0')



h[200].sum tensor(97.1332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.2338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0332, 0.0000, 0.0206,  ..., 0.0234, 0.0000, 0.0737],
        [0.0405, 0.0000, 0.0252,  ..., 0.0275, 0.0000, 0.0898],
        [0.0356, 0.0000, 0.0220,  ..., 0.0246, 0.0000, 0.0782],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83042.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0248, 0.0000, 0.1968,  ..., 0.1602, 0.0000, 0.3906],
        [0.0287, 0.0000, 0.1916,  ..., 0.1569, 0.0000, 0.3794],
        [0.0301, 0.0000, 0.1867,  ..., 0.1556, 0.0000, 0.3714],
        ...,
        [0.0997, 0.1262, 0.0016,  ..., 0.0820, 0.1262, 0.0527],
        [0.0996, 0.1261, 0.0016,  ..., 0.0819, 0.1261, 0.0526],
        [0.0996, 0.1261, 0.0016,  ..., 0.0819, 0.1261, 0.0526]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(750307.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9901.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(928.0706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(493.4811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(376.8456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2794],
        [ 0.2740],
        [ 0.2674],
        ...,
        [-3.5825],
        [-3.5745],
        [-3.5722]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281102.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0290],
        [1.0338],
        ...,
        [0.9973],
        [0.9963],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371148.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.4923, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0291],
        [1.0339],
        ...,
        [0.9973],
        [0.9962],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371153.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.4923, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2935.5842, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-9.8205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6269, device='cuda:0')



h[200].sum tensor(87.8150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68631.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0965, 0.1137, 0.0059,  ..., 0.0788, 0.1122, 0.0586],
        [0.0969, 0.1120, 0.0067,  ..., 0.0791, 0.1106, 0.0605],
        [0.0971, 0.1117, 0.0070,  ..., 0.0790, 0.1104, 0.0607],
        ...,
        [0.1000, 0.1265, 0.0016,  ..., 0.0820, 0.1263, 0.0528],
        [0.0999, 0.1264, 0.0016,  ..., 0.0819, 0.1262, 0.0527],
        [0.0999, 0.1264, 0.0016,  ..., 0.0820, 0.1262, 0.0527]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678943.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10024.0596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.0280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(407.3112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(305.5856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2357],
        [-2.0655],
        [-1.9745],
        ...,
        [-3.5912],
        [-3.5830],
        [-3.5807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330696.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0291],
        [1.0339],
        ...,
        [0.9973],
        [0.9962],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371153.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.6724, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0291],
        [1.0339],
        ...,
        [0.9972],
        [0.9962],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371158.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.6724, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083, -0.0009,  0.0051,  ...,  0.0059, -0.0013,  0.0188],
        [ 0.0032, -0.0004,  0.0021,  ...,  0.0036, -0.0006,  0.0095],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0015,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2841.0356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-9.1974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6001, device='cuda:0')



h[200].sum tensor(87.1854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0100, 0.0000, 0.0063,  ..., 0.0119, 0.0000, 0.0284],
        [0.0110, 0.0000, 0.0069,  ..., 0.0124, 0.0000, 0.0302],
        [0.0058, 0.0000, 0.0038,  ..., 0.0100, 0.0000, 0.0206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66003.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0874, 0.0204, 0.0511,  ..., 0.0896, 0.0254, 0.1276],
        [0.0888, 0.0242, 0.0456,  ..., 0.0886, 0.0303, 0.1196],
        [0.0927, 0.0573, 0.0298,  ..., 0.0842, 0.0631, 0.0952],
        ...,
        [0.1001, 0.1266, 0.0016,  ..., 0.0819, 0.1262, 0.0529],
        [0.1000, 0.1265, 0.0016,  ..., 0.0819, 0.1260, 0.0529],
        [0.1000, 0.1265, 0.0016,  ..., 0.0819, 0.1260, 0.0529]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665282.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10088.9434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.7302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(388.9331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.2639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3393e-04],
        [-3.1551e-01],
        [-9.1670e-01],
        ...,
        [-3.5927e+00],
        [-3.5846e+00],
        [-3.5823e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325778.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0291],
        [1.0339],
        ...,
        [0.9972],
        [0.9962],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371158.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.9375, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0292],
        [1.0339],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371164., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.9375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3314.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-11.8823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7729, device='cuda:0')



h[200].sum tensor(92.7867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9872, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0053]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73749.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0964, 0.1222, 0.0021,  ..., 0.0791, 0.1202, 0.0520],
        [0.0967, 0.1233, 0.0016,  ..., 0.0795, 0.1215, 0.0516],
        [0.0968, 0.1236, 0.0015,  ..., 0.0795, 0.1219, 0.0513],
        ...,
        [0.1002, 0.1266, 0.0018,  ..., 0.0818, 0.1257, 0.0531],
        [0.1001, 0.1265, 0.0018,  ..., 0.0817, 0.1256, 0.0530],
        [0.1001, 0.1265, 0.0018,  ..., 0.0817, 0.1256, 0.0530]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700326.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9983.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(895.6840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(434.3765, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.9548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1713],
        [-3.2206],
        [-3.1871],
        ...,
        [-3.5854],
        [-3.5775],
        [-3.5752]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287524.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0292],
        [1.0339],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371164., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5527],
        [0.3008],
        [0.4023],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0292],
        [1.0340],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371169.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5527],
        [0.3008],
        [0.4023],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0006,  0.0031,  ...,  0.0043, -0.0009,  0.0127],
        [ 0.0229, -0.0023,  0.0141,  ...,  0.0127, -0.0033,  0.0459],
        [ 0.0183, -0.0019,  0.0112,  ...,  0.0106, -0.0027,  0.0374],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3065.5176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0180, device='cuda:0')



h[100].sum tensor(-10.3244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6664, device='cuda:0')



h[200].sum tensor(90.3940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0641, 0.0000, 0.0395,  ..., 0.0383, 0.0000, 0.1333],
        [0.0616, 0.0000, 0.0379,  ..., 0.0372, 0.0000, 0.1287],
        [0.0815, 0.0000, 0.0500,  ..., 0.0465, 0.0000, 0.1654],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69907.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0474, 0.0000, 0.1442,  ..., 0.1326, 0.0000, 0.2928],
        [0.0279, 0.0000, 0.1937,  ..., 0.1546, 0.0000, 0.3800],
        [0.0094, 0.0000, 0.2514,  ..., 0.1823, 0.0000, 0.4852],
        ...,
        [0.1002, 0.1267, 0.0021,  ..., 0.0815, 0.1252, 0.0533],
        [0.1001, 0.1266, 0.0021,  ..., 0.0814, 0.1250, 0.0533],
        [0.1001, 0.1266, 0.0021,  ..., 0.0814, 0.1250, 0.0533]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681760.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10071.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.8027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(409.1832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(310.9838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1316],
        [ 0.1971],
        [ 0.1987],
        ...,
        [-3.5740],
        [-3.5660],
        [-3.5636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280975.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0292],
        [1.0340],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371169.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3804],
        [0.0000],
        [0.4143],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8927, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0292],
        [1.0340],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371174.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3804],
        [0.0000],
        [0.4143],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8927, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0141, -0.0015,  0.0087,  ...,  0.0086, -0.0021,  0.0297],
        [ 0.0217, -0.0022,  0.0133,  ...,  0.0122, -0.0031,  0.0436],
        [ 0.0052, -0.0006,  0.0033,  ...,  0.0045, -0.0009,  0.0133],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2935.8398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-9.5129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6198, device='cuda:0')



h[200].sum tensor(89.0599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2166, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1002, 0.0000, 0.0614,  ..., 0.0552, 0.0000, 0.1999],
        [0.0662, 0.0000, 0.0407,  ..., 0.0393, 0.0000, 0.1372],
        [0.0940, 0.0000, 0.0576,  ..., 0.0523, 0.0000, 0.1885],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66839.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1656e-03, 0.0000e+00, 2.5794e-01,  ..., 1.8452e-01, 0.0000e+00,
         4.9606e-01],
        [6.9997e-03, 0.0000e+00, 2.4959e-01,  ..., 1.8034e-01, 0.0000e+00,
         4.8030e-01],
        [5.6760e-05, 0.0000e+00, 2.7210e-01,  ..., 1.9154e-01, 0.0000e+00,
         5.2235e-01],
        ...,
        [1.0032e-01, 1.2685e-01, 2.2194e-03,  ..., 8.1402e-02, 1.2501e-01,
         5.3506e-02],
        [1.0022e-01, 1.2674e-01, 2.2119e-03,  ..., 8.1328e-02, 1.2489e-01,
         5.3453e-02],
        [1.0022e-01, 1.2675e-01, 2.2080e-03,  ..., 8.1328e-02, 1.2488e-01,
         5.3451e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663174.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10172.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(864.7980, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(389.2738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.9396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2357],
        [ 0.2225],
        [ 0.2174],
        ...,
        [-3.5687],
        [-3.5595],
        [-3.5489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277503.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0292],
        [1.0340],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371174.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.1837, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0292],
        [1.0341],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371179.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.1837, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [ 0.0099, -0.0011,  0.0061,  ...,  0.0066, -0.0015,  0.0218],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2737.9941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-8.3412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5468, device='cuda:0')



h[200].sum tensor(86.7567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0053],
        [0.0130, 0.0000, 0.0081,  ..., 0.0131, 0.0000, 0.0341],
        [0.0139, 0.0000, 0.0087,  ..., 0.0142, 0.0000, 0.0382],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64276.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0930, 0.0772, 0.0195,  ..., 0.0828, 0.0798, 0.0806],
        [0.0865, 0.0346, 0.0480,  ..., 0.0906, 0.0364, 0.1243],
        [0.0835, 0.0128, 0.0657,  ..., 0.0941, 0.0137, 0.1499],
        ...,
        [0.1006, 0.1270, 0.0022,  ..., 0.0815, 0.1253, 0.0535],
        [0.1005, 0.1269, 0.0022,  ..., 0.0814, 0.1252, 0.0535],
        [0.1005, 0.1269, 0.0022,  ..., 0.0814, 0.1252, 0.0535]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655502.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10197.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(851.0870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(378.0014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(279.9578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1854],
        [ 0.0148],
        [ 0.1882],
        ...,
        [-3.5846],
        [-3.5768],
        [-3.5745]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305563.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0292],
        [1.0341],
        ...,
        [0.9972],
        [0.9962],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371179.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.4307, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0293],
        [1.0342],
        ...,
        [0.9972],
        [0.9961],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371185.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.4307, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2803.7854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.1062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0153, device='cuda:0')



h[100].sum tensor(-8.6992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5666, device='cuda:0')



h[200].sum tensor(87.4150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2545, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0054],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66687.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0970, 0.1237, 0.0018,  ..., 0.0791, 0.1212, 0.0514],
        [0.0971, 0.1171, 0.0045,  ..., 0.0796, 0.1154, 0.0565],
        [0.0968, 0.1066, 0.0100,  ..., 0.0800, 0.1061, 0.0638],
        ...,
        [0.1008, 0.1271, 0.0023,  ..., 0.0816, 0.1255, 0.0535],
        [0.1007, 0.1270, 0.0023,  ..., 0.0815, 0.1254, 0.0534],
        [0.1007, 0.1270, 0.0023,  ..., 0.0815, 0.1254, 0.0534]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671600.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10195.4150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.3491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(394.3447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.6267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8275],
        [-2.3219],
        [-1.6733],
        ...,
        [-3.5797],
        [-3.5711],
        [-3.5685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297999.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0293],
        [1.0342],
        ...,
        [0.9972],
        [0.9961],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371185.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 630.0 event: 3150 loss: tensor(464.3342, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5586],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.6490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0293],
        [1.0342],
        ...,
        [0.9972],
        [0.9961],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371190.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5586],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.6490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [ 0.0183, -0.0018,  0.0112,  ...,  0.0106, -0.0027,  0.0374],
        [ 0.0247, -0.0024,  0.0151,  ...,  0.0136, -0.0035,  0.0493],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2832.9468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-8.8630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5754, device='cuda:0')



h[200].sum tensor(87.5489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0319, 0.0000, 0.0196,  ..., 0.0219, 0.0000, 0.0690],
        [0.0676, 0.0000, 0.0414,  ..., 0.0392, 0.0000, 0.1374],
        [0.1288, 0.0000, 0.0788,  ..., 0.0685, 0.0000, 0.2528],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66908.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0517, 0.0091, 0.1229,  ..., 0.1270, 0.0111, 0.2601],
        [0.0238, 0.0000, 0.2222,  ..., 0.1712, 0.0000, 0.4370],
        [0.0035, 0.0000, 0.3319,  ..., 0.2216, 0.0000, 0.6356],
        ...,
        [0.1012, 0.1272, 0.0023,  ..., 0.0818, 0.1259, 0.0535],
        [0.1011, 0.1271, 0.0023,  ..., 0.0818, 0.1258, 0.0534],
        [0.1011, 0.1271, 0.0023,  ..., 0.0818, 0.1258, 0.0534]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671155.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10281.1318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(866.0886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(396.9710, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.4724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0534],
        [ 0.0733],
        [ 0.1026],
        ...,
        [-3.6073],
        [-3.5994],
        [-3.5971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292211.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0293],
        [1.0342],
        ...,
        [0.9972],
        [0.9961],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371190.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6592],
        [0.5249],
        [0.4922],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6085, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0294],
        [1.0343],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371195.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6592],
        [0.5249],
        [0.4922],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6085, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0425, -0.0041,  0.0260,  ...,  0.0219, -0.0059,  0.0821],
        [ 0.0386, -0.0037,  0.0236,  ...,  0.0200, -0.0054,  0.0748],
        [ 0.0365, -0.0035,  0.0223,  ...,  0.0190, -0.0051,  0.0709],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2905.8433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0164, device='cuda:0')



h[100].sum tensor(-9.2783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6053, device='cuda:0')



h[200].sum tensor(88.0835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9554, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1692, 0.0000, 0.1033,  ..., 0.0873, 0.0000, 0.3272],
        [0.1712, 0.0000, 0.1046,  ..., 0.0883, 0.0000, 0.3311],
        [0.1366, 0.0000, 0.0835,  ..., 0.0721, 0.0000, 0.2672],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67073.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.4354,  ..., 0.2716, 0.0000, 0.8284],
        [0.0000, 0.0000, 0.4519,  ..., 0.2797, 0.0000, 0.8589],
        [0.0000, 0.0000, 0.3997,  ..., 0.2534, 0.0000, 0.7601],
        ...,
        [0.1015, 0.1274, 0.0023,  ..., 0.0821, 0.1263, 0.0535],
        [0.1014, 0.1273, 0.0023,  ..., 0.0821, 0.1262, 0.0534],
        [0.1014, 0.1273, 0.0023,  ..., 0.0821, 0.1262, 0.0534]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669610.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10374.2295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(868.6974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(398.6826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.4055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0488],
        [ 0.0453],
        [ 0.0543],
        ...,
        [-3.6206],
        [-3.6126],
        [-3.6102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289034.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0294],
        [1.0343],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371195.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4165],
        [0.3311],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.3259, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0294],
        [1.0343],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371200.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4165],
        [0.3311],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.3259, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056, -0.0006,  0.0035,  ...,  0.0046, -0.0009,  0.0140],
        [ 0.0074, -0.0008,  0.0046,  ...,  0.0054, -0.0012,  0.0172],
        [ 0.0124, -0.0013,  0.0076,  ...,  0.0078, -0.0018,  0.0265],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3187.1860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0193, device='cuda:0')



h[100].sum tensor(-10.8998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7138, device='cuda:0')



h[200].sum tensor(90.7526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0521, 0.0000, 0.0320,  ..., 0.0325, 0.0000, 0.1111],
        [0.0598, 0.0000, 0.0367,  ..., 0.0362, 0.0000, 0.1254],
        [0.0588, 0.0000, 0.0361,  ..., 0.0357, 0.0000, 0.1235],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71395.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0409, 0.0000, 0.1724,  ..., 0.1409, 0.0000, 0.3346],
        [0.0288, 0.0000, 0.1988,  ..., 0.1540, 0.0000, 0.3833],
        [0.0181, 0.0000, 0.2223,  ..., 0.1654, 0.0000, 0.4259],
        ...,
        [0.1018, 0.1275, 0.0023,  ..., 0.0825, 0.1268, 0.0534],
        [0.1018, 0.1274, 0.0023,  ..., 0.0824, 0.1267, 0.0533],
        [0.1017, 0.1274, 0.0023,  ..., 0.0824, 0.1267, 0.0533]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689684.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10339.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.8858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(428.1763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.8460, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0996],
        [ 0.0852],
        [ 0.0685],
        ...,
        [-3.6354],
        [-3.6271],
        [-3.6246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300212.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0294],
        [1.0343],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371200.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.6946, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0295],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371205.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.6946, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3484.1689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.8722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0224, device='cuda:0')



h[100].sum tensor(-12.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8296, device='cuda:0')



h[200].sum tensor(93.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0134, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0057]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78115.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0959, 0.1007, 0.0098,  ..., 0.0825, 0.1025, 0.0653],
        [0.0986, 0.1239, 0.0023,  ..., 0.0804, 0.1225, 0.0520],
        [0.0973, 0.1105, 0.0064,  ..., 0.0820, 0.1115, 0.0598],
        ...,
        [0.1021, 0.1276, 0.0023,  ..., 0.0828, 0.1273, 0.0532],
        [0.1020, 0.1275, 0.0023,  ..., 0.0827, 0.1271, 0.0532],
        [0.1020, 0.1275, 0.0023,  ..., 0.0827, 0.1271, 0.0532]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727038.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10289.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(919.2568, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(469.2177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.1089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4938],
        [-1.8093],
        [-1.6489],
        ...,
        [-3.6494],
        [-3.6412],
        [-3.6388]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253414.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0295],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371205.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8984],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(268.4886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0295],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371205.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8984],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(268.4886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0293, -0.0028,  0.0179,  ...,  0.0157, -0.0041,  0.0578],
        [ 0.0151, -0.0015,  0.0092,  ...,  0.0090, -0.0022,  0.0314],
        [ 0.0336, -0.0032,  0.0206,  ...,  0.0177, -0.0047,  0.0657],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3238.0513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0198, device='cuda:0')



h[100].sum tensor(-11.1830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7334, device='cuda:0')



h[200].sum tensor(90.8706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2730, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0638, 0.0000, 0.0392,  ..., 0.0380, 0.0000, 0.1328],
        [0.1075, 0.0000, 0.0658,  ..., 0.0585, 0.0000, 0.2135],
        [0.0424, 0.0000, 0.0260,  ..., 0.0274, 0.0000, 0.0909],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0057]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72492.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0285, 0.0000, 0.1903,  ..., 0.1533, 0.0000, 0.3728],
        [0.0224, 0.0000, 0.2017,  ..., 0.1600, 0.0000, 0.3954],
        [0.0482, 0.0000, 0.1422,  ..., 0.1338, 0.0000, 0.2895],
        ...,
        [0.1021, 0.1276, 0.0023,  ..., 0.0828, 0.1273, 0.0532],
        [0.1020, 0.1275, 0.0023,  ..., 0.0827, 0.1271, 0.0532],
        [0.1020, 0.1275, 0.0023,  ..., 0.0827, 0.1271, 0.0532]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696628.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10361.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(890.5306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(437.6664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.2405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3404],
        [ 0.3344],
        [ 0.2304],
        ...,
        [-3.6494],
        [-3.6412],
        [-3.6388]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298811.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0295],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371205.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.4861, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0295],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371210.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.4861, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2644.6553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-7.7755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5121, device='cuda:0')



h[200].sum tensor(83.7118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0091, 0.0000, 0.0056,  ..., 0.0107, 0.0000, 0.0248],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63694.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0938, 0.0771, 0.0183,  ..., 0.0855, 0.0828, 0.0795],
        [0.0958, 0.0940, 0.0119,  ..., 0.0841, 0.0974, 0.0698],
        [0.0907, 0.0619, 0.0278,  ..., 0.0898, 0.0669, 0.0957],
        ...,
        [0.1024, 0.1278, 0.0022,  ..., 0.0832, 0.1276, 0.0533],
        [0.1023, 0.1277, 0.0022,  ..., 0.0831, 0.1275, 0.0532],
        [0.1023, 0.1277, 0.0022,  ..., 0.0831, 0.1275, 0.0532]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658971.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10485.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(848.0174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(388.7406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.7462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3459],
        [-1.5313],
        [-1.2360],
        ...,
        [-3.6621],
        [-3.6539],
        [-3.6515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351129.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0295],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371210.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.8663, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0296],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371215.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.8663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2952.2334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-9.5437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6279, device='cuda:0')



h[200].sum tensor(86.5205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68909.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0961, 0.1013, 0.0092,  ..., 0.0836, 0.1037, 0.0649],
        [0.0990, 0.1250, 0.0017,  ..., 0.0812, 0.1238, 0.0516],
        [0.0992, 0.1249, 0.0018,  ..., 0.0812, 0.1239, 0.0517],
        ...,
        [0.1026, 0.1280, 0.0022,  ..., 0.0835, 0.1278, 0.0534],
        [0.1025, 0.1279, 0.0022,  ..., 0.0834, 0.1277, 0.0534],
        [0.1025, 0.1279, 0.0022,  ..., 0.0834, 0.1277, 0.0534]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681922.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10513.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(876.3272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(416.5428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.7930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9010],
        [-2.6368],
        [-3.1246],
        ...,
        [-3.6655],
        [-3.6617],
        [-3.6594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296959., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0296],
        [1.0344],
        ...,
        [0.9971],
        [0.9961],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371215.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.0510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0297],
        [1.0345],
        ...,
        [0.9971],
        [0.9960],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371219.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.0510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2818.6145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.9489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-8.7665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5738, device='cuda:0')



h[200].sum tensor(84.4199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0057],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67289.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0972, 0.0977, 0.0125,  ..., 0.0829, 0.0997, 0.0691],
        [0.0987, 0.1059, 0.0094,  ..., 0.0819, 0.1064, 0.0653],
        [0.0992, 0.1068, 0.0103,  ..., 0.0815, 0.1070, 0.0652],
        ...,
        [0.1028, 0.1280, 0.0021,  ..., 0.0838, 0.1278, 0.0537],
        [0.1027, 0.1279, 0.0021,  ..., 0.0837, 0.1276, 0.0537],
        [0.1027, 0.1279, 0.0021,  ..., 0.0837, 0.1276, 0.0537]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676171.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10507.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(861.1279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.0548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.0893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9452],
        [-1.2303],
        [-1.4554],
        ...,
        [-3.6736],
        [-3.6653],
        [-3.6630]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328738.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0297],
        [1.0345],
        ...,
        [0.9971],
        [0.9960],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371219.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.8588, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0298],
        [1.0345],
        ...,
        [0.9971],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371224.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.8588, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [ 0.0110, -0.0011,  0.0068,  ...,  0.0071, -0.0016,  0.0241],
        [ 0.0110, -0.0011,  0.0068,  ...,  0.0071, -0.0016,  0.0241],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2999.1472, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-9.7759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6415, device='cuda:0')



h[200].sum tensor(85.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0203, 0.0000, 0.0125,  ..., 0.0165, 0.0000, 0.0481],
        [0.0204, 0.0000, 0.0126,  ..., 0.0166, 0.0000, 0.0484],
        [0.0204, 0.0000, 0.0126,  ..., 0.0165, 0.0000, 0.0483],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69883.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0786, 0.0000, 0.0696,  ..., 0.1041, 0.0000, 0.1632],
        [0.0799, 0.0000, 0.0679,  ..., 0.1034, 0.0000, 0.1602],
        [0.0823, 0.0105, 0.0607,  ..., 0.1009, 0.0125, 0.1483],
        ...,
        [0.1030, 0.1281, 0.0021,  ..., 0.0841, 0.1275, 0.0541],
        [0.1029, 0.1280, 0.0021,  ..., 0.0840, 0.1274, 0.0540],
        [0.1029, 0.1280, 0.0021,  ..., 0.0840, 0.1274, 0.0540]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687908.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10510.3574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.9809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(426.4074, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.2181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1162],
        [-0.0420],
        [-0.2816],
        ...,
        [-3.6738],
        [-3.6656],
        [-3.6632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293299., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0298],
        [1.0345],
        ...,
        [0.9971],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371224.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5361],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.4365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0306],
        [1.0298],
        [1.0345],
        ...,
        [0.9971],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371228.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5361],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.4365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [ 0.0098, -0.0010,  0.0060,  ...,  0.0066, -0.0015,  0.0219],
        [ 0.0192, -0.0019,  0.0118,  ...,  0.0110, -0.0027,  0.0393],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2931.8525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0166, device='cuda:0')



h[100].sum tensor(-9.3338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6158, device='cuda:0')



h[200].sum tensor(84.8617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1446, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0101, 0.0000, 0.0062,  ..., 0.0110, 0.0000, 0.0269],
        [0.0353, 0.0000, 0.0216,  ..., 0.0235, 0.0000, 0.0760],
        [0.0911, 0.0000, 0.0557,  ..., 0.0508, 0.0000, 0.1838],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67706.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0781, 0.0062, 0.0648,  ..., 0.1041, 0.0129, 0.1593],
        [0.0577, 0.0000, 0.1173,  ..., 0.1267, 0.0011, 0.2506],
        [0.0275, 0.0000, 0.1953,  ..., 0.1589, 0.0000, 0.3853],
        ...,
        [0.1030, 0.1282, 0.0021,  ..., 0.0842, 0.1271, 0.0544],
        [0.1029, 0.1281, 0.0021,  ..., 0.0841, 0.1270, 0.0544],
        [0.1029, 0.1281, 0.0021,  ..., 0.0841, 0.1270, 0.0544]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673892.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10528.1748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(854.9894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(414.5461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(278.0930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5071],
        [ 0.4842],
        [ 0.4500],
        ...,
        [-3.6679],
        [-3.6593],
        [-3.6562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303991.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0306],
        [1.0298],
        [1.0345],
        ...,
        [0.9971],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371228.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 640.0 event: 3200 loss: tensor(413.8598, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.8281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0306],
        [1.0298],
        [1.0345],
        ...,
        [0.9970],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371232.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.8281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2754.2480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-8.2752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5486, device='cuda:0')



h[200].sum tensor(82.9001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66118.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0976, 0.1067, 0.0079,  ..., 0.0835, 0.1068, 0.0635],
        [0.0994, 0.1228, 0.0015,  ..., 0.0823, 0.1207, 0.0542],
        [0.0997, 0.1252, 0.0017,  ..., 0.0821, 0.1230, 0.0527],
        ...,
        [0.1031, 0.1283, 0.0020,  ..., 0.0844, 0.1268, 0.0545],
        [0.1030, 0.1282, 0.0020,  ..., 0.0844, 0.1267, 0.0545],
        [0.1030, 0.1282, 0.0020,  ..., 0.0844, 0.1267, 0.0545]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670001.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10571.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(846.4594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(404.2405, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(270.2240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1334],
        [-2.7937],
        [-3.2271],
        ...,
        [-3.6690],
        [-3.6605],
        [-3.6577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317800.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0306],
        [1.0298],
        [1.0345],
        ...,
        [0.9970],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371232.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3665],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.9174, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0297],
        [1.0345],
        ...,
        [0.9970],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371237.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3665],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.9174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [ 0.0129, -0.0013,  0.0079,  ...,  0.0080, -0.0019,  0.0276],
        [ 0.0053, -0.0006,  0.0033,  ...,  0.0045, -0.0009,  0.0136],
        [ 0.0063, -0.0007,  0.0039,  ...,  0.0049, -0.0010,  0.0154]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2942.1113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0166, device='cuda:0')



h[100].sum tensor(-9.2821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6144, device='cuda:0')



h[200].sum tensor(85.4241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0020, 0.0000, 0.0013,  ..., 0.0073, 0.0000, 0.0121],
        [0.0041, 0.0000, 0.0026,  ..., 0.0089, 0.0000, 0.0183],
        ...,
        [0.0152, 0.0000, 0.0095,  ..., 0.0150, 0.0000, 0.0418],
        [0.0366, 0.0000, 0.0226,  ..., 0.0256, 0.0000, 0.0838],
        [0.0166, 0.0000, 0.0103,  ..., 0.0150, 0.0000, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67912.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0991, 0.1040, 0.0119,  ..., 0.0816, 0.1022, 0.0678],
        [0.0986, 0.0838, 0.0207,  ..., 0.0830, 0.0845, 0.0818],
        [0.0978, 0.0684, 0.0280,  ..., 0.0840, 0.0713, 0.0918],
        ...,
        [0.0897, 0.0127, 0.0642,  ..., 0.0999, 0.0141, 0.1503],
        [0.0841, 0.0000, 0.0804,  ..., 0.1061, 0.0000, 0.1771],
        [0.0901, 0.0242, 0.0563,  ..., 0.0992, 0.0291, 0.1397]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672404.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10584.9756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(859.9441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.1276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(282.8023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6377],
        [-1.3553],
        [-1.2415],
        ...,
        [-1.1393],
        [-0.9875],
        [-1.4206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289699.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0297],
        [1.0345],
        ...,
        [0.9970],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371237.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.4104, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0297],
        [1.0345],
        ...,
        [0.9970],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371241.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.4104, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3343.5474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0205, device='cuda:0')



h[100].sum tensor(-11.5186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7578, device='cuda:0')



h[200].sum tensor(90.1476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74506.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0993, 0.1250, 0.0013,  ..., 0.0823, 0.1220, 0.0522],
        [0.0997, 0.1254, 0.0013,  ..., 0.0826, 0.1225, 0.0524],
        [0.0999, 0.1253, 0.0015,  ..., 0.0826, 0.1225, 0.0524],
        ...,
        [0.1034, 0.1284, 0.0018,  ..., 0.0850, 0.1264, 0.0543],
        [0.1033, 0.1283, 0.0018,  ..., 0.0849, 0.1262, 0.0542],
        [0.1032, 0.1282, 0.0018,  ..., 0.0849, 0.1262, 0.0542]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703744.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10432.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(885.5139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(454.6251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(317.7907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5515],
        [-3.6545],
        [-3.7359],
        ...,
        [-3.6804],
        [-3.6724],
        [-3.6702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304659.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0297],
        [1.0345],
        ...,
        [0.9970],
        [0.9960],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371241.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2585],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.8909, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0296],
        [1.0345],
        ...,
        [0.9970],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371245.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2585],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.8909, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [ 0.0096, -0.0010,  0.0059,  ...,  0.0065, -0.0014,  0.0215],
        [ 0.0132, -0.0013,  0.0081,  ...,  0.0082, -0.0019,  0.0281],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2671.3843, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.8752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0139, device='cuda:0')



h[100].sum tensor(-7.6917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5132, device='cuda:0')



h[200].sum tensor(83.0557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0132, 0.0000, 0.0082,  ..., 0.0132, 0.0000, 0.0350],
        [0.0310, 0.0000, 0.0191,  ..., 0.0222, 0.0000, 0.0703],
        [0.0486, 0.0000, 0.0299,  ..., 0.0311, 0.0000, 0.1053],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63554.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0852, 0.0334, 0.0559,  ..., 0.0986, 0.0358, 0.1412],
        [0.0735, 0.0031, 0.0954,  ..., 0.1130, 0.0059, 0.2057],
        [0.0660, 0.0000, 0.1193,  ..., 0.1219, 0.0000, 0.2448],
        ...,
        [0.1035, 0.1284, 0.0017,  ..., 0.0853, 0.1265, 0.0540],
        [0.1034, 0.1283, 0.0017,  ..., 0.0852, 0.1264, 0.0540],
        [0.1034, 0.1283, 0.0017,  ..., 0.0852, 0.1264, 0.0540]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655136.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10681.3018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(849.7344, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(384.4210, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.0124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3212],
        [ 0.2112],
        [ 0.4351],
        ...,
        [-3.6941],
        [-3.6859],
        [-3.6836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321061.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0296],
        [1.0345],
        ...,
        [0.9970],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371245.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4353],
        [0.0000],
        [0.2944],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.8271, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0296],
        [1.0345],
        ...,
        [0.9970],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371250.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4353],
        [0.0000],
        [0.2944],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.8271, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0145, -0.0014,  0.0089,  ...,  0.0088, -0.0020,  0.0304],
        [ 0.0277, -0.0026,  0.0169,  ...,  0.0150, -0.0037,  0.0549],
        [ 0.0100, -0.0010,  0.0062,  ...,  0.0067, -0.0015,  0.0222],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3334.1758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0205, device='cuda:0')



h[100].sum tensor(-11.3908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7589, device='cuda:0')



h[200].sum tensor(90.5990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1177, 0.0000, 0.0721,  ..., 0.0634, 0.0000, 0.2328],
        [0.0760, 0.0000, 0.0466,  ..., 0.0439, 0.0000, 0.1558],
        [0.0904, 0.0000, 0.0554,  ..., 0.0506, 0.0000, 0.1824],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72631.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8753e-04, 0.0000e+00, 2.8114e-01,  ..., 1.9878e-01, 0.0000e+00,
         5.4125e-01],
        [1.1983e-02, 0.0000e+00, 2.5022e-01,  ..., 1.8353e-01, 0.0000e+00,
         4.8289e-01],
        [1.0118e-02, 0.0000e+00, 2.5001e-01,  ..., 1.8322e-01, 0.0000e+00,
         4.8203e-01],
        ...,
        [1.0359e-01, 1.2839e-01, 1.6576e-03,  ..., 8.5412e-02, 1.2649e-01,
         5.3818e-02],
        [1.0350e-01, 1.2829e-01, 1.6509e-03,  ..., 8.5341e-02, 1.2638e-01,
         5.3769e-02],
        [1.0347e-01, 1.2827e-01, 1.6468e-03,  ..., 8.5324e-02, 1.2636e-01,
         5.3757e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691376.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10574.8643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(892.4440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(439.0202, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.0885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1874],
        [ 0.1884],
        [ 0.1943],
        ...,
        [-3.7018],
        [-3.6937],
        [-3.6913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284149.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0296],
        [1.0345],
        ...,
        [0.9970],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371250.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.3016, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0296],
        [1.0345],
        ...,
        [0.9970],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371254.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.3016, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2744.2317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.1024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0144, device='cuda:0')



h[100].sum tensor(-8.0284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5335, device='cuda:0')



h[200].sum tensor(84.3882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6549, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64511.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0997, 0.1250, 0.0011,  ..., 0.0828, 0.1221, 0.0517],
        [0.1001, 0.1255, 0.0012,  ..., 0.0832, 0.1226, 0.0519],
        [0.1002, 0.1253, 0.0013,  ..., 0.0832, 0.1226, 0.0519],
        ...,
        [0.1038, 0.1284, 0.0016,  ..., 0.0856, 0.1265, 0.0537],
        [0.1037, 0.1283, 0.0016,  ..., 0.0855, 0.1264, 0.0537],
        [0.1037, 0.1283, 0.0016,  ..., 0.0855, 0.1263, 0.0537]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657550.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10737.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(861.9637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(388.5319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(276.7092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6511],
        [-3.5591],
        [-3.4067],
        ...,
        [-3.7036],
        [-3.6946],
        [-3.6895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296381.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0296],
        [1.0345],
        ...,
        [0.9970],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371254.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.3179, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0296],
        [1.0345],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371259.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.3179, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2707.1021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0141, device='cuda:0')



h[100].sum tensor(-7.7804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5226, device='cuda:0')



h[200].sum tensor(84.1698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65088.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0998, 0.1251, 0.0011,  ..., 0.0830, 0.1220, 0.0517],
        [0.1002, 0.1256, 0.0012,  ..., 0.0833, 0.1224, 0.0519],
        [0.1003, 0.1254, 0.0013,  ..., 0.0833, 0.1225, 0.0520],
        ...,
        [0.1039, 0.1285, 0.0016,  ..., 0.0857, 0.1263, 0.0538],
        [0.1038, 0.1284, 0.0016,  ..., 0.0856, 0.1262, 0.0537],
        [0.1038, 0.1284, 0.0016,  ..., 0.0856, 0.1262, 0.0537]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664321.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10707.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(864.4067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(392.8824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(280.7018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7310],
        [-3.6865],
        [-3.6075],
        ...,
        [-3.7113],
        [-3.7032],
        [-3.7008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298238.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0296],
        [1.0345],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371259.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.2014, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0296],
        [1.0345],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371263.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.2014, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2751.2778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0147, device='cuda:0')



h[100].sum tensor(-8.0280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5441, device='cuda:0')



h[200].sum tensor(84.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8477, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0057],
        [0.0185, 0.0000, 0.0114,  ..., 0.0151, 0.0000, 0.0424],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0058],
        [0.0041, 0.0000, 0.0026,  ..., 0.0086, 0.0000, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64879.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0989, 0.1099, 0.0050,  ..., 0.0844, 0.1086, 0.0617],
        [0.0961, 0.0865, 0.0132,  ..., 0.0886, 0.0898, 0.0746],
        [0.0863, 0.0363, 0.0411,  ..., 0.1002, 0.0447, 0.1241],
        ...,
        [0.1040, 0.1264, 0.0016,  ..., 0.0862, 0.1243, 0.0556],
        [0.1008, 0.0928, 0.0142,  ..., 0.0898, 0.0959, 0.0753],
        [0.0936, 0.0464, 0.0424,  ..., 0.0983, 0.0479, 0.1210]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659918.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10776.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.0934, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(392.3468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(278.1176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7456],
        [-1.3719],
        [-0.7133],
        ...,
        [-3.0525],
        [-2.1982],
        [-1.0893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298818.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0296],
        [1.0345],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371263.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.7444, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0309],
        [1.0296],
        [1.0346],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371268.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.7444, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2783.6147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-8.1579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5484, device='cuda:0')



h[200].sum tensor(84.8216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0057],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65839.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1003, 0.1230, 0.0028,  ..., 0.0829, 0.1189, 0.0544],
        [0.1006, 0.1202, 0.0033,  ..., 0.0833, 0.1164, 0.0568],
        [0.0977, 0.0876, 0.0157,  ..., 0.0869, 0.0888, 0.0762],
        ...,
        [0.1042, 0.1289, 0.0016,  ..., 0.0861, 0.1260, 0.0543],
        [0.1041, 0.1288, 0.0016,  ..., 0.0860, 0.1259, 0.0543],
        [0.1041, 0.1288, 0.0016,  ..., 0.0860, 0.1259, 0.0543]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666053.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10744.0654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.8988, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(400.0686, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(282.6002, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2267],
        [-1.7201],
        [-1.0022],
        ...,
        [-3.7179],
        [-3.7093],
        [-3.7058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302165.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0309],
        [1.0296],
        [1.0346],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371268.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.8663, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0309],
        [1.0297],
        [1.0346],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371273.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.8663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3962.2725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0262, device='cuda:0')



h[100].sum tensor(-14.6899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9694, device='cuda:0')



h[200].sum tensor(97.6308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.5431, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0047, 0.0000, 0.0029,  ..., 0.0085, 0.0000, 0.0168],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0057],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85060.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0957, 0.0619, 0.0258,  ..., 0.0886, 0.0657, 0.0924],
        [0.0995, 0.1057, 0.0091,  ..., 0.0849, 0.1035, 0.0660],
        [0.1007, 0.1219, 0.0023,  ..., 0.0837, 0.1180, 0.0557],
        ...,
        [0.1043, 0.1291, 0.0016,  ..., 0.0862, 0.1257, 0.0547],
        [0.1042, 0.1290, 0.0016,  ..., 0.0861, 0.1256, 0.0546],
        [0.1042, 0.1289, 0.0016,  ..., 0.0861, 0.1255, 0.0546]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(759229.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10527.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(938.8242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(519.6031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(378.5152, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8167],
        [-1.6612],
        [-2.4061],
        ...,
        [-3.7173],
        [-3.7092],
        [-3.7069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265920.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0309],
        [1.0297],
        [1.0346],
        ...,
        [0.9969],
        [0.9959],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371273.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 650.0 event: 3250 loss: tensor(469.7772, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.5504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0310],
        [1.0297],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371278.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.5504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2677.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0137, device='cuda:0')



h[100].sum tensor(-7.5012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5069, device='cuda:0')



h[200].sum tensor(83.7184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0057],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0045, 0.0000, 0.0028,  ..., 0.0085, 0.0000, 0.0166],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64750.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1004, 0.1212, 0.0028,  ..., 0.0832, 0.1164, 0.0565],
        [0.0997, 0.1065, 0.0091,  ..., 0.0848, 0.1037, 0.0662],
        [0.0963, 0.0609, 0.0267,  ..., 0.0891, 0.0647, 0.0940],
        ...,
        [0.1044, 0.1293, 0.0016,  ..., 0.0862, 0.1255, 0.0550],
        [0.1044, 0.1292, 0.0016,  ..., 0.0862, 0.1254, 0.0550],
        [0.1043, 0.1292, 0.0016,  ..., 0.0861, 0.1254, 0.0550]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663912.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10811.9150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(858.9728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(392.8716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(276.9193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0236],
        [-1.4103],
        [-0.6966],
        ...,
        [-3.7201],
        [-3.7121],
        [-3.7098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284750.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0310],
        [1.0297],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371278.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.8948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371283.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.8948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [ 0.0030, -0.0004,  0.0020,  ...,  0.0034, -0.0006,  0.0095],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3093.5491, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0178, device='cuda:0')



h[100].sum tensor(-9.8151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6580, device='cuda:0')



h[200].sum tensor(88.0558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9088, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0101, 0.0000, 0.0064,  ..., 0.0117, 0.0000, 0.0295],
        [0.0178, 0.0000, 0.0112,  ..., 0.0160, 0.0000, 0.0463],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69474.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0969, 0.0772, 0.0201,  ..., 0.0882, 0.0793, 0.0833],
        [0.0914, 0.0326, 0.0484,  ..., 0.0956, 0.0346, 0.1281],
        [0.0871, 0.0092, 0.0700,  ..., 0.1009, 0.0103, 0.1606],
        ...,
        [0.1047, 0.1297, 0.0016,  ..., 0.0865, 0.1257, 0.0554],
        [0.1047, 0.1296, 0.0016,  ..., 0.0864, 0.1256, 0.0553],
        [0.1046, 0.1295, 0.0016,  ..., 0.0864, 0.1256, 0.0553]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679970.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10793.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(876.6865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(424.1584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(299.1408, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2920],
        [-0.4266],
        [ 0.1480],
        ...,
        [-3.7317],
        [-3.7236],
        [-3.7213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292187.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371283.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4824],
        [0.5054],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(286.5651, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371283.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4824],
        [0.5054],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(286.5651, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0220, -0.0020,  0.0135,  ...,  0.0123, -0.0029,  0.0444],
        [ 0.0247, -0.0023,  0.0152,  ...,  0.0136, -0.0033,  0.0495],
        [ 0.0166, -0.0016,  0.0102,  ...,  0.0098, -0.0023,  0.0345],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3400.4888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0212, device='cuda:0')



h[100].sum tensor(-11.5198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7828, device='cuda:0')



h[200].sum tensor(91.3704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0951, 0.0000, 0.0584,  ..., 0.0528, 0.0000, 0.1915],
        [0.0863, 0.0000, 0.0530,  ..., 0.0487, 0.0000, 0.1752],
        [0.0626, 0.0000, 0.0384,  ..., 0.0370, 0.0000, 0.1289],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74878.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0245, 0.0000, 0.2250,  ..., 0.1714, 0.0000, 0.4388],
        [0.0269, 0.0000, 0.2194,  ..., 0.1697, 0.0000, 0.4292],
        [0.0346, 0.0000, 0.1987,  ..., 0.1612, 0.0000, 0.3928],
        ...,
        [0.1047, 0.1297, 0.0016,  ..., 0.0865, 0.1257, 0.0554],
        [0.1047, 0.1296, 0.0016,  ..., 0.0864, 0.1256, 0.0553],
        [0.1043, 0.1261, 0.0019,  ..., 0.0867, 0.1227, 0.0573]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708305.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10755.9971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(905.7910, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(453.2484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.6136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4620],
        [ 0.4462],
        [ 0.4519],
        ...,
        [-3.7131],
        [-3.6445],
        [-3.4689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250515.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371283.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.2114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371288.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.2114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2736.4211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-7.8372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5305, device='cuda:0')



h[200].sum tensor(84.2411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63898.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0992, 0.1051, 0.0082,  ..., 0.0860, 0.1033, 0.0662],
        [0.1010, 0.1219, 0.0018,  ..., 0.0847, 0.1178, 0.0566],
        [0.1015, 0.1268, 0.0012,  ..., 0.0842, 0.1222, 0.0536],
        ...,
        [0.1051, 0.1300, 0.0015,  ..., 0.0867, 0.1260, 0.0555],
        [0.1050, 0.1299, 0.0015,  ..., 0.0866, 0.1259, 0.0555],
        [0.1050, 0.1298, 0.0015,  ..., 0.0866, 0.1258, 0.0554]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655946.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10938.4268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(851.6049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(392.1576, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(269.1335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1787],
        [-2.9327],
        [-3.4043],
        ...,
        [-3.7466],
        [-3.7384],
        [-3.7361]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315842.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371288.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0088, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0313],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371293.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0088, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2746.2852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-7.8545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5272, device='cuda:0')



h[200].sum tensor(84.6253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66104.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1011, 0.1267, 0.0011,  ..., 0.0839, 0.1216, 0.0535],
        [0.1016, 0.1264, 0.0016,  ..., 0.0842, 0.1213, 0.0544],
        [0.1019, 0.1244, 0.0030,  ..., 0.0839, 0.1193, 0.0560],
        ...,
        [0.1053, 0.1302, 0.0016,  ..., 0.0868, 0.1259, 0.0557],
        [0.1052, 0.1301, 0.0016,  ..., 0.0867, 0.1258, 0.0556],
        [0.1052, 0.1300, 0.0016,  ..., 0.0867, 0.1258, 0.0556]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670366.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10930.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(863.2540, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(405.4821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.4570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4181],
        [-3.2009],
        [-2.9106],
        ...,
        [-3.7522],
        [-3.7440],
        [-3.7417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299365.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0313],
        [1.0298],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371293.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.1099, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0314],
        [1.0299],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371299.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.1099, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2932.2229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-8.8418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5958, device='cuda:0')



h[200].sum tensor(86.9858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7824, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66362.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1010, 0.1201, 0.0045,  ..., 0.0843, 0.1157, 0.0580],
        [0.1016, 0.1252, 0.0023,  ..., 0.0844, 0.1202, 0.0552],
        [0.1019, 0.1272, 0.0014,  ..., 0.0842, 0.1222, 0.0538],
        ...,
        [0.1055, 0.1304, 0.0017,  ..., 0.0867, 0.1260, 0.0557],
        [0.1054, 0.1303, 0.0017,  ..., 0.0867, 0.1259, 0.0556],
        [0.1053, 0.1303, 0.0017,  ..., 0.0866, 0.1258, 0.0556]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665599.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10965.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(861.0605, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(409.6428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.4659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2223],
        [-2.6605],
        [-3.0197],
        ...,
        [-3.7513],
        [-3.7447],
        [-3.7443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315286.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0314],
        [1.0299],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371299.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6001, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0315],
        [1.0299],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371304.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6001, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0198, -0.0018,  0.0122,  ...,  0.0113, -0.0026,  0.0404],
        [ 0.0092, -0.0009,  0.0057,  ...,  0.0063, -0.0013,  0.0209],
        [ 0.0119, -0.0011,  0.0074,  ...,  0.0076, -0.0017,  0.0259],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3519.4695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0222, device='cuda:0')



h[100].sum tensor(-12.0341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8211, device='cuda:0')



h[200].sum tensor(93.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8604, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0692, 0.0000, 0.0426,  ..., 0.0406, 0.0000, 0.1436],
        [0.0648, 0.0000, 0.0399,  ..., 0.0386, 0.0000, 0.1355],
        [0.0561, 0.0000, 0.0346,  ..., 0.0345, 0.0000, 0.1194],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79546.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0000, 0.2159,  ..., 0.1658, 0.0000, 0.4193],
        [0.0316, 0.0000, 0.2150,  ..., 0.1657, 0.0000, 0.4173],
        [0.0392, 0.0000, 0.1974,  ..., 0.1573, 0.0000, 0.3848],
        ...,
        [0.1057, 0.1306, 0.0018,  ..., 0.0868, 0.1260, 0.0557],
        [0.1056, 0.1305, 0.0018,  ..., 0.0867, 0.1259, 0.0557],
        [0.1056, 0.1305, 0.0017,  ..., 0.0867, 0.1259, 0.0557]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(745644.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10737.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(912.7519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(493.5748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(347.2276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3451],
        [ 0.3308],
        [ 0.3364],
        ...,
        [-3.7672],
        [-3.7588],
        [-3.7564]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318054.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0315],
        [1.0299],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371304.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.4741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0315],
        [1.0299],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371304.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.4741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2705.8557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.8021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0138, device='cuda:0')



h[100].sum tensor(-7.5554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5094, device='cuda:0')



h[200].sum tensor(84.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2185, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65597.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1003, 0.1108, 0.0070,  ..., 0.0854, 0.1077, 0.0637],
        [0.1018, 0.1253, 0.0013,  ..., 0.0845, 0.1202, 0.0553],
        [0.1021, 0.1274, 0.0014,  ..., 0.0843, 0.1222, 0.0538],
        ...,
        [0.1057, 0.1306, 0.0018,  ..., 0.0868, 0.1260, 0.0557],
        [0.1056, 0.1305, 0.0018,  ..., 0.0867, 0.1259, 0.0557],
        [0.1056, 0.1305, 0.0017,  ..., 0.0867, 0.1259, 0.0557]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671520.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11012.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(864.6306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(403.1987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(280.1397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2622],
        [-2.9115],
        [-3.3299],
        ...,
        [-3.7620],
        [-3.7501],
        [-3.7393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293619.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0315],
        [1.0299],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371304.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.4048, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0315],
        [1.0300],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371310.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.4048, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3254.7864, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0191, device='cuda:0')



h[100].sum tensor(-10.5001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7086, device='cuda:0')



h[200].sum tensor(91.3235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0000, 0.0048,  ..., 0.0098, 0.0000, 0.0225],
        [0.0239, 0.0000, 0.0148,  ..., 0.0181, 0.0000, 0.0550],
        [0.0315, 0.0000, 0.0194,  ..., 0.0217, 0.0000, 0.0690],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73583.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0750, 0.0109, 0.0844,  ..., 0.1144, 0.0120, 0.1951],
        [0.0565, 0.0000, 0.1335,  ..., 0.1357, 0.0000, 0.2818],
        [0.0444, 0.0000, 0.1658,  ..., 0.1491, 0.0000, 0.3383],
        ...,
        [0.1057, 0.1307, 0.0019,  ..., 0.0867, 0.1258, 0.0558],
        [0.1057, 0.1306, 0.0019,  ..., 0.0866, 0.1257, 0.0558],
        [0.1056, 0.1306, 0.0019,  ..., 0.0865, 0.1256, 0.0558]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708153.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10918.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(900.0832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(450.9820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(321.7724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2109],
        [ 0.1868],
        [ 0.1744],
        ...,
        [-3.7678],
        [-3.7596],
        [-3.7572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264705.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0315],
        [1.0300],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371310.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0316],
        [1.0300],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371316.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [ 0.0120, -0.0011,  0.0074,  ...,  0.0076, -0.0016,  0.0259],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2848.9907, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0151, device='cuda:0')



h[100].sum tensor(-8.2145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5571, device='cuda:0')



h[200].sum tensor(87.2872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0831, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0442, 0.0000, 0.0274,  ..., 0.0288, 0.0000, 0.0974],
        [0.0099, 0.0000, 0.0061,  ..., 0.0109, 0.0000, 0.0265],
        [0.0123, 0.0000, 0.0076,  ..., 0.0120, 0.0000, 0.0311],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66145., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0756, 0.0000, 0.0908,  ..., 0.1141, 0.0000, 0.2011],
        [0.0894, 0.0299, 0.0450,  ..., 0.0989, 0.0326, 0.1271],
        [0.0936, 0.0560, 0.0298,  ..., 0.0941, 0.0592, 0.1029],
        ...,
        [0.1059, 0.1309, 0.0020,  ..., 0.0866, 0.1258, 0.0559],
        [0.1058, 0.1308, 0.0020,  ..., 0.0865, 0.1257, 0.0558],
        [0.1057, 0.1307, 0.0020,  ..., 0.0865, 0.1256, 0.0558]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670219., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11045.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(868.1780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(406.4990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.0814, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0143],
        [-0.8021],
        [-1.7873],
        ...,
        [-3.7721],
        [-3.7639],
        [-3.7615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290165.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0316],
        [1.0300],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371316.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 660.0 event: 3300 loss: tensor(394.2005, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5923],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.8676, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0316],
        [1.0300],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371322.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5923],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.8676, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [ 0.0109, -0.0010,  0.0068,  ...,  0.0071, -0.0015,  0.0240],
        [ 0.0086, -0.0008,  0.0053,  ...,  0.0060, -0.0012,  0.0197],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3469.6890, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.4960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0212, device='cuda:0')



h[100].sum tensor(-11.5632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7863, device='cuda:0')



h[200].sum tensor(94.2357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0112, 0.0000, 0.0070,  ..., 0.0115, 0.0000, 0.0290],
        [0.0262, 0.0000, 0.0162,  ..., 0.0191, 0.0000, 0.0592],
        [0.0677, 0.0000, 0.0417,  ..., 0.0398, 0.0000, 0.1408],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78365.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0870, 0.0288, 0.0556,  ..., 0.1005, 0.0306, 0.1435],
        [0.0725, 0.0048, 0.0976,  ..., 0.1180, 0.0070, 0.2151],
        [0.0543, 0.0000, 0.1496,  ..., 0.1387, 0.0000, 0.3035],
        ...,
        [0.1061, 0.1311, 0.0020,  ..., 0.0866, 0.1259, 0.0559],
        [0.1060, 0.1310, 0.0020,  ..., 0.0865, 0.1258, 0.0559],
        [0.1059, 0.1309, 0.0020,  ..., 0.0865, 0.1257, 0.0559]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739749.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10879.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(920.8975, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(481.3968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.5996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1476],
        [ 0.3386],
        [ 0.4224],
        ...,
        [-3.7753],
        [-3.7423],
        [-3.6605]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256314.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0316],
        [1.0300],
        [1.0347],
        ...,
        [0.9969],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371322.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2876],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.2004, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0300],
        [1.0347],
        ...,
        [0.9968],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371327.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2876],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.2004, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046, -0.0005,  0.0029,  ...,  0.0041, -0.0007,  0.0124],
        [ 0.0041, -0.0005,  0.0026,  ...,  0.0039, -0.0007,  0.0114],
        [ 0.0100, -0.0009,  0.0062,  ...,  0.0067, -0.0014,  0.0223],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2608.7314, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.1001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0126, device='cuda:0')



h[100].sum tensor(-6.8174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4677, device='cuda:0')



h[200].sum tensor(85.0682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4634, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0331, 0.0000, 0.0207,  ..., 0.0236, 0.0000, 0.0770],
        [0.0415, 0.0000, 0.0258,  ..., 0.0276, 0.0000, 0.0925],
        [0.0164, 0.0000, 0.0103,  ..., 0.0152, 0.0000, 0.0436],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62881.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.0000, 0.1329,  ..., 0.1226, 0.0000, 0.2634],
        [0.0724, 0.0000, 0.1181,  ..., 0.1182, 0.0000, 0.2397],
        [0.0850, 0.0116, 0.0770,  ..., 0.1040, 0.0125, 0.1726],
        ...,
        [0.1062, 0.1312, 0.0020,  ..., 0.0865, 0.1260, 0.0561],
        [0.1061, 0.1311, 0.0020,  ..., 0.0864, 0.1259, 0.0560],
        [0.1060, 0.1311, 0.0020,  ..., 0.0864, 0.1258, 0.0560]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658222.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11129.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(855.0717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(387.6096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.3205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4850],
        [ 0.3715],
        [-0.0362],
        ...,
        [-3.7871],
        [-3.7787],
        [-3.7763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305046.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0300],
        [1.0347],
        ...,
        [0.9968],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371327.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2524],
        [0.3745],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.9636, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0299],
        [1.0346],
        ...,
        [0.9968],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371332.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2524],
        [0.3745],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.9636, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0119, -0.0011,  0.0073,  ...,  0.0075, -0.0016,  0.0257],
        [ 0.0101, -0.0010,  0.0063,  ...,  0.0067, -0.0014,  0.0225],
        [ 0.0173, -0.0016,  0.0107,  ...,  0.0101, -0.0023,  0.0358],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3747.6018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0241, device='cuda:0')



h[100].sum tensor(-12.9652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8931, device='cuda:0')



h[200].sum tensor(97.3810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0467, 0.0000, 0.0289,  ..., 0.0300, 0.0000, 0.1020],
        [0.0589, 0.0000, 0.0364,  ..., 0.0357, 0.0000, 0.1247],
        [0.0507, 0.0000, 0.0314,  ..., 0.0319, 0.0000, 0.1095],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81649.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0632, 0.0000, 0.1400,  ..., 0.1272, 0.0000, 0.2774],
        [0.0582, 0.0000, 0.1530,  ..., 0.1335, 0.0000, 0.3010],
        [0.0583, 0.0000, 0.1536,  ..., 0.1335, 0.0000, 0.3015],
        ...,
        [0.1062, 0.1314, 0.0021,  ..., 0.0864, 0.1259, 0.0563],
        [0.1061, 0.1313, 0.0021,  ..., 0.0864, 0.1258, 0.0562],
        [0.1061, 0.1312, 0.0021,  ..., 0.0863, 0.1257, 0.0562]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(750198.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10823.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(931.2890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(502.9615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.2270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5170],
        [ 0.4916],
        [ 0.4736],
        ...,
        [-3.7899],
        [-3.7814],
        [-3.7791]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273345.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0299],
        [1.0346],
        ...,
        [0.9968],
        [0.9958],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371332.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5435],
        [0.6279],
        [0.5815],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0299],
        [1.0346],
        ...,
        [0.9968],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371338.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5435],
        [0.6279],
        [0.5815],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0312, -0.0027,  0.0192,  ...,  0.0166, -0.0040,  0.0616],
        [ 0.0314, -0.0027,  0.0193,  ...,  0.0167, -0.0040,  0.0619],
        [ 0.0243, -0.0021,  0.0149,  ...,  0.0134, -0.0031,  0.0488],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2998.8591, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-8.8096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5995, device='cuda:0')



h[200].sum tensor(89.4847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1395, 0.0000, 0.0854,  ..., 0.0734, 0.0000, 0.2734],
        [0.1286, 0.0000, 0.0788,  ..., 0.0683, 0.0000, 0.2534],
        [0.1255, 0.0000, 0.0769,  ..., 0.0669, 0.0000, 0.2477],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68346.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.3931,  ..., 0.2449, 0.0000, 0.7432],
        [0.0000, 0.0000, 0.3642,  ..., 0.2316, 0.0000, 0.6893],
        [0.0025, 0.0000, 0.3298,  ..., 0.2154, 0.0000, 0.6252],
        ...,
        [0.1060, 0.1314, 0.0023,  ..., 0.0862, 0.1255, 0.0565],
        [0.1059, 0.1313, 0.0023,  ..., 0.0861, 0.1254, 0.0565],
        [0.1058, 0.1313, 0.0023,  ..., 0.0861, 0.1253, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680183.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11001.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(873.9512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(420.0635, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.5207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1387],
        [ 0.1519],
        [ 0.1591],
        ...,
        [-3.7263],
        [-3.7111],
        [-3.6697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280886.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0299],
        [1.0346],
        ...,
        [0.9968],
        [0.9957],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371338.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2922],
        [0.3457],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.2211, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0299],
        [1.0346],
        ...,
        [0.9968],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371343.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2922],
        [0.3457],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.2211, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0215, -0.0019,  0.0132,  ...,  0.0120, -0.0028,  0.0435],
        [ 0.0101, -0.0009,  0.0063,  ...,  0.0067, -0.0014,  0.0225],
        [ 0.0058, -0.0006,  0.0037,  ...,  0.0047, -0.0009,  0.0146],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3147.2339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0177, device='cuda:0')



h[100].sum tensor(-9.5883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6562, device='cuda:0')



h[200].sum tensor(91.1571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0666, 0.0000, 0.0411,  ..., 0.0393, 0.0000, 0.1389],
        [0.0513, 0.0000, 0.0318,  ..., 0.0322, 0.0000, 0.1108],
        [0.0196, 0.0000, 0.0122,  ..., 0.0160, 0.0000, 0.0471],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70901.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0352, 0.0000, 0.1983,  ..., 0.1534, 0.0000, 0.3816],
        [0.0468, 0.0000, 0.1712,  ..., 0.1422, 0.0000, 0.3344],
        [0.0667, 0.0000, 0.1174,  ..., 0.1215, 0.0013, 0.2432],
        ...,
        [0.1060, 0.1316, 0.0023,  ..., 0.0862, 0.1257, 0.0567],
        [0.1059, 0.1315, 0.0023,  ..., 0.0861, 0.1256, 0.0566],
        [0.1059, 0.1314, 0.0023,  ..., 0.0861, 0.1255, 0.0566]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692768.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10944.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.4465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(434.1090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(306.3250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2778],
        [ 0.2823],
        [ 0.3139],
        ...,
        [-3.7900],
        [-3.7818],
        [-3.7796]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271918.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0299],
        [1.0346],
        ...,
        [0.9968],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371343.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0346],
        ...,
        [0.9968],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371348.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0104, -0.0010,  0.0064,  ...,  0.0068, -0.0014,  0.0230],
        [ 0.0095, -0.0009,  0.0059,  ...,  0.0064, -0.0013,  0.0213],
        [ 0.0157, -0.0014,  0.0097,  ...,  0.0093, -0.0021,  0.0328],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3410.3818, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.5997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0206, device='cuda:0')



h[100].sum tensor(-11.0014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7625, device='cuda:0')



h[200].sum tensor(93.9999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0538, 0.0000, 0.0333,  ..., 0.0333, 0.0000, 0.1152],
        [0.0465, 0.0000, 0.0289,  ..., 0.0299, 0.0000, 0.1019],
        [0.0319, 0.0000, 0.0200,  ..., 0.0231, 0.0000, 0.0748],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75539.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0475, 0.0000, 0.1738,  ..., 0.1404, 0.0000, 0.3359],
        [0.0562, 0.0000, 0.1548,  ..., 0.1322, 0.0000, 0.3022],
        [0.0664, 0.0000, 0.1304,  ..., 0.1217, 0.0000, 0.2594],
        ...,
        [0.1062, 0.1318, 0.0022,  ..., 0.0862, 0.1261, 0.0567],
        [0.1061, 0.1317, 0.0022,  ..., 0.0861, 0.1260, 0.0567],
        [0.1060, 0.1316, 0.0022,  ..., 0.0861, 0.1259, 0.0567]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(719380.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10867.7803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(901.8131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(466.3898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(327.0289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4770],
        [ 0.5110],
        [ 0.5386],
        ...,
        [-3.7976],
        [-3.7494],
        [-3.5998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292263.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0346],
        ...,
        [0.9968],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371348.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.0244, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9968],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371353.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.0244, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2616.7939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.2028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0125, device='cuda:0')



h[100].sum tensor(-6.7226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4617, device='cuda:0')



h[200].sum tensor(85.4571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0140, 0.0000, 0.0088,  ..., 0.0134, 0.0000, 0.0367],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62686.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0907, 0.0338, 0.0511,  ..., 0.0952, 0.0354, 0.1319],
        [0.0990, 0.0835, 0.0196,  ..., 0.0873, 0.0841, 0.0826],
        [0.1020, 0.1158, 0.0068,  ..., 0.0843, 0.1113, 0.0633],
        ...,
        [0.1064, 0.1320, 0.0021,  ..., 0.0863, 0.1266, 0.0567],
        [0.1063, 0.1319, 0.0021,  ..., 0.0863, 0.1265, 0.0566],
        [0.1062, 0.1319, 0.0021,  ..., 0.0862, 0.1265, 0.0566]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656540., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11079.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(850.1995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(389.7517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(261.3384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4868],
        [-1.3410],
        [-2.1601],
        ...,
        [-3.8233],
        [-3.8147],
        [-3.8125]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332946.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9968],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371353.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3679],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3269, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9967],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371359.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3679],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3269, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049, -0.0005,  0.0031,  ...,  0.0043, -0.0008,  0.0130],
        [ 0.0063, -0.0006,  0.0040,  ...,  0.0049, -0.0009,  0.0155],
        [ 0.0113, -0.0010,  0.0070,  ...,  0.0073, -0.0015,  0.0246],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2986.0574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0163, device='cuda:0')



h[100].sum tensor(-8.6952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6018, device='cuda:0')



h[200].sum tensor(89.5214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0000, 0.0098,  ..., 0.0147, 0.0000, 0.0419],
        [0.0273, 0.0000, 0.0170,  ..., 0.0203, 0.0000, 0.0637],
        [0.0405, 0.0000, 0.0252,  ..., 0.0271, 0.0000, 0.0907],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68850.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0848, 0.0000, 0.0792,  ..., 0.1008, 0.0000, 0.1737],
        [0.0764, 0.0000, 0.0984,  ..., 0.1103, 0.0000, 0.2078],
        [0.0613, 0.0000, 0.1358,  ..., 0.1258, 0.0000, 0.2718],
        ...,
        [0.1064, 0.1322, 0.0021,  ..., 0.0863, 0.1269, 0.0567],
        [0.1063, 0.1321, 0.0021,  ..., 0.0863, 0.1268, 0.0566],
        [0.1063, 0.1320, 0.0021,  ..., 0.0862, 0.1267, 0.0566]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690471.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10974.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.3060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(426.6978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.9181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5782],
        [ 0.5054],
        [ 0.3912],
        ...,
        [-3.8338],
        [-3.8251],
        [-3.8229]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305255.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9967],
        [0.9957],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371359.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2832],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(268.1934, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9967],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371364.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2832],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(268.1934, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0095, -0.0009,  0.0059,  ...,  0.0064, -0.0013,  0.0214],
        [ 0.0200, -0.0017,  0.0123,  ...,  0.0113, -0.0026,  0.0408],
        [ 0.0081, -0.0008,  0.0051,  ...,  0.0058, -0.0011,  0.0188],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0014,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3351.8296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0198, device='cuda:0')



h[100].sum tensor(-10.6275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7326, device='cuda:0')



h[200].sum tensor(93.4486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2584, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0580, 0.0000, 0.0359,  ..., 0.0353, 0.0000, 0.1231],
        [0.0449, 0.0000, 0.0279,  ..., 0.0292, 0.0000, 0.0988],
        [0.0946, 0.0000, 0.0582,  ..., 0.0525, 0.0000, 0.1906],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72571.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0432, 0.0000, 0.1798,  ..., 0.1422, 0.0000, 0.3459],
        [0.0438, 0.0000, 0.1791,  ..., 0.1419, 0.0000, 0.3452],
        [0.0224, 0.0000, 0.2235,  ..., 0.1628, 0.0000, 0.4273],
        ...,
        [0.1064, 0.1322, 0.0022,  ..., 0.0863, 0.1270, 0.0568],
        [0.1063, 0.1321, 0.0022,  ..., 0.0862, 0.1269, 0.0567],
        [0.1063, 0.1321, 0.0022,  ..., 0.0862, 0.1268, 0.0567]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700461.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10908.5254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(892.6409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(449.5487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(309.7888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4217],
        [ 0.4155],
        [ 0.3918],
        ...,
        [-3.8371],
        [-3.8287],
        [-3.8267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301122.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9967],
        [0.9956],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371364.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6021],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.3245, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9967],
        [0.9956],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371369.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6021],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.3245, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0221, -0.0019,  0.0136,  ...,  0.0123, -0.0028,  0.0446],
        [ 0.0096, -0.0009,  0.0060,  ...,  0.0065, -0.0013,  0.0217],
        [ 0.0111, -0.0010,  0.0069,  ...,  0.0072, -0.0015,  0.0244],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3265.3511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0189, device='cuda:0')



h[100].sum tensor(-10.1095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7002, device='cuda:0')



h[200].sum tensor(92.3876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0352, 0.0000, 0.0219,  ..., 0.0240, 0.0000, 0.0785],
        [0.0786, 0.0000, 0.0485,  ..., 0.0450, 0.0000, 0.1613],
        [0.0367, 0.0000, 0.0226,  ..., 0.0240, 0.0000, 0.0787],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71613.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0592, 0.0000, 0.1267,  ..., 0.1253, 0.0000, 0.2604],
        [0.0440, 0.0000, 0.1641,  ..., 0.1406, 0.0000, 0.3250],
        [0.0611, 0.0000, 0.1190,  ..., 0.1241, 0.0000, 0.2496],
        ...,
        [0.1063, 0.1322, 0.0023,  ..., 0.0863, 0.1269, 0.0570],
        [0.1056, 0.1266, 0.0036,  ..., 0.0868, 0.1223, 0.0601],
        [0.1037, 0.1082, 0.0105,  ..., 0.0886, 0.1072, 0.0706]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693645.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10905.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.9698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(441.1657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.3615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3787],
        [ 0.4374],
        [ 0.3443],
        ...,
        [-3.7386],
        [-3.5461],
        [-3.1837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294877.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0347],
        ...,
        [0.9967],
        [0.9956],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371369.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 670.0 event: 3350 loss: tensor(492.4258, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3423],
        [0.3425],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.3403, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0348],
        ...,
        [0.9967],
        [0.9956],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371375.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3423],
        [0.3425],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.3403, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0267, -0.0023,  0.0164,  ...,  0.0145, -0.0033,  0.0532],
        [ 0.0226, -0.0019,  0.0139,  ...,  0.0125, -0.0028,  0.0456],
        [ 0.0223, -0.0019,  0.0137,  ...,  0.0124, -0.0028,  0.0451],
        ...,
        [ 0.0165, -0.0014,  0.0102,  ...,  0.0097, -0.0021,  0.0344],
        [ 0.0074, -0.0007,  0.0046,  ...,  0.0054, -0.0010,  0.0175],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2911.2537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0152, device='cuda:0')



h[100].sum tensor(-8.1520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5636, device='cuda:0')



h[200].sum tensor(88.5045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2006, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1094, 0.0000, 0.0672,  ..., 0.0594, 0.0000, 0.2182],
        [0.0951, 0.0000, 0.0584,  ..., 0.0527, 0.0000, 0.1918],
        [0.0841, 0.0000, 0.0517,  ..., 0.0475, 0.0000, 0.1715],
        ...,
        [0.0401, 0.0000, 0.0250,  ..., 0.0272, 0.0000, 0.0909],
        [0.0360, 0.0000, 0.0223,  ..., 0.0246, 0.0000, 0.0805],
        [0.0140, 0.0000, 0.0088,  ..., 0.0136, 0.0000, 0.0374]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66295.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.0532e-04, 0.0000e+00, 2.9200e-01,  ..., 1.9212e-01, 0.0000e+00,
         5.4806e-01],
        [4.8447e-03, 0.0000e+00, 2.6713e-01,  ..., 1.8137e-01, 0.0000e+00,
         5.0338e-01],
        [1.2088e-02, 0.0000e+00, 2.4045e-01,  ..., 1.6897e-01, 0.0000e+00,
         4.5435e-01],
        ...,
        [6.7703e-02, 0.0000e+00, 1.3029e-01,  ..., 1.2337e-01, 0.0000e+00,
         2.5835e-01],
        [7.2385e-02, 0.0000e+00, 1.1224e-01,  ..., 1.1865e-01, 2.6547e-03,
         2.3105e-01],
        [8.7272e-02, 3.1186e-02, 6.6530e-02,  ..., 1.0418e-01, 3.4655e-02,
         1.5813e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671023.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10974.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.0248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(406.2618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(272.5266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2786],
        [ 0.2942],
        [ 0.3140],
        ...,
        [ 0.2845],
        [-0.0996],
        [-1.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302406.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0348],
        ...,
        [0.9967],
        [0.9956],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371375.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.0686, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0348],
        ...,
        [0.9966],
        [0.9956],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371381., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.0686, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3668.6194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0228, device='cuda:0')



h[100].sum tensor(-12.1119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8443, device='cuda:0')



h[200].sum tensor(96.6983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.2791, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77858.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1005, 0.1067, 0.0115,  ..., 0.0843, 0.1035, 0.0690],
        [0.1022, 0.1242, 0.0041,  ..., 0.0836, 0.1184, 0.0587],
        [0.1024, 0.1285, 0.0027,  ..., 0.0836, 0.1224, 0.0557],
        ...,
        [0.1061, 0.1322, 0.0028,  ..., 0.0862, 0.1267, 0.0573],
        [0.1060, 0.1321, 0.0028,  ..., 0.0861, 0.1266, 0.0573],
        [0.1059, 0.1320, 0.0028,  ..., 0.0861, 0.1266, 0.0572]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723111.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10724.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(907.0007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(476.1657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.3880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7903],
        [-2.4917],
        [-2.9935],
        ...,
        [-3.6113],
        [-3.7581],
        [-3.7974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265761.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0348],
        ...,
        [0.9966],
        [0.9956],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371381., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4448],
        [0.2600],
        [0.4456],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.8593, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0348],
        ...,
        [0.9966],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371386.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4448],
        [0.2600],
        [0.4456],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.8593, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0309, -0.0026,  0.0190,  ...,  0.0164, -0.0038,  0.0611],
        [ 0.0319, -0.0026,  0.0195,  ...,  0.0169, -0.0039,  0.0628],
        [ 0.0282, -0.0024,  0.0173,  ...,  0.0151, -0.0035,  0.0560],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2916.3967, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0150, device='cuda:0')



h[100].sum tensor(-8.0643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5569, device='cuda:0')



h[200].sum tensor(88.7405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1039, 0.0000, 0.0638,  ..., 0.0567, 0.0000, 0.2080],
        [0.1268, 0.0000, 0.0778,  ..., 0.0675, 0.0000, 0.2505],
        [0.1164, 0.0000, 0.0715,  ..., 0.0626, 0.0000, 0.2314],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67539.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0000, 0.2754,  ..., 0.1837, 0.0000, 0.5160],
        [0.0000, 0.0000, 0.3128,  ..., 0.2005, 0.0000, 0.5830],
        [0.0027, 0.0000, 0.2915,  ..., 0.1910, 0.0000, 0.5446],
        ...,
        [0.1061, 0.1322, 0.0028,  ..., 0.0864, 0.1270, 0.0571],
        [0.1060, 0.1321, 0.0028,  ..., 0.0863, 0.1269, 0.0571],
        [0.1060, 0.1320, 0.0028,  ..., 0.0863, 0.1269, 0.0571]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681172.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10858.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(866.1622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(413.4207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(277.0879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2921],
        [ 0.2707],
        [ 0.2928],
        ...,
        [-3.8339],
        [-3.8256],
        [-3.8235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295568.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0348],
        ...,
        [0.9966],
        [0.9955],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371386.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(461.8785, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0348],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371391.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(461.8785, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4887.1665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0341, device='cuda:0')



h[100].sum tensor(-18.4827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.2617, device='cuda:0')



h[200].sum tensor(110.1514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96727.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0993, 0.1038, 0.0118,  ..., 0.0861, 0.1024, 0.0691],
        [0.1013, 0.1161, 0.0079,  ..., 0.0850, 0.1123, 0.0629],
        [0.1024, 0.1255, 0.0043,  ..., 0.0841, 0.1202, 0.0574],
        ...,
        [0.1061, 0.1323, 0.0028,  ..., 0.0866, 0.1272, 0.0569],
        [0.1060, 0.1322, 0.0028,  ..., 0.0865, 0.1271, 0.0569],
        [0.1060, 0.1322, 0.0028,  ..., 0.0865, 0.1270, 0.0568]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(818076.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10477.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(980.9473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(594.8469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(422.2569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3739],
        [-1.8335],
        [-2.3112],
        ...,
        [-3.8417],
        [-3.8333],
        [-3.8313]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263046.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0348],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371391.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(258.0253, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0348],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371395.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(258.0253, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3339.9238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0190, device='cuda:0')



h[100].sum tensor(-10.2214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7048, device='cuda:0')



h[200].sum tensor(93.9046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0000, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73175.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1008, 0.1163, 0.0064,  ..., 0.0852, 0.1126, 0.0618],
        [0.1024, 0.1291, 0.0025,  ..., 0.0843, 0.1231, 0.0549],
        [0.1026, 0.1279, 0.0033,  ..., 0.0841, 0.1221, 0.0558],
        ...,
        [0.1062, 0.1325, 0.0028,  ..., 0.0869, 0.1274, 0.0566],
        [0.1061, 0.1324, 0.0028,  ..., 0.0868, 0.1273, 0.0566],
        [0.1061, 0.1323, 0.0027,  ..., 0.0868, 0.1272, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(705034.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10757.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(889.8239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(449.5858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(306.8481, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9322],
        [-3.1396],
        [-3.0493],
        ...,
        [-3.8538],
        [-3.8453],
        [-3.8433]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301433.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0348],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371395.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.2830, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0349],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371401.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.2830, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [ 0.0061, -0.0006,  0.0039,  ...,  0.0048, -0.0009,  0.0152],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3303.8867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.2353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-9.9727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6973, device='cuda:0')



h[200].sum tensor(93.7867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0359, 0.0000, 0.0225,  ..., 0.0247, 0.0000, 0.0824],
        [0.0049, 0.0000, 0.0032,  ..., 0.0083, 0.0000, 0.0177],
        [0.0063, 0.0000, 0.0040,  ..., 0.0090, 0.0000, 0.0202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71165.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0719, 0.0000, 0.1020,  ..., 0.1113, 0.0004, 0.2100],
        [0.0870, 0.0187, 0.0604,  ..., 0.0983, 0.0202, 0.1435],
        [0.0878, 0.0049, 0.0570,  ..., 0.0978, 0.0092, 0.1382],
        ...,
        [0.1062, 0.1325, 0.0028,  ..., 0.0871, 0.1274, 0.0564],
        [0.1061, 0.1324, 0.0027,  ..., 0.0871, 0.1273, 0.0564],
        [0.1061, 0.1324, 0.0027,  ..., 0.0870, 0.1273, 0.0564]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690870.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10854.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(891.8340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(432.3609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.3888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2787],
        [ 0.2778],
        [ 0.3009],
        ...,
        [-3.8607],
        [-3.8523],
        [-3.8503]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274612.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0349],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371401.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6621],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0349],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371406.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6621],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [ 0.0232, -0.0019,  0.0143,  ...,  0.0128, -0.0028,  0.0467],
        [ 0.0095, -0.0008,  0.0059,  ...,  0.0064, -0.0013,  0.0215],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3179.8857, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-9.2664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6447, device='cuda:0')



h[200].sum tensor(92.2555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6674, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0566, 0.0000, 0.0350,  ..., 0.0338, 0.0000, 0.1182],
        [0.0547, 0.0000, 0.0338,  ..., 0.0329, 0.0000, 0.1147],
        [0.0753, 0.0000, 0.0466,  ..., 0.0432, 0.0000, 0.1554],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70327.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0000, 0.1848,  ..., 0.1465, 0.0000, 0.3540],
        [0.0365, 0.0000, 0.1742,  ..., 0.1431, 0.0000, 0.3362],
        [0.0379, 0.0000, 0.1709,  ..., 0.1419, 0.0000, 0.3309],
        ...,
        [0.1061, 0.1326, 0.0028,  ..., 0.0873, 0.1275, 0.0565],
        [0.1060, 0.1325, 0.0028,  ..., 0.0872, 0.1274, 0.0564],
        [0.1060, 0.1324, 0.0028,  ..., 0.0871, 0.1273, 0.0564]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(694573.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10728.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.5585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(433.3499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(291.0556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3856],
        [ 0.4213],
        [ 0.4596],
        ...,
        [-3.8624],
        [-3.8542],
        [-3.8523]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314933.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0349],
        ...,
        [0.9966],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371406.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1798, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0349],
        ...,
        [0.9965],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371410.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1798, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0408, -0.0033,  0.0250,  ...,  0.0210, -0.0049,  0.0793],
        [ 0.0114, -0.0010,  0.0071,  ...,  0.0073, -0.0015,  0.0250],
        [ 0.0131, -0.0011,  0.0081,  ...,  0.0080, -0.0017,  0.0281],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3290.9399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-9.7884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6861, device='cuda:0')



h[200].sum tensor(93.4265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4173, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1221, 0.0000, 0.0750,  ..., 0.0651, 0.0000, 0.2418],
        [0.0969, 0.0000, 0.0597,  ..., 0.0533, 0.0000, 0.1953],
        [0.0225, 0.0000, 0.0140,  ..., 0.0172, 0.0000, 0.0529],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70958.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0000, 0.3081,  ..., 0.1987, 0.0000, 0.5740],
        [0.0227, 0.0000, 0.2221,  ..., 0.1637, 0.0000, 0.4232],
        [0.0548, 0.0000, 0.1233,  ..., 0.1246, 0.0000, 0.2524],
        ...,
        [0.1059, 0.1328, 0.0031,  ..., 0.0869, 0.1273, 0.0566],
        [0.1058, 0.1327, 0.0031,  ..., 0.0869, 0.1272, 0.0566],
        [0.1057, 0.1326, 0.0031,  ..., 0.0868, 0.1272, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689456.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10715.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(882.4639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(431.3022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.6792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2462],
        [ 0.3334],
        [ 0.4400],
        ...,
        [-3.8545],
        [-3.8464],
        [-3.8447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273668.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0297],
        [1.0349],
        ...,
        [0.9965],
        [0.9955],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371410.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2563],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.1670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0349],
        ...,
        [0.9965],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371414.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2563],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.1670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040, -0.0004,  0.0026,  ...,  0.0038, -0.0006,  0.0113],
        [ 0.0041, -0.0004,  0.0026,  ...,  0.0038, -0.0006,  0.0115],
        [ 0.0094, -0.0008,  0.0059,  ...,  0.0063, -0.0012,  0.0213],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [ 0.0078, -0.0007,  0.0049,  ...,  0.0056, -0.0011,  0.0185],
        [ 0.0078, -0.0007,  0.0049,  ...,  0.0056, -0.0011,  0.0185]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3276.0769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-9.6883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6779, device='cuda:0')



h[200].sum tensor(92.8727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0000, 0.0132,  ..., 0.0171, 0.0000, 0.0527],
        [0.0246, 0.0000, 0.0156,  ..., 0.0194, 0.0000, 0.0619],
        [0.0105, 0.0000, 0.0069,  ..., 0.0122, 0.0000, 0.0334],
        ...,
        [0.0150, 0.0000, 0.0094,  ..., 0.0139, 0.0000, 0.0395],
        [0.0150, 0.0000, 0.0094,  ..., 0.0138, 0.0000, 0.0395],
        [0.0150, 0.0000, 0.0094,  ..., 0.0138, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70442.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0784, 0.0000, 0.0887,  ..., 0.1026, 0.0000, 0.1828],
        [0.0820, 0.0000, 0.0850,  ..., 0.0999, 0.0000, 0.1752],
        [0.0882, 0.0174, 0.0644,  ..., 0.0949, 0.0176, 0.1443],
        ...,
        [0.0937, 0.0379, 0.0472,  ..., 0.0969, 0.0403, 0.1224],
        [0.0911, 0.0165, 0.0566,  ..., 0.0989, 0.0207, 0.1361],
        [0.0910, 0.0165, 0.0566,  ..., 0.0989, 0.0207, 0.1361]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684513.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10696.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(877.5308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(427.1634, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(286.5696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5872],
        [ 0.4469],
        [ 0.0590],
        ...,
        [-2.0188],
        [-1.5393],
        [-1.5377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267611.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0349],
        ...,
        [0.9965],
        [0.9954],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371414.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2668],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.9952, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0349],
        ...,
        [0.9965],
        [0.9954],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371418.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2668],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.9952, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0016],
        [ 0.0042, -0.0004,  0.0027,  ...,  0.0039, -0.0006,  0.0117],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0016],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2776.1201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.7769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0135, device='cuda:0')



h[100].sum tensor(-7.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4999, device='cuda:0')



h[200].sum tensor(87.0493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0151, 0.0000, 0.0098,  ..., 0.0150, 0.0000, 0.0445],
        [0.0033, 0.0000, 0.0022,  ..., 0.0075, 0.0000, 0.0150],
        [0.0043, 0.0000, 0.0028,  ..., 0.0080, 0.0000, 0.0169],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63863.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0958, 0.0566, 0.0355,  ..., 0.0879, 0.0619, 0.1010],
        [0.0987, 0.0875, 0.0215,  ..., 0.0865, 0.0878, 0.0822],
        [0.0994, 0.0967, 0.0171,  ..., 0.0862, 0.0957, 0.0762],
        ...,
        [0.1057, 0.1336, 0.0032,  ..., 0.0868, 0.1277, 0.0570],
        [0.1056, 0.1335, 0.0032,  ..., 0.0868, 0.1276, 0.0569],
        [0.1055, 0.1335, 0.0032,  ..., 0.0867, 0.1276, 0.0569]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658782., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10751.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(842.8040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(389.9996, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(249.4084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8936],
        [-2.3974],
        [-2.8725],
        ...,
        [-3.8661],
        [-3.8580],
        [-3.8562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310979.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0298],
        [1.0349],
        ...,
        [0.9965],
        [0.9954],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371418.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 680.0 event: 3400 loss: tensor(491.7960, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(267.2025, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0318],
        [1.0298],
        [1.0349],
        ...,
        [0.9965],
        [0.9954],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371422.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(267.2025, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3420.5630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-10.4577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7299, device='cuda:0')



h[200].sum tensor(93.8648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75344.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1015, 0.1305, 0.0025,  ..., 0.0841, 0.1237, 0.0546],
        [0.1019, 0.1310, 0.0026,  ..., 0.0844, 0.1241, 0.0548],
        [0.1021, 0.1308, 0.0027,  ..., 0.0844, 0.1242, 0.0549],
        ...,
        [0.1057, 0.1341, 0.0031,  ..., 0.0869, 0.1281, 0.0568],
        [0.1056, 0.1340, 0.0031,  ..., 0.0868, 0.1280, 0.0568],
        [0.1056, 0.1340, 0.0031,  ..., 0.0868, 0.1279, 0.0568]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718524.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10513.0400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(893.6696, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(460.5237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(309.0311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4826],
        [-3.7249],
        [-3.8747],
        ...,
        [-3.8353],
        [-3.7620],
        [-3.6798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281744.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0318],
        [1.0298],
        [1.0349],
        ...,
        [0.9965],
        [0.9954],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371422.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5840],
        [0.5957],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.4683, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0318],
        [1.0298],
        [1.0350],
        ...,
        [0.9965],
        [0.9954],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371426.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5840],
        [0.5957],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.4683, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0237, -0.0019,  0.0146,  ...,  0.0130, -0.0029,  0.0477],
        [ 0.0107, -0.0009,  0.0067,  ...,  0.0070, -0.0014,  0.0238],
        [ 0.0269, -0.0022,  0.0166,  ...,  0.0146, -0.0032,  0.0537],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3069.1147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-8.6498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6104, device='cuda:0')



h[200].sum tensor(89.9846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0978, 0.0000, 0.0603,  ..., 0.0538, 0.0000, 0.1970],
        [0.0881, 0.0000, 0.0544,  ..., 0.0493, 0.0000, 0.1792],
        [0.0322, 0.0000, 0.0201,  ..., 0.0224, 0.0000, 0.0734],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68146.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0063, 0.0000, 0.2703,  ..., 0.1795, 0.0000, 0.4984],
        [0.0165, 0.0000, 0.2186,  ..., 0.1594, 0.0000, 0.4098],
        [0.0466, 0.0000, 0.1447,  ..., 0.1312, 0.0000, 0.2842],
        ...,
        [0.1059, 0.1347, 0.0029,  ..., 0.0870, 0.1287, 0.0565],
        [0.1058, 0.1346, 0.0029,  ..., 0.0869, 0.1286, 0.0565],
        [0.1057, 0.1345, 0.0029,  ..., 0.0869, 0.1286, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677002.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10707.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(869.8901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(418.3507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(275.3635, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3348],
        [ 0.3904],
        [ 0.4594],
        ...,
        [-3.9088],
        [-3.9004],
        [-3.8987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291688.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0318],
        [1.0298],
        [1.0350],
        ...,
        [0.9965],
        [0.9954],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371426.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.8387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0298],
        [1.0351],
        ...,
        [0.9964],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371430.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.8387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [ 0.0037, -0.0004,  0.0024,  ...,  0.0037, -0.0006,  0.0108],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3329.9204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.3684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0193, device='cuda:0')



h[100].sum tensor(-10.0023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7152, device='cuda:0')



h[200].sum tensor(93.0676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0062],
        [0.0038, 0.0000, 0.0025,  ..., 0.0078, 0.0000, 0.0158],
        [0.0107, 0.0000, 0.0069,  ..., 0.0117, 0.0000, 0.0310],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72198.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0997, 0.0948, 0.0193,  ..., 0.0848, 0.0930, 0.0788],
        [0.0972, 0.0722, 0.0277,  ..., 0.0877, 0.0749, 0.0916],
        [0.0926, 0.0395, 0.0436,  ..., 0.0918, 0.0446, 0.1152],
        ...,
        [0.1059, 0.1351, 0.0027,  ..., 0.0871, 0.1291, 0.0561],
        [0.1059, 0.1350, 0.0027,  ..., 0.0870, 0.1290, 0.0561],
        [0.1058, 0.1349, 0.0027,  ..., 0.0870, 0.1290, 0.0561]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695482., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10609.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.9586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.0831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.4604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3716],
        [-1.1998],
        [-1.0113],
        ...,
        [-3.9311],
        [-3.9223],
        [-3.9203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307920.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0298],
        [1.0351],
        ...,
        [0.9964],
        [0.9953],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371430.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5737, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0298],
        [1.0352],
        ...,
        [0.9964],
        [0.9953],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371434.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5737, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0229, -0.0018,  0.0141,  ...,  0.0127, -0.0028,  0.0463],
        [ 0.0256, -0.0020,  0.0157,  ...,  0.0139, -0.0030,  0.0511],
        [ 0.0182, -0.0015,  0.0113,  ...,  0.0105, -0.0022,  0.0376],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3262.1113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-9.6152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6790, device='cuda:0')



h[200].sum tensor(92.7021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0919, 0.0000, 0.0567,  ..., 0.0510, 0.0000, 0.1860],
        [0.0797, 0.0000, 0.0493,  ..., 0.0453, 0.0000, 0.1636],
        [0.0713, 0.0000, 0.0441,  ..., 0.0413, 0.0000, 0.1479],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71216.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0054, 0.0000, 0.2366,  ..., 0.1647, 0.0000, 0.4359],
        [0.0141, 0.0000, 0.2200,  ..., 0.1581, 0.0000, 0.4074],
        [0.0232, 0.0000, 0.2025,  ..., 0.1507, 0.0000, 0.3768],
        ...,
        [0.1060, 0.1353, 0.0027,  ..., 0.0871, 0.1292, 0.0560],
        [0.1059, 0.1352, 0.0026,  ..., 0.0871, 0.1291, 0.0559],
        [0.1058, 0.1352, 0.0026,  ..., 0.0870, 0.1290, 0.0559]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691088.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10649.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.7419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(440.6158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(295.1363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3566],
        [ 0.3724],
        [ 0.3930],
        ...,
        [-3.9390],
        [-3.9303],
        [-3.9283]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309286.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0298],
        [1.0352],
        ...,
        [0.9964],
        [0.9953],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371434.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.8618, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0298],
        [1.0352],
        ...,
        [0.9963],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371438.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.8618, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3079.7710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-8.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6115, device='cuda:0')



h[200].sum tensor(90.9907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0668, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0095, 0.0000, 0.0060,  ..., 0.0104, 0.0000, 0.0263],
        [0.0192, 0.0000, 0.0120,  ..., 0.0156, 0.0000, 0.0467],
        [0.0096, 0.0000, 0.0060,  ..., 0.0104, 0.0000, 0.0264],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68452.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0895, 0.0436, 0.0393,  ..., 0.0947, 0.0479, 0.1120],
        [0.0849, 0.0175, 0.0551,  ..., 0.0991, 0.0215, 0.1357],
        [0.0900, 0.0438, 0.0396,  ..., 0.0950, 0.0483, 0.1124],
        ...,
        [0.1041, 0.1178, 0.0093,  ..., 0.0887, 0.1145, 0.0663],
        [0.1054, 0.1312, 0.0038,  ..., 0.0875, 0.1254, 0.0584],
        [0.1058, 0.1353, 0.0027,  ..., 0.0871, 0.1287, 0.0560]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678833.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10637.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(872.9401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(423.7047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(280.9250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6302],
        [-1.4019],
        [-1.6541],
        ...,
        [-3.4414],
        [-3.7122],
        [-3.8527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321351.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0298],
        [1.0352],
        ...,
        [0.9963],
        [0.9952],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371438.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.5405]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.2250, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0353],
        ...,
        [0.9963],
        [0.9952],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371443.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.5405]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.2250, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [ 0.0098, -0.0008,  0.0061,  ...,  0.0065, -0.0013,  0.0221],
        [ 0.0083, -0.0007,  0.0052,  ...,  0.0058, -0.0011,  0.0194]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3568.7622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0213, device='cuda:0')



h[100].sum tensor(-11.0637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7873, device='cuda:0')



h[200].sum tensor(96.5320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0063],
        ...,
        [0.0105, 0.0000, 0.0066,  ..., 0.0110, 0.0000, 0.0285],
        [0.0256, 0.0000, 0.0160,  ..., 0.0188, 0.0000, 0.0591],
        [0.0564, 0.0000, 0.0350,  ..., 0.0345, 0.0000, 0.1211]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77342.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1007, 0.1182, 0.0082,  ..., 0.0847, 0.1123, 0.0628],
        [0.1014, 0.1194, 0.0078,  ..., 0.0845, 0.1130, 0.0631],
        [0.0993, 0.0964, 0.0183,  ..., 0.0861, 0.0942, 0.0768],
        ...,
        [0.0906, 0.0478, 0.0452,  ..., 0.0994, 0.0502, 0.1224],
        [0.0774, 0.0097, 0.0827,  ..., 0.1101, 0.0141, 0.1799],
        [0.0644, 0.0000, 0.1177,  ..., 0.1206, 0.0000, 0.2343]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(724108.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10478.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(909.4732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(474.7905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.9415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1286],
        [-1.9383],
        [-1.4209],
        ...,
        [-1.8990],
        [-0.9481],
        [-0.4090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283877.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0353],
        ...,
        [0.9963],
        [0.9952],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371443.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3193],
        [0.2859],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0316, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0353],
        ...,
        [0.9962],
        [0.9951],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371448.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3193],
        [0.2859],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0316, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053, -0.0005,  0.0034,  ...,  0.0043, -0.0007,  0.0137],
        [ 0.0090, -0.0008,  0.0056,  ...,  0.0061, -0.0012,  0.0207],
        [ 0.0193, -0.0016,  0.0119,  ...,  0.0109, -0.0023,  0.0397],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0013,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3254.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.0028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-9.3163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6721, device='cuda:0')



h[200].sum tensor(93.2969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1628, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0172, 0.0000, 0.0108,  ..., 0.0145, 0.0000, 0.0432],
        [0.0500, 0.0000, 0.0311,  ..., 0.0312, 0.0000, 0.1089],
        [0.0681, 0.0000, 0.0421,  ..., 0.0397, 0.0000, 0.1423],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70546.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0744, 0.0235, 0.0781,  ..., 0.1056, 0.0274, 0.1716],
        [0.0466, 0.0000, 0.1471,  ..., 0.1286, 0.0000, 0.2819],
        [0.0208, 0.0000, 0.2051,  ..., 0.1497, 0.0000, 0.3772],
        ...,
        [0.1054, 0.1355, 0.0031,  ..., 0.0868, 0.1278, 0.0566],
        [0.1054, 0.1354, 0.0031,  ..., 0.0867, 0.1277, 0.0565],
        [0.1053, 0.1353, 0.0031,  ..., 0.0867, 0.1276, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686228.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10475.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(878.9462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(431.5764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(290.0730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0120],
        [ 0.2675],
        [ 0.3353],
        ...,
        [-3.9122],
        [-3.8987],
        [-3.8672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283232.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0353],
        ...,
        [0.9962],
        [0.9951],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371448.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.1014, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0352],
        ...,
        [0.9962],
        [0.9950],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371452.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.1014, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3286.7993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0181, device='cuda:0')



h[100].sum tensor(-9.3722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6695, device='cuda:0')



h[200].sum tensor(93.6415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1168, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0064],
        [0.0315, 0.0000, 0.0195,  ..., 0.0212, 0.0000, 0.0697],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71579.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0959, 0.0936, 0.0155,  ..., 0.0882, 0.0924, 0.0757],
        [0.0887, 0.0547, 0.0379,  ..., 0.0945, 0.0525, 0.1096],
        [0.0687, 0.0074, 0.0914,  ..., 0.1106, 0.0095, 0.1931],
        ...,
        [0.1051, 0.1355, 0.0032,  ..., 0.0868, 0.1273, 0.0570],
        [0.1050, 0.1354, 0.0032,  ..., 0.0868, 0.1272, 0.0570],
        [0.1050, 0.1354, 0.0032,  ..., 0.0867, 0.1272, 0.0569]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692159., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10357.1299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.4728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(437.4112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.9354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3763],
        [-0.8467],
        [-0.2022],
        ...,
        [-3.9047],
        [-3.8965],
        [-3.8947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288498.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0352],
        ...,
        [0.9962],
        [0.9950],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371452.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.8385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0352],
        ...,
        [0.9961],
        [0.9950],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371457.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.8385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3255.9863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.2059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0178, device='cuda:0')



h[100].sum tensor(-9.1691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6579, device='cuda:0')



h[200].sum tensor(93.7950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0064],
        [0.0051, 0.0000, 0.0033,  ..., 0.0081, 0.0000, 0.0185],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70881.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0984, 0.1054, 0.0127,  ..., 0.0865, 0.1010, 0.0700],
        [0.0935, 0.0731, 0.0259,  ..., 0.0909, 0.0709, 0.0909],
        [0.0838, 0.0315, 0.0538,  ..., 0.0986, 0.0319, 0.1340],
        ...,
        [0.1054, 0.1356, 0.0033,  ..., 0.0870, 0.1269, 0.0569],
        [0.1053, 0.1355, 0.0033,  ..., 0.0869, 0.1268, 0.0568],
        [0.1053, 0.1354, 0.0033,  ..., 0.0869, 0.1268, 0.0568]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687056.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10429.9395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(876.3718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(433.9596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.2289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6903],
        [-0.4133],
        [-0.1007],
        ...,
        [-3.9049],
        [-3.8967],
        [-3.8949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293598.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0298],
        [1.0352],
        ...,
        [0.9961],
        [0.9950],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371457.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.7816, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0298],
        [1.0352],
        ...,
        [0.9961],
        [0.9949],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371462.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.7816, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [ 0.0093, -0.0008,  0.0058,  ...,  0.0062, -0.0012,  0.0212],
        [ 0.0078, -0.0007,  0.0049,  ...,  0.0055, -0.0010,  0.0185],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3674.6018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0221, device='cuda:0')



h[100].sum tensor(-11.2815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8162, device='cuda:0')



h[200].sum tensor(98.8657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0096, 0.0000, 0.0060,  ..., 0.0101, 0.0000, 0.0266],
        [0.0157, 0.0000, 0.0099,  ..., 0.0137, 0.0000, 0.0405],
        [0.0521, 0.0000, 0.0323,  ..., 0.0320, 0.0000, 0.1128],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77800.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0897, 0.0471, 0.0413,  ..., 0.0935, 0.0496, 0.1132],
        [0.0809, 0.0236, 0.0650,  ..., 0.1015, 0.0266, 0.1502],
        [0.0639, 0.0000, 0.1079,  ..., 0.1156, 0.0000, 0.2178],
        ...,
        [0.1059, 0.1357, 0.0033,  ..., 0.0872, 0.1265, 0.0566],
        [0.1058, 0.1356, 0.0033,  ..., 0.0871, 0.1264, 0.0566],
        [0.1058, 0.1355, 0.0032,  ..., 0.0871, 0.1263, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723251.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10406.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(914.3726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(473.8905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(332.1375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3627],
        [-0.1739],
        [-0.0327],
        ...,
        [-3.9075],
        [-3.8993],
        [-3.8976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250240.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0298],
        [1.0352],
        ...,
        [0.9961],
        [0.9949],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371462.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 690.0 event: 3450 loss: tensor(477.4244, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3057],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0298],
        [1.0352],
        ...,
        [0.9960],
        [0.9949],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371466.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3057],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0005,  0.0032,  ...,  0.0042, -0.0007,  0.0132],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [ 0.0113, -0.0009,  0.0071,  ...,  0.0071, -0.0014,  0.0249],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3177.1045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0168, device='cuda:0')



h[100].sum tensor(-8.7088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6218, device='cuda:0')



h[200].sum tensor(93.7428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0039, 0.0000, 0.0026,  ..., 0.0075, 0.0000, 0.0161],
        [0.0282, 0.0000, 0.0178,  ..., 0.0208, 0.0000, 0.0685],
        [0.0213, 0.0000, 0.0134,  ..., 0.0169, 0.0000, 0.0532],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70804.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0934, 0.0478, 0.0398,  ..., 0.0909, 0.0484, 0.1086],
        [0.0828, 0.0056, 0.0785,  ..., 0.0999, 0.0074, 0.1652],
        [0.0793, 0.0009, 0.0916,  ..., 0.1027, 0.0035, 0.1840],
        ...,
        [0.1063, 0.1359, 0.0031,  ..., 0.0875, 0.1264, 0.0566],
        [0.1062, 0.1358, 0.0031,  ..., 0.0875, 0.1263, 0.0565],
        [0.1062, 0.1357, 0.0031,  ..., 0.0874, 0.1262, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(694383.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10601.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(882.5617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(435.2091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(297.2917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3297],
        [ 0.1822],
        [ 0.4647],
        ...,
        [-3.9130],
        [-3.9057],
        [-3.9045]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295672.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0298],
        [1.0352],
        ...,
        [0.9960],
        [0.9949],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371466.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3735],
        [0.3916],
        [0.3811],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.0632, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0298],
        [1.0352],
        ...,
        [0.9960],
        [0.9949],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371470.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3735],
        [0.3916],
        [0.3811],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.0632, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0293, -0.0023,  0.0180,  ...,  0.0155, -0.0034,  0.0580],
        [ 0.0287, -0.0022,  0.0176,  ...,  0.0153, -0.0033,  0.0569],
        [ 0.0236, -0.0018,  0.0146,  ...,  0.0129, -0.0027,  0.0476],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3355.8457, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.2153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0185, device='cuda:0')



h[100].sum tensor(-9.6056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6831, device='cuda:0')



h[200].sum tensor(95.5502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1157, 0.0000, 0.0712,  ..., 0.0617, 0.0000, 0.2301],
        [0.1024, 0.0000, 0.0630,  ..., 0.0555, 0.0000, 0.2054],
        [0.0967, 0.0000, 0.0596,  ..., 0.0529, 0.0000, 0.1950],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73265.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.3289,  ..., 0.2027, 0.0000, 0.5970],
        [0.0000, 0.0000, 0.3073,  ..., 0.1936, 0.0000, 0.5584],
        [0.0027, 0.0000, 0.2844,  ..., 0.1836, 0.0000, 0.5169],
        ...,
        [0.1063, 0.1341, 0.0038,  ..., 0.0880, 0.1246, 0.0580],
        [0.1064, 0.1360, 0.0029,  ..., 0.0878, 0.1262, 0.0567],
        [0.1063, 0.1359, 0.0029,  ..., 0.0877, 0.1261, 0.0567]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702654.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10599.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(897.1555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(449.9445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(311.2482, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1131],
        [ 0.1248],
        [ 0.1395],
        ...,
        [-3.5441],
        [-3.7220],
        [-3.8432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279349., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0298],
        [1.0352],
        ...,
        [0.9960],
        [0.9949],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371470.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.7362, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0298],
        [1.0352],
        ...,
        [0.9960],
        [0.9948],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371474.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.7362, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3193.0298, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0173, device='cuda:0')



h[100].sum tensor(-8.7780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6385, device='cuda:0')



h[200].sum tensor(93.1500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5549, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0000, 0.0105,  ..., 0.0135, 0.0000, 0.0401],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70845.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0867, 0.0444, 0.0412,  ..., 0.0985, 0.0484, 0.1193],
        [0.0979, 0.0955, 0.0144,  ..., 0.0896, 0.0923, 0.0756],
        [0.1013, 0.1138, 0.0094,  ..., 0.0865, 0.1063, 0.0672],
        ...,
        [0.1065, 0.1363, 0.0028,  ..., 0.0881, 0.1262, 0.0572],
        [0.1065, 0.1362, 0.0028,  ..., 0.0880, 0.1261, 0.0571],
        [0.1064, 0.1362, 0.0028,  ..., 0.0880, 0.1260, 0.0571]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693562.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10704.4658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.0277, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(434.8340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.8431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4419],
        [-1.7333],
        [-1.6717],
        ...,
        [-3.9347],
        [-3.9264],
        [-3.9246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270423.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0298],
        [1.0352],
        ...,
        [0.9960],
        [0.9948],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371474.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.9326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0298],
        [1.0352],
        ...,
        [0.9959],
        [0.9948],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371477.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.9326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048, -0.0005,  0.0031,  ...,  0.0041, -0.0007,  0.0129],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3136.9292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.4028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-8.5230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6172, device='cuda:0')



h[200].sum tensor(91.2109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0000, 0.0025,  ..., 0.0074, 0.0000, 0.0160],
        [0.0050, 0.0000, 0.0032,  ..., 0.0080, 0.0000, 0.0182],
        [0.0196, 0.0000, 0.0121,  ..., 0.0148, 0.0000, 0.0451],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70206.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0971, 0.0612, 0.0337,  ..., 0.0887, 0.0614, 0.1008],
        [0.0939, 0.0449, 0.0363,  ..., 0.0926, 0.0498, 0.1077],
        [0.0834, 0.0226, 0.0536,  ..., 0.1023, 0.0303, 0.1395],
        ...,
        [0.1067, 0.1366, 0.0026,  ..., 0.0884, 0.1264, 0.0577],
        [0.1066, 0.1365, 0.0026,  ..., 0.0884, 0.1263, 0.0576],
        [0.1065, 0.1365, 0.0026,  ..., 0.0883, 0.1262, 0.0576]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688680.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10694.1602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(881.1254, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(434.0121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(289.2582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6175],
        [-0.4783],
        [-0.2944],
        ...,
        [-3.9464],
        [-3.9380],
        [-3.9362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274221., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0298],
        [1.0352],
        ...,
        [0.9959],
        [0.9948],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371477.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7485],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.1840, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0299],
        [1.0353],
        ...,
        [0.9959],
        [0.9948],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371480.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7485],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.1840, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0258, -0.0020,  0.0159,  ...,  0.0139, -0.0030,  0.0517],
        [ 0.0103, -0.0009,  0.0065,  ...,  0.0067, -0.0013,  0.0232],
        [ 0.0141, -0.0011,  0.0088,  ...,  0.0085, -0.0017,  0.0301],
        ...,
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0016],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0016],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3269.2603, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0181, device='cuda:0')



h[100].sum tensor(-9.2470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6697, device='cuda:0')



h[200].sum tensor(91.1896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1209, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0307, 0.0000, 0.0192,  ..., 0.0214, 0.0000, 0.0710],
        [0.0744, 0.0000, 0.0460,  ..., 0.0425, 0.0000, 0.1544],
        [0.0321, 0.0000, 0.0199,  ..., 0.0214, 0.0000, 0.0711],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72046.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0658, 0.0000, 0.1041,  ..., 0.1179, 0.0000, 0.2183],
        [0.0521, 0.0000, 0.1350,  ..., 0.1301, 0.0000, 0.2708],
        [0.0687, 0.0033, 0.0932,  ..., 0.1160, 0.0062, 0.2033],
        ...,
        [0.1070, 0.1371, 0.0024,  ..., 0.0888, 0.1269, 0.0581],
        [0.1069, 0.1369, 0.0024,  ..., 0.0888, 0.1268, 0.0580],
        [0.1068, 0.1369, 0.0024,  ..., 0.0887, 0.1267, 0.0580]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699658.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10678.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(881.4526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(452.7937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.3252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1619],
        [ 0.0582],
        [-0.2994],
        ...,
        [-3.9649],
        [-3.9564],
        [-3.9546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302330.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0299],
        [1.0353],
        ...,
        [0.9959],
        [0.9948],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371480.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.6472, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0324],
        [1.0300],
        [1.0353],
        ...,
        [0.9959],
        [0.9947],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371483.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.6472, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0004,  0.0027,  ...,  0.0038, -0.0006,  0.0117],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0016],
        [ 0.0206, -0.0016,  0.0127,  ...,  0.0115, -0.0024,  0.0422],
        ...,
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0016],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0016],
        [-0.0014,  0.0000, -0.0007,  ...,  0.0012,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2884.9351, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.4257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0144, device='cuda:0')



h[100].sum tensor(-7.3533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5317, device='cuda:0')



h[200].sum tensor(86.0519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0100, 0.0000, 0.0065,  ..., 0.0110, 0.0000, 0.0303],
        [0.0324, 0.0000, 0.0203,  ..., 0.0222, 0.0000, 0.0742],
        [0.0325, 0.0000, 0.0204,  ..., 0.0223, 0.0000, 0.0745],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67101.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0782, 0.0000, 0.0870,  ..., 0.1079, 0.0000, 0.1862],
        [0.0576, 0.0000, 0.1308,  ..., 0.1264, 0.0000, 0.2619],
        [0.0436, 0.0000, 0.1604,  ..., 0.1388, 0.0000, 0.3128],
        ...,
        [0.1073, 0.1375, 0.0022,  ..., 0.0892, 0.1274, 0.0583],
        [0.1072, 0.1374, 0.0021,  ..., 0.0891, 0.1273, 0.0583],
        [0.1072, 0.1373, 0.0021,  ..., 0.0891, 0.1272, 0.0582]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678619.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10838.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(862.3760, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(425.3882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(264.6689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4510],
        [ 0.3990],
        [ 0.3480],
        ...,
        [-3.9863],
        [-3.9777],
        [-3.9759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306122.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0324],
        [1.0300],
        [1.0353],
        ...,
        [0.9959],
        [0.9947],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371483.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6162],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.2983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0325],
        [1.0300],
        [1.0354],
        ...,
        [0.9958],
        [0.9947],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371486.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6162],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.2983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0113, -0.0009,  0.0071,  ...,  0.0072, -0.0014,  0.0251],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0016],
        [ 0.0233, -0.0018,  0.0144,  ...,  0.0128, -0.0027,  0.0471],
        ...,
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0016],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0016],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3055.9585, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.4454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0160, device='cuda:0')



h[100].sum tensor(-8.2486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5936, device='cuda:0')



h[200].sum tensor(87.7678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7423, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0093, 0.0000, 0.0059,  ..., 0.0101, 0.0000, 0.0262],
        [0.0646, 0.0000, 0.0401,  ..., 0.0380, 0.0000, 0.1362],
        [0.0304, 0.0000, 0.0189,  ..., 0.0206, 0.0000, 0.0678],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69502.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0848, 0.0307, 0.0527,  ..., 0.1037, 0.0327, 0.1406],
        [0.0633, 0.0000, 0.1114,  ..., 0.1235, 0.0000, 0.2351],
        [0.0700, 0.0000, 0.0953,  ..., 0.1177, 0.0000, 0.2084],
        ...,
        [0.1077, 0.1378, 0.0018,  ..., 0.0896, 0.1277, 0.0583],
        [0.1076, 0.1377, 0.0018,  ..., 0.0896, 0.1276, 0.0583],
        [0.1076, 0.1376, 0.0018,  ..., 0.0895, 0.1275, 0.0582]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688626.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10857.5928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(867.2392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.5827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(276.9781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8343],
        [-0.4739],
        [-0.4908],
        ...,
        [-3.9611],
        [-3.9896],
        [-3.9935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-348791.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0325],
        [1.0300],
        [1.0354],
        ...,
        [0.9958],
        [0.9947],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371486.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(434.6508, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0326],
        [1.0301],
        [1.0355],
        ...,
        [0.9958],
        [0.9947],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371490.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(434.6508, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4726.8135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0321, device='cuda:0')



h[100].sum tensor(-16.6380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.1873, device='cuda:0')



h[200].sum tensor(106.2721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.4873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0000, 0.0032,  ..., 0.0080, 0.0000, 0.0181],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93201.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0979, 0.0690, 0.0253,  ..., 0.0923, 0.0699, 0.0943],
        [0.1025, 0.1155, 0.0078,  ..., 0.0888, 0.1079, 0.0675],
        [0.1042, 0.1318, 0.0014,  ..., 0.0875, 0.1214, 0.0580],
        ...,
        [0.1081, 0.1380, 0.0017,  ..., 0.0898, 0.1275, 0.0583],
        [0.1080, 0.1379, 0.0017,  ..., 0.0898, 0.1274, 0.0583],
        [0.1080, 0.1378, 0.0017,  ..., 0.0897, 0.1273, 0.0583]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(797722.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10612.2793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(973.2985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(590.9662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(400.6357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5639],
        [-2.4784],
        [-3.1956],
        ...,
        [-4.0196],
        [-4.0105],
        [-4.0083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286761.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0326],
        [1.0301],
        [1.0355],
        ...,
        [0.9958],
        [0.9947],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371490.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(612.9684, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0326],
        [1.0302],
        [1.0355],
        ...,
        [0.9958],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371494.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(612.9684, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(6040.8467, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.8472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0452, device='cuda:0')



h[100].sum tensor(-23.1650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.6744, device='cuda:0')



h[200].sum tensor(121.1382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(120101.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1042, 0.1343, 0.0012,  ..., 0.0871, 0.1227, 0.0562],
        [0.1046, 0.1347, 0.0012,  ..., 0.0874, 0.1231, 0.0564],
        [0.1047, 0.1346, 0.0014,  ..., 0.0874, 0.1231, 0.0565],
        ...,
        [0.1084, 0.1380, 0.0017,  ..., 0.0900, 0.1269, 0.0584],
        [0.1083, 0.1379, 0.0017,  ..., 0.0899, 0.1268, 0.0584],
        [0.1083, 0.1378, 0.0017,  ..., 0.0899, 0.1268, 0.0584]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(980274.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10236.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1074.0598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(760.0859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(535.6141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9275],
        [-3.9896],
        [-4.0127],
        ...,
        [-4.0176],
        [-4.0088],
        [-4.0069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300370.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0326],
        [1.0302],
        [1.0355],
        ...,
        [0.9958],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371494.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(324.5529, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0302],
        [1.0356],
        ...,
        [0.9958],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371498.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(324.5529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3924.5708, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0240, device='cuda:0')



h[100].sum tensor(-12.4246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8866, device='cuda:0')



h[200].sum tensor(99.7738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.0445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83972.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1042, 0.1342, 0.0014,  ..., 0.0869, 0.1220, 0.0563],
        [0.1021, 0.1112, 0.0088,  ..., 0.0896, 0.1037, 0.0694],
        [0.0946, 0.0577, 0.0308,  ..., 0.0966, 0.0583, 0.1041],
        ...,
        [0.1084, 0.1378, 0.0019,  ..., 0.0898, 0.1262, 0.0585],
        [0.1083, 0.1377, 0.0019,  ..., 0.0897, 0.1261, 0.0585],
        [0.1083, 0.1377, 0.0019,  ..., 0.0897, 0.1260, 0.0585]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(763254.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10714.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(943.7980, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(522.4806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.1350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8197],
        [-2.0931],
        [-1.1117],
        ...,
        [-4.0000],
        [-3.9916],
        [-3.9874]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274698.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0302],
        [1.0356],
        ...,
        [0.9958],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371498.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 700.0 event: 3500 loss: tensor(493.1211, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0303],
        [1.0357],
        ...,
        [0.9957],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371502.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3238.7446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.1648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-8.8615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6449, device='cuda:0')



h[200].sum tensor(93.9192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0000, 0.0027,  ..., 0.0075, 0.0000, 0.0165],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70665.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1011, 0.0886, 0.0206,  ..., 0.0887, 0.0825, 0.0847],
        [0.1039, 0.1207, 0.0079,  ..., 0.0868, 0.1091, 0.0659],
        [0.1041, 0.1240, 0.0051,  ..., 0.0868, 0.1122, 0.0634],
        ...,
        [0.1083, 0.1377, 0.0023,  ..., 0.0894, 0.1252, 0.0586],
        [0.1082, 0.1376, 0.0023,  ..., 0.0893, 0.1251, 0.0586],
        [0.1082, 0.1375, 0.0023,  ..., 0.0893, 0.1250, 0.0586]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684635.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10955.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(884.2778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(434.8192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.8384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3112],
        [-1.8008],
        [-1.9765],
        ...,
        [-3.7841],
        [-3.8945],
        [-3.9461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289188., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0303],
        [1.0357],
        ...,
        [0.9957],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371502.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(184.9910, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0303],
        [1.0357],
        ...,
        [0.9957],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371502.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(184.9910, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2871.5459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.1152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0137, device='cuda:0')



h[100].sum tensor(-7.0234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5053, device='cuda:0')



h[200].sum tensor(90.0554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66138.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1041, 0.1341, 0.0018,  ..., 0.0866, 0.1211, 0.0564],
        [0.1038, 0.1280, 0.0031,  ..., 0.0875, 0.1163, 0.0601],
        [0.1029, 0.1193, 0.0062,  ..., 0.0884, 0.1096, 0.0648],
        ...,
        [0.1083, 0.1377, 0.0023,  ..., 0.0894, 0.1252, 0.0586],
        [0.1082, 0.1376, 0.0023,  ..., 0.0893, 0.1251, 0.0586],
        [0.1082, 0.1375, 0.0023,  ..., 0.0893, 0.1250, 0.0586]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669193.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11020.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(864.7763, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(407.2389, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(270.7365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7990],
        [-3.6315],
        [-3.3037],
        ...,
        [-3.9737],
        [-3.9654],
        [-3.9636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304305., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0303],
        [1.0357],
        ...,
        [0.9957],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371502.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.1703, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0303],
        [1.0358],
        ...,
        [0.9957],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371506.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.1703, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0014,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2660.3950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-33.2571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0116, device='cuda:0')



h[100].sum tensor(-5.8664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4293, device='cuda:0')



h[200].sum tensor(89.1585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7698, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63385.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1041, 0.1339, 0.0020,  ..., 0.0863, 0.1203, 0.0564],
        [0.1045, 0.1343, 0.0021,  ..., 0.0865, 0.1207, 0.0565],
        [0.1046, 0.1342, 0.0022,  ..., 0.0865, 0.1207, 0.0566],
        ...,
        [0.1083, 0.1375, 0.0026,  ..., 0.0891, 0.1243, 0.0586],
        [0.1082, 0.1374, 0.0026,  ..., 0.0890, 0.1242, 0.0585],
        [0.1081, 0.1374, 0.0026,  ..., 0.0890, 0.1242, 0.0585]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659903.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11005.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(851.6704, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(385.7484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.4525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6807],
        [-3.8712],
        [-3.9656],
        ...,
        [-3.9529],
        [-3.9448],
        [-3.9430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311712.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0303],
        [1.0358],
        ...,
        [0.9957],
        [0.9946],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371506.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.1138, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0305],
        [1.0359],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371510.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.1138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3356.5554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-9.2210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6750, device='cuda:0')



h[200].sum tensor(98.1215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0063],
        [0.0204, 0.0000, 0.0128,  ..., 0.0157, 0.0000, 0.0492],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72427.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1028, 0.1203, 0.0068,  ..., 0.0870, 0.1083, 0.0640],
        [0.0956, 0.0658, 0.0297,  ..., 0.0940, 0.0621, 0.1000],
        [0.0737, 0.0227, 0.0906,  ..., 0.1134, 0.0240, 0.1982],
        ...,
        [0.1083, 0.1375, 0.0028,  ..., 0.0888, 0.1234, 0.0584],
        [0.1082, 0.1374, 0.0028,  ..., 0.0887, 0.1233, 0.0583],
        [0.1082, 0.1373, 0.0028,  ..., 0.0887, 0.1232, 0.0583]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693292.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10928.2275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(898.4860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(432.3806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(310.4276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3463],
        [-0.6502],
        [-0.0194],
        ...,
        [-3.9359],
        [-3.9279],
        [-3.9261]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252618.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0305],
        [1.0359],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371510.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7803],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.1868, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0306],
        [1.0360],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371514.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7803],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.1868, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [ 0.0148, -0.0011,  0.0092,  ...,  0.0087, -0.0017,  0.0313],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3369.1372, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0182, device='cuda:0')



h[100].sum tensor(-9.1874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6725, device='cuda:0')



h[200].sum tensor(99.7795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0000, 0.0095,  ..., 0.0126, 0.0000, 0.0369],
        [0.0123, 0.0000, 0.0077,  ..., 0.0112, 0.0000, 0.0314],
        [0.0683, 0.0000, 0.0423,  ..., 0.0395, 0.0000, 0.1426],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72931.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0907, 0.0496, 0.0403,  ..., 0.0986, 0.0496, 0.1179],
        [0.0850, 0.0233, 0.0596,  ..., 0.1045, 0.0249, 0.1477],
        [0.0641, 0.0000, 0.1189,  ..., 0.1236, 0.0000, 0.2415],
        ...,
        [0.1085, 0.1375, 0.0029,  ..., 0.0887, 0.1228, 0.0581],
        [0.1084, 0.1374, 0.0029,  ..., 0.0886, 0.1227, 0.0581],
        [0.1084, 0.1374, 0.0029,  ..., 0.0886, 0.1227, 0.0581]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695825.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10945.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(905.6395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(431.1747, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(318.7841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1196],
        [-0.2906],
        [ 0.2968],
        ...,
        [-3.9290],
        [-3.9208],
        [-3.9187]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245260.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0306],
        [1.0360],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371514.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.6891, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0306],
        [1.0360],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371514.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.6891, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0012,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2983.2627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.0970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-7.2685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5291, device='cuda:0')



h[200].sum tensor(95.7289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5752, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0062],
        [0.0113, 0.0000, 0.0072,  ..., 0.0114, 0.0000, 0.0321],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67287.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1012, 0.0971, 0.0167,  ..., 0.0889, 0.0888, 0.0773],
        [0.0938, 0.0499, 0.0405,  ..., 0.0963, 0.0456, 0.1145],
        [0.0742, 0.0252, 0.0964,  ..., 0.1143, 0.0221, 0.2037],
        ...,
        [0.1085, 0.1375, 0.0029,  ..., 0.0887, 0.1228, 0.0581],
        [0.1084, 0.1374, 0.0029,  ..., 0.0886, 0.1227, 0.0581],
        [0.1084, 0.1374, 0.0029,  ..., 0.0886, 0.1227, 0.0581]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672175.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11008.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(876.7047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(399.5163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.2942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3383],
        [-0.0187],
        [ 0.2304],
        ...,
        [-3.9277],
        [-3.9200],
        [-3.9185]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276867.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0306],
        [1.0360],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371514.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2869],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0331],
        [1.0307],
        [1.0361],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371518.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2869],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0096, -0.0008,  0.0060,  ...,  0.0063, -0.0011,  0.0217],
        [ 0.0166, -0.0012,  0.0103,  ...,  0.0096, -0.0019,  0.0346],
        [ 0.0098, -0.0008,  0.0062,  ...,  0.0064, -0.0012,  0.0221],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3487.6631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0193, device='cuda:0')



h[100].sum tensor(-9.6908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7126, device='cuda:0')



h[200].sum tensor(102.2758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8963, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0335, 0.0000, 0.0209,  ..., 0.0224, 0.0000, 0.0756],
        [0.0481, 0.0000, 0.0300,  ..., 0.0299, 0.0000, 0.1051],
        [0.0729, 0.0000, 0.0451,  ..., 0.0416, 0.0000, 0.1509],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74187.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0646, 0.0000, 0.1338,  ..., 0.1235, 0.0000, 0.2577],
        [0.0516, 0.0000, 0.1692,  ..., 0.1363, 0.0000, 0.3149],
        [0.0379, 0.0000, 0.2007,  ..., 0.1487, 0.0000, 0.3678],
        ...,
        [0.1087, 0.1375, 0.0031,  ..., 0.0887, 0.1225, 0.0579],
        [0.1086, 0.1374, 0.0031,  ..., 0.0886, 0.1224, 0.0579],
        [0.1086, 0.1374, 0.0031,  ..., 0.0886, 0.1223, 0.0578]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703730.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10972.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(917.2826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(434.2055, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.5067, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6532],
        [ 0.6210],
        [ 0.5934],
        ...,
        [-3.9259],
        [-3.9182],
        [-3.9164]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230883.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0331],
        [1.0307],
        [1.0361],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371518.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(337.0240, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0332],
        [1.0308],
        [1.0363],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371521.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(337.0240, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0183, -0.0014,  0.0113,  ...,  0.0104, -0.0021,  0.0377],
        [ 0.0085, -0.0007,  0.0054,  ...,  0.0058, -0.0010,  0.0196],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4078.3765, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0249, device='cuda:0')



h[100].sum tensor(-12.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9206, device='cuda:0')



h[200].sum tensor(109.1082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0523, 0.0000, 0.0325,  ..., 0.0318, 0.0000, 0.1125],
        [0.0329, 0.0000, 0.0205,  ..., 0.0221, 0.0000, 0.0743],
        [0.0158, 0.0000, 0.0099,  ..., 0.0134, 0.0000, 0.0402],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0000, 0.0063],
        [0.0085, 0.0000, 0.0053,  ..., 0.0095, 0.0000, 0.0245]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85149.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0587, 0.0000, 0.1476,  ..., 0.1293, 0.0000, 0.2804],
        [0.0723, 0.0043, 0.1090,  ..., 0.1173, 0.0057, 0.2200],
        [0.0875, 0.0356, 0.0638,  ..., 0.1031, 0.0342, 0.1494],
        ...,
        [0.1085, 0.1330, 0.0046,  ..., 0.0890, 0.1188, 0.0605],
        [0.1067, 0.1135, 0.0121,  ..., 0.0907, 0.1030, 0.0713],
        [0.1009, 0.0644, 0.0333,  ..., 0.0963, 0.0613, 0.1032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(763834.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10831.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(957.4377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(504.3752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(386.7430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2620],
        [-0.2041],
        [-0.9121],
        ...,
        [-3.6147],
        [-3.1577],
        [-2.4501]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237426.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0332],
        [1.0308],
        [1.0363],
        ...,
        [0.9957],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371521.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6118],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.8782, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0309],
        [1.0364],
        ...,
        [0.9956],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371524.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6118],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.8782, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0095, -0.0007,  0.0060,  ...,  0.0062, -0.0011,  0.0215],
        [ 0.0222, -0.0016,  0.0136,  ...,  0.0122, -0.0024,  0.0448],
        [ 0.0090, -0.0007,  0.0056,  ...,  0.0059, -0.0011,  0.0204],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3868.8977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0229, device='cuda:0')



h[100].sum tensor(-11.5123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8465, device='cuda:0')



h[200].sum tensor(107.1406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.3191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0459, 0.0000, 0.0284,  ..., 0.0281, 0.0000, 0.0981],
        [0.0548, 0.0000, 0.0338,  ..., 0.0323, 0.0000, 0.1146],
        [0.0988, 0.0000, 0.0607,  ..., 0.0536, 0.0000, 0.1984],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81002.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0536, 0.0000, 0.1637,  ..., 0.1340, 0.0000, 0.3055],
        [0.0398, 0.0000, 0.1936,  ..., 0.1467, 0.0000, 0.3574],
        [0.0169, 0.0000, 0.2479,  ..., 0.1675, 0.0000, 0.4478],
        ...,
        [0.1095, 0.1380, 0.0033,  ..., 0.0884, 0.1228, 0.0577],
        [0.1094, 0.1379, 0.0033,  ..., 0.0883, 0.1227, 0.0576],
        [0.1094, 0.1378, 0.0033,  ..., 0.0883, 0.1227, 0.0576]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739389.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11074.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(944.2307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(479.1241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(369.1893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4910],
        [ 0.4299],
        [ 0.3759],
        ...,
        [-3.8308],
        [-3.8713],
        [-3.9083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233617.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0309],
        [1.0364],
        ...,
        [0.9956],
        [0.9945],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371524.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3923],
        [0.3774],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0310],
        [1.0365],
        ...,
        [0.9956],
        [0.9944],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371527.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3923],
        [0.3774],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0337, -0.0024,  0.0206,  ...,  0.0175, -0.0036,  0.0660],
        [ 0.0193, -0.0014,  0.0119,  ...,  0.0108, -0.0021,  0.0395],
        [ 0.0277, -0.0020,  0.0170,  ...,  0.0147, -0.0030,  0.0550],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0011,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4257.4849, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0264, device='cuda:0')



h[100].sum tensor(-13.4415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9784, device='cuda:0')



h[200].sum tensor(110.7819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.7069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0885, 0.0000, 0.0543,  ..., 0.0486, 0.0000, 0.1791],
        [0.1242, 0.0000, 0.0760,  ..., 0.0654, 0.0000, 0.2451],
        [0.1148, 0.0000, 0.0703,  ..., 0.0610, 0.0000, 0.2277],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85371.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0000, 0.2986,  ..., 0.1867, 0.0000, 0.5333],
        [0.0000, 0.0000, 0.3477,  ..., 0.2062, 0.0000, 0.6177],
        [0.0000, 0.0000, 0.3542,  ..., 0.2087, 0.0000, 0.6285],
        ...,
        [0.1102, 0.1385, 0.0034,  ..., 0.0882, 0.1234, 0.0575],
        [0.1101, 0.1384, 0.0034,  ..., 0.0881, 0.1233, 0.0575],
        [0.1101, 0.1384, 0.0034,  ..., 0.0880, 0.1233, 0.0574]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(756963.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11126.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(960.9174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(510.1673, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(392.9591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2453],
        [ 0.2473],
        [ 0.2481],
        ...,
        [-3.9372],
        [-3.8869],
        [-3.8328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259823.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0310],
        [1.0365],
        ...,
        [0.9956],
        [0.9944],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371527.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 710.0 event: 3550 loss: tensor(385.9782, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.7439, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0310],
        [1.0365],
        ...,
        [0.9956],
        [0.9944],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371530.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.7439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [ 0.0125, -0.0009,  0.0077,  ...,  0.0076, -0.0014,  0.0268],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3826.3035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0226, device='cuda:0')



h[100].sum tensor(-11.3368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8352, device='cuda:0')



h[200].sum tensor(105.4353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1147, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0000, 0.0071,  ..., 0.0111, 0.0000, 0.0317],
        [0.0232, 0.0000, 0.0143,  ..., 0.0167, 0.0000, 0.0538],
        [0.0233, 0.0000, 0.0144,  ..., 0.0167, 0.0000, 0.0539],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80483.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0934, 0.0000, 0.0703,  ..., 0.0980, 0.0016, 0.1517],
        [0.0880, 0.0000, 0.0820,  ..., 0.1039, 0.0000, 0.1721],
        [0.0860, 0.0000, 0.0833,  ..., 0.1059, 0.0002, 0.1757],
        ...,
        [0.1107, 0.1390, 0.0035,  ..., 0.0877, 0.1239, 0.0576],
        [0.1106, 0.1389, 0.0035,  ..., 0.0877, 0.1238, 0.0576],
        [0.1105, 0.1388, 0.0035,  ..., 0.0876, 0.1237, 0.0575]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739175.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11356.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(947.8520, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(477.2676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(370.5137, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5757],
        [ 0.6142],
        [ 0.6189],
        ...,
        [-3.9826],
        [-3.9747],
        [-3.9728]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240476.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0310],
        [1.0365],
        ...,
        [0.9956],
        [0.9944],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371530.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5269],
        [0.4692],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.1628, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0311],
        [1.0366],
        ...,
        [0.9955],
        [0.9944],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371533.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5269],
        [0.4692],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.1628, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0415, -0.0029,  0.0253,  ...,  0.0212, -0.0044,  0.0805],
        [ 0.0179, -0.0013,  0.0109,  ...,  0.0101, -0.0020,  0.0368],
        [ 0.0084, -0.0007,  0.0052,  ...,  0.0056, -0.0010,  0.0194],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3835.8411, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0227, device='cuda:0')



h[100].sum tensor(-11.4230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8390, device='cuda:0')



h[200].sum tensor(104.2980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1848, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1387, 0.0000, 0.0846,  ..., 0.0721, 0.0000, 0.2720],
        [0.0962, 0.0000, 0.0588,  ..., 0.0521, 0.0000, 0.1935],
        [0.0323, 0.0000, 0.0198,  ..., 0.0209, 0.0000, 0.0706],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81272.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0057, 0.0000, 0.3520,  ..., 0.2044, 0.0000, 0.6212],
        [0.0276, 0.0000, 0.2529,  ..., 0.1683, 0.0000, 0.4570],
        [0.0630, 0.0087, 0.1339,  ..., 0.1267, 0.0099, 0.2631],
        ...,
        [0.1112, 0.1395, 0.0036,  ..., 0.0873, 0.1247, 0.0577],
        [0.1111, 0.1394, 0.0036,  ..., 0.0873, 0.1246, 0.0576],
        [0.1110, 0.1393, 0.0036,  ..., 0.0872, 0.1245, 0.0576]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(748850.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11429.0176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(947.4626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(486.0876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(372.5452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1262],
        [ 0.1530],
        [ 0.1469],
        ...,
        [-4.0007],
        [-3.9929],
        [-3.9912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244506.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0311],
        [1.0366],
        ...,
        [0.9955],
        [0.9944],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371533.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.3401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0312],
        [1.0366],
        ...,
        [0.9955],
        [0.9943],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371535.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.3401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0151, -0.0011,  0.0092,  ...,  0.0088, -0.0017,  0.0317],
        [ 0.0319, -0.0022,  0.0194,  ...,  0.0166, -0.0034,  0.0627],
        [ 0.0221, -0.0016,  0.0135,  ...,  0.0121, -0.0024,  0.0447],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3463.4458, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0192, device='cuda:0')



h[100].sum tensor(-9.6538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7111, device='cuda:0')



h[200].sum tensor(99.0966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0656, 0.0000, 0.0401,  ..., 0.0378, 0.0000, 0.1370],
        [0.0639, 0.0000, 0.0391,  ..., 0.0370, 0.0000, 0.1339],
        [0.1109, 0.0000, 0.0676,  ..., 0.0590, 0.0000, 0.2208],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74391.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0450, 0.0000, 0.2113,  ..., 0.1452, 0.0000, 0.3766],
        [0.0392, 0.0000, 0.2257,  ..., 0.1515, 0.0000, 0.4010],
        [0.0213, 0.0000, 0.2656,  ..., 0.1674, 0.0000, 0.4693],
        ...,
        [0.1118, 0.1401, 0.0037,  ..., 0.0871, 0.1257, 0.0577],
        [0.1117, 0.1400, 0.0036,  ..., 0.0870, 0.1256, 0.0577],
        [0.1116, 0.1399, 0.0036,  ..., 0.0870, 0.1255, 0.0576]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709456.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11711.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(920.4920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(445.5739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(336.9945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3735],
        [ 0.3292],
        [ 0.2973],
        ...,
        [-4.0260],
        [-4.0179],
        [-4.0160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256066.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0312],
        [1.0366],
        ...,
        [0.9955],
        [0.9943],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371535.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.2148, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0312],
        [1.0367],
        ...,
        [0.9955],
        [0.9943],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371538.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.2148, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0312, -0.0022,  0.0190,  ...,  0.0163, -0.0033,  0.0614],
        [ 0.0420, -0.0029,  0.0256,  ...,  0.0214, -0.0044,  0.0814],
        [ 0.0299, -0.0021,  0.0182,  ...,  0.0157, -0.0032,  0.0591],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2935.1335, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0140, device='cuda:0')



h[100].sum tensor(-7.1451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5169, device='cuda:0')



h[200].sum tensor(92.4657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3540, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1390, 0.0000, 0.0846,  ..., 0.0721, 0.0000, 0.2725],
        [0.1660, 0.0000, 0.1010,  ..., 0.0848, 0.0000, 0.3224],
        [0.1686, 0.0000, 0.1026,  ..., 0.0860, 0.0000, 0.3271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67352.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0077, 0.0000, 0.3605,  ..., 0.2072, 0.0000, 0.6340],
        [0.0000, 0.0000, 0.4311,  ..., 0.2334, 0.0000, 0.7511],
        [0.0000, 0.0000, 0.4318,  ..., 0.2335, 0.0000, 0.7517],
        ...,
        [0.1126, 0.1407, 0.0035,  ..., 0.0874, 0.1269, 0.0575],
        [0.1125, 0.1406, 0.0035,  ..., 0.0873, 0.1268, 0.0575],
        [0.1124, 0.1405, 0.0035,  ..., 0.0873, 0.1267, 0.0575]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683227.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11921.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.3701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.2646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(299.7761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0479],
        [-0.0742],
        [-0.0728],
        ...,
        [-4.0621],
        [-4.0536],
        [-4.0514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327427.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0312],
        [1.0367],
        ...,
        [0.9955],
        [0.9943],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371538.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2522],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.4434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0312],
        [1.0367],
        ...,
        [0.9955],
        [0.9943],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371541.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2522],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.4434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0098, -0.0007,  0.0060,  ...,  0.0063, -0.0011,  0.0219],
        [ 0.0089, -0.0007,  0.0054,  ...,  0.0059, -0.0010,  0.0202],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2853.0562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0137, device='cuda:0')



h[100].sum tensor(-6.8056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5066, device='cuda:0')



h[200].sum tensor(90.6872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1675, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0529, 0.0000, 0.0323,  ..., 0.0318, 0.0000, 0.1135],
        [0.0195, 0.0000, 0.0120,  ..., 0.0156, 0.0000, 0.0494],
        [0.0120, 0.0000, 0.0074,  ..., 0.0114, 0.0000, 0.0330],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65380.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0749, 0.0000, 0.1481,  ..., 0.1222, 0.0000, 0.2723],
        [0.0892, 0.0053, 0.1057,  ..., 0.1076, 0.0071, 0.2057],
        [0.0976, 0.0131, 0.0755,  ..., 0.0985, 0.0148, 0.1597],
        ...,
        [0.1134, 0.1412, 0.0033,  ..., 0.0877, 0.1278, 0.0574],
        [0.1133, 0.1411, 0.0033,  ..., 0.0877, 0.1277, 0.0574],
        [0.1132, 0.1410, 0.0033,  ..., 0.0876, 0.1276, 0.0574]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672512.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12175.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.1850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(400.7051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.0659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3177],
        [ 0.3360],
        [ 0.2843],
        ...,
        [-4.0927],
        [-4.0839],
        [-4.0816]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330272.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0312],
        [1.0367],
        ...,
        [0.9955],
        [0.9943],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371541.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0312],
        [1.0367],
        ...,
        [0.9954],
        [0.9943],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371544.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3105.6821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-8.0450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5964, device='cuda:0')



h[200].sum tensor(92.7535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7938, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71529.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1087, 0.1199, 0.0093,  ..., 0.0862, 0.1100, 0.0653],
        [0.1097, 0.1269, 0.0079,  ..., 0.0859, 0.1155, 0.0623],
        [0.1098, 0.1257, 0.0085,  ..., 0.0858, 0.1147, 0.0629],
        ...,
        [0.1138, 0.1413, 0.0032,  ..., 0.0882, 0.1283, 0.0574],
        [0.1137, 0.1412, 0.0032,  ..., 0.0881, 0.1282, 0.0574],
        [0.1137, 0.1411, 0.0032,  ..., 0.0881, 0.1281, 0.0573]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711502.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12167.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(912.4336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(443.2148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.6229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0203],
        [-1.3630],
        [-1.5597],
        ...,
        [-3.6504],
        [-3.9601],
        [-4.0665]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317005.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0312],
        [1.0367],
        ...,
        [0.9954],
        [0.9943],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371544.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(314.8053, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0312],
        [1.0368],
        ...,
        [0.9954],
        [0.9942],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371548.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(314.8053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0150, -0.0011,  0.0091,  ...,  0.0087, -0.0016,  0.0315],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3871.7642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0232, device='cuda:0')



h[100].sum tensor(-11.7601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8599, device='cuda:0')



h[200].sum tensor(100.2856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.5627, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0274, 0.0000, 0.0167,  ..., 0.0186, 0.0000, 0.0616],
        [0.0155, 0.0000, 0.0094,  ..., 0.0124, 0.0000, 0.0371],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81632.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0835, 0.0092, 0.1012,  ..., 0.1148, 0.0110, 0.2077],
        [0.0932, 0.0090, 0.0673,  ..., 0.1050, 0.0161, 0.1558],
        [0.0980, 0.0096, 0.0534,  ..., 0.0999, 0.0166, 0.1332],
        ...,
        [0.1142, 0.1413, 0.0031,  ..., 0.0885, 0.1285, 0.0575],
        [0.1141, 0.1412, 0.0031,  ..., 0.0884, 0.1284, 0.0575],
        [0.1140, 0.1412, 0.0031,  ..., 0.0884, 0.1283, 0.0575]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(754505.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12106.0439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(950.0449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(511.3365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(373.2631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0978],
        [ 0.1136],
        [ 0.1235],
        ...,
        [-4.1201],
        [-4.1113],
        [-4.1093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334043.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0312],
        [1.0368],
        ...,
        [0.9954],
        [0.9942],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371548.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.7367, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0312],
        [1.0368],
        ...,
        [0.9954],
        [0.9942],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371552.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.7367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0109, -0.0008,  0.0066,  ...,  0.0068, -0.0012,  0.0239],
        [ 0.0251, -0.0017,  0.0153,  ...,  0.0134, -0.0027,  0.0502],
        [ 0.0154, -0.0011,  0.0094,  ...,  0.0089, -0.0017,  0.0322],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3379.6465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-9.3439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6958, device='cuda:0')



h[200].sum tensor(95.0209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5931, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0416, 0.0000, 0.0254,  ..., 0.0258, 0.0000, 0.0904],
        [0.0757, 0.0000, 0.0461,  ..., 0.0423, 0.0000, 0.1557],
        [0.1483, 0.0000, 0.0901,  ..., 0.0763, 0.0000, 0.2896],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74478.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0542, 0.0000, 0.1957,  ..., 0.1455, 0.0000, 0.3578],
        [0.0288, 0.0000, 0.2812,  ..., 0.1765, 0.0000, 0.4980],
        [0.0069, 0.0000, 0.3911,  ..., 0.2181, 0.0000, 0.6818],
        ...,
        [0.1142, 0.1412, 0.0032,  ..., 0.0887, 0.1283, 0.0578],
        [0.1141, 0.1411, 0.0032,  ..., 0.0886, 0.1282, 0.0578],
        [0.1140, 0.1410, 0.0032,  ..., 0.0885, 0.1281, 0.0577]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717893.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12222.5850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(922.0912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(466.4415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.9110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4421],
        [-0.5697],
        [-0.7171],
        ...,
        [-4.1199],
        [-4.1110],
        [-4.1087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323490.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0312],
        [1.0368],
        ...,
        [0.9954],
        [0.9942],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371552.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.4321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0312],
        [1.0368],
        ...,
        [0.9954],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371556.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.4321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3292.1006, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.8281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-8.8743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6622, device='cuda:0')



h[200].sum tensor(94.3850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9848, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71723.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1097, 0.1372, 0.0026,  ..., 0.0859, 0.1236, 0.0560],
        [0.1101, 0.1377, 0.0027,  ..., 0.0862, 0.1241, 0.0562],
        [0.1103, 0.1375, 0.0028,  ..., 0.0862, 0.1241, 0.0563],
        ...,
        [0.1142, 0.1409, 0.0032,  ..., 0.0887, 0.1278, 0.0582],
        [0.1141, 0.1408, 0.0032,  ..., 0.0886, 0.1277, 0.0581],
        [0.1140, 0.1408, 0.0032,  ..., 0.0885, 0.1277, 0.0581]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700001.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12245.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(909.6312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.9187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.5703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0899],
        [-4.1004],
        [-4.0807],
        ...,
        [-4.1127],
        [-4.1039],
        [-4.1016]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309669.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0312],
        [1.0368],
        ...,
        [0.9954],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371556.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.6089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0312],
        [1.0369],
        ...,
        [0.9954],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371560.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.6089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033, -0.0003,  0.0020,  ...,  0.0032, -0.0005,  0.0100],
        [ 0.0044, -0.0004,  0.0027,  ...,  0.0037, -0.0006,  0.0120],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3769.4927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0227, device='cuda:0')



h[100].sum tensor(-11.1065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8403, device='cuda:0')



h[200].sum tensor(100.1283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.2069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0332, 0.0000, 0.0203,  ..., 0.0224, 0.0000, 0.0775],
        [0.0151, 0.0000, 0.0092,  ..., 0.0127, 0.0000, 0.0392],
        [0.0045, 0.0000, 0.0028,  ..., 0.0071, 0.0000, 0.0172],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81592.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0962, 0.0000, 0.0942,  ..., 0.1024, 0.0000, 0.1866],
        [0.1018, 0.0234, 0.0635,  ..., 0.0962, 0.0263, 0.1432],
        [0.1065, 0.0600, 0.0336,  ..., 0.0906, 0.0595, 0.1006],
        ...,
        [0.1141, 0.1406, 0.0033,  ..., 0.0884, 0.1270, 0.0587],
        [0.1140, 0.1405, 0.0033,  ..., 0.0884, 0.1269, 0.0586],
        [0.1139, 0.1404, 0.0033,  ..., 0.0883, 0.1268, 0.0586]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(758817., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12094.3604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(946.3417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(507.7406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(372.4609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3182],
        [-0.2640],
        [-1.2012],
        ...,
        [-4.0953],
        [-4.0867],
        [-4.0845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288473.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0312],
        [1.0369],
        ...,
        [0.9954],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371560.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 720.0 event: 3600 loss: tensor(489.2783, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3074],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.1002, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0369],
        ...,
        [0.9954],
        [0.9942],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371565.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3074],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.1002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0004,  0.0031,  ...,  0.0040, -0.0006,  0.0132],
        [ 0.0030, -0.0003,  0.0018,  ...,  0.0030, -0.0004,  0.0094],
        [ 0.0093, -0.0007,  0.0057,  ...,  0.0060, -0.0011,  0.0211],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3678.3303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0215, device='cuda:0')



h[100].sum tensor(-10.5742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7952, device='cuda:0')



h[200].sum tensor(100.3028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0000, 0.0065,  ..., 0.0105, 0.0000, 0.0310],
        [0.0265, 0.0000, 0.0162,  ..., 0.0192, 0.0000, 0.0652],
        [0.0094, 0.0000, 0.0058,  ..., 0.0105, 0.0000, 0.0311],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77288.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1056, 0.0358, 0.0480,  ..., 0.0906, 0.0382, 0.1198],
        [0.1035, 0.0035, 0.0680,  ..., 0.0943, 0.0059, 0.1477],
        [0.1067, 0.0209, 0.0547,  ..., 0.0903, 0.0258, 0.1281],
        ...,
        [0.1137, 0.1364, 0.0045,  ..., 0.0885, 0.1229, 0.0612],
        [0.1138, 0.1401, 0.0034,  ..., 0.0882, 0.1258, 0.0592],
        [0.1137, 0.1400, 0.0034,  ..., 0.0881, 0.1257, 0.0592]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(724738.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12124.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(929.2072, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(476.9995, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(351.8258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6448],
        [-0.2424],
        [-0.3456],
        ...,
        [-3.1694],
        [-3.7383],
        [-3.9725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263874.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0369],
        ...,
        [0.9954],
        [0.9942],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371565.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.6489, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0370],
        ...,
        [0.9953],
        [0.9942],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371568.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.6489, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015],
        [ 0.0050, -0.0004,  0.0031,  ...,  0.0040, -0.0006,  0.0132],
        ...,
        [-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0008,  ...,  0.0010,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3092.8936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.2014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0155, device='cuda:0')



h[100].sum tensor(-7.7050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5754, device='cuda:0')



h[200].sum tensor(95.4508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0062],
        [0.0052, 0.0000, 0.0032,  ..., 0.0073, 0.0000, 0.0184],
        [0.0111, 0.0000, 0.0068,  ..., 0.0107, 0.0000, 0.0318],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69473.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1083, 0.0997, 0.0164,  ..., 0.0874, 0.0923, 0.0771],
        [0.1067, 0.0545, 0.0338,  ..., 0.0905, 0.0557, 0.1021],
        [0.1038, 0.0269, 0.0586,  ..., 0.0947, 0.0283, 0.1366],
        ...,
        [0.1140, 0.1401, 0.0033,  ..., 0.0882, 0.1250, 0.0597],
        [0.1139, 0.1400, 0.0033,  ..., 0.0882, 0.1249, 0.0597],
        [0.1139, 0.1399, 0.0033,  ..., 0.0881, 0.1249, 0.0596]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692436.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12165.6455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(890.5784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(430.0465, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.2157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9341],
        [-0.3996],
        [ 0.0459],
        ...,
        [-4.0639],
        [-4.0558],
        [-4.0537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306584.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0370],
        ...,
        [0.9953],
        [0.9942],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371568.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2485],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0371],
        ...,
        [0.9953],
        [0.9941],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371571.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2485],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0124, -0.0009,  0.0075,  ...,  0.0074, -0.0014,  0.0267],
        [ 0.0104, -0.0008,  0.0063,  ...,  0.0065, -0.0012,  0.0230],
        [ 0.0052, -0.0004,  0.0032,  ...,  0.0041, -0.0006,  0.0135],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3127.6833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-7.8185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5896, device='cuda:0')



h[200].sum tensor(97.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0585, 0.0000, 0.0357,  ..., 0.0340, 0.0000, 0.1239],
        [0.0381, 0.0000, 0.0233,  ..., 0.0245, 0.0000, 0.0863],
        [0.0247, 0.0000, 0.0151,  ..., 0.0176, 0.0000, 0.0592],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69712.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0791, 0.0000, 0.1604,  ..., 0.1249, 0.0000, 0.2938],
        [0.0851, 0.0000, 0.1413,  ..., 0.1188, 0.0000, 0.2635],
        [0.0906, 0.0000, 0.1156,  ..., 0.1121, 0.0000, 0.2246],
        ...,
        [0.1142, 0.1400, 0.0031,  ..., 0.0883, 0.1242, 0.0601],
        [0.1141, 0.1399, 0.0031,  ..., 0.0882, 0.1241, 0.0601],
        [0.1140, 0.1398, 0.0031,  ..., 0.0882, 0.1240, 0.0601]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692313.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12326.0098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(904.0417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(422.9763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(319.4225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3451],
        [ 0.3158],
        [ 0.3039],
        ...,
        [-4.0568],
        [-4.0486],
        [-4.0463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254908.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0371],
        ...,
        [0.9953],
        [0.9941],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371571.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0372],
        ...,
        [0.9953],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371574.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [ 0.0038, -0.0003,  0.0024,  ...,  0.0034, -0.0005,  0.0109],
        [ 0.0038, -0.0003,  0.0024,  ...,  0.0034, -0.0005,  0.0109],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3286.5847, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.0074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-8.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6448, device='cuda:0')



h[200].sum tensor(99.6859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0206, 0.0000, 0.0126,  ..., 0.0156, 0.0000, 0.0514],
        [0.0235, 0.0000, 0.0144,  ..., 0.0170, 0.0000, 0.0568],
        [0.0297, 0.0000, 0.0182,  ..., 0.0199, 0.0000, 0.0683],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72074.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0821, 0.0000, 0.1415,  ..., 0.1226, 0.0000, 0.2700],
        [0.0810, 0.0000, 0.1483,  ..., 0.1248, 0.0000, 0.2808],
        [0.0781, 0.0000, 0.1575,  ..., 0.1283, 0.0000, 0.2957],
        ...,
        [0.1145, 0.1401, 0.0029,  ..., 0.0887, 0.1238, 0.0605],
        [0.1144, 0.1400, 0.0029,  ..., 0.0886, 0.1237, 0.0604],
        [0.1144, 0.1399, 0.0029,  ..., 0.0886, 0.1236, 0.0604]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702895.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12271.4551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(907.5923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(442.5600, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.9312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0705],
        [-0.0952],
        [-0.1121],
        ...,
        [-4.0631],
        [-4.0550],
        [-4.0530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284477., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0313],
        [1.0372],
        ...,
        [0.9953],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371574.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8030, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0314],
        [1.0372],
        ...,
        [0.9953],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371577.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8030, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3329.4915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0178, device='cuda:0')



h[100].sum tensor(-8.7452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6605, device='cuda:0')



h[200].sum tensor(100.7978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0060],
        [0.0049, 0.0000, 0.0030,  ..., 0.0070, 0.0000, 0.0175],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0062],
        [0.0057, 0.0000, 0.0035,  ..., 0.0075, 0.0000, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72171.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1103, 0.1336, 0.0024,  ..., 0.0864, 0.1173, 0.0601],
        [0.1099, 0.1139, 0.0097,  ..., 0.0881, 0.1021, 0.0706],
        [0.1066, 0.0613, 0.0328,  ..., 0.0934, 0.0586, 0.1055],
        ...,
        [0.1147, 0.1367, 0.0033,  ..., 0.0892, 0.1208, 0.0626],
        [0.1132, 0.1053, 0.0144,  ..., 0.0913, 0.0965, 0.0785],
        [0.1102, 0.0531, 0.0395,  ..., 0.0963, 0.0498, 0.1153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700898.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12420.7012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(915.9768, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(439.1414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(334.8894, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9851],
        [-2.2977],
        [-1.2994],
        ...,
        [-3.4710],
        [-2.6310],
        [-1.4857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253254., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0314],
        [1.0372],
        ...,
        [0.9953],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371577.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.2821, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0314],
        [1.0373],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371580.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.2821, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2975.4526, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.0486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-7.0730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5307, device='cuda:0')



h[200].sum tensor(97.0745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0000, 0.0098,  ..., 0.0128, 0.0000, 0.0402],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66639.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1058, 0.0382, 0.0425,  ..., 0.0954, 0.0366, 0.1197],
        [0.1095, 0.0969, 0.0151,  ..., 0.0899, 0.0891, 0.0793],
        [0.1107, 0.1222, 0.0063,  ..., 0.0880, 0.1087, 0.0665],
        ...,
        [0.1152, 0.1402, 0.0024,  ..., 0.0894, 0.1235, 0.0611],
        [0.1151, 0.1401, 0.0024,  ..., 0.0894, 0.1234, 0.0610],
        [0.1150, 0.1401, 0.0024,  ..., 0.0893, 0.1234, 0.0610]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672804.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12456.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.4254, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.7797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(304.5930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8094],
        [-2.7127],
        [-3.3730],
        ...,
        [-3.7598],
        [-3.6139],
        [-3.5562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296228.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0314],
        [1.0373],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371580.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3318],
        [0.3875],
        [0.2817],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.8182, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0314],
        [1.0373],
        ...,
        [0.9952],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371583.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3318],
        [0.3875],
        [0.2817],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.8182, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0235, -0.0016,  0.0143,  ...,  0.0126, -0.0024,  0.0471],
        [ 0.0214, -0.0014,  0.0131,  ...,  0.0116, -0.0022,  0.0433],
        [ 0.0225, -0.0015,  0.0137,  ...,  0.0121, -0.0023,  0.0452],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3546.2690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0197, device='cuda:0')



h[100].sum tensor(-9.8064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7288, device='cuda:0')



h[200].sum tensor(102.4855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0841, 0.0000, 0.0513,  ..., 0.0459, 0.0000, 0.1708],
        [0.0908, 0.0000, 0.0554,  ..., 0.0491, 0.0000, 0.1833],
        [0.0776, 0.0000, 0.0474,  ..., 0.0430, 0.0000, 0.1590],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76608.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0662, 0.0000, 0.2107,  ..., 0.1509, 0.0000, 0.3887],
        [0.0631, 0.0000, 0.2242,  ..., 0.1558, 0.0000, 0.4107],
        [0.0694, 0.0000, 0.2048,  ..., 0.1478, 0.0000, 0.3778],
        ...,
        [0.1154, 0.1403, 0.0022,  ..., 0.0898, 0.1237, 0.0614],
        [0.1153, 0.1402, 0.0022,  ..., 0.0897, 0.1236, 0.0613],
        [0.1152, 0.1402, 0.0022,  ..., 0.0897, 0.1235, 0.0613]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727757.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12403.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(927.8441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(474.7177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(353.8732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3898],
        [ 0.3776],
        [ 0.3764],
        ...,
        [-4.0943],
        [-4.0864],
        [-4.0846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271440., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0314],
        [1.0373],
        ...,
        [0.9952],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371583.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0315],
        [1.0374],
        ...,
        [0.9952],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371585.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3146.6028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0159, device='cuda:0')



h[100].sum tensor(-7.9625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5897, device='cuda:0')



h[200].sum tensor(96.9999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72293.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1105, 0.1038, 0.0135,  ..., 0.0886, 0.0938, 0.0775],
        [0.1116, 0.1326, 0.0029,  ..., 0.0874, 0.1165, 0.0627],
        [0.1116, 0.1368, 0.0021,  ..., 0.0875, 0.1202, 0.0600],
        ...,
        [0.1155, 0.1406, 0.0021,  ..., 0.0901, 0.1242, 0.0617],
        [0.1154, 0.1405, 0.0021,  ..., 0.0900, 0.1241, 0.0616],
        [0.1153, 0.1404, 0.0021,  ..., 0.0900, 0.1240, 0.0616]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712986.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12439.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(904.3928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(451.9909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(326.8620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6635],
        [-2.5124],
        [-3.1264],
        ...,
        [-4.1085],
        [-4.1005],
        [-4.0977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289007.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0315],
        [1.0374],
        ...,
        [0.9952],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371585.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(258.8369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0315],
        [1.0374],
        ...,
        [0.9952],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371588.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(258.8369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [ 0.0173, -0.0012,  0.0106,  ...,  0.0097, -0.0018,  0.0358],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3434.9614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0191, device='cuda:0')



h[100].sum tensor(-9.3838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7070, device='cuda:0')



h[200].sum tensor(98.7937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7958, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0062],
        [0.0179, 0.0000, 0.0110,  ..., 0.0133, 0.0000, 0.0418],
        [0.0216, 0.0000, 0.0133,  ..., 0.0157, 0.0000, 0.0511],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75132.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1088, 0.0769, 0.0197,  ..., 0.0923, 0.0741, 0.0903],
        [0.1017, 0.0348, 0.0551,  ..., 0.1040, 0.0344, 0.1464],
        [0.0945, 0.0097, 0.0987,  ..., 0.1159, 0.0107, 0.2131],
        ...,
        [0.1157, 0.1409, 0.0019,  ..., 0.0905, 0.1247, 0.0620],
        [0.1156, 0.1408, 0.0019,  ..., 0.0905, 0.1246, 0.0619],
        [0.1155, 0.1407, 0.0019,  ..., 0.0904, 0.1246, 0.0619]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(721620.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12491.8135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(915.7897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(470.6481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(337.9826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4619],
        [-0.0646],
        [ 0.2258],
        ...,
        [-4.1271],
        [-4.1190],
        [-4.1172]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275908.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0315],
        [1.0374],
        ...,
        [0.9952],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371588.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.5878, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0374],
        ...,
        [0.9952],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371591.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.5878, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [ 0.0113, -0.0008,  0.0070,  ...,  0.0070, -0.0012,  0.0248],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3273.1138, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-8.6827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6627, device='cuda:0')



h[200].sum tensor(96.4360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0163, 0.0000, 0.0101,  ..., 0.0133, 0.0000, 0.0413],
        [0.0139, 0.0000, 0.0087,  ..., 0.0128, 0.0000, 0.0394],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0186, 0.0000, 0.0114,  ..., 0.0139, 0.0000, 0.0433]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71573.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1103, 0.0749, 0.0227,  ..., 0.0924, 0.0721, 0.0928],
        [0.1086, 0.0294, 0.0506,  ..., 0.0985, 0.0298, 0.1352],
        [0.1091, 0.0156, 0.0630,  ..., 0.0996, 0.0148, 0.1517],
        ...,
        [0.1155, 0.1310, 0.0035,  ..., 0.0921, 0.1174, 0.0671],
        [0.1130, 0.0874, 0.0169,  ..., 0.0962, 0.0842, 0.0880],
        [0.1050, 0.0395, 0.0549,  ..., 0.1089, 0.0389, 0.1503]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696241.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12615.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(899.4108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(454.0721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(318.8145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1525],
        [ 0.1557],
        [ 0.4152],
        ...,
        [-3.5767],
        [-2.8222],
        [-1.7751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303581.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0374],
        ...,
        [0.9952],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371591.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 730.0 event: 3650 loss: tensor(476.5347, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.4248, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0374],
        ...,
        [0.9951],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371594.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.4248, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2771.5498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.6722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0131, device='cuda:0')



h[100].sum tensor(-6.3658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4847, device='cuda:0')



h[200].sum tensor(91.0052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64617.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1120, 0.1379, 0.0008,  ..., 0.0887, 0.1216, 0.0599],
        [0.1125, 0.1384, 0.0008,  ..., 0.0890, 0.1220, 0.0602],
        [0.1126, 0.1382, 0.0010,  ..., 0.0890, 0.1220, 0.0602],
        ...,
        [0.1166, 0.1416, 0.0012,  ..., 0.0915, 0.1257, 0.0623],
        [0.1165, 0.1415, 0.0012,  ..., 0.0915, 0.1256, 0.0622],
        [0.1164, 0.1415, 0.0012,  ..., 0.0914, 0.1255, 0.0622]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668525.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12784.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(875.3478, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(412.1259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(284.4038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1038],
        [-4.1688],
        [-4.2076],
        ...,
        [-4.1777],
        [-4.1692],
        [-4.1672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323816.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0374],
        ...,
        [0.9951],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371594.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.4218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0374],
        ...,
        [0.9951],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371594.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.4218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0127, -0.0009,  0.0078,  ...,  0.0076, -0.0013,  0.0273],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [ 0.0309, -0.0020,  0.0189,  ...,  0.0162, -0.0031,  0.0610],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3233.9592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0175, device='cuda:0')



h[100].sum tensor(-8.5412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6458, device='cuda:0')



h[200].sum tensor(95.7831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0525, 0.0000, 0.0322,  ..., 0.0309, 0.0000, 0.1105],
        [0.0667, 0.0000, 0.0409,  ..., 0.0382, 0.0000, 0.1393],
        [0.0650, 0.0000, 0.0398,  ..., 0.0368, 0.0000, 0.1335],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72200.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0783, 0.0000, 0.1851,  ..., 0.1474, 0.0000, 0.3559],
        [0.0740, 0.0000, 0.2040,  ..., 0.1546, 0.0000, 0.3875],
        [0.0703, 0.0000, 0.2128,  ..., 0.1592, 0.0000, 0.4041],
        ...,
        [0.1166, 0.1416, 0.0012,  ..., 0.0915, 0.1257, 0.0623],
        [0.1165, 0.1415, 0.0012,  ..., 0.0915, 0.1256, 0.0622],
        [0.1164, 0.1415, 0.0012,  ..., 0.0914, 0.1255, 0.0622]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702075.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12767.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(908.8998, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(457.5724, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.5354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2779],
        [ 0.2468],
        [ 0.2293],
        ...,
        [-4.1778],
        [-4.1693],
        [-4.1675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288332.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0374],
        ...,
        [0.9951],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371594.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.7472, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0375],
        ...,
        [0.9951],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371597.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.7472, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3138.1221, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.5460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0168, device='cuda:0')



h[100].sum tensor(-8.1082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6221, device='cuda:0')



h[200].sum tensor(94.9801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2589, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70101.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1124, 0.0992, 0.0135,  ..., 0.0910, 0.0908, 0.0816],
        [0.1121, 0.0734, 0.0218,  ..., 0.0934, 0.0708, 0.0955],
        [0.1110, 0.0556, 0.0287,  ..., 0.0957, 0.0537, 0.1062],
        ...,
        [0.1161, 0.0964, 0.0147,  ..., 0.0954, 0.0903, 0.0860],
        [0.1162, 0.1075, 0.0118,  ..., 0.0945, 0.0990, 0.0801],
        [0.1166, 0.1303, 0.0038,  ..., 0.0927, 0.1168, 0.0682]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693051.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12853.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(903.2143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.9598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(314.4343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7529],
        [-0.5352],
        [-0.2300],
        ...,
        [-2.7181],
        [-3.0467],
        [-3.5081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317922.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0375],
        ...,
        [0.9951],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371597.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.2898, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0375],
        ...,
        [0.9951],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371601.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.2898, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2884.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.9932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-6.8808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5253, device='cuda:0')



h[200].sum tensor(93.2331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66228.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1125, 0.1382, 0.0005,  ..., 0.0890, 0.1217, 0.0599],
        [0.1129, 0.1356, 0.0005,  ..., 0.0895, 0.1197, 0.0618],
        [0.1130, 0.1208, 0.0059,  ..., 0.0904, 0.1081, 0.0698],
        ...,
        [0.1171, 0.1419, 0.0010,  ..., 0.0919, 0.1258, 0.0623],
        [0.1170, 0.1418, 0.0010,  ..., 0.0918, 0.1257, 0.0622],
        [0.1170, 0.1417, 0.0009,  ..., 0.0918, 0.1256, 0.0622]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676422.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12836.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(883.7734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(427.2867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(294.1078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0265],
        [-2.4373],
        [-1.6454],
        ...,
        [-4.1887],
        [-4.1837],
        [-4.1857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-352302.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0375],
        ...,
        [0.9951],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371601.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2423],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7074, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0376],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371606.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2423],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7074, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037, -0.0003,  0.0023,  ...,  0.0034, -0.0005,  0.0107],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [ 0.0037, -0.0003,  0.0023,  ...,  0.0034, -0.0005,  0.0107],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3088.9302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0160, device='cuda:0')



h[100].sum tensor(-7.7395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5920, device='cuda:0')



h[200].sum tensor(96.4563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0029, 0.0000, 0.0019,  ..., 0.0064, 0.0000, 0.0139],
        [0.0134, 0.0000, 0.0086,  ..., 0.0133, 0.0000, 0.0408],
        [0.0029, 0.0000, 0.0019,  ..., 0.0064, 0.0000, 0.0139],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69344.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1099, 0.0341, 0.0424,  ..., 0.0969, 0.0357, 0.1252],
        [0.1094, 0.0052, 0.0683,  ..., 0.1015, 0.0064, 0.1629],
        [0.1090, 0.0105, 0.0613,  ..., 0.1011, 0.0134, 0.1533],
        ...,
        [0.1168, 0.1417, 0.0013,  ..., 0.0915, 0.1251, 0.0624],
        [0.1167, 0.1416, 0.0012,  ..., 0.0914, 0.1250, 0.0623],
        [0.1167, 0.1415, 0.0012,  ..., 0.0914, 0.1250, 0.0623]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691243.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12805.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(900.8840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(438.3972, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(311.8680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0236],
        [ 0.2477],
        [ 0.3327],
        ...,
        [-4.1815],
        [-4.1714],
        [-4.1680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304513.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0376],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371606.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.1354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0377],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371611.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.1354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3298.1816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-8.6042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6614, device='cuda:0')



h[200].sum tensor(99.7025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73552.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1120, 0.1378, 0.0011,  ..., 0.0882, 0.1205, 0.0601],
        [0.1124, 0.1382, 0.0011,  ..., 0.0885, 0.1209, 0.0603],
        [0.1125, 0.1381, 0.0013,  ..., 0.0885, 0.1209, 0.0604],
        ...,
        [0.1165, 0.1414, 0.0016,  ..., 0.0910, 0.1244, 0.0624],
        [0.1164, 0.1413, 0.0016,  ..., 0.0910, 0.1244, 0.0624],
        [0.1164, 0.1413, 0.0016,  ..., 0.0909, 0.1243, 0.0624]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714780.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12581.8145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(908.1034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(465.5260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.6529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8096],
        [-4.0058],
        [-4.1442],
        ...,
        [-4.1662],
        [-4.1581],
        [-4.1566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322847.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0315],
        [1.0377],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371611.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.4562, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0378],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371616.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.4562, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0171, -0.0011,  0.0105,  ...,  0.0097, -0.0017,  0.0354],
        [ 0.0074, -0.0005,  0.0045,  ...,  0.0051, -0.0008,  0.0175],
        [ 0.0185, -0.0012,  0.0113,  ...,  0.0103, -0.0019,  0.0379],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3035.7568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0152, device='cuda:0')



h[100].sum tensor(-7.2735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5640, device='cuda:0')



h[200].sum tensor(97.9635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0272, 0.0000, 0.0168,  ..., 0.0190, 0.0000, 0.0638],
        [0.0580, 0.0000, 0.0356,  ..., 0.0340, 0.0000, 0.1231],
        [0.0442, 0.0000, 0.0272,  ..., 0.0276, 0.0000, 0.0977],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67381.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0948, 0.0000, 0.1270,  ..., 0.1209, 0.0000, 0.2535],
        [0.0896, 0.0000, 0.1477,  ..., 0.1291, 0.0000, 0.2881],
        [0.0894, 0.0000, 0.1479,  ..., 0.1294, 0.0000, 0.2886],
        ...,
        [0.1162, 0.1412, 0.0019,  ..., 0.0906, 0.1238, 0.0626],
        [0.1161, 0.1411, 0.0019,  ..., 0.0905, 0.1237, 0.0626],
        [0.1160, 0.1410, 0.0019,  ..., 0.0905, 0.1236, 0.0625]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677727.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12631.0811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(887.2814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(419.9731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(303.2500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4148],
        [ 0.4113],
        [ 0.3964],
        ...,
        [-4.1271],
        [-4.1224],
        [-4.1248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288959.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0378],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371616.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(252.7692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0379],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371620.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(252.7692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0248, -0.0016,  0.0151,  ...,  0.0133, -0.0024,  0.0496],
        [ 0.0188, -0.0012,  0.0115,  ...,  0.0104, -0.0019,  0.0385],
        [ 0.0133, -0.0009,  0.0081,  ...,  0.0079, -0.0014,  0.0284],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3414.9958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0187, device='cuda:0')



h[100].sum tensor(-8.9585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6905, device='cuda:0')



h[200].sum tensor(102.3749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4958, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1248, 0.0000, 0.0760,  ..., 0.0653, 0.0000, 0.2461],
        [0.1047, 0.0000, 0.0639,  ..., 0.0559, 0.0000, 0.2092],
        [0.1087, 0.0000, 0.0663,  ..., 0.0578, 0.0000, 0.2165],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0030, 0.0000, 0.0019,  ..., 0.0066, 0.0000, 0.0147],
        [0.0061, 0.0000, 0.0038,  ..., 0.0086, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74975.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0381, 0.0000, 0.3141,  ..., 0.1914, 0.0000, 0.5688],
        [0.0432, 0.0000, 0.3009,  ..., 0.1865, 0.0000, 0.5459],
        [0.0466, 0.0000, 0.2920,  ..., 0.1828, 0.0000, 0.5299],
        ...,
        [0.1167, 0.1184, 0.0112,  ..., 0.0906, 0.1049, 0.0763],
        [0.1172, 0.0963, 0.0200,  ..., 0.0908, 0.0869, 0.0894],
        [0.1177, 0.0785, 0.0279,  ..., 0.0910, 0.0724, 0.1000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(720368., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12531.3330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(918.9377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(464.2588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(341.9316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0805],
        [ 0.0965],
        [ 0.1334],
        ...,
        [-3.4140],
        [-3.0973],
        [-2.9446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273718.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0379],
        ...,
        [0.9951],
        [0.9939],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371620.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(369.8602, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0379],
        ...,
        [0.9951],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371625.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(369.8602, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4347.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0273, device='cuda:0')



h[100].sum tensor(-13.2239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.0103, device='cuda:0')



h[200].sum tensor(112.4627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2843, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92830.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1115, 0.1375, 0.0017,  ..., 0.0873, 0.1194, 0.0604],
        [0.1120, 0.1380, 0.0017,  ..., 0.0876, 0.1198, 0.0606],
        [0.1121, 0.1378, 0.0018,  ..., 0.0876, 0.1198, 0.0607],
        ...,
        [0.1161, 0.1411, 0.0022,  ..., 0.0901, 0.1233, 0.0628],
        [0.1160, 0.1410, 0.0022,  ..., 0.0900, 0.1232, 0.0627],
        [0.1159, 0.1410, 0.0022,  ..., 0.0900, 0.1232, 0.0627]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(824962.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12358.1104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(991.9349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(570.9537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.3841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0563],
        [-4.0915],
        [-4.1090],
        ...,
        [-4.1202],
        [-4.1076],
        [-4.1023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239454.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0379],
        ...,
        [0.9951],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371625.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(183.9170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0379],
        ...,
        [0.9951],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371625.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(183.9170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2902.9175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.9011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0136, device='cuda:0')



h[100].sum tensor(-6.5319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5024, device='cuda:0')



h[200].sum tensor(97.5712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0921, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0000, 0.0082,  ..., 0.0118, 0.0000, 0.0357],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66910.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1090, 0.0377, 0.0477,  ..., 0.0953, 0.0351, 0.1297],
        [0.1117, 0.0911, 0.0187,  ..., 0.0896, 0.0824, 0.0862],
        [0.1126, 0.1227, 0.0076,  ..., 0.0873, 0.1071, 0.0700],
        ...,
        [0.1161, 0.1411, 0.0022,  ..., 0.0901, 0.1233, 0.0628],
        [0.1160, 0.1410, 0.0022,  ..., 0.0900, 0.1232, 0.0627],
        [0.1159, 0.1410, 0.0022,  ..., 0.0900, 0.1232, 0.0627]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679011.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12596.0693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(886.3258, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.8990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.9387, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5293],
        [-1.3814],
        [-2.1590],
        ...,
        [-4.1322],
        [-4.1246],
        [-4.1232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280241.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0379],
        ...,
        [0.9951],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371625.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 740.0 event: 3700 loss: tensor(382.3522, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6113],
        [0.6641],
        [0.5474],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.1608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0380],
        ...,
        [0.9951],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371629.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6113],
        [0.6641],
        [0.5474],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.1608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0213, -0.0014,  0.0130,  ...,  0.0117, -0.0021,  0.0432],
        [ 0.0323, -0.0020,  0.0197,  ...,  0.0168, -0.0031,  0.0635],
        [ 0.0335, -0.0021,  0.0204,  ...,  0.0174, -0.0032,  0.0657],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3692.4326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.5736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0209, device='cuda:0')



h[100].sum tensor(-10.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7735, device='cuda:0')



h[200].sum tensor(106.1679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1276, 0.0000, 0.0777,  ..., 0.0666, 0.0000, 0.2512],
        [0.1302, 0.0000, 0.0793,  ..., 0.0678, 0.0000, 0.2561],
        [0.1108, 0.0000, 0.0675,  ..., 0.0587, 0.0000, 0.2203],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78674.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0408, 0.0000, 0.3055,  ..., 0.1867, 0.0000, 0.5536],
        [0.0393, 0.0000, 0.3098,  ..., 0.1891, 0.0000, 0.5618],
        [0.0448, 0.0000, 0.2917,  ..., 0.1822, 0.0000, 0.5314],
        ...,
        [0.1163, 0.1413, 0.0022,  ..., 0.0900, 0.1233, 0.0629],
        [0.1162, 0.1412, 0.0022,  ..., 0.0899, 0.1232, 0.0628],
        [0.1161, 0.1411, 0.0022,  ..., 0.0899, 0.1231, 0.0628]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739135.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12565.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(940.0516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(480.9270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.6477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2044],
        [ 0.1988],
        [ 0.2094],
        ...,
        [-4.1367],
        [-4.1291],
        [-4.1278]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248195.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0316],
        [1.0380],
        ...,
        [0.9951],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371629.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.1698, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0316],
        [1.0380],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371632.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.1698, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3032.3955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.4808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-7.0928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5468, device='cuda:0')



h[200].sum tensor(99.5359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68236.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1129, 0.1282, 0.0057,  ..., 0.0864, 0.1108, 0.0675],
        [0.1128, 0.1365, 0.0028,  ..., 0.0872, 0.1181, 0.0624],
        [0.1127, 0.1378, 0.0020,  ..., 0.0875, 0.1196, 0.0612],
        ...,
        [0.1166, 0.1416, 0.0020,  ..., 0.0900, 0.1235, 0.0630],
        [0.1165, 0.1415, 0.0020,  ..., 0.0900, 0.1234, 0.0630],
        [0.1165, 0.1415, 0.0020,  ..., 0.0899, 0.1234, 0.0629]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684591.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12681.2529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(894.8723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(419.4142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.7195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4681],
        [-2.9409],
        [-3.3151],
        ...,
        [-4.1500],
        [-4.1425],
        [-4.1412]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295331.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0316],
        [1.0380],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371632.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.8467, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0315],
        [1.0381],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371636.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.8467, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3010.2256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0146, device='cuda:0')



h[100].sum tensor(-6.9480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5404, device='cuda:0')



h[200].sum tensor(99.7795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7807, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68068.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1119, 0.1023, 0.0139,  ..., 0.0893, 0.0912, 0.0801],
        [0.1123, 0.1186, 0.0088,  ..., 0.0889, 0.1042, 0.0715],
        [0.1128, 0.1275, 0.0060,  ..., 0.0881, 0.1111, 0.0673],
        ...,
        [0.1167, 0.1417, 0.0020,  ..., 0.0901, 0.1233, 0.0633],
        [0.1166, 0.1416, 0.0020,  ..., 0.0900, 0.1232, 0.0632],
        [0.1166, 0.1416, 0.0020,  ..., 0.0900, 0.1232, 0.0632]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684632.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12747.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(898.9815, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(414.2038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(308.1027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7101],
        [-1.1902],
        [-1.6656],
        ...,
        [-4.1510],
        [-4.1434],
        [-4.1423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274101.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0315],
        [1.0381],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371636.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.5541, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0315],
        [1.0381],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371639.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.5541, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2894.5039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0134, device='cuda:0')



h[100].sum tensor(-6.3934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4959, device='cuda:0')



h[200].sum tensor(98.6721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66430.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1124, 0.1383, 0.0014,  ..., 0.0875, 0.1195, 0.0611],
        [0.1128, 0.1388, 0.0015,  ..., 0.0878, 0.1200, 0.0613],
        [0.1130, 0.1386, 0.0016,  ..., 0.0877, 0.1199, 0.0614],
        ...,
        [0.1170, 0.1420, 0.0019,  ..., 0.0903, 0.1234, 0.0635],
        [0.1169, 0.1419, 0.0019,  ..., 0.0902, 0.1233, 0.0634],
        [0.1168, 0.1418, 0.0019,  ..., 0.0901, 0.1233, 0.0634]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677289.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12762.3877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(890.4416, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(406.0683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.7942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4850],
        [-3.7644],
        [-3.9422],
        ...,
        [-4.1575],
        [-4.1485],
        [-4.1451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296012.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0315],
        [1.0381],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371639.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.9775, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0315],
        [1.0381],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371639.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.9775, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3290.3306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.6094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0171, device='cuda:0')



h[100].sum tensor(-8.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6337, device='cuda:0')



h[200].sum tensor(102.7446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4680, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72556.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1124, 0.1223, 0.0066,  ..., 0.0883, 0.1068, 0.0698],
        [0.1128, 0.1356, 0.0015,  ..., 0.0880, 0.1174, 0.0630],
        [0.1130, 0.1386, 0.0016,  ..., 0.0877, 0.1199, 0.0614],
        ...,
        [0.1170, 0.1420, 0.0019,  ..., 0.0903, 0.1234, 0.0635],
        [0.1169, 0.1419, 0.0019,  ..., 0.0902, 0.1233, 0.0634],
        [0.1168, 0.1418, 0.0019,  ..., 0.0901, 0.1233, 0.0634]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(705625.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12783.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(920.6396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(440.4005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.7868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9837],
        [-3.5658],
        [-3.8742],
        ...,
        [-4.1606],
        [-4.1529],
        [-4.1517]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266902.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0315],
        [1.0381],
        ...,
        [0.9950],
        [0.9938],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371639.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0315],
        [1.0382],
        ...,
        [0.9950],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371643.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3341.7590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-8.4304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6516, device='cuda:0')



h[200].sum tensor(103.2114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0188, 0.0000, 0.0115,  ..., 0.0144, 0.0000, 0.0457],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72758.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1120, 0.1214, 0.0058,  ..., 0.0892, 0.1065, 0.0697],
        [0.1115, 0.0928, 0.0156,  ..., 0.0918, 0.0845, 0.0844],
        [0.1063, 0.0345, 0.0529,  ..., 0.1021, 0.0331, 0.1428],
        ...,
        [0.1171, 0.1422, 0.0018,  ..., 0.0907, 0.1236, 0.0636],
        [0.1171, 0.1421, 0.0018,  ..., 0.0906, 0.1235, 0.0635],
        [0.1170, 0.1420, 0.0018,  ..., 0.0906, 0.1235, 0.0635]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703478.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12818.7354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(922.7509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(444.2180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.9130, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7987],
        [-1.9596],
        [-0.8646],
        ...,
        [-4.1749],
        [-4.1671],
        [-4.1658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266273.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0315],
        [1.0382],
        ...,
        [0.9950],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371643.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(176.4320, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0315],
        [1.0382],
        ...,
        [0.9949],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371647.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(176.4320, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2855.6245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.5273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0130, device='cuda:0')



h[100].sum tensor(-6.1923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4819, device='cuda:0')



h[200].sum tensor(97.8731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0063],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65687.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1127, 0.1387, 0.0012,  ..., 0.0883, 0.1201, 0.0612],
        [0.1132, 0.1392, 0.0012,  ..., 0.0886, 0.1205, 0.0615],
        [0.1133, 0.1390, 0.0014,  ..., 0.0886, 0.1205, 0.0615],
        ...,
        [0.1173, 0.1424, 0.0017,  ..., 0.0911, 0.1240, 0.0636],
        [0.1172, 0.1423, 0.0017,  ..., 0.0911, 0.1239, 0.0636],
        [0.1172, 0.1423, 0.0017,  ..., 0.0910, 0.1239, 0.0636]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675037.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12880.0615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(893.6039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(404.4361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(295.2117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0085],
        [-3.9608],
        [-3.8168],
        ...,
        [-4.1753],
        [-4.1735],
        [-4.1737]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288906.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0315],
        [1.0382],
        ...,
        [0.9949],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371647.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2683],
        [0.2489],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.5684, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0342],
        [1.0315],
        [1.0383],
        ...,
        [0.9949],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371651., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2683],
        [0.2489],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.5684, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0003,  0.0027,  ...,  0.0037, -0.0005,  0.0117],
        [ 0.0081, -0.0005,  0.0050,  ...,  0.0054, -0.0008,  0.0187],
        [ 0.0180, -0.0011,  0.0110,  ...,  0.0101, -0.0018,  0.0370],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2950.2224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0139, device='cuda:0')



h[100].sum tensor(-6.6225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5151, device='cuda:0')



h[200].sum tensor(98.2645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0236, 0.0000, 0.0146,  ..., 0.0173, 0.0000, 0.0570],
        [0.0496, 0.0000, 0.0304,  ..., 0.0301, 0.0000, 0.1073],
        [0.0579, 0.0000, 0.0355,  ..., 0.0340, 0.0000, 0.1227],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66484.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1008, 0.0000, 0.1143,  ..., 0.1164, 0.0000, 0.2338],
        [0.0937, 0.0000, 0.1430,  ..., 0.1274, 0.0000, 0.2812],
        [0.0886, 0.0000, 0.1586,  ..., 0.1342, 0.0000, 0.3078],
        ...,
        [0.1174, 0.1426, 0.0016,  ..., 0.0916, 0.1244, 0.0637],
        [0.1173, 0.1425, 0.0016,  ..., 0.0916, 0.1243, 0.0637],
        [0.1173, 0.1425, 0.0016,  ..., 0.0915, 0.1243, 0.0637]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676439.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12818.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(890.7546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(416.6412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.2876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5685],
        [ 0.5488],
        [ 0.5333],
        ...,
        [-4.2074],
        [-4.1996],
        [-4.1984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325318.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0342],
        [1.0315],
        [1.0383],
        ...,
        [0.9949],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371651., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.3093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0316],
        [1.0384],
        ...,
        [0.9949],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371655., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.3093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0100, -0.0007,  0.0061,  ...,  0.0063, -0.0010,  0.0223],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3460.9424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0189, device='cuda:0')



h[100].sum tensor(-8.9247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7001, device='cuda:0')



h[200].sum tensor(102.9373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6709, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0145, 0.0000, 0.0091,  ..., 0.0131, 0.0000, 0.0404],
        [0.0132, 0.0000, 0.0082,  ..., 0.0119, 0.0000, 0.0356],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0065],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71639.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1044, 0.0143, 0.0834,  ..., 0.1079, 0.0135, 0.1875],
        [0.1086, 0.0312, 0.0527,  ..., 0.1004, 0.0303, 0.1415],
        [0.1105, 0.0457, 0.0313,  ..., 0.0959, 0.0481, 0.1098],
        ...,
        [0.1173, 0.1427, 0.0016,  ..., 0.0920, 0.1246, 0.0639],
        [0.1172, 0.1426, 0.0016,  ..., 0.0919, 0.1245, 0.0638],
        [0.1172, 0.1426, 0.0016,  ..., 0.0919, 0.1245, 0.0638]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693375.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12804.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(913.5229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.0511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.7462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4064],
        [ 0.3001],
        [ 0.2231],
        ...,
        [-4.2151],
        [-4.2073],
        [-4.2062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288371.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0316],
        [1.0384],
        ...,
        [0.9949],
        [0.9937],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371655., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.3676, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0316],
        [1.0385],
        ...,
        [0.9949],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371659.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.3676, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [ 0.0042, -0.0003,  0.0026,  ...,  0.0036, -0.0005,  0.0117],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3028.9526, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0146, device='cuda:0')



h[100].sum tensor(-6.9333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5391, device='cuda:0')



h[200].sum tensor(97.8225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0066],
        [0.0125, 0.0000, 0.0077,  ..., 0.0115, 0.0000, 0.0344],
        [0.0262, 0.0000, 0.0161,  ..., 0.0186, 0.0000, 0.0621],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68456.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1101, 0.0576, 0.0301,  ..., 0.0955, 0.0543, 0.1055],
        [0.1038, 0.0226, 0.0721,  ..., 0.1076, 0.0207, 0.1713],
        [0.0947, 0.0013, 0.1198,  ..., 0.1220, 0.0036, 0.2451],
        ...,
        [0.1172, 0.1428, 0.0017,  ..., 0.0923, 0.1249, 0.0640],
        [0.1171, 0.1427, 0.0017,  ..., 0.0922, 0.1248, 0.0639],
        [0.1170, 0.1426, 0.0017,  ..., 0.0922, 0.1248, 0.0639]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688876., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12724.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(895.3338, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(433.0495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(301.5336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3382],
        [ 0.0083],
        [ 0.1909],
        ...,
        [-4.0710],
        [-4.1795],
        [-4.2070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316108.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0316],
        [1.0385],
        ...,
        [0.9949],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371659.2812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 750.0 event: 3750 loss: tensor(428.3769, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.7551, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0385],
        ...,
        [0.9949],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371663.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.7551, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3350.2141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0177, device='cuda:0')



h[100].sum tensor(-8.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6549, device='cuda:0')



h[200].sum tensor(100.8845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0000, 0.0048,  ..., 0.0098, 0.0000, 0.0277],
        [0.0046, 0.0000, 0.0029,  ..., 0.0078, 0.0000, 0.0200],
        [0.0023, 0.0000, 0.0015,  ..., 0.0061, 0.0000, 0.0134],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72834.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1151, 0.0397, 0.0434,  ..., 0.0906, 0.0395, 0.1225],
        [0.1155, 0.0718, 0.0305,  ..., 0.0897, 0.0653, 0.1049],
        [0.1147, 0.1029, 0.0165,  ..., 0.0893, 0.0910, 0.0857],
        ...,
        [0.1169, 0.1427, 0.0018,  ..., 0.0924, 0.1248, 0.0642],
        [0.1168, 0.1426, 0.0018,  ..., 0.0924, 0.1247, 0.0641],
        [0.1168, 0.1426, 0.0018,  ..., 0.0923, 0.1247, 0.0641]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709688.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12649.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(913.4034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(460.0194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.0188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7741],
        [-1.5600],
        [-2.2697],
        ...,
        [-4.2171],
        [-4.2095],
        [-4.2085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293030.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0385],
        ...,
        [0.9949],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371663.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(306.4144, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0386],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371667.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(306.4144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [ 0.0050, -0.0004,  0.0031,  ...,  0.0040, -0.0006,  0.0132],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3868.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0226, device='cuda:0')



h[100].sum tensor(-10.6413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8370, device='cuda:0')



h[200].sum tensor(105.9651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1478, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0068],
        [0.0052, 0.0000, 0.0032,  ..., 0.0075, 0.0000, 0.0189],
        [0.0111, 0.0000, 0.0069,  ..., 0.0109, 0.0000, 0.0322],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80084.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1125, 0.1156, 0.0097,  ..., 0.0904, 0.1017, 0.0754],
        [0.1115, 0.0659, 0.0271,  ..., 0.0944, 0.0619, 0.1023],
        [0.1097, 0.0323, 0.0511,  ..., 0.0990, 0.0310, 0.1368],
        ...,
        [0.1170, 0.1429, 0.0017,  ..., 0.0927, 0.1249, 0.0644],
        [0.1169, 0.1428, 0.0017,  ..., 0.0926, 0.1248, 0.0643],
        [0.1169, 0.1427, 0.0017,  ..., 0.0926, 0.1248, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(740819., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12538.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(936.5242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(509.0201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(356.4756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0786],
        [-1.1869],
        [-0.3489],
        ...,
        [-4.2238],
        [-4.2160],
        [-4.2150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310356.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0386],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371667.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3979, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0386],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371667.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3979, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3024.0713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.4063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0145, device='cuda:0')



h[100].sum tensor(-6.8222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5365, device='cuda:0')



h[200].sum tensor(97.3048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7091, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68369.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1125, 0.1392, 0.0012,  ..., 0.0898, 0.1210, 0.0620],
        [0.1129, 0.1397, 0.0013,  ..., 0.0901, 0.1214, 0.0622],
        [0.1130, 0.1395, 0.0014,  ..., 0.0900, 0.1214, 0.0623],
        ...,
        [0.1170, 0.1429, 0.0017,  ..., 0.0927, 0.1249, 0.0644],
        [0.1169, 0.1428, 0.0017,  ..., 0.0926, 0.1248, 0.0643],
        [0.1169, 0.1427, 0.0017,  ..., 0.0926, 0.1248, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691091.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12715.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(895.6993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(432.7410, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(299.7870, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0842],
        [-4.0103],
        [-3.8655],
        ...,
        [-4.2246],
        [-4.2169],
        [-4.2160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295302.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0386],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371667.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.0573, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0386],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371671.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.0573, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [ 0.0032, -0.0003,  0.0020,  ...,  0.0032, -0.0004,  0.0099],
        [ 0.0032, -0.0003,  0.0020,  ...,  0.0032, -0.0004,  0.0099],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3032.2720, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0145, device='cuda:0')



h[100].sum tensor(-6.8377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5383, device='cuda:0')



h[200].sum tensor(97.0143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7417, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0103, 0.0000, 0.0064,  ..., 0.0105, 0.0000, 0.0306],
        [0.0135, 0.0000, 0.0084,  ..., 0.0126, 0.0000, 0.0389],
        [0.0090, 0.0000, 0.0057,  ..., 0.0105, 0.0000, 0.0307],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68191.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1118, 0.0435, 0.0415,  ..., 0.0954, 0.0414, 0.1223],
        [0.1117, 0.0203, 0.0555,  ..., 0.0980, 0.0200, 0.1422],
        [0.1124, 0.0261, 0.0521,  ..., 0.0969, 0.0245, 0.1370],
        ...,
        [0.1173, 0.1431, 0.0016,  ..., 0.0929, 0.1252, 0.0646],
        [0.1172, 0.1430, 0.0016,  ..., 0.0928, 0.1250, 0.0646],
        [0.1171, 0.1430, 0.0016,  ..., 0.0928, 0.1250, 0.0646]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688437.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12756.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(895.7083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(433.6700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(298.3237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5796],
        [-0.0233],
        [ 0.2048],
        ...,
        [-4.2360],
        [-4.2283],
        [-4.2274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300549.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0316],
        [1.0386],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371671.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.2397, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0317],
        [1.0387],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371675.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.2397, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041, -0.0003,  0.0026,  ...,  0.0036, -0.0005,  0.0115],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3264.8076, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.8753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0170, device='cuda:0')



h[100].sum tensor(-7.8653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6289, device='cuda:0')



h[200].sum tensor(99.1882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0000, 0.0021,  ..., 0.0066, 0.0000, 0.0152],
        [0.0043, 0.0000, 0.0027,  ..., 0.0071, 0.0000, 0.0171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71160.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1134, 0.0957, 0.0168,  ..., 0.0919, 0.0861, 0.0878],
        [0.1137, 0.0962, 0.0166,  ..., 0.0924, 0.0865, 0.0878],
        [0.1140, 0.1058, 0.0130,  ..., 0.0915, 0.0941, 0.0827],
        ...,
        [0.1176, 0.1434, 0.0015,  ..., 0.0931, 0.1253, 0.0649],
        [0.1175, 0.1433, 0.0015,  ..., 0.0930, 0.1252, 0.0648],
        [0.1175, 0.1433, 0.0015,  ..., 0.0930, 0.1252, 0.0648]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704292.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12721.1885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(905.6022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(456.8828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(312.6004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6373],
        [-2.3319],
        [-1.9754],
        ...,
        [-4.2502],
        [-4.2424],
        [-4.2415]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334586.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0317],
        [1.0387],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371675.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.2220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0317],
        [1.0387],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371675.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.2220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043, -0.0003,  0.0027,  ...,  0.0037, -0.0005,  0.0119],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3491.2744, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0189, device='cuda:0')



h[100].sum tensor(-8.8846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6999, device='cuda:0')



h[200].sum tensor(101.5073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0162, 0.0000, 0.0100,  ..., 0.0133, 0.0000, 0.0414],
        [0.0045, 0.0000, 0.0028,  ..., 0.0072, 0.0000, 0.0175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74360.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1079, 0.0198, 0.0600,  ..., 0.1024, 0.0215, 0.1515],
        [0.1116, 0.0553, 0.0329,  ..., 0.0962, 0.0525, 0.1115],
        [0.1131, 0.1050, 0.0133,  ..., 0.0922, 0.0939, 0.0820],
        ...,
        [0.1176, 0.1434, 0.0015,  ..., 0.0931, 0.1253, 0.0649],
        [0.1175, 0.1433, 0.0015,  ..., 0.0930, 0.1252, 0.0648],
        [0.1175, 0.1433, 0.0015,  ..., 0.0930, 0.1252, 0.0648]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713851.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12745.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(920.6376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(474.5886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.5759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1681],
        [-0.9840],
        [-1.9299],
        ...,
        [-4.2502],
        [-4.2424],
        [-4.2415]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305304.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0317],
        [1.0387],
        ...,
        [0.9948],
        [0.9936],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371675.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4414],
        [0.6055],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.3613, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0317],
        [1.0387],
        ...,
        [0.9948],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371679.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4414],
        [0.6055],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.3613, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0408, -0.0024,  0.0248,  ...,  0.0208, -0.0037,  0.0791],
        [ 0.0177, -0.0011,  0.0108,  ...,  0.0100, -0.0017,  0.0365],
        [ 0.0169, -0.0010,  0.0103,  ...,  0.0096, -0.0016,  0.0351],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3440.5916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0183, device='cuda:0')



h[100].sum tensor(-8.6201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6784, device='cuda:0')



h[200].sum tensor(101.0270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1050, 0.0000, 0.0641,  ..., 0.0561, 0.0000, 0.2099],
        [0.1129, 0.0000, 0.0689,  ..., 0.0598, 0.0000, 0.2246],
        [0.0663, 0.0000, 0.0406,  ..., 0.0380, 0.0000, 0.1387],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75169.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0491, 0.0000, 0.2567,  ..., 0.1757, 0.0000, 0.4771],
        [0.0520, 0.0000, 0.2533,  ..., 0.1734, 0.0000, 0.4704],
        [0.0684, 0.0000, 0.2110,  ..., 0.1555, 0.0000, 0.3967],
        ...,
        [0.1178, 0.1437, 0.0014,  ..., 0.0932, 0.1253, 0.0652],
        [0.1176, 0.1435, 0.0014,  ..., 0.0932, 0.1252, 0.0651],
        [0.1176, 0.1435, 0.0014,  ..., 0.0931, 0.1252, 0.0651]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723917.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12830.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(931.9517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(476.2804, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(335.6134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2307],
        [ 0.2250],
        [ 0.2432],
        ...,
        [-4.2572],
        [-4.2494],
        [-4.2486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277492.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0317],
        [1.0387],
        ...,
        [0.9948],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371679.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.4999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0346],
        [1.0317],
        [1.0387],
        ...,
        [0.9947],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371683.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.4999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2987.5071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0141, device='cuda:0')



h[100].sum tensor(-6.5421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5204, device='cuda:0')



h[200].sum tensor(96.3945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0150, 0.0000, 0.0093,  ..., 0.0128, 0.0000, 0.0393],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66885.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1062, 0.0268, 0.0613,  ..., 0.1043, 0.0268, 0.1563],
        [0.1121, 0.0707, 0.0254,  ..., 0.0950, 0.0669, 0.1004],
        [0.1138, 0.1110, 0.0115,  ..., 0.0920, 0.0982, 0.0799],
        ...,
        [0.1179, 0.1439, 0.0013,  ..., 0.0933, 0.1253, 0.0654],
        [0.1178, 0.1438, 0.0013,  ..., 0.0932, 0.1252, 0.0653],
        [0.1178, 0.1438, 0.0013,  ..., 0.0932, 0.1251, 0.0653]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681405.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12872.0752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(893.4381, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(428.7456, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(292.6887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1420],
        [-0.2749],
        [-0.7082],
        ...,
        [-4.2625],
        [-4.2547],
        [-4.2539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321334.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0346],
        [1.0317],
        [1.0387],
        ...,
        [0.9947],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371683.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.0197, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0346],
        [1.0317],
        [1.0387],
        ...,
        [0.9947],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371683.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.0197, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3033.9165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.3815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0144, device='cuda:0')



h[100].sum tensor(-6.7500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5327, device='cuda:0')



h[200].sum tensor(96.8693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0151, 0.0000, 0.0093,  ..., 0.0128, 0.0000, 0.0393],
        [0.0076, 0.0000, 0.0047,  ..., 0.0087, 0.0000, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68847.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1057, 0.0211, 0.0566,  ..., 0.1048, 0.0215, 0.1495],
        [0.1070, 0.0252, 0.0497,  ..., 0.1034, 0.0247, 0.1397],
        [0.1083, 0.0327, 0.0402,  ..., 0.1011, 0.0305, 0.1255],
        ...,
        [0.1179, 0.1439, 0.0013,  ..., 0.0933, 0.1253, 0.0654],
        [0.1178, 0.1438, 0.0013,  ..., 0.0932, 0.1252, 0.0653],
        [0.1178, 0.1438, 0.0013,  ..., 0.0932, 0.1251, 0.0653]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692350.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12836.9121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(900.7895, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(441.3936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.2399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1233],
        [ 0.3272],
        [ 0.4047],
        ...,
        [-4.2625],
        [-4.2547],
        [-4.2539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317408.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0346],
        [1.0317],
        [1.0387],
        ...,
        [0.9947],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371683.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6045],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.4401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0346],
        [1.0317],
        [1.0388],
        ...,
        [0.9947],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371687.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6045],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.4401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0173, -0.0010,  0.0106,  ...,  0.0098, -0.0016,  0.0358],
        [ 0.0112, -0.0007,  0.0069,  ...,  0.0069, -0.0011,  0.0247],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3125.1411, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0152, device='cuda:0')



h[100].sum tensor(-7.0867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5612, device='cuda:0')



h[200].sum tensor(98.2997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1561, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1140, 0.0000, 0.0696,  ..., 0.0603, 0.0000, 0.2266],
        [0.0417, 0.0000, 0.0255,  ..., 0.0253, 0.0000, 0.0885],
        [0.0116, 0.0000, 0.0071,  ..., 0.0105, 0.0000, 0.0306],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69131.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0429, 0.0000, 0.2671,  ..., 0.1809, 0.0000, 0.4984],
        [0.0764, 0.0000, 0.1506,  ..., 0.1407, 0.0000, 0.3077],
        [0.0988, 0.0415, 0.0664,  ..., 0.1120, 0.0388, 0.1710],
        ...,
        [0.1181, 0.1442, 0.0013,  ..., 0.0934, 0.1249, 0.0656],
        [0.1180, 0.1440, 0.0013,  ..., 0.0933, 0.1248, 0.0656],
        [0.1179, 0.1440, 0.0013,  ..., 0.0933, 0.1248, 0.0656]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691720.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12851.3184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(901.9731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(443.2706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(305.7817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1239],
        [ 0.0270],
        [-0.3593],
        ...,
        [-4.2605],
        [-4.2510],
        [-4.2486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322990., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0346],
        [1.0317],
        [1.0388],
        ...,
        [0.9947],
        [0.9935],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371687.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 760.0 event: 3800 loss: tensor(334.5939, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8331, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0347],
        [1.0317],
        [1.0388],
        ...,
        [0.9947],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371691.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8331, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [ 0.0099, -0.0006,  0.0061,  ...,  0.0063, -0.0010,  0.0221],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3516.6245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0187, device='cuda:0')



h[100].sum tensor(-8.7480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6934, device='cuda:0')



h[200].sum tensor(103.1650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5484, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0067],
        [0.0130, 0.0000, 0.0081,  ..., 0.0117, 0.0000, 0.0354],
        [0.0140, 0.0000, 0.0088,  ..., 0.0128, 0.0000, 0.0397],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73926.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1122, 0.0848, 0.0202,  ..., 0.0939, 0.0765, 0.0932],
        [0.1095, 0.0343, 0.0498,  ..., 0.1009, 0.0320, 0.1388],
        [0.1087, 0.0151, 0.0680,  ..., 0.1040, 0.0137, 0.1645],
        ...,
        [0.1180, 0.1443, 0.0013,  ..., 0.0933, 0.1243, 0.0660],
        [0.1179, 0.1442, 0.0013,  ..., 0.0932, 0.1242, 0.0660],
        [0.1179, 0.1442, 0.0013,  ..., 0.0932, 0.1241, 0.0660]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711902., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12821.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(925.0541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(469.1338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.4156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9449],
        [-0.1610],
        [ 0.2730],
        ...,
        [-4.2442],
        [-4.1847],
        [-4.0066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302307.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0347],
        [1.0317],
        [1.0388],
        ...,
        [0.9947],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371691.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3785, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0347],
        [1.0317],
        [1.0388],
        ...,
        [0.9947],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371696.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3785, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028, -0.0002,  0.0018,  ...,  0.0030, -0.0004,  0.0092],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3316.7446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0167, device='cuda:0')



h[100].sum tensor(-7.7657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6184, device='cuda:0')



h[200].sum tensor(102.1179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0000, 0.0033,  ..., 0.0079, 0.0000, 0.0208],
        [0.0051, 0.0000, 0.0033,  ..., 0.0080, 0.0000, 0.0209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71267.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1153, 0.0857, 0.0239,  ..., 0.0903, 0.0742, 0.0989],
        [0.1153, 0.0964, 0.0191,  ..., 0.0907, 0.0831, 0.0926],
        [0.1144, 0.1263, 0.0066,  ..., 0.0906, 0.1079, 0.0735],
        ...,
        [0.1179, 0.1444, 0.0014,  ..., 0.0932, 0.1235, 0.0664],
        [0.1178, 0.1443, 0.0014,  ..., 0.0931, 0.1234, 0.0663],
        [0.1177, 0.1443, 0.0014,  ..., 0.0931, 0.1234, 0.0663]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699070.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12821.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(916.0148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.8922, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.7817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0853],
        [-2.5861],
        [-3.0271],
        ...,
        [-4.2459],
        [-4.2385],
        [-4.2379]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283277.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0347],
        [1.0317],
        [1.0388],
        ...,
        [0.9947],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371696.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.0815, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0316],
        [1.0388],
        ...,
        [0.9947],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371701.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.0815, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0003,  0.0031,  ...,  0.0040, -0.0005,  0.0132],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0042, -0.0003,  0.0026,  ...,  0.0036, -0.0005,  0.0116],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3037.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0139, device='cuda:0')



h[100].sum tensor(-6.4408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5138, device='cuda:0')



h[200].sum tensor(99.8437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0333, 0.0000, 0.0205,  ..., 0.0217, 0.0000, 0.0753],
        [0.0364, 0.0000, 0.0224,  ..., 0.0237, 0.0000, 0.0833],
        [0.0217, 0.0000, 0.0134,  ..., 0.0163, 0.0000, 0.0540],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68472.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0945, 0.0000, 0.1214,  ..., 0.1197, 0.0000, 0.2469],
        [0.0985, 0.0000, 0.1176,  ..., 0.1169, 0.0000, 0.2386],
        [0.1041, 0.0000, 0.0963,  ..., 0.1094, 0.0000, 0.2049],
        ...,
        [0.1174, 0.1444, 0.0017,  ..., 0.0929, 0.1229, 0.0667],
        [0.1173, 0.1443, 0.0017,  ..., 0.0928, 0.1228, 0.0666],
        [0.1173, 0.1443, 0.0017,  ..., 0.0928, 0.1227, 0.0666]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691767.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12614.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(891.4171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(434.8095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.8059, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6171],
        [ 0.6682],
        [ 0.7014],
        ...,
        [-4.2267],
        [-4.2194],
        [-4.2188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315320.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0316],
        [1.0388],
        ...,
        [0.9947],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371701.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.2899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0316],
        [1.0388],
        ...,
        [0.9946],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371705.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.2899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053, -0.0004,  0.0033,  ...,  0.0041, -0.0006,  0.0137],
        [ 0.0101, -0.0006,  0.0062,  ...,  0.0064, -0.0010,  0.0226],
        [ 0.0274, -0.0016,  0.0167,  ...,  0.0144, -0.0025,  0.0544],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3546.6357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0187, device='cuda:0')



h[100].sum tensor(-8.6388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6919, device='cuda:0')



h[200].sum tensor(105.4350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0000, 0.0116,  ..., 0.0143, 0.0000, 0.0462],
        [0.0557, 0.0000, 0.0341,  ..., 0.0328, 0.0000, 0.1191],
        [0.0560, 0.0000, 0.0343,  ..., 0.0329, 0.0000, 0.1197],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74640.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0988, 0.0198, 0.0823,  ..., 0.1108, 0.0171, 0.1888],
        [0.0873, 0.0000, 0.1366,  ..., 0.1267, 0.0000, 0.2731],
        [0.0836, 0.0000, 0.1531,  ..., 0.1314, 0.0000, 0.2989],
        ...,
        [0.1172, 0.1446, 0.0019,  ..., 0.0927, 0.1224, 0.0671],
        [0.1171, 0.1444, 0.0018,  ..., 0.0926, 0.1223, 0.0670],
        [0.1171, 0.1444, 0.0018,  ..., 0.0926, 0.1222, 0.0670]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713355., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12619.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(923.2350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(464.7058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(341.6840, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5139],
        [ 0.5370],
        [ 0.5248],
        ...,
        [-4.2143],
        [-4.2072],
        [-4.2066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251315.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0316],
        [1.0388],
        ...,
        [0.9946],
        [0.9934],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371705.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2703],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.9098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0316],
        [1.0389],
        ...,
        [0.9946],
        [0.9934],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371710.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2703],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.9098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0043, -0.0003,  0.0027,  ...,  0.0036, -0.0005,  0.0119],
        [ 0.0034, -0.0003,  0.0021,  ...,  0.0032, -0.0004,  0.0102],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0067, -0.0004,  0.0041,  ...,  0.0047, -0.0007,  0.0163]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2871.7803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.0300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0122, device='cuda:0')



h[100].sum tensor(-5.6196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4505, device='cuda:0')



h[200].sum tensor(98.8154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0028,  ..., 0.0069, 0.0000, 0.0174],
        [0.0151, 0.0000, 0.0094,  ..., 0.0125, 0.0000, 0.0395],
        [0.0332, 0.0000, 0.0205,  ..., 0.0222, 0.0000, 0.0775],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0071],
        [0.0071, 0.0000, 0.0044,  ..., 0.0083, 0.0000, 0.0227],
        [0.0127, 0.0000, 0.0079,  ..., 0.0116, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65437.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1106, 0.0608, 0.0316,  ..., 0.0943, 0.0532, 0.1103],
        [0.1080, 0.0246, 0.0583,  ..., 0.1003, 0.0228, 0.1493],
        [0.1052, 0.0000, 0.0823,  ..., 0.1056, 0.0000, 0.1838],
        ...,
        [0.1161, 0.0902, 0.0210,  ..., 0.0959, 0.0789, 0.0967],
        [0.1134, 0.0590, 0.0335,  ..., 0.0994, 0.0533, 0.1155],
        [0.1102, 0.0364, 0.0527,  ..., 0.1043, 0.0329, 0.1441]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676363.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12608.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(881.5132, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.4328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.0602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9720],
        [-0.2020],
        [ 0.1834],
        ...,
        [-1.9896],
        [-1.7593],
        [-1.2529]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293222.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0316],
        [1.0389],
        ...,
        [0.9946],
        [0.9934],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371710.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6494],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.3320, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0316],
        [1.0389],
        ...,
        [0.9946],
        [0.9933],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371714.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6494],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.3320, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0122, -0.0007,  0.0075,  ...,  0.0073, -0.0012,  0.0264],
        [ 0.0142, -0.0008,  0.0087,  ...,  0.0082, -0.0013,  0.0301],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3823.8286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0212, device='cuda:0')



h[100].sum tensor(-9.8145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7849, device='cuda:0')



h[200].sum tensor(108.6046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2045, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0125, 0.0000, 0.0077,  ..., 0.0107, 0.0000, 0.0323],
        [0.0330, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0725],
        [0.1026, 0.0000, 0.0626,  ..., 0.0547, 0.0000, 0.2056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79606.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0957, 0.0031, 0.0707,  ..., 0.1116, 0.0050, 0.1770],
        [0.0790, 0.0000, 0.1317,  ..., 0.1320, 0.0000, 0.2752],
        [0.0455, 0.0000, 0.2449,  ..., 0.1697, 0.0000, 0.4587],
        ...,
        [0.1172, 0.1452, 0.0017,  ..., 0.0927, 0.1222, 0.0675],
        [0.1171, 0.1451, 0.0017,  ..., 0.0926, 0.1221, 0.0674],
        [0.1171, 0.1451, 0.0017,  ..., 0.0926, 0.1221, 0.0674]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(741231., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12551.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(948.5215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(495.4106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(370.8541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4974],
        [ 0.3855],
        [ 0.2583],
        ...,
        [-4.2167],
        [-4.2002],
        [-4.1902]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249256.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0316],
        [1.0389],
        ...,
        [0.9946],
        [0.9933],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371714.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.7964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0316],
        [1.0389],
        ...,
        [0.9946],
        [0.9933],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371718.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.7964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0108, -0.0007,  0.0067,  ...,  0.0067, -0.0010,  0.0239],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3444.2957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.0258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-8.1354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6523, device='cuda:0')



h[200].sum tensor(104.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0153, 0.0000, 0.0095,  ..., 0.0132, 0.0000, 0.0420],
        [0.0138, 0.0000, 0.0086,  ..., 0.0119, 0.0000, 0.0369],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74099.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1059, 0.0134, 0.0691,  ..., 0.1029, 0.0118, 0.1665],
        [0.1078, 0.0379, 0.0503,  ..., 0.0999, 0.0332, 0.1401],
        [0.1120, 0.0915, 0.0191,  ..., 0.0927, 0.0785, 0.0925],
        ...,
        [0.1173, 0.1458, 0.0015,  ..., 0.0928, 0.1224, 0.0675],
        [0.1172, 0.1456, 0.0015,  ..., 0.0927, 0.1223, 0.0674],
        [0.1172, 0.1456, 0.0015,  ..., 0.0927, 0.1222, 0.0674]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715913.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12555.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(922.5745, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(467.5930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(343.5993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1329],
        [-0.4825],
        [-1.4063],
        ...,
        [-4.2492],
        [-4.2419],
        [-4.2414]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293159.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0316],
        [1.0389],
        ...,
        [0.9946],
        [0.9933],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371718.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3124, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0316],
        [1.0389],
        ...,
        [0.9945],
        [0.9933],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371722.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3124, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0124, -0.0007,  0.0076,  ...,  0.0074, -0.0012,  0.0268],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0113, -0.0007,  0.0070,  ...,  0.0069, -0.0011,  0.0247],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3476.1221, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.5986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0179, device='cuda:0')



h[100].sum tensor(-8.2680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6619, device='cuda:0')



h[200].sum tensor(105.3301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9789, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0482, 0.0000, 0.0295,  ..., 0.0286, 0.0000, 0.1026],
        [0.0442, 0.0000, 0.0272,  ..., 0.0274, 0.0000, 0.0977],
        [0.0211, 0.0000, 0.0130,  ..., 0.0154, 0.0000, 0.0503],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75770.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0758, 0.0000, 0.1651,  ..., 0.1388, 0.0000, 0.3224],
        [0.0860, 0.0000, 0.1397,  ..., 0.1288, 0.0000, 0.2793],
        [0.0970, 0.0000, 0.0937,  ..., 0.1148, 0.0000, 0.2076],
        ...,
        [0.1175, 0.1462, 0.0013,  ..., 0.0929, 0.1225, 0.0675],
        [0.1174, 0.1461, 0.0013,  ..., 0.0928, 0.1224, 0.0675],
        [0.1174, 0.1461, 0.0013,  ..., 0.0928, 0.1224, 0.0674]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(733100.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12547.4814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(931.8822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(479.9616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(353.1811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4625],
        [ 0.5194],
        [ 0.5354],
        ...,
        [-4.2676],
        [-4.2603],
        [-4.2598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294034.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0316],
        [1.0389],
        ...,
        [0.9945],
        [0.9933],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371722.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8364],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8113, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0390],
        ...,
        [0.9945],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371726.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8364],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8113, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0160, -0.0009,  0.0098,  ...,  0.0091, -0.0015,  0.0334],
        [ 0.0161, -0.0009,  0.0099,  ...,  0.0091, -0.0015,  0.0335],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3872.6184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0216, device='cuda:0')



h[100].sum tensor(-9.9916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7998, device='cuda:0')



h[200].sum tensor(109.5110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0482, 0.0000, 0.0296,  ..., 0.0286, 0.0000, 0.1026],
        [0.0750, 0.0000, 0.0458,  ..., 0.0412, 0.0000, 0.1521],
        [0.1209, 0.0000, 0.0738,  ..., 0.0633, 0.0000, 0.2391],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81241.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0697, 0.0000, 0.1740,  ..., 0.1433, 0.0000, 0.3406],
        [0.0464, 0.0000, 0.2423,  ..., 0.1688, 0.0000, 0.4548],
        [0.0263, 0.0000, 0.3043,  ..., 0.1904, 0.0000, 0.5567],
        ...,
        [0.1175, 0.1466, 0.0011,  ..., 0.0930, 0.1226, 0.0676],
        [0.1174, 0.1464, 0.0011,  ..., 0.0929, 0.1225, 0.0676],
        [0.1174, 0.1464, 0.0011,  ..., 0.0929, 0.1225, 0.0676]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(755402.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12514.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(959.4380, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(512.9957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(382.8256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3440],
        [ 0.2457],
        [ 0.1836],
        ...,
        [-4.2800],
        [-4.2726],
        [-4.2721]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284162.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0390],
        ...,
        [0.9945],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371726.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3784],
        [0.4558],
        [0.3884],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0331, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371730.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3784],
        [0.4558],
        [0.3884],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0331, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0302, -0.0017,  0.0184,  ...,  0.0158, -0.0027,  0.0596],
        [ 0.0209, -0.0012,  0.0128,  ...,  0.0114, -0.0019,  0.0425],
        [ 0.0220, -0.0012,  0.0135,  ...,  0.0119, -0.0020,  0.0445],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3674.2080, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.7356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0196, device='cuda:0')



h[100].sum tensor(-9.1118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7267, device='cuda:0')



h[200].sum tensor(107.4212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1071, 0.0000, 0.0654,  ..., 0.0569, 0.0000, 0.2137],
        [0.1120, 0.0000, 0.0684,  ..., 0.0592, 0.0000, 0.2228],
        [0.0973, 0.0000, 0.0595,  ..., 0.0523, 0.0000, 0.1956],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75197.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0312, 0.0000, 0.2895,  ..., 0.1840, 0.0000, 0.5303],
        [0.0328, 0.0000, 0.2870,  ..., 0.1832, 0.0000, 0.5261],
        [0.0414, 0.0000, 0.2663,  ..., 0.1750, 0.0000, 0.4902],
        ...,
        [0.1175, 0.1470, 0.0011,  ..., 0.0929, 0.1227, 0.0678],
        [0.1174, 0.1468, 0.0010,  ..., 0.0928, 0.1226, 0.0677],
        [0.1174, 0.1468, 0.0010,  ..., 0.0928, 0.1226, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713274.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12561.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(931.9995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(478.8674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(351.3040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2667],
        [ 0.2562],
        [ 0.2521],
        ...,
        [-4.2919],
        [-4.2845],
        [-4.2840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320169., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371730.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.7552, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371730.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.7552, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        ...,
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0012,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3187.1968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0152, device='cuda:0')



h[100].sum tensor(-6.9822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5620, device='cuda:0')



h[200].sum tensor(102.4618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0066],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69192.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1096, 0.0955, 0.0137,  ..., 0.0943, 0.0827, 0.0878],
        [0.1130, 0.1366, 0.0013,  ..., 0.0909, 0.1140, 0.0688],
        [0.1135, 0.1434, 0.0008,  ..., 0.0903, 0.1193, 0.0655],
        ...,
        [0.1175, 0.1470, 0.0011,  ..., 0.0929, 0.1227, 0.0678],
        [0.1174, 0.1468, 0.0010,  ..., 0.0928, 0.1226, 0.0677],
        [0.1174, 0.1468, 0.0010,  ..., 0.0928, 0.1226, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689897., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12709.7764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(915.0839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(436.4721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(322.6293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2805],
        [-3.2629],
        [-3.8822],
        ...,
        [-4.2919],
        [-4.2844],
        [-4.2840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281288.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371730.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.6167],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371733.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.6167],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0115, -0.0007,  0.0071,  ...,  0.0070, -0.0011,  0.0251],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3058.3513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.3421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0139, device='cuda:0')



h[100].sum tensor(-6.4199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5155, device='cuda:0')



h[200].sum tensor(100.7571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0067],
        ...,
        [0.0442, 0.0000, 0.0272,  ..., 0.0277, 0.0000, 0.0983],
        [0.0098, 0.0000, 0.0061,  ..., 0.0097, 0.0000, 0.0274],
        [0.0123, 0.0000, 0.0076,  ..., 0.0108, 0.0000, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68418.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1129, 0.1435, 0.0006,  ..., 0.0899, 0.1191, 0.0653],
        [0.1134, 0.1433, 0.0011,  ..., 0.0902, 0.1189, 0.0661],
        [0.1135, 0.1401, 0.0026,  ..., 0.0902, 0.1164, 0.0680],
        ...,
        [0.1044, 0.0000, 0.0681,  ..., 0.1106, 0.0000, 0.1712],
        [0.1099, 0.0416, 0.0392,  ..., 0.1030, 0.0386, 0.1281],
        [0.1113, 0.0654, 0.0299,  ..., 0.1008, 0.0581, 0.1146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689743.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12615.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(905.8852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(435.8170, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.1381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4751],
        [-3.2314],
        [-2.8785],
        ...,
        [-1.9561],
        [-2.2641],
        [-2.8268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307865.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9932],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371733.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4348],
        [0.5127],
        [0.4358],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.8394, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9931],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371737.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4348],
        [0.5127],
        [0.4358],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.8394, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0262, -0.0015,  0.0160,  ...,  0.0139, -0.0023,  0.0523],
        [ 0.0243, -0.0014,  0.0149,  ...,  0.0130, -0.0021,  0.0488],
        [ 0.0256, -0.0014,  0.0156,  ...,  0.0136, -0.0023,  0.0512],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3926.4978, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0220, device='cuda:0')



h[100].sum tensor(-10.2009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8136, device='cuda:0')



h[200].sum tensor(109.2698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1088, 0.0000, 0.0664,  ..., 0.0577, 0.0000, 0.2169],
        [0.1183, 0.0000, 0.0722,  ..., 0.0622, 0.0000, 0.2346],
        [0.0899, 0.0000, 0.0550,  ..., 0.0489, 0.0000, 0.1823],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82033.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0202, 0.0000, 0.3078,  ..., 0.1904, 0.0000, 0.5593],
        [0.0194, 0.0000, 0.3115,  ..., 0.1920, 0.0000, 0.5655],
        [0.0345, 0.0000, 0.2751,  ..., 0.1776, 0.0000, 0.5033],
        ...,
        [0.1174, 0.1478, 0.0010,  ..., 0.0929, 0.1232, 0.0680],
        [0.1173, 0.1476, 0.0010,  ..., 0.0928, 0.1231, 0.0679],
        [0.1173, 0.1476, 0.0010,  ..., 0.0928, 0.1231, 0.0679]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(758312.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12428.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(960.8536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(520.8094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(382.7771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1603],
        [ 0.1751],
        [ 0.2015],
        ...,
        [-4.3047],
        [-4.2942],
        [-4.2914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294110.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9931],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371737.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.6227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371741.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.6227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0212, -0.0012,  0.0130,  ...,  0.0116, -0.0019,  0.0432],
        [ 0.0216, -0.0012,  0.0132,  ...,  0.0118, -0.0019,  0.0437],
        [ 0.0155, -0.0009,  0.0095,  ...,  0.0089, -0.0014,  0.0326],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3447.4509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0176, device='cuda:0')



h[100].sum tensor(-8.1259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6518, device='cuda:0')



h[200].sum tensor(103.8214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7965, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0586, 0.0000, 0.0360,  ..., 0.0343, 0.0000, 0.1245],
        [0.0643, 0.0000, 0.0395,  ..., 0.0370, 0.0000, 0.1351],
        [0.0435, 0.0000, 0.0267,  ..., 0.0266, 0.0000, 0.0942],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72881.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0758, 0.0000, 0.1539,  ..., 0.1349, 0.0000, 0.3043],
        [0.0740, 0.0000, 0.1630,  ..., 0.1378, 0.0000, 0.3186],
        [0.0834, 0.0000, 0.1310,  ..., 0.1271, 0.0000, 0.2682],
        ...,
        [0.1175, 0.1482, 0.0009,  ..., 0.0931, 0.1235, 0.0681],
        [0.1174, 0.1481, 0.0009,  ..., 0.0930, 0.1234, 0.0681],
        [0.1173, 0.1481, 0.0009,  ..., 0.0930, 0.1234, 0.0681]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706240.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12519.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(921.0804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(468.9289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(335.5399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5947],
        [ 0.6171],
        [ 0.5802],
        ...,
        [-4.3304],
        [-4.3228],
        [-4.3223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325912.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371741.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.3585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371744.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.3585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4064.1426, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0233, device='cuda:0')



h[100].sum tensor(-10.8009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.8614, device='cuda:0')



h[200].sum tensor(109.7104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.5900, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0000, 0.0037,  ..., 0.0078, 0.0000, 0.0201],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83331.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1115, 0.0977, 0.0155,  ..., 0.0933, 0.0826, 0.0908],
        [0.1130, 0.1306, 0.0044,  ..., 0.0915, 0.1087, 0.0738],
        [0.1134, 0.1417, 0.0005,  ..., 0.0908, 0.1176, 0.0678],
        ...,
        [0.1176, 0.1487, 0.0008,  ..., 0.0932, 0.1237, 0.0683],
        [0.1174, 0.1486, 0.0008,  ..., 0.0931, 0.1236, 0.0683],
        [0.1174, 0.1486, 0.0008,  ..., 0.0931, 0.1236, 0.0683]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(761712., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12465.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(968.7128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(529.1066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(386.9716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7760],
        [-3.4778],
        [-3.9536],
        ...,
        [-4.3424],
        [-4.3346],
        [-4.3341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288025.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9944],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371744.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9943],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371748.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062, -0.0004,  0.0039,  ...,  0.0045, -0.0006,  0.0154],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [ 0.0062, -0.0004,  0.0039,  ...,  0.0045, -0.0006,  0.0154],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3272.9644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-7.3378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5950, device='cuda:0')



h[200].sum tensor(101.7875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0000, 0.0031,  ..., 0.0073, 0.0000, 0.0184],
        [0.0228, 0.0000, 0.0143,  ..., 0.0175, 0.0000, 0.0586],
        [0.0050, 0.0000, 0.0031,  ..., 0.0073, 0.0000, 0.0184],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71906.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1109, 0.0790, 0.0229,  ..., 0.0942, 0.0671, 0.1014],
        [0.1094, 0.0340, 0.0397,  ..., 0.0980, 0.0321, 0.1252],
        [0.1112, 0.0809, 0.0219,  ..., 0.0949, 0.0693, 0.1002],
        ...,
        [0.1175, 0.1489, 0.0008,  ..., 0.0934, 0.1236, 0.0685],
        [0.1174, 0.1487, 0.0008,  ..., 0.0933, 0.1235, 0.0684],
        [0.1173, 0.1487, 0.0008,  ..., 0.0933, 0.1235, 0.0684]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708284.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12483.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(915.8970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(462.9244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(329.3739, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0783],
        [-2.3011],
        [-2.7731],
        ...,
        [-4.3456],
        [-4.3379],
        [-4.3374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327572.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9943],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371748.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.2207, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9943],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371748.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.2207, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3282.5977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0162, device='cuda:0')



h[100].sum tensor(-7.3795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5988, device='cuda:0')



h[200].sum tensor(101.8854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0068],
        [0.0025, 0.0000, 0.0016,  ..., 0.0062, 0.0000, 0.0138],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71032.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1131, 0.1377, 0.0022,  ..., 0.0905, 0.1138, 0.0704],
        [0.1132, 0.1159, 0.0115,  ..., 0.0918, 0.0962, 0.0834],
        [0.1127, 0.0753, 0.0270,  ..., 0.0934, 0.0636, 0.1065],
        ...,
        [0.1175, 0.1489, 0.0008,  ..., 0.0934, 0.1236, 0.0685],
        [0.1174, 0.1487, 0.0008,  ..., 0.0933, 0.1235, 0.0684],
        [0.1173, 0.1487, 0.0008,  ..., 0.0933, 0.1235, 0.0684]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701882.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12554.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(917.1876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(453.9012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(325.6745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0281],
        [-2.2347],
        [-1.3111],
        ...,
        [-4.3456],
        [-4.3379],
        [-4.3374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305819.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0316],
        [1.0390],
        ...,
        [0.9943],
        [0.9931],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371748.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(560.0065, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0315],
        [1.0390],
        ...,
        [0.9943],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371752.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(560.0065, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [ 0.0083, -0.0005,  0.0052,  ...,  0.0055, -0.0008,  0.0193],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5972.4360, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.3485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0413, device='cuda:0')



h[100].sum tensor(-18.9356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.5297, device='cuda:0')



h[200].sum tensor(129.6631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.6844, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0307, 0.0000, 0.0191,  ..., 0.0212, 0.0000, 0.0732],
        [0.0068, 0.0000, 0.0043,  ..., 0.0081, 0.0000, 0.0218],
        [0.0086, 0.0000, 0.0053,  ..., 0.0089, 0.0000, 0.0250],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(120390.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1049, 0.0027, 0.0483,  ..., 0.1016, 0.0068, 0.1387],
        [0.1087, 0.0620, 0.0273,  ..., 0.0973, 0.0539, 0.1089],
        [0.1096, 0.0789, 0.0206,  ..., 0.0960, 0.0684, 0.0994],
        ...,
        [0.1173, 0.1488, 0.0009,  ..., 0.0936, 0.1233, 0.0685],
        [0.1172, 0.1486, 0.0009,  ..., 0.0935, 0.1232, 0.0685],
        [0.1172, 0.1486, 0.0009,  ..., 0.0935, 0.1232, 0.0685]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(992203.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11755.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1105.8235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(764.4976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(570.0516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0128],
        [-2.5602],
        [-3.1395],
        ...,
        [-4.3454],
        [-4.3377],
        [-4.3373]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277889.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0315],
        [1.0390],
        ...,
        [0.9943],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371752.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.2150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0315],
        [1.0390],
        ...,
        [0.9943],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371757.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.2150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3381.9087, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.4848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0169, device='cuda:0')



h[100].sum tensor(-7.6793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6261, device='cuda:0')



h[200].sum tensor(104.0312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72535.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1124, 0.1445, 0.0007,  ..., 0.0907, 0.1189, 0.0660],
        [0.1128, 0.1450, 0.0007,  ..., 0.0911, 0.1194, 0.0663],
        [0.1129, 0.1448, 0.0009,  ..., 0.0910, 0.1193, 0.0663],
        ...,
        [0.1169, 0.1484, 0.0012,  ..., 0.0937, 0.1227, 0.0687],
        [0.1168, 0.1483, 0.0012,  ..., 0.0936, 0.1226, 0.0686],
        [0.1168, 0.1483, 0.0012,  ..., 0.0936, 0.1226, 0.0686]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709735.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12372.8809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(917.8204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(462.7594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.1581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1743],
        [-4.1576],
        [-4.1114],
        ...,
        [-4.3309],
        [-4.3233],
        [-4.3230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301230.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0315],
        [1.0390],
        ...,
        [0.9943],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371757.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.1172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0315],
        [1.0390],
        ...,
        [0.9943],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371761.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.1172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042, -0.0003,  0.0026,  ...,  0.0035, -0.0004,  0.0117],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3592.3394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0188, device='cuda:0')



h[100].sum tensor(-8.5119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6969, device='cuda:0')



h[200].sum tensor(106.9424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6119, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0000, 0.0097,  ..., 0.0127, 0.0000, 0.0405],
        [0.0043, 0.0000, 0.0027,  ..., 0.0068, 0.0000, 0.0173],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74287.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1005, 0.0220, 0.0642,  ..., 0.1044, 0.0202, 0.1607],
        [0.1073, 0.0614, 0.0324,  ..., 0.0976, 0.0528, 0.1144],
        [0.1110, 0.1158, 0.0100,  ..., 0.0931, 0.0963, 0.0812],
        ...,
        [0.1166, 0.1482, 0.0012,  ..., 0.0938, 0.1222, 0.0688],
        [0.1165, 0.1480, 0.0012,  ..., 0.0937, 0.1220, 0.0687],
        [0.1164, 0.1480, 0.0012,  ..., 0.0937, 0.1220, 0.0687]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712656., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12268.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(924.2024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(473.5412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(343.3368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2815],
        [-1.3133],
        [-2.4032],
        ...,
        [-4.3232],
        [-4.3158],
        [-4.3154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302525.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0315],
        [1.0390],
        ...,
        [0.9943],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371761.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 780.0 event: 3900 loss: tensor(838.4943, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0314],
        [1.0390],
        ...,
        [0.9942],
        [0.9930],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371766.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2879.6880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0119, device='cuda:0')



h[100].sum tensor(-5.3862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4421, device='cuda:0')



h[200].sum tensor(100.7491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0000, 0.0029,  ..., 0.0068, 0.0000, 0.0176],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65115.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1038, 0.0445, 0.0428,  ..., 0.1001, 0.0374, 0.1293],
        [0.1089, 0.0890, 0.0194,  ..., 0.0952, 0.0750, 0.0947],
        [0.1117, 0.1252, 0.0077,  ..., 0.0922, 0.1030, 0.0769],
        ...,
        [0.1163, 0.1480, 0.0012,  ..., 0.0939, 0.1216, 0.0688],
        [0.1162, 0.1479, 0.0012,  ..., 0.0938, 0.1215, 0.0687],
        [0.1162, 0.1479, 0.0012,  ..., 0.0938, 0.1215, 0.0687]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673197.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12314.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(890.5738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(415.7200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.8674, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1751],
        [-0.9385],
        [-1.7282],
        ...,
        [-4.3205],
        [-4.3131],
        [-4.3127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303179.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0314],
        [1.0390],
        ...,
        [0.9942],
        [0.9930],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371766.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8296],
        [0.8379],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.0504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0314],
        [1.0390],
        ...,
        [0.9942],
        [0.9930],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371770.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8296],
        [0.8379],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.0504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0160, -0.0009,  0.0099,  ...,  0.0091, -0.0014,  0.0336],
        [ 0.0304, -0.0016,  0.0186,  ...,  0.0158, -0.0026,  0.0600],
        [ 0.0447, -0.0023,  0.0273,  ...,  0.0225, -0.0037,  0.0864],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3141.2053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-6.4417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5246, device='cuda:0')



h[200].sum tensor(104.2831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1010, 0.0000, 0.0619,  ..., 0.0538, 0.0000, 0.2027],
        [0.1287, 0.0000, 0.0787,  ..., 0.0668, 0.0000, 0.2539],
        [0.0812, 0.0000, 0.0498,  ..., 0.0446, 0.0000, 0.1662],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69334.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0367, 0.0000, 0.2182,  ..., 0.1590, 0.0000, 0.4091],
        [0.0222, 0.0000, 0.2539,  ..., 0.1720, 0.0000, 0.4681],
        [0.0388, 0.0000, 0.2160,  ..., 0.1581, 0.0000, 0.4051],
        ...,
        [0.1160, 0.1478, 0.0013,  ..., 0.0940, 0.1211, 0.0688],
        [0.1159, 0.1477, 0.0013,  ..., 0.0939, 0.1210, 0.0688],
        [0.1159, 0.1477, 0.0013,  ..., 0.0939, 0.1209, 0.0688]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695902.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12255.5732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(916.4648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(435.5940, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(325.3097, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4609],
        [ 0.3978],
        [ 0.2761],
        ...,
        [-4.3157],
        [-4.3083],
        [-4.3079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261061.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0314],
        [1.0390],
        ...,
        [0.9942],
        [0.9930],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371770.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.4963, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371774.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.4963, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0012,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3460.5200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0172, device='cuda:0')



h[100].sum tensor(-7.7494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6351, device='cuda:0')



h[200].sum tensor(108.1450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73168.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1113, 0.1411, 0.0008,  ..., 0.0912, 0.1148, 0.0677],
        [0.1119, 0.1444, 0.0008,  ..., 0.0914, 0.1174, 0.0665],
        [0.1119, 0.1441, 0.0010,  ..., 0.0913, 0.1173, 0.0666],
        ...,
        [0.1159, 0.1477, 0.0012,  ..., 0.0940, 0.1207, 0.0689],
        [0.1158, 0.1476, 0.0012,  ..., 0.0939, 0.1206, 0.0688],
        [0.1157, 0.1475, 0.0012,  ..., 0.0939, 0.1205, 0.0688]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709424.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12085.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(927.8951, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(463.7088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(345.2085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5302],
        [-3.8613],
        [-3.9871],
        ...,
        [-4.3160],
        [-4.3086],
        [-4.3083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286256.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371774.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8271],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7954, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371779.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8271],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7954, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [ 0.0158, -0.0009,  0.0097,  ...,  0.0090, -0.0014,  0.0332],
        [ 0.0091, -0.0005,  0.0057,  ...,  0.0058, -0.0008,  0.0208],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3634.7712, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0189, device='cuda:0')



h[100].sum tensor(-8.4445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6987, device='cuda:0')



h[200].sum tensor(109.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0163, 0.0000, 0.0100,  ..., 0.0123, 0.0000, 0.0394],
        [0.0227, 0.0000, 0.0141,  ..., 0.0159, 0.0000, 0.0536],
        [0.0735, 0.0000, 0.0452,  ..., 0.0410, 0.0000, 0.1523],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75381.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0969, 0.0428, 0.0468,  ..., 0.1036, 0.0360, 0.1393],
        [0.0869, 0.0117, 0.0813,  ..., 0.1135, 0.0103, 0.1911],
        [0.0671, 0.0000, 0.1406,  ..., 0.1310, 0.0000, 0.2818],
        ...,
        [0.1157, 0.1476, 0.0013,  ..., 0.0940, 0.1204, 0.0691],
        [0.1156, 0.1474, 0.0013,  ..., 0.0939, 0.1203, 0.0690],
        [0.1156, 0.1474, 0.0013,  ..., 0.0939, 0.1203, 0.0690]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(720436.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12011.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(938.7244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(475.8359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(355.9090, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8764],
        [-0.0396],
        [ 0.4455],
        ...,
        [-4.3138],
        [-4.3064],
        [-4.3062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262118., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371779.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(174.8684, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371779.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(174.8684, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3019.4487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0129, device='cuda:0')



h[100].sum tensor(-5.8244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4777, device='cuda:0')



h[200].sum tensor(103.7517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66798.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1112, 0.1437, 0.0009,  ..., 0.0910, 0.1168, 0.0664],
        [0.1115, 0.1410, 0.0022,  ..., 0.0916, 0.1146, 0.0685],
        [0.1112, 0.1325, 0.0057,  ..., 0.0919, 0.1080, 0.0730],
        ...,
        [0.1157, 0.1476, 0.0013,  ..., 0.0940, 0.1204, 0.0691],
        [0.1156, 0.1474, 0.0013,  ..., 0.0939, 0.1203, 0.0690],
        [0.1156, 0.1474, 0.0013,  ..., 0.0939, 0.1203, 0.0690]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679428.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12127.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(900.6941, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(424.5093, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(311.7271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5297],
        [-3.1682],
        [-2.6782],
        ...,
        [-4.3138],
        [-4.3064],
        [-4.3062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293567.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371779.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(352.7565, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371779.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(352.7565, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4420.8115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0260, device='cuda:0')



h[100].sum tensor(-11.7915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.9636, device='cuda:0')



h[200].sum tensor(117.9499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.4388, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(90089.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1112, 0.1378, 0.0025,  ..., 0.0909, 0.1117, 0.0701],
        [0.1117, 0.1434, 0.0014,  ..., 0.0913, 0.1164, 0.0674],
        [0.1118, 0.1440, 0.0010,  ..., 0.0913, 0.1171, 0.0667],
        ...,
        [0.1157, 0.1476, 0.0013,  ..., 0.0940, 0.1204, 0.0691],
        [0.1156, 0.1474, 0.0013,  ..., 0.0939, 0.1203, 0.0690],
        [0.1156, 0.1474, 0.0013,  ..., 0.0939, 0.1203, 0.0690]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(811148.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11795.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(997.8421, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(566.4836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(429.1585, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7686],
        [-3.3649],
        [-3.7379],
        ...,
        [-4.3098],
        [-4.3062],
        [-4.3062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253712.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0314],
        [1.0391],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371779.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2554],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.1156, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0315],
        [1.0392],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371782.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2554],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.1156, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [ 0.0040, -0.0003,  0.0026,  ...,  0.0035, -0.0004,  0.0114],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3139.7644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0140, device='cuda:0')



h[100].sum tensor(-6.3241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5166, device='cuda:0')



h[200].sum tensor(104.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3491, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0000, 0.0094,  ..., 0.0134, 0.0000, 0.0436],
        [0.0032, 0.0000, 0.0021,  ..., 0.0062, 0.0000, 0.0154],
        [0.0041, 0.0000, 0.0026,  ..., 0.0066, 0.0000, 0.0171],
        ...,
        [0.0135, 0.0000, 0.0085,  ..., 0.0118, 0.0000, 0.0373],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70760.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1083, 0.0686, 0.0307,  ..., 0.0935, 0.0572, 0.1087],
        [0.1100, 0.1004, 0.0174,  ..., 0.0928, 0.0825, 0.0916],
        [0.1102, 0.1096, 0.0135,  ..., 0.0926, 0.0900, 0.0860],
        ...,
        [0.1019, 0.0350, 0.0592,  ..., 0.1061, 0.0305, 0.1539],
        [0.1108, 0.0891, 0.0207,  ..., 0.0979, 0.0758, 0.0979],
        [0.1147, 0.1366, 0.0045,  ..., 0.0944, 0.1122, 0.0748]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(705898.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12006.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(918.3228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.2985, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.5488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3299],
        [-3.3934],
        [-3.4816],
        ...,
        [-0.7988],
        [-2.0935],
        [-3.2051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259040.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0315],
        [1.0392],
        ...,
        [0.9942],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371782.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.0566, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0315],
        [1.0392],
        ...,
        [0.9941],
        [0.9929],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371786.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.0566, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0138, -0.0008,  0.0085,  ...,  0.0081, -0.0012,  0.0295],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [ 0.0044, -0.0003,  0.0028,  ...,  0.0037, -0.0005,  0.0122],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3327.5322, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0161, device='cuda:0')



h[100].sum tensor(-7.1351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5956, device='cuda:0')



h[200].sum tensor(105.0193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7798, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0000, 0.0162,  ..., 0.0177, 0.0000, 0.0605],
        [0.0188, 0.0000, 0.0117,  ..., 0.0142, 0.0000, 0.0469],
        [0.0374, 0.0000, 0.0230,  ..., 0.0230, 0.0000, 0.0812],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70631.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0692, 0.0000, 0.1219,  ..., 0.1262, 0.0000, 0.2519],
        [0.0725, 0.0000, 0.1178,  ..., 0.1243, 0.0000, 0.2448],
        [0.0591, 0.0000, 0.1513,  ..., 0.1352, 0.0000, 0.2971],
        ...,
        [0.1154, 0.1476, 0.0015,  ..., 0.0942, 0.1212, 0.0692],
        [0.1153, 0.1474, 0.0015,  ..., 0.0941, 0.1211, 0.0691],
        [0.1152, 0.1474, 0.0015,  ..., 0.0941, 0.1211, 0.0691]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698189.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11986.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(914.3125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(450.7091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(323.9820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3690],
        [ 0.3887],
        [ 0.3859],
        ...,
        [-4.3318],
        [-4.3244],
        [-4.3241]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275185.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0315],
        [1.0392],
        ...,
        [0.9941],
        [0.9929],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371786.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6343],
        [0.3738],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0393],
        ...,
        [0.9941],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371789.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6343],
        [0.3738],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0219, -0.0012,  0.0135,  ...,  0.0119, -0.0018,  0.0446],
        [ 0.0144, -0.0008,  0.0089,  ...,  0.0084, -0.0013,  0.0308],
        [ 0.0198, -0.0011,  0.0122,  ...,  0.0109, -0.0017,  0.0407],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3529.5542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0178, device='cuda:0')



h[100].sum tensor(-8.0037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6573, device='cuda:0')



h[200].sum tensor(106.1016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8965, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0705, 0.0000, 0.0433,  ..., 0.0398, 0.0000, 0.1471],
        [0.0911, 0.0000, 0.0559,  ..., 0.0495, 0.0000, 0.1854],
        [0.0803, 0.0000, 0.0493,  ..., 0.0444, 0.0000, 0.1653],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72175.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0302, 0.0000, 0.2282,  ..., 0.1576, 0.0000, 0.4135],
        [0.0289, 0.0000, 0.2315,  ..., 0.1590, 0.0000, 0.4198],
        [0.0389, 0.0000, 0.2092,  ..., 0.1508, 0.0000, 0.3841],
        ...,
        [0.1154, 0.1478, 0.0013,  ..., 0.0942, 0.1217, 0.0692],
        [0.1153, 0.1476, 0.0013,  ..., 0.0941, 0.1216, 0.0691],
        [0.1153, 0.1476, 0.0013,  ..., 0.0941, 0.1216, 0.0691]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698666.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11933.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(916.9067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(467.6669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.8959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3877],
        [ 0.4158],
        [ 0.4676],
        ...,
        [-4.3542],
        [-4.3467],
        [-4.3463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318091.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0393],
        ...,
        [0.9941],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371789.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.4203, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0394],
        ...,
        [0.9941],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371793.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.4203, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018],
        ...,
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0006,  ...,  0.0010,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3399.6096, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0165, device='cuda:0')



h[100].sum tensor(-7.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6103, device='cuda:0')



h[200].sum tensor(104.1823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0450, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72642.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1105, 0.1392, 0.0010,  ..., 0.0916, 0.1145, 0.0688],
        [0.1114, 0.1445, 0.0008,  ..., 0.0916, 0.1187, 0.0668],
        [0.1115, 0.1443, 0.0010,  ..., 0.0916, 0.1186, 0.0669],
        ...,
        [0.1154, 0.1478, 0.0012,  ..., 0.0942, 0.1220, 0.0692],
        [0.1153, 0.1477, 0.0012,  ..., 0.0941, 0.1219, 0.0691],
        [0.1152, 0.1477, 0.0012,  ..., 0.0941, 0.1219, 0.0691]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708231.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11939.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(923.7912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(469.4745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(330.1692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4306],
        [-3.0669],
        [-3.4327],
        ...,
        [-4.3152],
        [-4.1471],
        [-3.7080]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295685.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0316],
        [1.0394],
        ...,
        [0.9941],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371793.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 790.0 event: 3950 loss: tensor(381.3441, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2600],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.2268, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0317],
        [1.0394],
        ...,
        [0.9941],
        [0.9928],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371797.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2600],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.2268, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042, -0.0003,  0.0027,  ...,  0.0036, -0.0004,  0.0119],
        [ 0.0096, -0.0005,  0.0060,  ...,  0.0062, -0.0009,  0.0219],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3154.7373, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.0683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0143, device='cuda:0')



h[100].sum tensor(-6.3822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5278, device='cuda:0')



h[200].sum tensor(101.3892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0318, 0.0000, 0.0199,  ..., 0.0218, 0.0000, 0.0759],
        [0.0143, 0.0000, 0.0091,  ..., 0.0130, 0.0000, 0.0411],
        [0.0132, 0.0000, 0.0083,  ..., 0.0118, 0.0000, 0.0367],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69752.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0934, 0.0000, 0.0805,  ..., 0.1057, 0.0000, 0.1796],
        [0.0989, 0.0165, 0.0630,  ..., 0.1019, 0.0139, 0.1549],
        [0.1018, 0.0440, 0.0440,  ..., 0.0995, 0.0376, 0.1298],
        ...,
        [0.1154, 0.1479, 0.0012,  ..., 0.0942, 0.1221, 0.0692],
        [0.1153, 0.1478, 0.0012,  ..., 0.0941, 0.1220, 0.0692],
        [0.1153, 0.1478, 0.0012,  ..., 0.0941, 0.1220, 0.0691]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698790.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11992.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(918.4387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(450.2467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(316.0207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0818],
        [-0.3360],
        [-1.0775],
        ...,
        [-4.3667],
        [-4.3595],
        [-4.3605]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282752., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0317],
        [1.0394],
        ...,
        [0.9941],
        [0.9928],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371797.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4559, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0318],
        [1.0395],
        ...,
        [0.9940],
        [0.9928],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371801.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4559, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3499.4707, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.5013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0174, device='cuda:0')



h[100].sum tensor(-7.8076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6432, device='cuda:0')



h[200].sum tensor(104.7696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74079.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1111, 0.1443, 0.0007,  ..., 0.0912, 0.1185, 0.0666],
        [0.1116, 0.1440, 0.0012,  ..., 0.0914, 0.1181, 0.0676],
        [0.1117, 0.1392, 0.0028,  ..., 0.0912, 0.1142, 0.0706],
        ...,
        [0.1156, 0.1481, 0.0011,  ..., 0.0941, 0.1222, 0.0693],
        [0.1155, 0.1480, 0.0011,  ..., 0.0941, 0.1221, 0.0692],
        [0.1155, 0.1480, 0.0011,  ..., 0.0940, 0.1221, 0.0692]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11921.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(935.4922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(480.4370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(336.8562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9211],
        [-3.6937],
        [-3.2766],
        ...,
        [-4.3936],
        [-4.3857],
        [-4.3853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297010.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0318],
        [1.0395],
        ...,
        [0.9940],
        [0.9928],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371801.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.1149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0319],
        [1.0396],
        ...,
        [0.9940],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371805.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.1149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3209.1841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0148, device='cuda:0')



h[100].sum tensor(-6.5561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5466, device='cuda:0')



h[200].sum tensor(101.7738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8928, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        [0.0047, 0.0000, 0.0030,  ..., 0.0073, 0.0000, 0.0185],
        [0.0209, 0.0000, 0.0131,  ..., 0.0161, 0.0000, 0.0534],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70233.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1077, 0.0903, 0.0207,  ..., 0.0936, 0.0758, 0.0953],
        [0.1017, 0.0457, 0.0449,  ..., 0.0993, 0.0382, 0.1314],
        [0.0886, 0.0121, 0.0878,  ..., 0.1098, 0.0104, 0.1929],
        ...,
        [0.1157, 0.1483, 0.0012,  ..., 0.0941, 0.1222, 0.0693],
        [0.1156, 0.1481, 0.0011,  ..., 0.0940, 0.1221, 0.0692],
        [0.1156, 0.1481, 0.0011,  ..., 0.0940, 0.1220, 0.0692]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700118.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11989.0176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(920.2109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(457.3510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(317.2096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3137],
        [-0.3785],
        [ 0.2307],
        ...,
        [-4.3994],
        [-4.3918],
        [-4.3915]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311393.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0319],
        [1.0396],
        ...,
        [0.9940],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371805.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0320],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371810.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3161.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.3068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0142, device='cuda:0')



h[100].sum tensor(-6.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5249, device='cuda:0')



h[200].sum tensor(101.4615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68546.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1112, 0.1444, 0.0009,  ..., 0.0910, 0.1180, 0.0667],
        [0.1110, 0.1386, 0.0015,  ..., 0.0918, 0.1137, 0.0700],
        [0.1099, 0.1254, 0.0060,  ..., 0.0928, 0.1037, 0.0762],
        ...,
        [0.1156, 0.1483, 0.0013,  ..., 0.0939, 0.1218, 0.0694],
        [0.1155, 0.1481, 0.0013,  ..., 0.0938, 0.1217, 0.0693],
        [0.1155, 0.1481, 0.0013,  ..., 0.0938, 0.1216, 0.0693]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689864.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11997.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(911.9593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(445.0299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.5798, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2375],
        [-4.0341],
        [-3.6223],
        ...,
        [-4.3954],
        [-4.3876],
        [-4.3871]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315651., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0320],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371810.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(252.4797, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0320],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371815.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(252.4797, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0011,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3655.1470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0186, device='cuda:0')



h[100].sum tensor(-8.2748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.6897, device='cuda:0')



h[200].sum tensor(107.1984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4815, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0045, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75593.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1110, 0.1374, 0.0040,  ..., 0.0901, 0.1110, 0.0716],
        [0.1113, 0.1368, 0.0052,  ..., 0.0907, 0.1109, 0.0722],
        [0.1109, 0.1300, 0.0077,  ..., 0.0913, 0.1059, 0.0754],
        ...,
        [0.1154, 0.1480, 0.0016,  ..., 0.0936, 0.1208, 0.0696],
        [0.1153, 0.1479, 0.0016,  ..., 0.0935, 0.1207, 0.0696],
        [0.1153, 0.1479, 0.0016,  ..., 0.0935, 0.1207, 0.0695]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723196.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11837.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(943.2357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(481.8917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(342.8949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9129],
        [-2.6728],
        [-2.2619],
        ...,
        [-4.3745],
        [-4.3668],
        [-4.3664]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271657.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0320],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371815.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6094],
        [0.6118],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(268.3956, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0321],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371820.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6094],
        [0.6118],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(268.3956, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0222, -0.0011,  0.0136,  ...,  0.0121, -0.0018,  0.0452],
        [ 0.0222, -0.0011,  0.0135,  ...,  0.0120, -0.0018,  0.0451],
        [ 0.0452, -0.0022,  0.0275,  ...,  0.0228, -0.0036,  0.0876],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0018],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3801.1201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0198, device='cuda:0')



h[100].sum tensor(-8.7931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7332, device='cuda:0')



h[200].sum tensor(109.5177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2684, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0493, 0.0000, 0.0302,  ..., 0.0293, 0.0000, 0.1057],
        [0.1176, 0.0000, 0.0718,  ..., 0.0620, 0.0000, 0.2344],
        [0.1362, 0.0000, 0.0831,  ..., 0.0707, 0.0000, 0.2688],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77986.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0397, 0.0000, 0.1908,  ..., 0.1402, 0.0000, 0.3515],
        [0.0135, 0.0000, 0.2784,  ..., 0.1705, 0.0000, 0.4926],
        [0.0016, 0.0000, 0.3350,  ..., 0.1901, 0.0000, 0.5841],
        ...,
        [0.1154, 0.1480, 0.0018,  ..., 0.0933, 0.1199, 0.0698],
        [0.1153, 0.1479, 0.0018,  ..., 0.0932, 0.1198, 0.0698],
        [0.1153, 0.1478, 0.0018,  ..., 0.0932, 0.1198, 0.0698]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(732732.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11794.7354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(954.9249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(491.5611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(356.1238, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3211],
        [ 0.2890],
        [ 0.2648],
        ...,
        [-4.3615],
        [-4.3538],
        [-4.3535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254493.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0321],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371820.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(167.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0322],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371824.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(167.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066, -0.0004,  0.0041,  ...,  0.0047, -0.0006,  0.0162],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3016.1309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.0274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0124, device='cuda:0')



h[100].sum tensor(-5.4702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.4580, device='cuda:0')



h[200].sum tensor(102.7014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2886, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0000, 0.0033,  ..., 0.0074, 0.0000, 0.0195],
        [0.0068, 0.0000, 0.0042,  ..., 0.0081, 0.0000, 0.0222],
        [0.0226, 0.0000, 0.0139,  ..., 0.0161, 0.0000, 0.0538],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66587.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1014, 0.0241, 0.0460,  ..., 0.0975, 0.0208, 0.1306],
        [0.0995, 0.0107, 0.0506,  ..., 0.0996, 0.0105, 0.1381],
        [0.0913, 0.0032, 0.0717,  ..., 0.1058, 0.0031, 0.1693],
        ...,
        [0.1157, 0.1484, 0.0016,  ..., 0.0931, 0.1192, 0.0700],
        [0.1156, 0.1483, 0.0016,  ..., 0.0930, 0.1191, 0.0699],
        [0.1156, 0.1482, 0.0016,  ..., 0.0930, 0.1190, 0.0699]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680787.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12019.6680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(909.1420, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(421.2201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(300.7819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5414],
        [ 0.5552],
        [ 0.5879],
        ...,
        [-4.3677],
        [-4.3600],
        [-4.3592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298076., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0322],
        [1.0397],
        ...,
        [0.9940],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371824.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(268.5973, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0322],
        [1.0398],
        ...,
        [0.9939],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371829., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(268.5973, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0074, -0.0004,  0.0046,  ...,  0.0051, -0.0007,  0.0178],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3832.4016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0198, device='cuda:0')



h[100].sum tensor(-8.8150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.7337, device='cuda:0')



h[200].sum tensor(111.7527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0180, 0.0000, 0.0111,  ..., 0.0139, 0.0000, 0.0451],
        [0.0077, 0.0000, 0.0048,  ..., 0.0085, 0.0000, 0.0238],
        [0.0064, 0.0000, 0.0040,  ..., 0.0079, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77616.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0861, 0.0146, 0.0799,  ..., 0.1089, 0.0128, 0.1836],
        [0.0995, 0.0284, 0.0456,  ..., 0.0998, 0.0244, 0.1334],
        [0.1008, 0.0260, 0.0462,  ..., 0.0985, 0.0217, 0.1331],
        ...,
        [0.1161, 0.1487, 0.0015,  ..., 0.0930, 0.1185, 0.0701],
        [0.1160, 0.1486, 0.0015,  ..., 0.0930, 0.1184, 0.0701],
        [0.1160, 0.1486, 0.0015,  ..., 0.0929, 0.1184, 0.0701]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727873., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11951.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(964.0562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(484.4549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(360.1714, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4270],
        [ 0.4529],
        [ 0.4873],
        ...,
        [-4.3769],
        [-4.3691],
        [-4.3687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257442.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0322],
        [1.0398],
        ...,
        [0.9939],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371829., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.4019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0323],
        [1.0398],
        ...,
        [0.9939],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(371833.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.4019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099, -0.0005,  0.0061,  ...,  0.0063, -0.0009,  0.0223],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        ...,
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017],
        [-0.0013,  0.0000, -0.0007,  ...,  0.0010,  0.0000,  0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3429.5771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.8911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.0160, device='cuda:0')



h[100].sum tensor(-7.1112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.5911, device='cuda:0')



h[200].sum tensor(107.9363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0324, 0.0000, 0.0200,  ..., 0.0213, 0.0000, 0.0742],
        [0.0130, 0.0000, 0.0081,  ..., 0.0116, 0.0000, 0.0359],
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0070],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75920.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0724, 0.0023, 0.1178,  ..., 0.1181, 0.0023, 0.2401],
        [0.0932, 0.0399, 0.0627,  ..., 0.1039, 0.0320, 0.1592],
        [0.1066, 0.0839, 0.0209,  ..., 0.0944, 0.0677, 0.0975],
        ...,
        [0.1146, 0.1286, 0.0076,  ..., 0.0942, 0.1022, 0.0803],
        [0.1158, 0.1441, 0.0018,  ..., 0.0932, 0.1143, 0.0726],
        [0.1162, 0.1488, 0.0014,  ..., 0.0929, 0.1180, 0.0703]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(743569.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11967.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(954.2081, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(475.3012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(349.9876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3379],
        [ 0.1500],
        [-0.0436],
        ...,
        [-3.7727],
        [-4.1027],
        [-4.2798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282313.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0323],
        [1.0398],
        ...,
        [0.9939],
        [0.9927],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(371833.2500, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:01:09.513202
evaluation loss: 499.43389892578125
epoch: 0 mean loss: 480.9395446777344
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 138, in <module>
    writer.add_hparams({'Lr': LrVal, 'weight_decay': weight_decay_val, 'BatchSize':BatchSize},\
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 313, in add_hparams
    w_hp.add_scalar(k, v)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 354, in add_scalar
    summary = scalar(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/summary.py", line 250, in scalar
    assert scalar.squeeze().ndim == 0, "scalar should be 0D"
AssertionError: scalar should be 0D

real	1m51.921s
user	1m12.198s
sys	0m25.232s
