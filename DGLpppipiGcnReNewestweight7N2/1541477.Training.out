0: gpu022.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-7a90e930-602c-d4f7-ff60-d36003d041db)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Fri Aug 12 22:14:01 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:1D:00.0 Off |                    0 |
| N/A   39C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2aea3ceae880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m3.748s
user	0m2.277s
sys	0m0.718s
[22:14:07] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.9238],
        [-0.0835],
        [-0.8121],
        ...,
        [ 0.4963],
        [-0.0668],
        [ 0.9718]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-115.2343, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0416, -0.0477, -0.0583, -0.0348, -0.0928, -0.0743,  0.0548,  0.0822,
         -0.0718, -0.0046,  0.1298, -0.0454,  0.1203,  0.0582, -0.1056, -0.0874,
          0.0971,  0.1442,  0.1327, -0.0970,  0.1365, -0.0027, -0.1021, -0.1277,
         -0.0193, -0.0559,  0.0986, -0.0715,  0.0909,  0.0272,  0.1194,  0.0085,
          0.0139,  0.1452, -0.0882,  0.1470, -0.0826,  0.1307,  0.0633,  0.0334,
         -0.0289,  0.1183, -0.0160,  0.1049,  0.1256, -0.1489, -0.0998,  0.0335,
         -0.0330, -0.0318,  0.0668, -0.0358, -0.0132,  0.0297,  0.0468,  0.0484,
          0.1363, -0.0350, -0.1046, -0.0390,  0.1040,  0.1320,  0.0989, -0.0553,
          0.0013, -0.0315,  0.1020, -0.1372,  0.1446, -0.1207, -0.0182, -0.0543,
         -0.1274, -0.0152, -0.1487, -0.1357, -0.0512,  0.0892, -0.0249,  0.0849,
         -0.0907,  0.0812, -0.0350, -0.1347, -0.0193,  0.1496,  0.0375,  0.0664,
          0.1052,  0.1266, -0.0872, -0.0267, -0.1012,  0.0319, -0.1347, -0.0502,
         -0.1496,  0.1106,  0.0673, -0.0088, -0.0289,  0.0619, -0.0990,  0.1274,
         -0.0468, -0.0185,  0.1420, -0.1012,  0.1412,  0.0947,  0.0618, -0.0897,
         -0.1165,  0.0601, -0.1096, -0.1511, -0.0861, -0.1090, -0.1282, -0.0800,
         -0.0806,  0.1112,  0.0191, -0.0937, -0.0547,  0.1225,  0.0669,  0.0829,
         -0.1482,  0.1252,  0.1499,  0.1059, -0.0323, -0.1203,  0.0670, -0.1152,
         -0.0278, -0.0755,  0.1432,  0.0628,  0.0707,  0.0544, -0.0243,  0.1095,
         -0.0146,  0.0240, -0.0121,  0.1142, -0.0071, -0.1381,  0.0091, -0.1123,
         -0.1227, -0.1221,  0.0356,  0.0575,  0.0649,  0.1327,  0.1460, -0.1411,
          0.0747,  0.0581, -0.0969, -0.1266,  0.1524,  0.0656,  0.0766,  0.1093,
         -0.0907, -0.1475,  0.1329, -0.0193,  0.0924, -0.0889,  0.0752, -0.1006,
         -0.0618,  0.0882,  0.0099, -0.0972,  0.0813, -0.1389, -0.1293, -0.1518,
          0.1422,  0.0156, -0.0124, -0.0685,  0.0965, -0.1304, -0.1246,  0.0455,
          0.0842, -0.0257, -0.1365,  0.0999,  0.1367, -0.0876,  0.0622,  0.0280,
         -0.0264,  0.1395, -0.0048, -0.0313, -0.0882, -0.0005,  0.0813,  0.0758,
         -0.0655,  0.0041,  0.1427, -0.0645,  0.0105, -0.0802, -0.0657,  0.1375,
          0.1238, -0.0911,  0.1167, -0.0634,  0.0775,  0.0323, -0.1454, -0.0523,
          0.0155, -0.0222,  0.0135,  0.0841,  0.0298,  0.0521,  0.1498,  0.1133,
          0.1517,  0.0329, -0.1245, -0.0244, -0.1382, -0.0041,  0.0120,  0.0550,
         -0.1042,  0.1357, -0.0836, -0.0577, -0.0087, -0.1455, -0.0021, -0.0758,
          0.1318, -0.0644, -0.1232, -0.0194, -0.1065,  0.1451,  0.0082,  0.1344]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0416, -0.0477, -0.0583, -0.0348, -0.0928, -0.0743,  0.0548,  0.0822,
         -0.0718, -0.0046,  0.1298, -0.0454,  0.1203,  0.0582, -0.1056, -0.0874,
          0.0971,  0.1442,  0.1327, -0.0970,  0.1365, -0.0027, -0.1021, -0.1277,
         -0.0193, -0.0559,  0.0986, -0.0715,  0.0909,  0.0272,  0.1194,  0.0085,
          0.0139,  0.1452, -0.0882,  0.1470, -0.0826,  0.1307,  0.0633,  0.0334,
         -0.0289,  0.1183, -0.0160,  0.1049,  0.1256, -0.1489, -0.0998,  0.0335,
         -0.0330, -0.0318,  0.0668, -0.0358, -0.0132,  0.0297,  0.0468,  0.0484,
          0.1363, -0.0350, -0.1046, -0.0390,  0.1040,  0.1320,  0.0989, -0.0553,
          0.0013, -0.0315,  0.1020, -0.1372,  0.1446, -0.1207, -0.0182, -0.0543,
         -0.1274, -0.0152, -0.1487, -0.1357, -0.0512,  0.0892, -0.0249,  0.0849,
         -0.0907,  0.0812, -0.0350, -0.1347, -0.0193,  0.1496,  0.0375,  0.0664,
          0.1052,  0.1266, -0.0872, -0.0267, -0.1012,  0.0319, -0.1347, -0.0502,
         -0.1496,  0.1106,  0.0673, -0.0088, -0.0289,  0.0619, -0.0990,  0.1274,
         -0.0468, -0.0185,  0.1420, -0.1012,  0.1412,  0.0947,  0.0618, -0.0897,
         -0.1165,  0.0601, -0.1096, -0.1511, -0.0861, -0.1090, -0.1282, -0.0800,
         -0.0806,  0.1112,  0.0191, -0.0937, -0.0547,  0.1225,  0.0669,  0.0829,
         -0.1482,  0.1252,  0.1499,  0.1059, -0.0323, -0.1203,  0.0670, -0.1152,
         -0.0278, -0.0755,  0.1432,  0.0628,  0.0707,  0.0544, -0.0243,  0.1095,
         -0.0146,  0.0240, -0.0121,  0.1142, -0.0071, -0.1381,  0.0091, -0.1123,
         -0.1227, -0.1221,  0.0356,  0.0575,  0.0649,  0.1327,  0.1460, -0.1411,
          0.0747,  0.0581, -0.0969, -0.1266,  0.1524,  0.0656,  0.0766,  0.1093,
         -0.0907, -0.1475,  0.1329, -0.0193,  0.0924, -0.0889,  0.0752, -0.1006,
         -0.0618,  0.0882,  0.0099, -0.0972,  0.0813, -0.1389, -0.1293, -0.1518,
          0.1422,  0.0156, -0.0124, -0.0685,  0.0965, -0.1304, -0.1246,  0.0455,
          0.0842, -0.0257, -0.1365,  0.0999,  0.1367, -0.0876,  0.0622,  0.0280,
         -0.0264,  0.1395, -0.0048, -0.0313, -0.0882, -0.0005,  0.0813,  0.0758,
         -0.0655,  0.0041,  0.1427, -0.0645,  0.0105, -0.0802, -0.0657,  0.1375,
          0.1238, -0.0911,  0.1167, -0.0634,  0.0775,  0.0323, -0.1454, -0.0523,
          0.0155, -0.0222,  0.0135,  0.0841,  0.0298,  0.0521,  0.1498,  0.1133,
          0.1517,  0.0329, -0.1245, -0.0244, -0.1382, -0.0041,  0.0120,  0.0550,
         -0.1042,  0.1357, -0.0836, -0.0577, -0.0087, -0.1455, -0.0021, -0.0758,
          0.1318, -0.0644, -0.1232, -0.0194, -0.1065,  0.1451,  0.0082,  0.1344]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0400,  0.0964,  0.1134,  ..., -0.0700, -0.0770,  0.0958],
        [-0.1072, -0.0814,  0.0868,  ..., -0.0573,  0.1090,  0.0096],
        [-0.0220, -0.0337,  0.0045,  ..., -0.0707, -0.0241, -0.0317],
        ...,
        [ 0.1112,  0.0610, -0.0204,  ...,  0.1132, -0.0609, -0.0396],
        [-0.1091,  0.1031,  0.0185,  ...,  0.1235, -0.1201, -0.0260],
        [-0.1060, -0.0947, -0.0299,  ...,  0.0812,  0.0730,  0.1128]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0400,  0.0964,  0.1134,  ..., -0.0700, -0.0770,  0.0958],
        [-0.1072, -0.0814,  0.0868,  ..., -0.0573,  0.1090,  0.0096],
        [-0.0220, -0.0337,  0.0045,  ..., -0.0707, -0.0241, -0.0317],
        ...,
        [ 0.1112,  0.0610, -0.0204,  ...,  0.1132, -0.0609, -0.0396],
        [-0.1091,  0.1031,  0.0185,  ...,  0.1235, -0.1201, -0.0260],
        [-0.1060, -0.0947, -0.0299,  ...,  0.0812,  0.0730,  0.1128]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0447,  0.1370, -0.1007,  ..., -0.0261, -0.0677,  0.1430],
        [ 0.0585,  0.0465, -0.0521,  ...,  0.0284,  0.0477,  0.0247],
        [ 0.0469, -0.1349, -0.0724,  ...,  0.0514,  0.0162,  0.1245],
        ...,
        [-0.0783,  0.0091,  0.0319,  ...,  0.1530, -0.1131, -0.0054],
        [ 0.0013, -0.0358, -0.0906,  ...,  0.0350, -0.0690, -0.0360],
        [ 0.1650, -0.1583, -0.0848,  ..., -0.0007, -0.0335, -0.1150]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0447,  0.1370, -0.1007,  ..., -0.0261, -0.0677,  0.1430],
        [ 0.0585,  0.0465, -0.0521,  ...,  0.0284,  0.0477,  0.0247],
        [ 0.0469, -0.1349, -0.0724,  ...,  0.0514,  0.0162,  0.1245],
        ...,
        [-0.0783,  0.0091,  0.0319,  ...,  0.1530, -0.1131, -0.0054],
        [ 0.0013, -0.0358, -0.0906,  ...,  0.0350, -0.0690, -0.0360],
        [ 0.1650, -0.1583, -0.0848,  ..., -0.0007, -0.0335, -0.1150]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0579, -0.1482,  0.2147,  ..., -0.1444, -0.0974,  0.0793],
        [-0.0133,  0.1719,  0.1462,  ...,  0.2428, -0.1410, -0.1800],
        [-0.1948, -0.1016, -0.2134,  ..., -0.1687,  0.1165, -0.0781],
        ...,
        [-0.0104, -0.0939,  0.0814,  ...,  0.0317,  0.2103,  0.1320],
        [-0.0911,  0.1501, -0.1183,  ...,  0.0928,  0.2130, -0.1662],
        [-0.1399,  0.0983, -0.2018,  ..., -0.1301,  0.1464, -0.1265]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0579, -0.1482,  0.2147,  ..., -0.1444, -0.0974,  0.0793],
        [-0.0133,  0.1719,  0.1462,  ...,  0.2428, -0.1410, -0.1800],
        [-0.1948, -0.1016, -0.2134,  ..., -0.1687,  0.1165, -0.0781],
        ...,
        [-0.0104, -0.0939,  0.0814,  ...,  0.0317,  0.2103,  0.1320],
        [-0.0911,  0.1501, -0.1183,  ...,  0.0928,  0.2130, -0.1662],
        [-0.1399,  0.0983, -0.2018,  ..., -0.1301,  0.1464, -0.1265]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.0337],
        [ 0.0904],
        [ 0.2833],
        [-0.3197],
        [ 0.2638],
        [ 0.2549],
        [-0.3148],
        [ 0.3424],
        [ 0.3272],
        [-0.3242],
        [ 0.3578],
        [-0.2671],
        [ 0.1506],
        [ 0.3523],
        [ 0.2004],
        [ 0.2930],
        [-0.3254],
        [-0.2524],
        [ 0.0626],
        [ 0.2577],
        [-0.3484],
        [-0.2867],
        [ 0.3852],
        [ 0.4058],
        [-0.4116],
        [-0.3708],
        [ 0.3858],
        [ 0.0715],
        [-0.0560],
        [ 0.3007],
        [-0.1029],
        [-0.4061]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.0337],
        [ 0.0904],
        [ 0.2833],
        [-0.3197],
        [ 0.2638],
        [ 0.2549],
        [-0.3148],
        [ 0.3424],
        [ 0.3272],
        [-0.3242],
        [ 0.3578],
        [-0.2671],
        [ 0.1506],
        [ 0.3523],
        [ 0.2004],
        [ 0.2930],
        [-0.3254],
        [-0.2524],
        [ 0.0626],
        [ 0.2577],
        [-0.3484],
        [-0.2867],
        [ 0.3852],
        [ 0.4058],
        [-0.4116],
        [-0.3708],
        [ 0.3858],
        [ 0.0715],
        [-0.0560],
        [ 0.3007],
        [-0.1029],
        [-0.4061]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-2.4730e-03, -3.5618e-02, -1.0982e-01, -8.5063e-02, -6.7553e-02,
          5.2786e-02,  3.6285e-02, -1.3113e-01,  1.3288e-01,  1.0515e-01,
         -2.5722e-02, -4.8937e-02, -1.1697e-01,  1.3749e-01, -1.4933e-01,
          1.3767e-01, -5.9980e-02,  6.7034e-02, -3.1805e-02,  1.4583e-01,
         -1.1703e-01,  4.5328e-02,  1.2114e-01, -1.3871e-01,  2.6445e-03,
         -9.1474e-02,  1.1217e-01, -1.4172e-03, -8.5198e-02,  5.9019e-02,
         -8.1163e-02,  3.8362e-02,  1.0029e-01, -2.3713e-02,  5.7519e-02,
         -2.2153e-02, -1.0365e-02, -3.3631e-02,  6.9004e-02, -9.6355e-02,
          1.2996e-01,  1.2429e-01, -3.0053e-02,  4.6872e-03, -8.4664e-02,
         -1.4908e-01, -7.5029e-02,  1.2843e-01,  5.2455e-02, -1.4669e-01,
         -1.2997e-01, -3.7745e-02, -3.8069e-02,  9.0450e-02,  3.2789e-03,
         -7.1091e-03,  1.3412e-01,  4.2411e-02, -8.1617e-02, -1.2469e-01,
         -1.9229e-02, -5.3169e-02, -4.0679e-03,  1.3592e-01,  5.2963e-02,
          1.4409e-01, -1.1735e-01, -5.1022e-02, -7.2755e-02,  7.1520e-02,
         -5.5313e-02, -5.8471e-02,  9.2268e-02, -2.8813e-03,  7.2913e-03,
         -4.5913e-02, -1.3366e-01, -8.6736e-02, -6.0857e-02,  1.0613e-01,
         -1.2791e-01,  9.8839e-02, -5.3269e-02,  1.5623e-02, -1.3925e-01,
         -1.1509e-01,  1.4075e-01, -1.0193e-01,  1.2177e-01, -4.3767e-02,
         -1.4628e-01,  7.7317e-02,  6.4458e-03, -8.9245e-02, -1.3446e-01,
          8.0383e-02,  1.3838e-01, -1.3453e-01, -1.0885e-01,  1.3120e-01,
         -4.5883e-02, -2.0920e-03,  2.1958e-02, -1.2201e-01, -6.3893e-02,
          2.4498e-02, -1.0499e-02, -5.1392e-02,  7.8671e-02,  1.3023e-01,
          1.2773e-01, -7.2454e-02, -4.0563e-03, -1.3985e-01, -3.1960e-02,
         -7.3361e-02, -1.0221e-01,  1.2290e-02, -1.0225e-01,  1.4757e-01,
          1.2629e-01, -1.0415e-01, -1.0524e-02,  1.3685e-02, -9.5966e-02,
         -8.6467e-02,  6.3040e-02, -1.3952e-01,  1.2736e-01,  9.3415e-02,
         -1.9957e-02, -9.2664e-02, -3.2477e-02, -7.0706e-02, -7.5819e-02,
         -4.3101e-03, -3.5865e-02,  1.3263e-02,  3.2023e-02,  1.4983e-01,
          4.1389e-03,  1.0943e-01,  9.6658e-02,  1.0761e-01, -1.2350e-01,
          6.7200e-02, -1.5057e-01,  2.2982e-02, -9.4396e-02, -8.9908e-02,
          6.7574e-02, -1.0232e-01,  1.2142e-01, -1.3961e-01,  5.0970e-02,
         -1.0967e-01, -3.6692e-02,  1.0796e-01, -6.2375e-02, -4.6243e-02,
          9.6741e-02, -1.3374e-02,  1.2438e-01, -1.2695e-01,  9.9015e-02,
          1.4354e-01, -8.1577e-02,  1.4676e-01,  4.5174e-02, -4.6731e-03,
         -7.4307e-03,  1.0154e-01, -9.3079e-02,  9.4521e-02,  4.2444e-02,
          1.0753e-01,  4.3657e-02, -6.5471e-02, -7.6588e-02, -4.0247e-02,
          1.2808e-01,  1.0499e-01,  1.9466e-02, -1.1735e-01, -4.8971e-02,
          9.8712e-02,  1.1144e-01,  1.8564e-04, -5.3166e-02, -1.4979e-01,
         -1.2497e-01,  1.3572e-01,  3.0840e-02, -3.8226e-02,  1.4400e-01,
          4.7842e-02,  7.7279e-02, -1.5239e-01,  1.3718e-01,  4.9490e-02,
         -6.3025e-02, -1.2236e-01, -3.1123e-03, -6.0260e-02, -5.9390e-02,
         -3.4472e-02,  4.2625e-02, -5.1991e-02, -7.8377e-02,  1.3073e-01,
         -8.8745e-02,  2.0069e-02, -3.6846e-03, -7.4430e-02,  1.5154e-01,
         -1.1554e-01,  1.8962e-02, -4.3772e-02,  3.0439e-02, -4.9252e-05,
         -4.1993e-02,  1.3823e-01,  7.9193e-02,  1.7833e-02, -1.0600e-01,
          1.3375e-02,  3.0237e-02, -7.9021e-02,  1.5234e-01,  1.0832e-01,
         -5.5114e-02,  1.3688e-01, -1.0459e-01, -9.1557e-02,  3.0290e-02,
          1.1852e-01,  2.1914e-02, -9.6154e-02,  1.2220e-01,  5.1461e-02,
          1.3735e-01,  5.5211e-02, -5.3666e-02,  1.2926e-01,  6.7326e-02,
         -1.4720e-01,  7.6707e-02,  2.0080e-02, -1.0209e-01,  1.4848e-01,
         -2.3224e-02,  1.1996e-01,  7.1576e-02,  1.0233e-01, -7.1975e-02,
         -6.9540e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-2.4730e-03, -3.5618e-02, -1.0982e-01, -8.5063e-02, -6.7553e-02,
          5.2786e-02,  3.6285e-02, -1.3113e-01,  1.3288e-01,  1.0515e-01,
         -2.5722e-02, -4.8937e-02, -1.1697e-01,  1.3749e-01, -1.4933e-01,
          1.3767e-01, -5.9980e-02,  6.7034e-02, -3.1805e-02,  1.4583e-01,
         -1.1703e-01,  4.5328e-02,  1.2114e-01, -1.3871e-01,  2.6445e-03,
         -9.1474e-02,  1.1217e-01, -1.4172e-03, -8.5198e-02,  5.9019e-02,
         -8.1163e-02,  3.8362e-02,  1.0029e-01, -2.3713e-02,  5.7519e-02,
         -2.2153e-02, -1.0365e-02, -3.3631e-02,  6.9004e-02, -9.6355e-02,
          1.2996e-01,  1.2429e-01, -3.0053e-02,  4.6872e-03, -8.4664e-02,
         -1.4908e-01, -7.5029e-02,  1.2843e-01,  5.2455e-02, -1.4669e-01,
         -1.2997e-01, -3.7745e-02, -3.8069e-02,  9.0450e-02,  3.2789e-03,
         -7.1091e-03,  1.3412e-01,  4.2411e-02, -8.1617e-02, -1.2469e-01,
         -1.9229e-02, -5.3169e-02, -4.0679e-03,  1.3592e-01,  5.2963e-02,
          1.4409e-01, -1.1735e-01, -5.1022e-02, -7.2755e-02,  7.1520e-02,
         -5.5313e-02, -5.8471e-02,  9.2268e-02, -2.8813e-03,  7.2913e-03,
         -4.5913e-02, -1.3366e-01, -8.6736e-02, -6.0857e-02,  1.0613e-01,
         -1.2791e-01,  9.8839e-02, -5.3269e-02,  1.5623e-02, -1.3925e-01,
         -1.1509e-01,  1.4075e-01, -1.0193e-01,  1.2177e-01, -4.3767e-02,
         -1.4628e-01,  7.7317e-02,  6.4458e-03, -8.9245e-02, -1.3446e-01,
          8.0383e-02,  1.3838e-01, -1.3453e-01, -1.0885e-01,  1.3120e-01,
         -4.5883e-02, -2.0920e-03,  2.1958e-02, -1.2201e-01, -6.3893e-02,
          2.4498e-02, -1.0499e-02, -5.1392e-02,  7.8671e-02,  1.3023e-01,
          1.2773e-01, -7.2454e-02, -4.0563e-03, -1.3985e-01, -3.1960e-02,
         -7.3361e-02, -1.0221e-01,  1.2290e-02, -1.0225e-01,  1.4757e-01,
          1.2629e-01, -1.0415e-01, -1.0524e-02,  1.3685e-02, -9.5966e-02,
         -8.6467e-02,  6.3040e-02, -1.3952e-01,  1.2736e-01,  9.3415e-02,
         -1.9957e-02, -9.2664e-02, -3.2477e-02, -7.0706e-02, -7.5819e-02,
         -4.3101e-03, -3.5865e-02,  1.3263e-02,  3.2023e-02,  1.4983e-01,
          4.1389e-03,  1.0943e-01,  9.6658e-02,  1.0761e-01, -1.2350e-01,
          6.7200e-02, -1.5057e-01,  2.2982e-02, -9.4396e-02, -8.9908e-02,
          6.7574e-02, -1.0232e-01,  1.2142e-01, -1.3961e-01,  5.0970e-02,
         -1.0967e-01, -3.6692e-02,  1.0796e-01, -6.2375e-02, -4.6243e-02,
          9.6741e-02, -1.3374e-02,  1.2438e-01, -1.2695e-01,  9.9015e-02,
          1.4354e-01, -8.1577e-02,  1.4676e-01,  4.5174e-02, -4.6731e-03,
         -7.4307e-03,  1.0154e-01, -9.3079e-02,  9.4521e-02,  4.2444e-02,
          1.0753e-01,  4.3657e-02, -6.5471e-02, -7.6588e-02, -4.0247e-02,
          1.2808e-01,  1.0499e-01,  1.9466e-02, -1.1735e-01, -4.8971e-02,
          9.8712e-02,  1.1144e-01,  1.8564e-04, -5.3166e-02, -1.4979e-01,
         -1.2497e-01,  1.3572e-01,  3.0840e-02, -3.8226e-02,  1.4400e-01,
          4.7842e-02,  7.7279e-02, -1.5239e-01,  1.3718e-01,  4.9490e-02,
         -6.3025e-02, -1.2236e-01, -3.1123e-03, -6.0260e-02, -5.9390e-02,
         -3.4472e-02,  4.2625e-02, -5.1991e-02, -7.8377e-02,  1.3073e-01,
         -8.8745e-02,  2.0069e-02, -3.6846e-03, -7.4430e-02,  1.5154e-01,
         -1.1554e-01,  1.8962e-02, -4.3772e-02,  3.0439e-02, -4.9252e-05,
         -4.1993e-02,  1.3823e-01,  7.9193e-02,  1.7833e-02, -1.0600e-01,
          1.3375e-02,  3.0237e-02, -7.9021e-02,  1.5234e-01,  1.0832e-01,
         -5.5114e-02,  1.3688e-01, -1.0459e-01, -9.1557e-02,  3.0290e-02,
          1.1852e-01,  2.1914e-02, -9.6154e-02,  1.2220e-01,  5.1461e-02,
          1.3735e-01,  5.5211e-02, -5.3666e-02,  1.2926e-01,  6.7326e-02,
         -1.4720e-01,  7.6707e-02,  2.0080e-02, -1.0209e-01,  1.4848e-01,
         -2.3224e-02,  1.1996e-01,  7.1576e-02,  1.0233e-01, -7.1975e-02,
         -6.9540e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0938, -0.0544, -0.0905,  ..., -0.0108,  0.0413, -0.0641],
        [ 0.0228,  0.0001, -0.0031,  ...,  0.0201,  0.0355,  0.0387],
        [ 0.0361,  0.0228, -0.0776,  ..., -0.0580,  0.0206,  0.0584],
        ...,
        [-0.0809,  0.0672, -0.0338,  ..., -0.0301,  0.0824, -0.1209],
        [-0.0445,  0.1203,  0.0375,  ...,  0.1109,  0.0381, -0.1169],
        [ 0.0906, -0.1082, -0.0925,  ..., -0.0682, -0.0642, -0.0464]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0938, -0.0544, -0.0905,  ..., -0.0108,  0.0413, -0.0641],
        [ 0.0228,  0.0001, -0.0031,  ...,  0.0201,  0.0355,  0.0387],
        [ 0.0361,  0.0228, -0.0776,  ..., -0.0580,  0.0206,  0.0584],
        ...,
        [-0.0809,  0.0672, -0.0338,  ..., -0.0301,  0.0824, -0.1209],
        [-0.0445,  0.1203,  0.0375,  ...,  0.1109,  0.0381, -0.1169],
        [ 0.0906, -0.1082, -0.0925,  ..., -0.0682, -0.0642, -0.0464]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0088, -0.0954, -0.1144,  ...,  0.1178, -0.1440,  0.0353],
        [ 0.0764,  0.0355,  0.0014,  ..., -0.0882, -0.1252, -0.0393],
        [-0.1164,  0.0604, -0.1339,  ...,  0.1174, -0.0277,  0.1765],
        ...,
        [ 0.0774,  0.0442, -0.1563,  ...,  0.0066,  0.1593, -0.1602],
        [-0.1069, -0.1274,  0.1104,  ..., -0.0608,  0.0748,  0.0344],
        [ 0.0819, -0.0366, -0.0595,  ..., -0.0732, -0.0801, -0.1519]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0088, -0.0954, -0.1144,  ...,  0.1178, -0.1440,  0.0353],
        [ 0.0764,  0.0355,  0.0014,  ..., -0.0882, -0.1252, -0.0393],
        [-0.1164,  0.0604, -0.1339,  ...,  0.1174, -0.0277,  0.1765],
        ...,
        [ 0.0774,  0.0442, -0.1563,  ...,  0.0066,  0.1593, -0.1602],
        [-0.1069, -0.1274,  0.1104,  ..., -0.0608,  0.0748,  0.0344],
        [ 0.0819, -0.0366, -0.0595,  ..., -0.0732, -0.0801, -0.1519]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1030, -0.0361, -0.0390,  ..., -0.0687, -0.1903,  0.1088],
        [ 0.1505,  0.0194, -0.0606,  ..., -0.0481,  0.1507, -0.0595],
        [ 0.2398,  0.1143,  0.1203,  ...,  0.1335, -0.1023,  0.2395],
        ...,
        [ 0.1155,  0.0735,  0.0949,  ..., -0.0252, -0.1984,  0.2059],
        [ 0.1391, -0.2090, -0.1915,  ..., -0.2476, -0.2398, -0.1618],
        [-0.2269, -0.1804,  0.1811,  ...,  0.0655,  0.0023,  0.1476]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1030, -0.0361, -0.0390,  ..., -0.0687, -0.1903,  0.1088],
        [ 0.1505,  0.0194, -0.0606,  ..., -0.0481,  0.1507, -0.0595],
        [ 0.2398,  0.1143,  0.1203,  ...,  0.1335, -0.1023,  0.2395],
        ...,
        [ 0.1155,  0.0735,  0.0949,  ..., -0.0252, -0.1984,  0.2059],
        [ 0.1391, -0.2090, -0.1915,  ..., -0.2476, -0.2398, -0.1618],
        [-0.2269, -0.1804,  0.1811,  ...,  0.0655,  0.0023,  0.1476]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.0373],
        [ 0.2882],
        [ 0.0919],
        [-0.3296],
        [-0.2700],
        [-0.0951],
        [-0.3305],
        [-0.0972],
        [-0.0740],
        [-0.2636],
        [ 0.2900],
        [ 0.2652],
        [-0.0564],
        [-0.3551],
        [ 0.3352],
        [-0.3264],
        [ 0.1488],
        [ 0.4074],
        [-0.3703],
        [ 0.1576],
        [ 0.2150],
        [-0.4216],
        [ 0.1430],
        [-0.0443],
        [ 0.2891],
        [-0.1339],
        [-0.0098],
        [ 0.1065],
        [-0.1679],
        [ 0.0782],
        [ 0.1931],
        [-0.2342]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.0373],
        [ 0.2882],
        [ 0.0919],
        [-0.3296],
        [-0.2700],
        [-0.0951],
        [-0.3305],
        [-0.0972],
        [-0.0740],
        [-0.2636],
        [ 0.2900],
        [ 0.2652],
        [-0.0564],
        [-0.3551],
        [ 0.3352],
        [-0.3264],
        [ 0.1488],
        [ 0.4074],
        [-0.3703],
        [ 0.1576],
        [ 0.2150],
        [-0.4216],
        [ 0.1430],
        [-0.0443],
        [ 0.2891],
        [-0.1339],
        [-0.0098],
        [ 0.1065],
        [-0.1679],
        [ 0.0782],
        [ 0.1931],
        [-0.2342]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-37.1218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-4.4130, device='cuda:0')



h[100].sum tensor(-0.2376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0.2439, device='cuda:0')



h[200].sum tensor(-4.8715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-4.9998, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2707.4058, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0010,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0019, 0.0051,  ..., 0.0000, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(12659.0693, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-32.7848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11.2862, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(0.9029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-22.8512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0685],
        [0.0838],
        [0.1209],
        ...,
        [0.0193],
        [0.0194],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(1727.6882, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0685],
        [0.0838],
        [0.1209],
        ...,
        [0.0193],
        [0.0194],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(95.2796, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-4.2980, device='cuda:0')



h[100].sum tensor(5.1030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0617, device='cuda:0')



h[200].sum tensor(11.7655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(16117.6543, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0162, 0.0000, 0.0000,  ..., 0.0301, 0.0000, 0.0074],
        [0.0034, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(98854.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1310.1821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(92.1774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(145.3680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(10.2273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3119.5815, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(219.4770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.2828],
        [0.1734],
        [0.1061],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(16610.5391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0685],
        [0.0838],
        [0.1209],
        ...,
        [0.0193],
        [0.0194],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 48, in <module>
    checkpoint_load(torch.load(F"{checkpoint_dir_path}/checkpoint_dir/{TraEvN}{EpochNum}{startmesh}saved_checkpoint.tar"))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/serialization.py", line 594, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/checkpoint_dir/90016284saved_checkpoint.tar'

real	0m21.839s
user	0m12.128s
sys	0m9.363s
