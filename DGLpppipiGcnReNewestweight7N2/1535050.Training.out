0: gpu027.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-60b276b2-4ce7-6f0f-583c-12a5a65d2ee5)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Tue Aug  9 18:41:20 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |
| N/A   46C    P0    75W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b20e21ba880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m2.985s
user	0m1.703s
sys	0m0.519s
[18:41:24] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.2558],
        [ 0.0764],
        [-0.0720],
        ...,
        [-0.7621],
        [ 1.5605],
        [-0.0786]], device='cuda:0', requires_grad=True) 
node features sum: tensor(13.2318, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.1137, -0.0728, -0.0717, -0.1216, -0.0355, -0.1028, -0.0256,  0.0122,
          0.1221, -0.1354,  0.0942,  0.1381, -0.1021,  0.1042, -0.0997,  0.1341,
         -0.1143, -0.0042, -0.0173,  0.0120,  0.1285, -0.1027, -0.0776,  0.0561,
         -0.1215, -0.1411,  0.0110,  0.0671,  0.0475, -0.1115,  0.0969,  0.1450,
         -0.1077, -0.1086,  0.0695,  0.0019,  0.0526, -0.0710,  0.0132, -0.0291,
          0.0923,  0.0765,  0.0683, -0.0333, -0.1370, -0.0773, -0.0530, -0.0769,
          0.0596,  0.1139,  0.0294,  0.0053,  0.1355,  0.1404, -0.0983,  0.1064,
         -0.0928, -0.0767,  0.0161,  0.0956,  0.0305,  0.1447,  0.0239,  0.0182,
         -0.1489,  0.0469, -0.1141, -0.0535, -0.0527,  0.1005, -0.0818,  0.1002,
         -0.0841,  0.0779,  0.0009, -0.0318,  0.1191, -0.0505,  0.0727,  0.1510,
          0.1184, -0.1078,  0.0322,  0.1108,  0.1430, -0.0902, -0.0374,  0.1296,
          0.0493, -0.0879,  0.0649, -0.0624,  0.1173, -0.0124, -0.0038,  0.0718,
          0.0289,  0.0531,  0.0412, -0.0679,  0.1186,  0.0904, -0.0897, -0.0754,
         -0.0132, -0.0967, -0.0319, -0.0940, -0.0605,  0.1130,  0.1351,  0.0581,
          0.1478,  0.1335,  0.0667, -0.1313, -0.0325,  0.0893,  0.0538,  0.1469,
          0.1456,  0.0105,  0.0654,  0.1125,  0.1118,  0.0681,  0.1049, -0.1075,
         -0.0560, -0.1247, -0.1280, -0.1010, -0.0413, -0.0508, -0.0630, -0.0956,
          0.0608,  0.0691,  0.1127, -0.1165, -0.0781,  0.1361,  0.0733, -0.0148,
         -0.1039,  0.0774, -0.1334, -0.1066,  0.1371,  0.0467,  0.0629,  0.0562,
          0.0055, -0.0880, -0.1385, -0.1086,  0.0380,  0.0225, -0.0902,  0.0988,
          0.0712, -0.1045, -0.1496, -0.1239, -0.1507,  0.0693,  0.0778, -0.0212,
          0.0510, -0.0005, -0.1343, -0.0064, -0.0755,  0.1348,  0.0658,  0.0108,
          0.1413, -0.0263, -0.1369, -0.0773, -0.0868, -0.0397,  0.0329,  0.1473,
         -0.1035,  0.0435, -0.1013, -0.1003,  0.0128,  0.0307,  0.0525, -0.0798,
          0.1496, -0.0920, -0.0922,  0.1415,  0.0063,  0.1008,  0.0215,  0.1394,
         -0.0816,  0.0344, -0.0728,  0.1147,  0.0910, -0.0689, -0.1029, -0.1272,
          0.0927,  0.1418, -0.0930, -0.1342,  0.0643,  0.1380, -0.0719, -0.0798,
          0.0005,  0.1415, -0.0087, -0.0152,  0.0961, -0.0544,  0.1086, -0.0522,
         -0.0500,  0.1138,  0.1243, -0.1232,  0.0969, -0.1387,  0.1053, -0.0349,
         -0.0234, -0.1400,  0.0531,  0.0838, -0.0874, -0.1356, -0.0675,  0.0635,
         -0.0979,  0.0871,  0.0619,  0.1104, -0.1143, -0.0990,  0.0917, -0.0747,
          0.0334,  0.0402,  0.1284,  0.1175, -0.0302,  0.0661, -0.0164, -0.0211]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1137, -0.0728, -0.0717, -0.1216, -0.0355, -0.1028, -0.0256,  0.0122,
          0.1221, -0.1354,  0.0942,  0.1381, -0.1021,  0.1042, -0.0997,  0.1341,
         -0.1143, -0.0042, -0.0173,  0.0120,  0.1285, -0.1027, -0.0776,  0.0561,
         -0.1215, -0.1411,  0.0110,  0.0671,  0.0475, -0.1115,  0.0969,  0.1450,
         -0.1077, -0.1086,  0.0695,  0.0019,  0.0526, -0.0710,  0.0132, -0.0291,
          0.0923,  0.0765,  0.0683, -0.0333, -0.1370, -0.0773, -0.0530, -0.0769,
          0.0596,  0.1139,  0.0294,  0.0053,  0.1355,  0.1404, -0.0983,  0.1064,
         -0.0928, -0.0767,  0.0161,  0.0956,  0.0305,  0.1447,  0.0239,  0.0182,
         -0.1489,  0.0469, -0.1141, -0.0535, -0.0527,  0.1005, -0.0818,  0.1002,
         -0.0841,  0.0779,  0.0009, -0.0318,  0.1191, -0.0505,  0.0727,  0.1510,
          0.1184, -0.1078,  0.0322,  0.1108,  0.1430, -0.0902, -0.0374,  0.1296,
          0.0493, -0.0879,  0.0649, -0.0624,  0.1173, -0.0124, -0.0038,  0.0718,
          0.0289,  0.0531,  0.0412, -0.0679,  0.1186,  0.0904, -0.0897, -0.0754,
         -0.0132, -0.0967, -0.0319, -0.0940, -0.0605,  0.1130,  0.1351,  0.0581,
          0.1478,  0.1335,  0.0667, -0.1313, -0.0325,  0.0893,  0.0538,  0.1469,
          0.1456,  0.0105,  0.0654,  0.1125,  0.1118,  0.0681,  0.1049, -0.1075,
         -0.0560, -0.1247, -0.1280, -0.1010, -0.0413, -0.0508, -0.0630, -0.0956,
          0.0608,  0.0691,  0.1127, -0.1165, -0.0781,  0.1361,  0.0733, -0.0148,
         -0.1039,  0.0774, -0.1334, -0.1066,  0.1371,  0.0467,  0.0629,  0.0562,
          0.0055, -0.0880, -0.1385, -0.1086,  0.0380,  0.0225, -0.0902,  0.0988,
          0.0712, -0.1045, -0.1496, -0.1239, -0.1507,  0.0693,  0.0778, -0.0212,
          0.0510, -0.0005, -0.1343, -0.0064, -0.0755,  0.1348,  0.0658,  0.0108,
          0.1413, -0.0263, -0.1369, -0.0773, -0.0868, -0.0397,  0.0329,  0.1473,
         -0.1035,  0.0435, -0.1013, -0.1003,  0.0128,  0.0307,  0.0525, -0.0798,
          0.1496, -0.0920, -0.0922,  0.1415,  0.0063,  0.1008,  0.0215,  0.1394,
         -0.0816,  0.0344, -0.0728,  0.1147,  0.0910, -0.0689, -0.1029, -0.1272,
          0.0927,  0.1418, -0.0930, -0.1342,  0.0643,  0.1380, -0.0719, -0.0798,
          0.0005,  0.1415, -0.0087, -0.0152,  0.0961, -0.0544,  0.1086, -0.0522,
         -0.0500,  0.1138,  0.1243, -0.1232,  0.0969, -0.1387,  0.1053, -0.0349,
         -0.0234, -0.1400,  0.0531,  0.0838, -0.0874, -0.1356, -0.0675,  0.0635,
         -0.0979,  0.0871,  0.0619,  0.1104, -0.1143, -0.0990,  0.0917, -0.0747,
          0.0334,  0.0402,  0.1284,  0.1175, -0.0302,  0.0661, -0.0164, -0.0211]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0611,  0.0700, -0.1229,  ..., -0.0158,  0.0102,  0.0577],
        [ 0.0564, -0.0851, -0.0980,  ..., -0.0324,  0.0695, -0.0585],
        [ 0.0469, -0.0271, -0.0108,  ...,  0.0082,  0.0758, -0.1101],
        ...,
        [-0.1104,  0.0638, -0.0933,  ...,  0.0559,  0.0810,  0.1018],
        [ 0.0966, -0.0099, -0.1148,  ..., -0.0904,  0.0644, -0.0345],
        [ 0.1065,  0.0013, -0.0225,  ..., -0.0935, -0.0343,  0.0569]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0611,  0.0700, -0.1229,  ..., -0.0158,  0.0102,  0.0577],
        [ 0.0564, -0.0851, -0.0980,  ..., -0.0324,  0.0695, -0.0585],
        [ 0.0469, -0.0271, -0.0108,  ...,  0.0082,  0.0758, -0.1101],
        ...,
        [-0.1104,  0.0638, -0.0933,  ...,  0.0559,  0.0810,  0.1018],
        [ 0.0966, -0.0099, -0.1148,  ..., -0.0904,  0.0644, -0.0345],
        [ 0.1065,  0.0013, -0.0225,  ..., -0.0935, -0.0343,  0.0569]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0702,  0.0545,  0.1731,  ..., -0.0093,  0.0629, -0.1595],
        [-0.1557, -0.1161, -0.1289,  ...,  0.0600, -0.0682,  0.0170],
        [ 0.0192, -0.1240, -0.0454,  ...,  0.1239,  0.1146,  0.1325],
        ...,
        [ 0.0440,  0.1174,  0.0405,  ..., -0.1018,  0.0595, -0.0179],
        [-0.1490, -0.0816,  0.1215,  ..., -0.0251, -0.0142, -0.1245],
        [-0.0915, -0.0204,  0.1571,  ...,  0.1586,  0.1323, -0.0539]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0702,  0.0545,  0.1731,  ..., -0.0093,  0.0629, -0.1595],
        [-0.1557, -0.1161, -0.1289,  ...,  0.0600, -0.0682,  0.0170],
        [ 0.0192, -0.1240, -0.0454,  ...,  0.1239,  0.1146,  0.1325],
        ...,
        [ 0.0440,  0.1174,  0.0405,  ..., -0.1018,  0.0595, -0.0179],
        [-0.1490, -0.0816,  0.1215,  ..., -0.0251, -0.0142, -0.1245],
        [-0.0915, -0.0204,  0.1571,  ...,  0.1586,  0.1323, -0.0539]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0978,  0.1804, -0.0602,  ...,  0.1129, -0.0133, -0.0627],
        [ 0.0800, -0.2114, -0.1562,  ...,  0.1831, -0.0616, -0.2342],
        [-0.1786,  0.0641, -0.2481,  ...,  0.1202,  0.0725, -0.0170],
        ...,
        [-0.0013, -0.0434,  0.0701,  ...,  0.1321, -0.0083, -0.2248],
        [ 0.2116, -0.1023,  0.0230,  ..., -0.0039, -0.1047,  0.2096],
        [ 0.0682,  0.0140, -0.0810,  ...,  0.0975,  0.1586,  0.2045]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0978,  0.1804, -0.0602,  ...,  0.1129, -0.0133, -0.0627],
        [ 0.0800, -0.2114, -0.1562,  ...,  0.1831, -0.0616, -0.2342],
        [-0.1786,  0.0641, -0.2481,  ...,  0.1202,  0.0725, -0.0170],
        ...,
        [-0.0013, -0.0434,  0.0701,  ...,  0.1321, -0.0083, -0.2248],
        [ 0.2116, -0.1023,  0.0230,  ..., -0.0039, -0.1047,  0.2096],
        [ 0.0682,  0.0140, -0.0810,  ...,  0.0975,  0.1586,  0.2045]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2666],
        [ 0.2611],
        [ 0.3889],
        [ 0.4207],
        [ 0.1978],
        [-0.0100],
        [ 0.2063],
        [ 0.2932],
        [-0.3646],
        [ 0.2809],
        [ 0.0752],
        [-0.3430],
        [ 0.1350],
        [-0.0710],
        [-0.0863],
        [ 0.3223],
        [ 0.0274],
        [-0.3286],
        [-0.2763],
        [-0.0142],
        [ 0.0909],
        [-0.1106],
        [-0.3862],
        [ 0.0583],
        [ 0.0658],
        [-0.3289],
        [-0.1379],
        [ 0.2815],
        [-0.0903],
        [-0.1948],
        [ 0.1016],
        [-0.1419]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2666],
        [ 0.2611],
        [ 0.3889],
        [ 0.4207],
        [ 0.1978],
        [-0.0100],
        [ 0.2063],
        [ 0.2932],
        [-0.3646],
        [ 0.2809],
        [ 0.0752],
        [-0.3430],
        [ 0.1350],
        [-0.0710],
        [-0.0863],
        [ 0.3223],
        [ 0.0274],
        [-0.3286],
        [-0.2763],
        [-0.0142],
        [ 0.0909],
        [-0.1106],
        [-0.3862],
        [ 0.0583],
        [ 0.0658],
        [-0.3289],
        [-0.1379],
        [ 0.2815],
        [-0.0903],
        [-0.1948],
        [ 0.1016],
        [-0.1419]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0531,  0.0518, -0.0513,  0.1068, -0.1056, -0.0699,  0.1271, -0.1009,
          0.0107, -0.0425,  0.0687, -0.0972, -0.1110,  0.1311,  0.0890,  0.0542,
          0.0136,  0.1454,  0.1276,  0.0832, -0.0521,  0.0739, -0.0873, -0.0263,
         -0.0113, -0.0364, -0.1525, -0.0188, -0.0211,  0.0342, -0.0035, -0.0515,
         -0.0821, -0.0215,  0.0843,  0.0165,  0.0748, -0.1348,  0.1052,  0.0428,
         -0.0364,  0.0460,  0.0552,  0.0029, -0.1343, -0.0930, -0.0970,  0.0752,
          0.1463,  0.1326,  0.0398,  0.0224,  0.1056,  0.0851, -0.1406,  0.0991,
          0.1017, -0.0908,  0.0409, -0.1194, -0.0663, -0.1513, -0.0747, -0.0999,
          0.1379, -0.0842, -0.0101, -0.0284, -0.1284,  0.0488, -0.1190, -0.0282,
          0.1358,  0.0219, -0.0528,  0.0639,  0.1089,  0.0280, -0.0341, -0.0042,
         -0.0756,  0.0050, -0.1338, -0.0138, -0.0945, -0.1141,  0.0725,  0.0446,
          0.0439,  0.0242,  0.1023,  0.0177, -0.1092,  0.1456, -0.1307, -0.0781,
          0.0766,  0.0136,  0.0717, -0.0919, -0.0746,  0.0685, -0.0758, -0.1111,
         -0.1526,  0.0232, -0.0948,  0.0802,  0.0468, -0.0618,  0.1135, -0.0385,
          0.1081,  0.0468, -0.1222,  0.0901,  0.0455, -0.0576,  0.0644,  0.0779,
         -0.1149,  0.0392,  0.0444, -0.0413, -0.1525,  0.1466, -0.0573, -0.1294,
         -0.0336,  0.1288, -0.0382, -0.1105,  0.0898, -0.1303, -0.1446, -0.1102,
          0.0971, -0.0394, -0.1456, -0.1199, -0.1019,  0.0478, -0.1313, -0.1057,
         -0.0399, -0.1388,  0.0143, -0.1019,  0.0208, -0.0909, -0.1124,  0.0649,
          0.0164, -0.1422,  0.0726,  0.0422,  0.0461, -0.0271, -0.1369,  0.0876,
         -0.0279,  0.1222, -0.0139, -0.0252,  0.0904,  0.0579, -0.1137,  0.0940,
          0.1067,  0.1227,  0.1234, -0.0904, -0.0833,  0.1490,  0.1467, -0.1357,
         -0.0732,  0.1222,  0.0609, -0.0267,  0.1164, -0.0733,  0.0056,  0.1357,
          0.0190,  0.1181, -0.0674,  0.0688, -0.1339,  0.1376, -0.0144,  0.1299,
          0.0786,  0.0090,  0.0492, -0.0455, -0.0860, -0.0416,  0.0411,  0.0959,
         -0.0368,  0.0779,  0.1485, -0.0756, -0.1200, -0.1404,  0.0542, -0.0665,
         -0.1213,  0.1316, -0.0179,  0.0490, -0.1184,  0.1465,  0.1383,  0.0747,
         -0.0314, -0.0200,  0.0170,  0.1477, -0.1010, -0.0566,  0.0204, -0.0362,
         -0.0107, -0.0891,  0.1283,  0.0122,  0.0457,  0.0886, -0.0778, -0.0237,
          0.0112, -0.0139,  0.0130,  0.0664, -0.1064,  0.1289, -0.1134,  0.0663,
         -0.0028,  0.1239,  0.0982,  0.0846, -0.1361, -0.0363, -0.0408, -0.1163,
          0.1356, -0.1292,  0.0033,  0.0652, -0.0185, -0.0824,  0.0792,  0.1153]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0531,  0.0518, -0.0513,  0.1068, -0.1056, -0.0699,  0.1271, -0.1009,
          0.0107, -0.0425,  0.0687, -0.0972, -0.1110,  0.1311,  0.0890,  0.0542,
          0.0136,  0.1454,  0.1276,  0.0832, -0.0521,  0.0739, -0.0873, -0.0263,
         -0.0113, -0.0364, -0.1525, -0.0188, -0.0211,  0.0342, -0.0035, -0.0515,
         -0.0821, -0.0215,  0.0843,  0.0165,  0.0748, -0.1348,  0.1052,  0.0428,
         -0.0364,  0.0460,  0.0552,  0.0029, -0.1343, -0.0930, -0.0970,  0.0752,
          0.1463,  0.1326,  0.0398,  0.0224,  0.1056,  0.0851, -0.1406,  0.0991,
          0.1017, -0.0908,  0.0409, -0.1194, -0.0663, -0.1513, -0.0747, -0.0999,
          0.1379, -0.0842, -0.0101, -0.0284, -0.1284,  0.0488, -0.1190, -0.0282,
          0.1358,  0.0219, -0.0528,  0.0639,  0.1089,  0.0280, -0.0341, -0.0042,
         -0.0756,  0.0050, -0.1338, -0.0138, -0.0945, -0.1141,  0.0725,  0.0446,
          0.0439,  0.0242,  0.1023,  0.0177, -0.1092,  0.1456, -0.1307, -0.0781,
          0.0766,  0.0136,  0.0717, -0.0919, -0.0746,  0.0685, -0.0758, -0.1111,
         -0.1526,  0.0232, -0.0948,  0.0802,  0.0468, -0.0618,  0.1135, -0.0385,
          0.1081,  0.0468, -0.1222,  0.0901,  0.0455, -0.0576,  0.0644,  0.0779,
         -0.1149,  0.0392,  0.0444, -0.0413, -0.1525,  0.1466, -0.0573, -0.1294,
         -0.0336,  0.1288, -0.0382, -0.1105,  0.0898, -0.1303, -0.1446, -0.1102,
          0.0971, -0.0394, -0.1456, -0.1199, -0.1019,  0.0478, -0.1313, -0.1057,
         -0.0399, -0.1388,  0.0143, -0.1019,  0.0208, -0.0909, -0.1124,  0.0649,
          0.0164, -0.1422,  0.0726,  0.0422,  0.0461, -0.0271, -0.1369,  0.0876,
         -0.0279,  0.1222, -0.0139, -0.0252,  0.0904,  0.0579, -0.1137,  0.0940,
          0.1067,  0.1227,  0.1234, -0.0904, -0.0833,  0.1490,  0.1467, -0.1357,
         -0.0732,  0.1222,  0.0609, -0.0267,  0.1164, -0.0733,  0.0056,  0.1357,
          0.0190,  0.1181, -0.0674,  0.0688, -0.1339,  0.1376, -0.0144,  0.1299,
          0.0786,  0.0090,  0.0492, -0.0455, -0.0860, -0.0416,  0.0411,  0.0959,
         -0.0368,  0.0779,  0.1485, -0.0756, -0.1200, -0.1404,  0.0542, -0.0665,
         -0.1213,  0.1316, -0.0179,  0.0490, -0.1184,  0.1465,  0.1383,  0.0747,
         -0.0314, -0.0200,  0.0170,  0.1477, -0.1010, -0.0566,  0.0204, -0.0362,
         -0.0107, -0.0891,  0.1283,  0.0122,  0.0457,  0.0886, -0.0778, -0.0237,
          0.0112, -0.0139,  0.0130,  0.0664, -0.1064,  0.1289, -0.1134,  0.0663,
         -0.0028,  0.1239,  0.0982,  0.0846, -0.1361, -0.0363, -0.0408, -0.1163,
          0.1356, -0.1292,  0.0033,  0.0652, -0.0185, -0.0824,  0.0792,  0.1153]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0191, -0.0705, -0.0979,  ..., -0.0166, -0.1245,  0.0069],
        [ 0.0039, -0.0803,  0.0528,  ...,  0.0717, -0.0846,  0.0344],
        [-0.0871, -0.1037,  0.0938,  ...,  0.0369, -0.0119,  0.0206],
        ...,
        [ 0.0795,  0.0698,  0.0402,  ...,  0.0587,  0.0822, -0.0267],
        [ 0.1077,  0.0359,  0.0572,  ..., -0.1105, -0.0065,  0.0719],
        [ 0.0128, -0.0611, -0.0711,  ..., -0.0848, -0.0876,  0.0803]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0191, -0.0705, -0.0979,  ..., -0.0166, -0.1245,  0.0069],
        [ 0.0039, -0.0803,  0.0528,  ...,  0.0717, -0.0846,  0.0344],
        [-0.0871, -0.1037,  0.0938,  ...,  0.0369, -0.0119,  0.0206],
        ...,
        [ 0.0795,  0.0698,  0.0402,  ...,  0.0587,  0.0822, -0.0267],
        [ 0.1077,  0.0359,  0.0572,  ..., -0.1105, -0.0065,  0.0719],
        [ 0.0128, -0.0611, -0.0711,  ..., -0.0848, -0.0876,  0.0803]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0573,  0.1021,  0.1550,  ..., -0.0466,  0.1167,  0.0244],
        [ 0.0137, -0.1267,  0.0229,  ..., -0.0852,  0.1025,  0.0380],
        [ 0.1058,  0.0752, -0.0427,  ..., -0.1650,  0.0358,  0.0312],
        ...,
        [-0.1758, -0.0354, -0.1573,  ..., -0.0080,  0.1541, -0.0889],
        [-0.1602, -0.1387,  0.1464,  ...,  0.1610,  0.0706,  0.0604],
        [-0.0286, -0.1193, -0.0589,  ...,  0.0230,  0.1697,  0.0350]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0573,  0.1021,  0.1550,  ..., -0.0466,  0.1167,  0.0244],
        [ 0.0137, -0.1267,  0.0229,  ..., -0.0852,  0.1025,  0.0380],
        [ 0.1058,  0.0752, -0.0427,  ..., -0.1650,  0.0358,  0.0312],
        ...,
        [-0.1758, -0.0354, -0.1573,  ..., -0.0080,  0.1541, -0.0889],
        [-0.1602, -0.1387,  0.1464,  ...,  0.1610,  0.0706,  0.0604],
        [-0.0286, -0.1193, -0.0589,  ...,  0.0230,  0.1697,  0.0350]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1353,  0.2261,  0.1257,  ...,  0.0591,  0.0424,  0.1598],
        [-0.1752, -0.2475, -0.1387,  ...,  0.1178, -0.1162,  0.0394],
        [ 0.1917,  0.1608, -0.2469,  ...,  0.0483, -0.1708,  0.2180],
        ...,
        [-0.0149,  0.0206, -0.1288,  ..., -0.2298, -0.0611, -0.1039],
        [ 0.1380,  0.2077,  0.1881,  ...,  0.0369,  0.2029, -0.0603],
        [-0.0706, -0.0423, -0.2235,  ..., -0.0689, -0.1865,  0.0959]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1353,  0.2261,  0.1257,  ...,  0.0591,  0.0424,  0.1598],
        [-0.1752, -0.2475, -0.1387,  ...,  0.1178, -0.1162,  0.0394],
        [ 0.1917,  0.1608, -0.2469,  ...,  0.0483, -0.1708,  0.2180],
        ...,
        [-0.0149,  0.0206, -0.1288,  ..., -0.2298, -0.0611, -0.1039],
        [ 0.1380,  0.2077,  0.1881,  ...,  0.0369,  0.2029, -0.0603],
        [-0.0706, -0.0423, -0.2235,  ..., -0.0689, -0.1865,  0.0959]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.4238],
        [ 0.3540],
        [-0.3977],
        [-0.3568],
        [-0.1875],
        [ 0.3529],
        [-0.2777],
        [-0.1002],
        [-0.2042],
        [ 0.3053],
        [ 0.2369],
        [ 0.3382],
        [ 0.0989],
        [-0.1474],
        [ 0.3580],
        [ 0.3526],
        [ 0.3638],
        [-0.2001],
        [ 0.2639],
        [-0.2474],
        [ 0.1363],
        [ 0.1126],
        [-0.0463],
        [-0.0667],
        [-0.0902],
        [ 0.1499],
        [ 0.0396],
        [-0.1721],
        [ 0.2638],
        [-0.4250],
        [-0.3611],
        [-0.3058]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.4238],
        [ 0.3540],
        [-0.3977],
        [-0.3568],
        [-0.1875],
        [ 0.3529],
        [-0.2777],
        [-0.1002],
        [-0.2042],
        [ 0.3053],
        [ 0.2369],
        [ 0.3382],
        [ 0.0989],
        [-0.1474],
        [ 0.3580],
        [ 0.3526],
        [ 0.3638],
        [-0.2001],
        [ 0.2639],
        [-0.2474],
        [ 0.1363],
        [ 0.1126],
        [-0.0463],
        [-0.0667],
        [-0.0902],
        [ 0.1499],
        [ 0.0396],
        [-0.1721],
        [ 0.2638],
        [-0.4250],
        [-0.3611],
        [-0.3058]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(69.0567, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.5389, device='cuda:0')



h[100].sum tensor(-4.6764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7995, device='cuda:0')



h[200].sum tensor(1.8072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(1.8547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3275.7349, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0006,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(15840.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-13.7808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-12.3866, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-8.5629, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0019],
        [-0.0023],
        [-0.0034],
        ...,
        [-0.0005],
        [-0.0005],
        [-0.0004]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-48.3830, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.0019],
        [-0.0023],
        [-0.0034],
        ...,
        [-0.0005],
        [-0.0005],
        [-0.0004]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(126.4534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.8983, device='cuda:0')



h[100].sum tensor(-4.8090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.7700, device='cuda:0')



h[200].sum tensor(19.3800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15577.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0237, 0.0000, 0.0158,  ..., 0.0000, 0.0207, 0.0000],
        [0.0050, 0.0000, 0.0033,  ..., 0.0000, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(95760.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1914.3933, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(134.6865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(412.7915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(29.0418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.9570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1737],
        [0.1065],
        [0.0652],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(10205.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0019],
        [-0.0023],
        [-0.0034],
        ...,
        [-0.0005],
        [-0.0005],
        [-0.0004]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 45, in <module>
    hpmesh = numpy.array(hpmesh)
  File "cupy/_core/core.pyx", line 1473, in cupy._core.core._ndarray_base.__array__
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.

real	0m15.516s
user	0m9.549s
sys	0m5.752s
