0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-adc0bb57-5f8a-ba9d-04e2-dbaa9f7c20f1)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        A2:17:1D:79:E5:47:38:13:77:F2:EE:66:EA:5D:DB:A0:A1:CA:F4:E7
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jan  9 17:09:08 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:1D:00.0 Off |                    0 |
| N/A   30C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089926656

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ab13e05e880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m3.822s
user	0m2.353s
sys	0m0.713s




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 6507, 6507, 6507],
        [   1,    2,    3,  ..., 6219, 6794, 6795]]) 

edge_index shape
 torch.Size([2, 1175277])
graph: Graph(num_nodes=6796, num_edges=1175277,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 6507, 6507, 6507], device='cuda:0'), tensor([   1,    2,    3,  ..., 6219, 6794, 6795], device='cuda:0'))

number of nodes: 6796

number of edges: 2350554

node features (random input): tensor([[-1.5450],
        [ 0.7880],
        [-0.2958],
        ...,
        [ 0.7458],
        [ 0.6040],
        [ 0.8628]], device='cuda:0', requires_grad=True) 
node features sum: tensor(197.2616, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(2350554., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 399

In degrees of node 234: 399





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLBhaGcnReNewestweight7N4SecNei

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (linear1): Linear(in_features=2350554, out_features=256, bias=True)
  (linear2): Linear(in_features=256, out_features=2350554, bias=True)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 1205878235

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[ 0.0033, -0.1507,  0.0570,  0.1160,  0.1062,  0.1371,  0.0290,  0.0346,
          0.0924, -0.0834,  0.0822, -0.1351,  0.0895, -0.1454, -0.0809, -0.0180,
          0.0787, -0.0233,  0.1316,  0.1417, -0.0994, -0.0762,  0.1398,  0.0283,
         -0.0403, -0.0050, -0.0484, -0.1243,  0.0739,  0.1242,  0.0074, -0.0773,
          0.0197,  0.1450,  0.1044,  0.0244, -0.0807, -0.0094, -0.1208,  0.1047,
          0.0549,  0.0401,  0.0422, -0.0657,  0.0660,  0.0080, -0.0108,  0.0577,
         -0.0993, -0.0597, -0.0010,  0.0352, -0.0104, -0.0606, -0.0222, -0.0478,
          0.0916, -0.0970, -0.0435, -0.0558, -0.0483,  0.1323,  0.0569,  0.1432,
         -0.0722, -0.0432,  0.0375,  0.0876, -0.1379,  0.0259,  0.0095,  0.0370,
          0.1295,  0.0162,  0.1176, -0.0111, -0.0743,  0.0856, -0.1511, -0.0755,
          0.0365, -0.1354, -0.1178, -0.0693, -0.1145, -0.1032,  0.0061,  0.0846,
         -0.0147, -0.0594, -0.0986, -0.0860, -0.0256,  0.1496, -0.1385, -0.0997,
         -0.1450, -0.0345, -0.0190,  0.1391,  0.1018, -0.0069, -0.0397, -0.1493,
          0.0876, -0.0404,  0.0492, -0.0883, -0.1430,  0.0471, -0.1271, -0.0117,
         -0.1466, -0.1399, -0.0116,  0.0017, -0.1172, -0.0431, -0.0227,  0.0509,
         -0.1320, -0.0867, -0.0396,  0.0748,  0.0987,  0.0347, -0.0632,  0.1305,
         -0.0356,  0.0243, -0.1181, -0.1441, -0.0390,  0.1146,  0.1320, -0.0086,
         -0.0681, -0.0071, -0.0988, -0.1103, -0.0395, -0.0165, -0.0563,  0.0723,
          0.1506, -0.0121, -0.0358,  0.0749, -0.1484,  0.0009,  0.1518, -0.1028,
         -0.0679,  0.0839,  0.0556,  0.1348, -0.0038, -0.0613, -0.1360, -0.0260,
          0.0115,  0.0165,  0.0333,  0.1228, -0.0040, -0.0841,  0.1521,  0.0575,
          0.0991, -0.1522, -0.0356,  0.1144,  0.1483, -0.0528,  0.1382, -0.0015,
         -0.0293, -0.0474,  0.0394, -0.1137,  0.0863,  0.0734, -0.1111,  0.0684,
          0.0985, -0.0995,  0.0778,  0.0958,  0.0257, -0.1016, -0.0843,  0.0931,
         -0.0863, -0.0813,  0.0114, -0.1494,  0.0181,  0.0324,  0.1329,  0.1438,
         -0.1077,  0.0194,  0.0732, -0.1497, -0.0974, -0.0341, -0.1221, -0.1407,
          0.0813,  0.0146,  0.0979, -0.0411,  0.0983,  0.0539, -0.0936,  0.0913,
         -0.1192,  0.0494, -0.0904, -0.0218, -0.0407, -0.0390,  0.1348, -0.1515,
         -0.0803,  0.0767, -0.0122, -0.1236,  0.0852,  0.0959,  0.0660, -0.1050,
          0.0866,  0.0757,  0.1071, -0.0850,  0.1158, -0.0846,  0.1313, -0.1166,
         -0.0530, -0.0156,  0.1489, -0.1189, -0.0309, -0.0522,  0.0828,  0.0555,
         -0.1265, -0.0162, -0.0439, -0.0498,  0.0252, -0.0174,  0.1216, -0.1024]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0033, -0.1507,  0.0570,  0.1160,  0.1062,  0.1371,  0.0290,  0.0346,
          0.0924, -0.0834,  0.0822, -0.1351,  0.0895, -0.1454, -0.0809, -0.0180,
          0.0787, -0.0233,  0.1316,  0.1417, -0.0994, -0.0762,  0.1398,  0.0283,
         -0.0403, -0.0050, -0.0484, -0.1243,  0.0739,  0.1242,  0.0074, -0.0773,
          0.0197,  0.1450,  0.1044,  0.0244, -0.0807, -0.0094, -0.1208,  0.1047,
          0.0549,  0.0401,  0.0422, -0.0657,  0.0660,  0.0080, -0.0108,  0.0577,
         -0.0993, -0.0597, -0.0010,  0.0352, -0.0104, -0.0606, -0.0222, -0.0478,
          0.0916, -0.0970, -0.0435, -0.0558, -0.0483,  0.1323,  0.0569,  0.1432,
         -0.0722, -0.0432,  0.0375,  0.0876, -0.1379,  0.0259,  0.0095,  0.0370,
          0.1295,  0.0162,  0.1176, -0.0111, -0.0743,  0.0856, -0.1511, -0.0755,
          0.0365, -0.1354, -0.1178, -0.0693, -0.1145, -0.1032,  0.0061,  0.0846,
         -0.0147, -0.0594, -0.0986, -0.0860, -0.0256,  0.1496, -0.1385, -0.0997,
         -0.1450, -0.0345, -0.0190,  0.1391,  0.1018, -0.0069, -0.0397, -0.1493,
          0.0876, -0.0404,  0.0492, -0.0883, -0.1430,  0.0471, -0.1271, -0.0117,
         -0.1466, -0.1399, -0.0116,  0.0017, -0.1172, -0.0431, -0.0227,  0.0509,
         -0.1320, -0.0867, -0.0396,  0.0748,  0.0987,  0.0347, -0.0632,  0.1305,
         -0.0356,  0.0243, -0.1181, -0.1441, -0.0390,  0.1146,  0.1320, -0.0086,
         -0.0681, -0.0071, -0.0988, -0.1103, -0.0395, -0.0165, -0.0563,  0.0723,
          0.1506, -0.0121, -0.0358,  0.0749, -0.1484,  0.0009,  0.1518, -0.1028,
         -0.0679,  0.0839,  0.0556,  0.1348, -0.0038, -0.0613, -0.1360, -0.0260,
          0.0115,  0.0165,  0.0333,  0.1228, -0.0040, -0.0841,  0.1521,  0.0575,
          0.0991, -0.1522, -0.0356,  0.1144,  0.1483, -0.0528,  0.1382, -0.0015,
         -0.0293, -0.0474,  0.0394, -0.1137,  0.0863,  0.0734, -0.1111,  0.0684,
          0.0985, -0.0995,  0.0778,  0.0958,  0.0257, -0.1016, -0.0843,  0.0931,
         -0.0863, -0.0813,  0.0114, -0.1494,  0.0181,  0.0324,  0.1329,  0.1438,
         -0.1077,  0.0194,  0.0732, -0.1497, -0.0974, -0.0341, -0.1221, -0.1407,
          0.0813,  0.0146,  0.0979, -0.0411,  0.0983,  0.0539, -0.0936,  0.0913,
         -0.1192,  0.0494, -0.0904, -0.0218, -0.0407, -0.0390,  0.1348, -0.1515,
         -0.0803,  0.0767, -0.0122, -0.1236,  0.0852,  0.0959,  0.0660, -0.1050,
          0.0866,  0.0757,  0.1071, -0.0850,  0.1158, -0.0846,  0.1313, -0.1166,
         -0.0530, -0.0156,  0.1489, -0.1189, -0.0309, -0.0522,  0.0828,  0.0555,
         -0.1265, -0.0162, -0.0439, -0.0498,  0.0252, -0.0174,  0.1216, -0.1024]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name linear1.weight 
shape:
 torch.Size([256, 2350554]) 
grad:
 True 
date:
 tensor([[ 3.4721e-04,  3.2427e-04,  3.2446e-04,  ...,  1.3806e-04,
          2.2778e-04, -5.6960e-04],
        [ 6.2141e-04, -2.7258e-04,  2.4611e-04,  ..., -4.6239e-04,
         -4.9443e-04, -7.9352e-05],
        [-4.6519e-04, -1.8263e-04, -5.5889e-04,  ..., -1.3105e-04,
          4.7892e-04,  4.4473e-04],
        ...,
        [ 3.4126e-07,  6.1613e-04, -7.5421e-05,  ..., -1.7572e-04,
          2.5465e-04,  4.1541e-04],
        [-5.7838e-04,  4.7932e-04,  1.5350e-04,  ..., -3.1498e-04,
          5.6693e-04, -2.5887e-04],
        [-5.4368e-04,  2.7716e-04, -2.4169e-05,  ...,  2.4244e-04,
         -6.5131e-04, -2.0370e-04]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 3.4721e-04,  3.2427e-04,  3.2446e-04,  ...,  1.3806e-04,
          2.2778e-04, -5.6960e-04],
        [ 6.2141e-04, -2.7258e-04,  2.4611e-04,  ..., -4.6239e-04,
         -4.9443e-04, -7.9352e-05],
        [-4.6519e-04, -1.8263e-04, -5.5889e-04,  ..., -1.3105e-04,
          4.7892e-04,  4.4473e-04],
        ...,
        [ 3.4126e-07,  6.1613e-04, -7.5421e-05,  ..., -1.7572e-04,
          2.5465e-04,  4.1541e-04],
        [-5.7838e-04,  4.7932e-04,  1.5350e-04,  ..., -3.1498e-04,
          5.6693e-04, -2.5887e-04],
        [-5.4368e-04,  2.7716e-04, -2.4169e-05,  ...,  2.4244e-04,
         -6.5131e-04, -2.0370e-04]], device='cuda:0', requires_grad=True)

name linear1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([-5.6674e-05,  4.6668e-04,  5.5720e-04,  4.9796e-04, -1.4547e-04,
         1.3303e-04, -2.1433e-04,  5.0851e-04, -3.5295e-04,  6.7858e-05,
         1.6960e-05, -2.8608e-04,  6.4597e-04, -4.1443e-04, -1.8440e-04,
         1.7239e-04, -2.1111e-04,  9.8869e-05, -2.2553e-04,  1.8064e-04,
        -3.1490e-04, -3.7565e-04, -7.7218e-05, -5.5580e-04,  4.2702e-04,
        -4.1178e-04, -2.4774e-04,  4.7235e-04, -4.0357e-04,  4.1176e-04,
        -4.3223e-04,  1.8076e-04, -4.1330e-04,  4.1713e-04, -2.4392e-05,
        -4.7169e-05,  1.0025e-05, -5.4739e-04,  4.9087e-04, -6.2398e-04,
        -3.5137e-04, -1.2138e-06, -4.4860e-05,  3.0447e-04,  4.5004e-04,
         2.3065e-04,  5.4404e-04,  4.5931e-04,  3.8905e-04,  9.4693e-05,
         1.9869e-04, -2.5666e-04, -1.9863e-04,  1.6542e-04, -5.9390e-04,
         3.4492e-04,  4.0246e-04, -5.6152e-04, -3.2986e-04, -1.5088e-04,
         5.7108e-04, -6.5215e-04, -2.6646e-04, -5.8501e-04,  1.2226e-04,
        -1.7531e-04, -3.6236e-04,  4.2752e-04,  2.4419e-04, -3.7534e-04,
        -2.3607e-04, -5.2379e-05, -5.5343e-04,  5.9166e-04, -6.1222e-04,
        -3.0526e-04, -4.9941e-04, -6.1475e-04,  3.3104e-04,  2.3844e-04,
         5.9344e-04,  3.8432e-04,  4.1604e-04,  4.8920e-04,  2.2803e-04,
         4.7003e-04,  1.1600e-04,  3.1868e-04,  4.6055e-04,  2.0742e-04,
        -4.0776e-04,  3.9016e-05, -5.5692e-04,  4.2425e-04, -2.2420e-04,
        -6.4308e-04,  2.6084e-04,  6.1756e-06,  5.4989e-04, -4.5069e-04,
        -3.4698e-04,  1.7472e-04, -3.1344e-04,  2.3406e-04,  4.9060e-04,
         3.1990e-04,  4.0450e-04, -5.5261e-04, -4.2277e-04,  2.2538e-05,
         5.1593e-04,  2.6794e-04, -5.8918e-04,  6.3355e-04, -3.2048e-04,
        -2.0474e-04,  4.1037e-04, -1.4941e-04,  5.4570e-04, -5.7578e-04,
         1.7473e-04,  1.0235e-04,  3.8293e-04,  3.8748e-04,  6.1332e-04,
        -2.6654e-05, -3.7056e-04, -2.6538e-04, -5.9369e-04, -5.0498e-04,
         8.3740e-05,  1.5986e-04, -1.1880e-04,  2.3928e-04,  9.4201e-05,
         1.8034e-04,  2.5378e-04, -1.5409e-04,  3.9635e-04,  5.6712e-04,
        -9.0217e-05, -3.6892e-04,  5.1289e-04, -2.2452e-04, -2.1015e-04,
        -6.1993e-05,  5.5481e-04, -1.7147e-04, -2.8898e-04,  5.8923e-04,
        -2.7809e-04,  2.7618e-04,  4.6072e-04, -5.9267e-05, -6.0222e-04,
        -3.3876e-04,  5.1534e-04, -4.1492e-04,  1.1376e-04,  3.7481e-04,
         2.4864e-04, -5.2614e-06,  4.7175e-04,  2.5161e-05,  5.7659e-05,
         1.4223e-04,  6.4884e-04,  6.2266e-04,  6.1157e-06,  5.5694e-04,
         4.9942e-04, -1.1255e-04,  4.3850e-04, -5.7550e-04, -6.0415e-04,
         4.5274e-04,  1.7843e-04,  5.4746e-05,  6.4693e-04,  4.2126e-04,
        -3.5731e-04, -1.1276e-04,  5.3560e-04,  7.2801e-05,  4.2498e-04,
         1.3813e-04,  5.9489e-04,  6.2948e-04,  4.6806e-04,  3.9199e-04,
        -4.8857e-05, -5.1388e-04, -2.5622e-04, -4.5904e-04,  1.5477e-04,
        -1.6647e-04, -6.2357e-05, -3.3877e-04, -4.9365e-04,  4.0310e-04,
         4.1691e-04,  3.7663e-04, -6.2661e-04, -5.3595e-04, -5.2319e-04,
        -6.3071e-05,  6.9231e-05,  1.4321e-04,  2.1415e-04,  4.4668e-04,
        -5.3844e-04, -2.6334e-04,  3.5150e-04,  7.7501e-05,  6.1122e-04,
         6.4652e-04,  5.4701e-04, -5.6149e-04,  2.2933e-04,  5.1135e-04,
        -2.7439e-04,  1.4211e-04,  3.8629e-04, -2.5802e-05, -7.3243e-05,
        -3.0991e-05,  5.7430e-04, -4.2587e-04, -6.3644e-04, -6.2868e-04,
         2.8424e-04, -1.5180e-04, -1.4841e-04,  2.1535e-06, -5.4997e-04,
         2.0975e-04, -2.5518e-04,  5.7829e-04,  4.6280e-04, -5.3495e-04,
        -3.1370e-04, -2.8975e-04,  3.8608e-05, -7.7515e-05, -1.9661e-04,
         4.9504e-04,  4.3003e-04, -3.8178e-04, -4.5703e-04, -2.8145e-04,
         5.2571e-04,  8.5348e-05, -2.9284e-04,  8.3897e-05, -1.5288e-04,
        -1.9866e-04], device='cuda:0') 
parameter:
 Parameter containing:
tensor([-5.6674e-05,  4.6668e-04,  5.5720e-04,  4.9796e-04, -1.4547e-04,
         1.3303e-04, -2.1433e-04,  5.0851e-04, -3.5295e-04,  6.7858e-05,
         1.6960e-05, -2.8608e-04,  6.4597e-04, -4.1443e-04, -1.8440e-04,
         1.7239e-04, -2.1111e-04,  9.8869e-05, -2.2553e-04,  1.8064e-04,
        -3.1490e-04, -3.7565e-04, -7.7218e-05, -5.5580e-04,  4.2702e-04,
        -4.1178e-04, -2.4774e-04,  4.7235e-04, -4.0357e-04,  4.1176e-04,
        -4.3223e-04,  1.8076e-04, -4.1330e-04,  4.1713e-04, -2.4392e-05,
        -4.7169e-05,  1.0025e-05, -5.4739e-04,  4.9087e-04, -6.2398e-04,
        -3.5137e-04, -1.2138e-06, -4.4860e-05,  3.0447e-04,  4.5004e-04,
         2.3065e-04,  5.4404e-04,  4.5931e-04,  3.8905e-04,  9.4693e-05,
         1.9869e-04, -2.5666e-04, -1.9863e-04,  1.6542e-04, -5.9390e-04,
         3.4492e-04,  4.0246e-04, -5.6152e-04, -3.2986e-04, -1.5088e-04,
         5.7108e-04, -6.5215e-04, -2.6646e-04, -5.8501e-04,  1.2226e-04,
        -1.7531e-04, -3.6236e-04,  4.2752e-04,  2.4419e-04, -3.7534e-04,
        -2.3607e-04, -5.2379e-05, -5.5343e-04,  5.9166e-04, -6.1222e-04,
        -3.0526e-04, -4.9941e-04, -6.1475e-04,  3.3104e-04,  2.3844e-04,
         5.9344e-04,  3.8432e-04,  4.1604e-04,  4.8920e-04,  2.2803e-04,
         4.7003e-04,  1.1600e-04,  3.1868e-04,  4.6055e-04,  2.0742e-04,
        -4.0776e-04,  3.9016e-05, -5.5692e-04,  4.2425e-04, -2.2420e-04,
        -6.4308e-04,  2.6084e-04,  6.1756e-06,  5.4989e-04, -4.5069e-04,
        -3.4698e-04,  1.7472e-04, -3.1344e-04,  2.3406e-04,  4.9060e-04,
         3.1990e-04,  4.0450e-04, -5.5261e-04, -4.2277e-04,  2.2538e-05,
         5.1593e-04,  2.6794e-04, -5.8918e-04,  6.3355e-04, -3.2048e-04,
        -2.0474e-04,  4.1037e-04, -1.4941e-04,  5.4570e-04, -5.7578e-04,
         1.7473e-04,  1.0235e-04,  3.8293e-04,  3.8748e-04,  6.1332e-04,
        -2.6654e-05, -3.7056e-04, -2.6538e-04, -5.9369e-04, -5.0498e-04,
         8.3740e-05,  1.5986e-04, -1.1880e-04,  2.3928e-04,  9.4201e-05,
         1.8034e-04,  2.5378e-04, -1.5409e-04,  3.9635e-04,  5.6712e-04,
        -9.0217e-05, -3.6892e-04,  5.1289e-04, -2.2452e-04, -2.1015e-04,
        -6.1993e-05,  5.5481e-04, -1.7147e-04, -2.8898e-04,  5.8923e-04,
        -2.7809e-04,  2.7618e-04,  4.6072e-04, -5.9267e-05, -6.0222e-04,
        -3.3876e-04,  5.1534e-04, -4.1492e-04,  1.1376e-04,  3.7481e-04,
         2.4864e-04, -5.2614e-06,  4.7175e-04,  2.5161e-05,  5.7659e-05,
         1.4223e-04,  6.4884e-04,  6.2266e-04,  6.1157e-06,  5.5694e-04,
         4.9942e-04, -1.1255e-04,  4.3850e-04, -5.7550e-04, -6.0415e-04,
         4.5274e-04,  1.7843e-04,  5.4746e-05,  6.4693e-04,  4.2126e-04,
        -3.5731e-04, -1.1276e-04,  5.3560e-04,  7.2801e-05,  4.2498e-04,
         1.3813e-04,  5.9489e-04,  6.2948e-04,  4.6806e-04,  3.9199e-04,
        -4.8857e-05, -5.1388e-04, -2.5622e-04, -4.5904e-04,  1.5477e-04,
        -1.6647e-04, -6.2357e-05, -3.3877e-04, -4.9365e-04,  4.0310e-04,
         4.1691e-04,  3.7663e-04, -6.2661e-04, -5.3595e-04, -5.2319e-04,
        -6.3071e-05,  6.9231e-05,  1.4321e-04,  2.1415e-04,  4.4668e-04,
        -5.3844e-04, -2.6334e-04,  3.5150e-04,  7.7501e-05,  6.1122e-04,
         6.4652e-04,  5.4701e-04, -5.6149e-04,  2.2933e-04,  5.1135e-04,
        -2.7439e-04,  1.4211e-04,  3.8629e-04, -2.5802e-05, -7.3243e-05,
        -3.0991e-05,  5.7430e-04, -4.2587e-04, -6.3644e-04, -6.2868e-04,
         2.8424e-04, -1.5180e-04, -1.4841e-04,  2.1535e-06, -5.4997e-04,
         2.0975e-04, -2.5518e-04,  5.7829e-04,  4.6280e-04, -5.3495e-04,
        -3.1370e-04, -2.8975e-04,  3.8608e-05, -7.7515e-05, -1.9661e-04,
         4.9504e-04,  4.3003e-04, -3.8178e-04, -4.5703e-04, -2.8145e-04,
         5.2571e-04,  8.5348e-05, -2.9284e-04,  8.3897e-05, -1.5288e-04,
        -1.9866e-04], device='cuda:0', requires_grad=True)

name linear2.weight 
shape:
 torch.Size([2350554, 256]) 
grad:
 True 
date:
 tensor([[ 0.0097, -0.0030, -0.0160,  ..., -0.0300,  0.0454,  0.0118],
        [ 0.0030, -0.0452,  0.0371,  ...,  0.0147,  0.0316,  0.0074],
        [ 0.0219,  0.0424,  0.0141,  ...,  0.0065, -0.0610,  0.0211],
        ...,
        [-0.0083,  0.0059, -0.0477,  ..., -0.0046,  0.0070,  0.0163],
        [-0.0084, -0.0433, -0.0385,  ...,  0.0115,  0.0391, -0.0022],
        [-0.0144, -0.0080, -0.0515,  ...,  0.0235,  0.0059, -0.0447]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0097, -0.0030, -0.0160,  ..., -0.0300,  0.0454,  0.0118],
        [ 0.0030, -0.0452,  0.0371,  ...,  0.0147,  0.0316,  0.0074],
        [ 0.0219,  0.0424,  0.0141,  ...,  0.0065, -0.0610,  0.0211],
        ...,
        [-0.0083,  0.0059, -0.0477,  ..., -0.0046,  0.0070,  0.0163],
        [-0.0084, -0.0433, -0.0385,  ...,  0.0115,  0.0391, -0.0022],
        [-0.0144, -0.0080, -0.0515,  ...,  0.0235,  0.0059, -0.0447]],
       device='cuda:0', requires_grad=True)

name linear2.bias 
shape:
 torch.Size([2350554]) 
grad:
 True 
date:
 tensor([ 0.0069,  0.0343,  0.0182,  ..., -0.0077, -0.0358,  0.0049],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([ 0.0069,  0.0343,  0.0182,  ..., -0.0077, -0.0358,  0.0049],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0266, -0.0728, -0.0114,  ..., -0.0859,  0.0774,  0.0226],
        [ 0.0178,  0.0328,  0.0534,  ...,  0.1208, -0.0898,  0.1096],
        [-0.0967, -0.0197,  0.0579,  ..., -0.0183, -0.1211,  0.0110],
        ...,
        [ 0.0142, -0.1005,  0.0802,  ..., -0.0248,  0.0078,  0.0513],
        [ 0.0865,  0.0791, -0.0152,  ..., -0.0334,  0.0964,  0.1216],
        [-0.0616, -0.0724, -0.1066,  ..., -0.0052,  0.1164,  0.0196]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0266, -0.0728, -0.0114,  ..., -0.0859,  0.0774,  0.0226],
        [ 0.0178,  0.0328,  0.0534,  ...,  0.1208, -0.0898,  0.1096],
        [-0.0967, -0.0197,  0.0579,  ..., -0.0183, -0.1211,  0.0110],
        ...,
        [ 0.0142, -0.1005,  0.0802,  ..., -0.0248,  0.0078,  0.0513],
        [ 0.0865,  0.0791, -0.0152,  ..., -0.0334,  0.0964,  0.1216],
        [-0.0616, -0.0724, -0.1066,  ..., -0.0052,  0.1164,  0.0196]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.1167,  0.1661,  0.0427,  ..., -0.1673,  0.1420, -0.1362],
        [ 0.0769,  0.0585,  0.0465,  ..., -0.0498, -0.1014, -0.0371],
        [ 0.1205, -0.0363,  0.0863,  ..., -0.0965, -0.1701, -0.0826],
        ...,
        [-0.0305, -0.0439, -0.0751,  ...,  0.1340,  0.0143,  0.1697],
        [-0.1178,  0.1099, -0.0812,  ...,  0.0484, -0.0704, -0.0741],
        [ 0.1720,  0.1066,  0.0834,  ...,  0.1534, -0.1234,  0.1389]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1167,  0.1661,  0.0427,  ..., -0.1673,  0.1420, -0.1362],
        [ 0.0769,  0.0585,  0.0465,  ..., -0.0498, -0.1014, -0.0371],
        [ 0.1205, -0.0363,  0.0863,  ..., -0.0965, -0.1701, -0.0826],
        ...,
        [-0.0305, -0.0439, -0.0751,  ...,  0.1340,  0.0143,  0.1697],
        [-0.1178,  0.1099, -0.0812,  ...,  0.0484, -0.0704, -0.0741],
        [ 0.1720,  0.1066,  0.0834,  ...,  0.1534, -0.1234,  0.1389]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.1561, -0.2070, -0.0892,  ...,  0.1021, -0.0299,  0.1998],
        [-0.1737, -0.0165, -0.1199,  ...,  0.0606,  0.2409, -0.1214],
        [-0.0669,  0.0910,  0.1759,  ..., -0.1901, -0.1988, -0.0149],
        ...,
        [ 0.0462, -0.2330, -0.1877,  ..., -0.2326, -0.1301, -0.1248],
        [ 0.2070, -0.1724,  0.0037,  ...,  0.0040,  0.0658, -0.2366],
        [-0.0458, -0.0361, -0.2117,  ..., -0.0354,  0.1583,  0.1192]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1561, -0.2070, -0.0892,  ...,  0.1021, -0.0299,  0.1998],
        [-0.1737, -0.0165, -0.1199,  ...,  0.0606,  0.2409, -0.1214],
        [-0.0669,  0.0910,  0.1759,  ..., -0.1901, -0.1988, -0.0149],
        ...,
        [ 0.0462, -0.2330, -0.1877,  ..., -0.2326, -0.1301, -0.1248],
        [ 0.2070, -0.1724,  0.0037,  ...,  0.0040,  0.0658, -0.2366],
        [-0.0458, -0.0361, -0.2117,  ..., -0.0354,  0.1583,  0.1192]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-4.1557e-01],
        [-3.9308e-01],
        [-1.2605e-01],
        [ 4.1854e-01],
        [ 2.3215e-01],
        [-1.5782e-01],
        [ 2.7331e-01],
        [-1.8551e-01],
        [-4.1382e-01],
        [ 1.1188e-01],
        [-2.5508e-01],
        [-1.7003e-04],
        [ 3.4149e-01],
        [ 2.1398e-01],
        [ 9.6029e-03],
        [-1.7822e-01],
        [ 1.2670e-01],
        [-1.7005e-01],
        [ 4.1953e-01],
        [ 6.5124e-02],
        [ 2.0435e-02],
        [ 3.5719e-01],
        [-1.7725e-01],
        [-2.0204e-01],
        [-2.0551e-01],
        [ 2.1278e-01],
        [ 2.7898e-01],
        [ 2.5443e-01],
        [-1.2747e-01],
        [-1.7147e-01],
        [ 1.5101e-02],
        [-1.3622e-01]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-4.1557e-01],
        [-3.9308e-01],
        [-1.2605e-01],
        [ 4.1854e-01],
        [ 2.3215e-01],
        [-1.5782e-01],
        [ 2.7331e-01],
        [-1.8551e-01],
        [-4.1382e-01],
        [ 1.1188e-01],
        [-2.5508e-01],
        [-1.7003e-04],
        [ 3.4149e-01],
        [ 2.1398e-01],
        [ 9.6029e-03],
        [-1.7822e-01],
        [ 1.2670e-01],
        [-1.7005e-01],
        [ 4.1953e-01],
        [ 6.5124e-02],
        [ 2.0435e-02],
        [ 3.5719e-01],
        [-1.7725e-01],
        [-2.0204e-01],
        [-2.0551e-01],
        [ 2.1278e-01],
        [ 2.7898e-01],
        [ 2.5443e-01],
        [-1.2747e-01],
        [-1.7147e-01],
        [ 1.5101e-02],
        [-1.3622e-01]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)





 shepe: torch.Size([2350554, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet] tensor([[0.0000],
        [0.1261],
        [0.0867],
        ...,
        [0.0000],
        [0.0000],
        [0.0082]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(195542.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-66.7792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.4766, device='cuda:0')



h[100].sum tensor(-1.6096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.7220, device='cuda:0')



h[200].sum tensor(-0.2477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.2650, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(27036.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0234, 0.0119, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0087, 0.0044, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0053, 0.0027, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(674083.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(21178.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(370.8288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.5438],
        [1.1082],
        [0.7563],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(499210.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.0000],
        [0.1261],
        [0.0867],
        ...,
        [0.0000],
        [0.0000],
        [0.0082]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet].sum tensor(195542.4375, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[1.5438],
        [1.1082],
        [0.7563],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])





 shepe: torch.Size([47011080, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet] tensor([[0.0796],
        [0.3374],
        [0.0000],
        ...,
        [0.0000],
        [0.7481],
        [0.1056]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(4886992.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0088, -0.0079,  0.0148,  ...,  0.0199,  0.0039,  0.0041],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(573.3557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-61.6475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-66.2131, device='cuda:0')



h[100].sum tensor(34.8549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.4362, device='cuda:0')



h[200].sum tensor(133.7015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(143.6034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0598,  ..., 0.0804, 0.0159, 0.0167],
        [0.0000, 0.0000, 0.0481,  ..., 0.0646, 0.0127, 0.0134],
        [0.0000, 0.0000, 0.0197,  ..., 0.0264, 0.0052, 0.0055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(328593.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0983, 0.3245,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0893, 0.2946,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0852, 0.2814,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(6414832., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5078.3965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[4.3243e+00],
        [4.5339e+00],
        [5.4417e+00],
        ...,
        [8.2278e-05],
        [1.2867e-04],
        [1.4990e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(2567571.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.0796],
        [0.3374],
        [0.0000],
        ...,
        [0.0000],
        [0.7481],
        [0.1056]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet].sum tensor(4886992.5000, device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[1.5438],
        [1.1082],
        [0.7563],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/./TrainingBha2ndneiefet.py", line 13, in <module>
    writer = SummaryWriter()
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 220, in __init__
    self._get_file_writer()
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 250, in _get_file_writer
    self.file_writer = FileWriter(self.log_dir, self.max_queue,
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 60, in __init__
    self.event_writer = EventFileWriter(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/summary/writer/event_file_writer.py", line 72, in __init__
    tf.io.gfile.makedirs(logdir)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 900, in makedirs
    return get_filesystem(path).makedirs(path)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 201, in makedirs
    os.makedirs(path, exist_ok=True)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/os.py", line 225, in makedirs
    mkdir(name, mode)
OSError: [Errno 122] Disk quota exceeded: 'runs/Jan09_17-10-03_gpu016.ihep.ac.cn'

real	0m52.977s
user	0m45.797s
sys	0m7.261s
