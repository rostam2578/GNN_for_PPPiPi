0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-adc0bb57-5f8a-ba9d-04e2-dbaa9f7c20f1)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        A2:17:1D:79:E5:47:38:13:77:F2:EE:66:EA:5D:DB:A0:A1:CA:F4:E7
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jan  9 17:10:55 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:1D:00.0 Off |                    0 |
| N/A   31C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089926656

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b30b388e880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.065s
user	0m2.345s
sys	0m0.732s




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 6507, 6507, 6507],
        [   1,    2,    3,  ..., 6219, 6794, 6795]]) 

edge_index shape
 torch.Size([2, 1175277])
graph: Graph(num_nodes=6796, num_edges=1175277,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 6507, 6507, 6507], device='cuda:0'), tensor([   1,    2,    3,  ..., 6219, 6794, 6795], device='cuda:0'))

number of nodes: 6796

number of edges: 2350554

node features (random input): tensor([[-1.3480],
        [-0.3008],
        [-0.5766],
        ...,
        [ 0.1636],
        [-1.2928],
        [-1.9321]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-9.9350, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(2350554., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 399

In degrees of node 234: 399





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLBhaGcnReNewestweight7N4SecNei

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (linear1): Linear(in_features=2350554, out_features=256, bias=True)
  (linear2): Linear(in_features=256, out_features=2350554, bias=True)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 1205878235

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[ 6.9025e-03,  1.2625e-01,  4.3839e-02, -8.2636e-03,  7.5485e-02,
          1.3564e-01,  1.1873e-01,  2.3961e-02,  4.2645e-02,  2.5010e-02,
          6.2627e-03, -9.5988e-02, -8.0336e-03,  6.3123e-02,  1.4092e-01,
         -1.5254e-01,  1.4060e-01,  3.0965e-07,  4.5135e-02,  2.9481e-02,
          1.3776e-01,  3.4601e-02,  5.8157e-02, -1.3147e-02,  1.8307e-03,
          1.3726e-01,  8.9914e-04,  4.2308e-02,  1.5056e-01,  1.3811e-01,
         -1.0243e-01,  9.1817e-02, -9.6851e-02, -1.4919e-01, -1.1182e-01,
          8.5369e-02,  5.5552e-02,  9.3828e-02, -1.5154e-01, -2.0154e-02,
         -9.8768e-02,  9.6999e-02, -1.5096e-01,  3.6096e-02, -2.2727e-03,
          1.8232e-02,  3.2738e-02, -4.4832e-02,  2.2220e-02, -1.4535e-01,
         -7.2979e-02,  1.2341e-02, -9.5682e-02, -6.2316e-02, -4.8968e-02,
         -3.2205e-02, -1.2571e-01, -5.2142e-02,  8.5417e-02,  2.3324e-02,
         -1.1668e-01,  1.7822e-02, -1.0247e-02,  1.9852e-02, -5.3105e-02,
          3.0825e-02, -4.9270e-02, -8.9600e-02, -8.1657e-02, -1.0250e-01,
         -1.2115e-01,  7.6043e-02,  2.9385e-02,  5.2745e-02,  7.3964e-02,
          1.4676e-01,  3.3982e-03,  3.7537e-04,  4.8634e-02,  2.5609e-02,
         -5.0226e-02, -8.7009e-03, -1.1835e-01,  9.7517e-02,  5.0063e-02,
          7.6017e-02,  4.5273e-02, -4.3649e-02,  2.2322e-02,  2.6217e-02,
          3.1318e-02,  7.8181e-02,  1.2906e-01,  1.2238e-01,  6.2519e-02,
         -5.3982e-02,  1.2552e-01,  1.1333e-01, -1.1449e-01, -1.0529e-01,
         -5.9304e-02, -5.8590e-02,  1.2486e-01, -1.1607e-01, -1.3581e-01,
          1.3577e-01, -3.0548e-02, -1.3939e-01, -1.4353e-01, -7.5119e-02,
          1.5604e-02, -6.1304e-02, -8.4260e-02, -1.3088e-01, -2.4561e-03,
         -1.4428e-01,  6.8499e-02,  8.8701e-02, -6.1719e-02, -1.4253e-01,
         -1.4521e-01,  4.5080e-03, -9.3410e-02, -8.1411e-02, -1.2580e-01,
          1.4015e-01,  1.4489e-01,  2.2404e-02, -8.9054e-02,  9.3515e-02,
          6.2152e-02,  1.1174e-01, -9.9269e-02, -7.8037e-02, -5.9991e-02,
          6.7902e-02,  2.1289e-02, -1.3523e-01,  1.0075e-01,  1.4784e-01,
         -4.6599e-02,  1.1403e-01, -7.1879e-02, -1.1681e-01, -1.4419e-01,
         -1.0758e-01,  1.1629e-01,  1.0231e-01,  5.1841e-02,  1.4857e-01,
          8.8051e-02, -1.1409e-01,  4.9630e-03, -1.2185e-01,  9.0052e-02,
          1.1236e-01, -1.4802e-01, -1.7764e-02, -1.1837e-01,  7.9378e-02,
         -9.5709e-02,  1.0227e-01,  1.1160e-02, -1.9065e-02, -9.8023e-02,
         -6.5633e-02, -1.1968e-01, -8.4952e-02, -1.1876e-01, -1.4158e-01,
         -1.1344e-01,  3.3275e-02, -7.5575e-02, -2.4253e-02,  1.5859e-02,
          6.1310e-02,  1.9268e-02, -1.1986e-01,  1.7696e-03,  5.1477e-02,
          1.3910e-01, -1.4339e-01, -5.4467e-02,  1.2430e-01, -1.4599e-01,
         -5.7935e-02, -6.5650e-02,  1.1875e-01, -3.9442e-02, -1.3604e-01,
          3.8160e-02, -1.1107e-01, -4.4433e-02, -8.8979e-02,  1.3422e-01,
         -4.8042e-02, -4.6374e-02, -5.5972e-02,  9.6945e-03,  3.1324e-02,
         -8.8512e-02,  8.0004e-02,  7.9216e-02,  2.5104e-02,  3.4587e-02,
         -3.3858e-02,  4.2374e-02,  5.3646e-02,  1.3139e-01, -8.7295e-02,
          5.2174e-03,  1.1662e-02, -1.4405e-01,  1.7760e-02,  5.4066e-02,
          8.9366e-02, -1.6333e-02, -8.7193e-03, -1.0116e-01, -1.3048e-01,
         -1.3830e-01,  4.8420e-02,  1.3894e-01, -5.0700e-02, -1.2348e-03,
         -1.0883e-01,  4.3735e-03,  9.9251e-02,  5.5923e-02, -1.0182e-01,
         -7.0379e-02, -1.0813e-01,  3.8097e-02, -1.1091e-01,  5.4212e-02,
         -1.2111e-01, -1.5083e-01, -5.5060e-02,  5.1857e-02,  3.0609e-02,
          1.1069e-01,  1.4001e-01,  8.7252e-02, -1.0222e-01,  9.7572e-03,
          1.4527e-02,  1.0535e-01, -1.5017e-01, -7.1068e-02,  1.1768e-01,
         -2.0801e-02, -2.3278e-02,  6.8392e-02, -9.8212e-02, -1.1413e-01,
          3.4936e-02]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 6.9025e-03,  1.2625e-01,  4.3839e-02, -8.2636e-03,  7.5485e-02,
          1.3564e-01,  1.1873e-01,  2.3961e-02,  4.2645e-02,  2.5010e-02,
          6.2627e-03, -9.5988e-02, -8.0336e-03,  6.3123e-02,  1.4092e-01,
         -1.5254e-01,  1.4060e-01,  3.0965e-07,  4.5135e-02,  2.9481e-02,
          1.3776e-01,  3.4601e-02,  5.8157e-02, -1.3147e-02,  1.8307e-03,
          1.3726e-01,  8.9914e-04,  4.2308e-02,  1.5056e-01,  1.3811e-01,
         -1.0243e-01,  9.1817e-02, -9.6851e-02, -1.4919e-01, -1.1182e-01,
          8.5369e-02,  5.5552e-02,  9.3828e-02, -1.5154e-01, -2.0154e-02,
         -9.8768e-02,  9.6999e-02, -1.5096e-01,  3.6096e-02, -2.2727e-03,
          1.8232e-02,  3.2738e-02, -4.4832e-02,  2.2220e-02, -1.4535e-01,
         -7.2979e-02,  1.2341e-02, -9.5682e-02, -6.2316e-02, -4.8968e-02,
         -3.2205e-02, -1.2571e-01, -5.2142e-02,  8.5417e-02,  2.3324e-02,
         -1.1668e-01,  1.7822e-02, -1.0247e-02,  1.9852e-02, -5.3105e-02,
          3.0825e-02, -4.9270e-02, -8.9600e-02, -8.1657e-02, -1.0250e-01,
         -1.2115e-01,  7.6043e-02,  2.9385e-02,  5.2745e-02,  7.3964e-02,
          1.4676e-01,  3.3982e-03,  3.7537e-04,  4.8634e-02,  2.5609e-02,
         -5.0226e-02, -8.7009e-03, -1.1835e-01,  9.7517e-02,  5.0063e-02,
          7.6017e-02,  4.5273e-02, -4.3649e-02,  2.2322e-02,  2.6217e-02,
          3.1318e-02,  7.8181e-02,  1.2906e-01,  1.2238e-01,  6.2519e-02,
         -5.3982e-02,  1.2552e-01,  1.1333e-01, -1.1449e-01, -1.0529e-01,
         -5.9304e-02, -5.8590e-02,  1.2486e-01, -1.1607e-01, -1.3581e-01,
          1.3577e-01, -3.0548e-02, -1.3939e-01, -1.4353e-01, -7.5119e-02,
          1.5604e-02, -6.1304e-02, -8.4260e-02, -1.3088e-01, -2.4561e-03,
         -1.4428e-01,  6.8499e-02,  8.8701e-02, -6.1719e-02, -1.4253e-01,
         -1.4521e-01,  4.5080e-03, -9.3410e-02, -8.1411e-02, -1.2580e-01,
          1.4015e-01,  1.4489e-01,  2.2404e-02, -8.9054e-02,  9.3515e-02,
          6.2152e-02,  1.1174e-01, -9.9269e-02, -7.8037e-02, -5.9991e-02,
          6.7902e-02,  2.1289e-02, -1.3523e-01,  1.0075e-01,  1.4784e-01,
         -4.6599e-02,  1.1403e-01, -7.1879e-02, -1.1681e-01, -1.4419e-01,
         -1.0758e-01,  1.1629e-01,  1.0231e-01,  5.1841e-02,  1.4857e-01,
          8.8051e-02, -1.1409e-01,  4.9630e-03, -1.2185e-01,  9.0052e-02,
          1.1236e-01, -1.4802e-01, -1.7764e-02, -1.1837e-01,  7.9378e-02,
         -9.5709e-02,  1.0227e-01,  1.1160e-02, -1.9065e-02, -9.8023e-02,
         -6.5633e-02, -1.1968e-01, -8.4952e-02, -1.1876e-01, -1.4158e-01,
         -1.1344e-01,  3.3275e-02, -7.5575e-02, -2.4253e-02,  1.5859e-02,
          6.1310e-02,  1.9268e-02, -1.1986e-01,  1.7696e-03,  5.1477e-02,
          1.3910e-01, -1.4339e-01, -5.4467e-02,  1.2430e-01, -1.4599e-01,
         -5.7935e-02, -6.5650e-02,  1.1875e-01, -3.9442e-02, -1.3604e-01,
          3.8160e-02, -1.1107e-01, -4.4433e-02, -8.8979e-02,  1.3422e-01,
         -4.8042e-02, -4.6374e-02, -5.5972e-02,  9.6945e-03,  3.1324e-02,
         -8.8512e-02,  8.0004e-02,  7.9216e-02,  2.5104e-02,  3.4587e-02,
         -3.3858e-02,  4.2374e-02,  5.3646e-02,  1.3139e-01, -8.7295e-02,
          5.2174e-03,  1.1662e-02, -1.4405e-01,  1.7760e-02,  5.4066e-02,
          8.9366e-02, -1.6333e-02, -8.7193e-03, -1.0116e-01, -1.3048e-01,
         -1.3830e-01,  4.8420e-02,  1.3894e-01, -5.0700e-02, -1.2348e-03,
         -1.0883e-01,  4.3735e-03,  9.9251e-02,  5.5923e-02, -1.0182e-01,
         -7.0379e-02, -1.0813e-01,  3.8097e-02, -1.1091e-01,  5.4212e-02,
         -1.2111e-01, -1.5083e-01, -5.5060e-02,  5.1857e-02,  3.0609e-02,
          1.1069e-01,  1.4001e-01,  8.7252e-02, -1.0222e-01,  9.7572e-03,
          1.4527e-02,  1.0535e-01, -1.5017e-01, -7.1068e-02,  1.1768e-01,
         -2.0801e-02, -2.3278e-02,  6.8392e-02, -9.8212e-02, -1.1413e-01,
          3.4936e-02]], device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name linear1.weight 
shape:
 torch.Size([256, 2350554]) 
grad:
 True 
date:
 tensor([[-1.6625e-04,  6.0298e-04,  3.2406e-04,  ..., -3.9000e-04,
         -2.1191e-04,  5.9004e-04],
        [ 3.1397e-04,  5.3498e-05,  4.0449e-04,  ...,  2.8653e-04,
         -3.3502e-04, -3.7921e-04],
        [-1.2079e-04, -5.5355e-04, -6.0137e-05,  ...,  3.2908e-04,
          4.7677e-04,  5.2407e-04],
        ...,
        [ 3.5964e-04,  2.9317e-04, -4.9436e-04,  ..., -4.3194e-04,
         -2.1665e-04,  2.2661e-04],
        [-1.5988e-04, -5.0286e-04, -5.2022e-04,  ..., -3.5656e-04,
          3.8224e-04,  4.0157e-04],
        [-3.5240e-04, -1.6245e-04,  2.7888e-04,  ...,  3.6379e-04,
          7.5856e-05, -2.9916e-04]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-1.6625e-04,  6.0298e-04,  3.2406e-04,  ..., -3.9000e-04,
         -2.1191e-04,  5.9004e-04],
        [ 3.1397e-04,  5.3498e-05,  4.0449e-04,  ...,  2.8653e-04,
         -3.3502e-04, -3.7921e-04],
        [-1.2079e-04, -5.5355e-04, -6.0137e-05,  ...,  3.2908e-04,
          4.7677e-04,  5.2407e-04],
        ...,
        [ 3.5964e-04,  2.9317e-04, -4.9436e-04,  ..., -4.3194e-04,
         -2.1665e-04,  2.2661e-04],
        [-1.5988e-04, -5.0286e-04, -5.2022e-04,  ..., -3.5656e-04,
          3.8224e-04,  4.0157e-04],
        [-3.5240e-04, -1.6245e-04,  2.7888e-04,  ...,  3.6379e-04,
          7.5856e-05, -2.9916e-04]], device='cuda:0', requires_grad=True)

name linear1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([ 3.7140e-04,  3.1798e-04, -1.4784e-04, -3.6774e-04,  2.9147e-04,
        -2.0570e-04,  4.1677e-05, -3.9801e-04,  2.1514e-04, -1.5520e-04,
         9.7687e-05,  5.7827e-04, -6.4772e-04, -4.1629e-04,  1.3951e-04,
        -1.0545e-04, -3.9088e-04, -5.7313e-04,  1.4189e-04, -6.4125e-04,
        -5.3687e-04, -3.5698e-04,  5.8727e-04, -4.5890e-04,  5.0742e-04,
        -3.4978e-04,  3.2143e-04, -4.7314e-04, -6.1513e-04,  4.4846e-04,
        -5.2222e-04,  1.7826e-04,  4.3768e-04, -1.7169e-04,  6.5151e-04,
        -2.0023e-04, -4.2236e-04,  2.0385e-04,  1.4831e-04, -2.1596e-04,
         2.9704e-04, -4.3139e-04, -4.7301e-04, -5.6425e-04,  4.2627e-04,
        -1.6630e-04, -6.1980e-05, -5.2211e-04, -1.4423e-04, -4.9137e-04,
         2.7637e-04,  4.7494e-05, -4.6789e-04,  1.7336e-04, -9.8143e-05,
         2.4571e-04, -3.8448e-04,  9.0131e-05, -4.7037e-04,  5.3051e-04,
        -2.0006e-04, -1.5068e-04, -1.3513e-04, -6.2076e-05, -2.4730e-04,
         3.0516e-04,  2.4492e-05,  1.2405e-04, -4.0948e-05, -3.0882e-05,
        -3.3965e-04, -3.8344e-04, -2.6716e-04,  1.8488e-04, -9.2172e-05,
         1.1937e-04, -1.2456e-04,  3.7829e-04, -2.3664e-04, -3.1740e-04,
         7.5333e-05,  3.9393e-04, -6.0801e-04, -4.7461e-04,  1.0186e-04,
         2.9565e-04,  4.2948e-04,  6.3988e-04,  4.0016e-04, -6.0990e-04,
         1.9861e-04, -2.4247e-05,  4.9075e-04,  2.5120e-04, -3.4638e-04,
        -4.5909e-04, -5.1310e-04, -3.8665e-04, -8.2571e-06,  5.5809e-04,
         5.1234e-04,  2.6070e-04, -1.8480e-04, -2.6223e-04,  9.4315e-05,
        -3.9646e-04, -3.2225e-04,  4.1845e-04,  1.1510e-04,  1.6175e-04,
        -1.2802e-05,  1.1867e-04,  5.8931e-04,  5.7324e-04,  3.4031e-04,
        -2.2367e-04, -2.9296e-05,  1.0944e-04, -4.3918e-04,  4.6495e-04,
        -6.5145e-04,  4.4725e-05, -3.1970e-04, -2.2741e-04, -1.0935e-04,
        -3.3650e-04,  4.8227e-04,  5.1112e-04, -1.7244e-04, -3.4106e-04,
         1.8976e-04,  5.8129e-04, -5.3814e-05, -1.4973e-04, -7.6524e-05,
         6.2249e-05, -2.5713e-04, -6.8096e-05, -1.6898e-04, -3.1307e-04,
         3.7324e-04, -3.0909e-04, -1.8836e-05,  6.0209e-04,  6.9273e-05,
        -1.3386e-04,  3.9639e-04, -4.9632e-04,  4.6171e-04,  5.0056e-04,
         4.4967e-05, -3.8590e-04,  5.0203e-05,  2.0043e-04, -1.7047e-04,
         1.8560e-04, -3.9851e-04,  9.2255e-05, -1.5972e-04,  9.8895e-05,
         5.2726e-04, -2.0019e-04,  4.9969e-04,  1.7578e-04, -4.1811e-04,
         1.5946e-04,  2.6302e-04,  5.8783e-04,  4.2185e-04, -5.7346e-04,
        -6.4543e-04,  1.1317e-04, -1.4659e-04, -2.3107e-04, -7.0287e-05,
        -6.2302e-04,  3.7521e-04,  8.6196e-05,  3.8342e-04,  6.1600e-04,
         2.5562e-04,  4.5162e-05,  6.0253e-05,  4.6628e-04, -5.9684e-04,
         4.8350e-04, -5.6599e-04,  4.7953e-04, -3.5604e-04, -1.0116e-04,
         6.6869e-05,  4.8799e-04,  4.0667e-05,  8.6108e-05,  1.4072e-04,
         5.7710e-04, -3.7764e-04,  5.3914e-04,  3.4074e-04, -2.8952e-04,
         4.0218e-04,  2.5582e-04,  9.5090e-05, -5.9760e-04, -4.1807e-04,
         3.3688e-04, -3.1754e-04,  5.9508e-06, -7.6873e-05,  6.2879e-04,
        -5.9335e-04,  7.5229e-05,  1.3910e-04, -1.7903e-05, -5.7767e-04,
        -4.7535e-05,  5.0636e-04,  3.2624e-04, -3.1492e-05, -4.4277e-04,
        -2.7919e-04,  1.9990e-04,  2.4362e-04, -4.2284e-04,  5.2185e-04,
         1.9572e-04, -1.1142e-04,  5.1129e-04,  5.1839e-04,  6.2670e-04,
        -5.8101e-04, -2.6023e-04, -3.3084e-04,  2.4535e-05, -7.6346e-05,
        -5.5182e-04, -1.9413e-04,  1.1816e-04,  5.4878e-04, -6.1417e-04,
         3.3498e-04, -6.3846e-04, -2.1189e-04,  4.5755e-04,  8.7503e-05,
        -4.6321e-05, -6.9942e-05,  2.6225e-04,  1.9374e-05,  4.0004e-04,
         3.7870e-06, -2.5753e-04,  3.2113e-04, -2.4910e-04,  4.2038e-04,
        -3.1268e-04], device='cuda:0') 
parameter:
 Parameter containing:
tensor([ 3.7140e-04,  3.1798e-04, -1.4784e-04, -3.6774e-04,  2.9147e-04,
        -2.0570e-04,  4.1677e-05, -3.9801e-04,  2.1514e-04, -1.5520e-04,
         9.7687e-05,  5.7827e-04, -6.4772e-04, -4.1629e-04,  1.3951e-04,
        -1.0545e-04, -3.9088e-04, -5.7313e-04,  1.4189e-04, -6.4125e-04,
        -5.3687e-04, -3.5698e-04,  5.8727e-04, -4.5890e-04,  5.0742e-04,
        -3.4978e-04,  3.2143e-04, -4.7314e-04, -6.1513e-04,  4.4846e-04,
        -5.2222e-04,  1.7826e-04,  4.3768e-04, -1.7169e-04,  6.5151e-04,
        -2.0023e-04, -4.2236e-04,  2.0385e-04,  1.4831e-04, -2.1596e-04,
         2.9704e-04, -4.3139e-04, -4.7301e-04, -5.6425e-04,  4.2627e-04,
        -1.6630e-04, -6.1980e-05, -5.2211e-04, -1.4423e-04, -4.9137e-04,
         2.7637e-04,  4.7494e-05, -4.6789e-04,  1.7336e-04, -9.8143e-05,
         2.4571e-04, -3.8448e-04,  9.0131e-05, -4.7037e-04,  5.3051e-04,
        -2.0006e-04, -1.5068e-04, -1.3513e-04, -6.2076e-05, -2.4730e-04,
         3.0516e-04,  2.4492e-05,  1.2405e-04, -4.0948e-05, -3.0882e-05,
        -3.3965e-04, -3.8344e-04, -2.6716e-04,  1.8488e-04, -9.2172e-05,
         1.1937e-04, -1.2456e-04,  3.7829e-04, -2.3664e-04, -3.1740e-04,
         7.5333e-05,  3.9393e-04, -6.0801e-04, -4.7461e-04,  1.0186e-04,
         2.9565e-04,  4.2948e-04,  6.3988e-04,  4.0016e-04, -6.0990e-04,
         1.9861e-04, -2.4247e-05,  4.9075e-04,  2.5120e-04, -3.4638e-04,
        -4.5909e-04, -5.1310e-04, -3.8665e-04, -8.2571e-06,  5.5809e-04,
         5.1234e-04,  2.6070e-04, -1.8480e-04, -2.6223e-04,  9.4315e-05,
        -3.9646e-04, -3.2225e-04,  4.1845e-04,  1.1510e-04,  1.6175e-04,
        -1.2802e-05,  1.1867e-04,  5.8931e-04,  5.7324e-04,  3.4031e-04,
        -2.2367e-04, -2.9296e-05,  1.0944e-04, -4.3918e-04,  4.6495e-04,
        -6.5145e-04,  4.4725e-05, -3.1970e-04, -2.2741e-04, -1.0935e-04,
        -3.3650e-04,  4.8227e-04,  5.1112e-04, -1.7244e-04, -3.4106e-04,
         1.8976e-04,  5.8129e-04, -5.3814e-05, -1.4973e-04, -7.6524e-05,
         6.2249e-05, -2.5713e-04, -6.8096e-05, -1.6898e-04, -3.1307e-04,
         3.7324e-04, -3.0909e-04, -1.8836e-05,  6.0209e-04,  6.9273e-05,
        -1.3386e-04,  3.9639e-04, -4.9632e-04,  4.6171e-04,  5.0056e-04,
         4.4967e-05, -3.8590e-04,  5.0203e-05,  2.0043e-04, -1.7047e-04,
         1.8560e-04, -3.9851e-04,  9.2255e-05, -1.5972e-04,  9.8895e-05,
         5.2726e-04, -2.0019e-04,  4.9969e-04,  1.7578e-04, -4.1811e-04,
         1.5946e-04,  2.6302e-04,  5.8783e-04,  4.2185e-04, -5.7346e-04,
        -6.4543e-04,  1.1317e-04, -1.4659e-04, -2.3107e-04, -7.0287e-05,
        -6.2302e-04,  3.7521e-04,  8.6196e-05,  3.8342e-04,  6.1600e-04,
         2.5562e-04,  4.5162e-05,  6.0253e-05,  4.6628e-04, -5.9684e-04,
         4.8350e-04, -5.6599e-04,  4.7953e-04, -3.5604e-04, -1.0116e-04,
         6.6869e-05,  4.8799e-04,  4.0667e-05,  8.6108e-05,  1.4072e-04,
         5.7710e-04, -3.7764e-04,  5.3914e-04,  3.4074e-04, -2.8952e-04,
         4.0218e-04,  2.5582e-04,  9.5090e-05, -5.9760e-04, -4.1807e-04,
         3.3688e-04, -3.1754e-04,  5.9508e-06, -7.6873e-05,  6.2879e-04,
        -5.9335e-04,  7.5229e-05,  1.3910e-04, -1.7903e-05, -5.7767e-04,
        -4.7535e-05,  5.0636e-04,  3.2624e-04, -3.1492e-05, -4.4277e-04,
        -2.7919e-04,  1.9990e-04,  2.4362e-04, -4.2284e-04,  5.2185e-04,
         1.9572e-04, -1.1142e-04,  5.1129e-04,  5.1839e-04,  6.2670e-04,
        -5.8101e-04, -2.6023e-04, -3.3084e-04,  2.4535e-05, -7.6346e-05,
        -5.5182e-04, -1.9413e-04,  1.1816e-04,  5.4878e-04, -6.1417e-04,
         3.3498e-04, -6.3846e-04, -2.1189e-04,  4.5755e-04,  8.7503e-05,
        -4.6321e-05, -6.9942e-05,  2.6225e-04,  1.9374e-05,  4.0004e-04,
         3.7870e-06, -2.5753e-04,  3.2113e-04, -2.4910e-04,  4.2038e-04,
        -3.1268e-04], device='cuda:0', requires_grad=True)

name linear2.weight 
shape:
 torch.Size([2350554, 256]) 
grad:
 True 
date:
 tensor([[ 0.0404, -0.0295,  0.0290,  ..., -0.0040, -0.0147, -0.0020],
        [ 0.0611,  0.0362, -0.0109,  ...,  0.0552,  0.0530,  0.0294],
        [ 0.0235, -0.0085,  0.0105,  ..., -0.0205,  0.0120,  0.0235],
        ...,
        [-0.0052, -0.0230, -0.0353,  ..., -0.0237, -0.0451,  0.0129],
        [ 0.0508, -0.0389, -0.0232,  ..., -0.0273, -0.0110, -0.0248],
        [ 0.0488,  0.0236,  0.0455,  ..., -0.0236, -0.0560,  0.0524]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0404, -0.0295,  0.0290,  ..., -0.0040, -0.0147, -0.0020],
        [ 0.0611,  0.0362, -0.0109,  ...,  0.0552,  0.0530,  0.0294],
        [ 0.0235, -0.0085,  0.0105,  ..., -0.0205,  0.0120,  0.0235],
        ...,
        [-0.0052, -0.0230, -0.0353,  ..., -0.0237, -0.0451,  0.0129],
        [ 0.0508, -0.0389, -0.0232,  ..., -0.0273, -0.0110, -0.0248],
        [ 0.0488,  0.0236,  0.0455,  ..., -0.0236, -0.0560,  0.0524]],
       device='cuda:0', requires_grad=True)

name linear2.bias 
shape:
 torch.Size([2350554]) 
grad:
 True 
date:
 tensor([-0.0390, -0.0322,  0.0041,  ..., -0.0322,  0.0569,  0.0550],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([-0.0390, -0.0322,  0.0041,  ..., -0.0322,  0.0569,  0.0550],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0288, -0.1217,  0.0383,  ...,  0.1220,  0.0907,  0.0435],
        [-0.0147,  0.0127,  0.0412,  ..., -0.0404,  0.0013, -0.0079],
        [-0.0920, -0.0014,  0.0287,  ...,  0.0202, -0.0777,  0.0732],
        ...,
        [-0.1027, -0.1204,  0.0569,  ..., -0.0879, -0.0112, -0.0464],
        [-0.0476,  0.0004,  0.1180,  ..., -0.1033,  0.0153, -0.0104],
        [-0.0824,  0.0735, -0.0127,  ...,  0.0453,  0.0343,  0.0607]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0288, -0.1217,  0.0383,  ...,  0.1220,  0.0907,  0.0435],
        [-0.0147,  0.0127,  0.0412,  ..., -0.0404,  0.0013, -0.0079],
        [-0.0920, -0.0014,  0.0287,  ...,  0.0202, -0.0777,  0.0732],
        ...,
        [-0.1027, -0.1204,  0.0569,  ..., -0.0879, -0.0112, -0.0464],
        [-0.0476,  0.0004,  0.1180,  ..., -0.1033,  0.0153, -0.0104],
        [-0.0824,  0.0735, -0.0127,  ...,  0.0453,  0.0343,  0.0607]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0940,  0.1435,  0.1555,  ..., -0.1419,  0.0543, -0.0658],
        [-0.0091, -0.1670, -0.0571,  ..., -0.1180, -0.1042,  0.0369],
        [ 0.0603,  0.1283, -0.0470,  ...,  0.1392,  0.0367, -0.0770],
        ...,
        [ 0.0441,  0.1552, -0.0692,  ..., -0.0129,  0.0259, -0.0153],
        [ 0.0656,  0.1523,  0.1404,  ...,  0.0641, -0.1738, -0.1005],
        [ 0.1305,  0.0473,  0.0569,  ..., -0.0110,  0.0056,  0.0552]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0940,  0.1435,  0.1555,  ..., -0.1419,  0.0543, -0.0658],
        [-0.0091, -0.1670, -0.0571,  ..., -0.1180, -0.1042,  0.0369],
        [ 0.0603,  0.1283, -0.0470,  ...,  0.1392,  0.0367, -0.0770],
        ...,
        [ 0.0441,  0.1552, -0.0692,  ..., -0.0129,  0.0259, -0.0153],
        [ 0.0656,  0.1523,  0.1404,  ...,  0.0641, -0.1738, -0.1005],
        [ 0.1305,  0.0473,  0.0569,  ..., -0.0110,  0.0056,  0.0552]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.1973, -0.0199, -0.0248,  ..., -0.2177, -0.1486, -0.1112],
        [-0.1635, -0.2437, -0.0755,  ...,  0.2083,  0.0270,  0.0944],
        [ 0.0335,  0.1224, -0.2452,  ...,  0.1717,  0.1579,  0.0136],
        ...,
        [-0.0771, -0.2164, -0.0872,  ..., -0.1907,  0.1330, -0.2194],
        [-0.0593,  0.0067,  0.0207,  ..., -0.1813,  0.2490,  0.0700],
        [-0.2368,  0.1549, -0.2169,  ...,  0.2118,  0.2450, -0.1699]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1973, -0.0199, -0.0248,  ..., -0.2177, -0.1486, -0.1112],
        [-0.1635, -0.2437, -0.0755,  ...,  0.2083,  0.0270,  0.0944],
        [ 0.0335,  0.1224, -0.2452,  ...,  0.1717,  0.1579,  0.0136],
        ...,
        [-0.0771, -0.2164, -0.0872,  ..., -0.1907,  0.1330, -0.2194],
        [-0.0593,  0.0067,  0.0207,  ..., -0.1813,  0.2490,  0.0700],
        [-0.2368,  0.1549, -0.2169,  ...,  0.2118,  0.2450, -0.1699]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.1635],
        [ 0.2947],
        [-0.2279],
        [-0.0971],
        [ 0.4253],
        [-0.2208],
        [-0.0520],
        [-0.0801],
        [ 0.2996],
        [-0.1765],
        [-0.1815],
        [-0.0932],
        [ 0.3076],
        [-0.2072],
        [-0.3728],
        [ 0.1222],
        [ 0.0702],
        [-0.4029],
        [ 0.0149],
        [ 0.0013],
        [ 0.2070],
        [ 0.3839],
        [ 0.1728],
        [-0.1048],
        [-0.1597],
        [-0.3909],
        [ 0.0452],
        [-0.0129],
        [-0.2043],
        [-0.1843],
        [ 0.2619],
        [-0.2032]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1635],
        [ 0.2947],
        [-0.2279],
        [-0.0971],
        [ 0.4253],
        [-0.2208],
        [-0.0520],
        [-0.0801],
        [ 0.2996],
        [-0.1765],
        [-0.1815],
        [-0.0932],
        [ 0.3076],
        [-0.2072],
        [-0.3728],
        [ 0.1222],
        [ 0.0702],
        [-0.4029],
        [ 0.0149],
        [ 0.0013],
        [ 0.2070],
        [ 0.3839],
        [ 0.1728],
        [-0.1048],
        [-0.1597],
        [-0.3909],
        [ 0.0452],
        [-0.0129],
        [-0.2043],
        [-0.1843],
        [ 0.2619],
        [-0.2032]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)





 shepe: torch.Size([2350554, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet] tensor([[0.0000],
        [0.0538],
        [0.0000],
        ...,
        [0.0000],
        [0.2137],
        [0.0297]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(209492.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-123.2021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6792, device='cuda:0')



h[100].sum tensor(2.1353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2845, device='cuda:0')



h[200].sum tensor(-2.6470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.8319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(27601.6855, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0090,  ..., 0.0054, 0.0065, 0.0027],
        [0.0012, 0.0000, 0.0027,  ..., 0.0016, 0.0020, 0.0008],
        [0.0005, 0.0000, 0.0011,  ..., 0.0007, 0.0008, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(631634.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5036.0381, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(81.8539, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2158],
        [-0.8223],
        [-0.5914],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-476054.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.0000],
        [0.0538],
        [0.0000],
        ...,
        [0.0000],
        [0.2137],
        [0.0297]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet].sum tensor(209492.3438, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-1.2158],
        [-0.8223],
        [-0.5914],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])





 shepe: torch.Size([47011080, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet] tensor([[0.4136],
        [0.0000],
        [0.2095],
        ...,
        [0.0000],
        [0.0000],
        [0.2603]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(4416284.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0122, -0.0191,  0.0008,  ...,  0.0178, -0.0084, -0.0144],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-3683.3452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.1130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(91.4165, device='cuda:0')



h[100].sum tensor(26.7912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.7754, device='cuda:0')



h[200].sum tensor(131.9570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(141.7297, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0489, 0.0000, 0.0032,  ..., 0.0714, 0.0000, 0.0000],
        [0.0400, 0.0000, 0.0026,  ..., 0.0584, 0.0000, 0.0000],
        [0.0040, 0.0000, 0.0003,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(232685.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0739, 0.1543,  ..., 0.0000, 0.1597, 0.1424],
        [0.0000, 0.0648, 0.1352,  ..., 0.0000, 0.1400, 0.1248],
        [0.0000, 0.0394, 0.0821,  ..., 0.0000, 0.0850, 0.0758],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(4554113.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1056.4043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[3.9070e+00],
        [4.5419e+00],
        [4.8427e+00],
        ...,
        [6.2304e-05],
        [8.6232e-05],
        [1.1117e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(1938239.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.4136],
        [0.0000],
        [0.2095],
        ...,
        [0.0000],
        [0.0000],
        [0.2603]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet].sum tensor(4416284.5000, device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-1.2158],
        [-0.8223],
        [-0.5914],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')



load_model False 
TraEvN 201 
BatchSize 15 
EpochNum 10 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 164 
endmesh 165 






optimizer.param_groups
 [{'params': [Parameter containing:
tensor([[ 0.0924, -0.1444,  0.0060,  0.0521, -0.0576, -0.0125,  0.0198, -0.0005,
          0.0345, -0.0387, -0.1268,  0.1058, -0.1115,  0.0734,  0.1248, -0.1250,
         -0.1387, -0.1240,  0.1473, -0.1025, -0.1163, -0.0138, -0.0569, -0.1477,
         -0.0210, -0.1088,  0.0555,  0.0727, -0.1369,  0.0793,  0.0305, -0.0720,
         -0.1331, -0.1387, -0.1050, -0.0896, -0.0729, -0.1272, -0.0720, -0.0240,
          0.1061, -0.0585,  0.0821,  0.0435, -0.0542,  0.0183, -0.0120, -0.0564,
         -0.0240,  0.0253,  0.0029,  0.0205,  0.0421, -0.0578, -0.0253,  0.0745,
         -0.0566,  0.0216,  0.0978, -0.0350,  0.1524,  0.0622,  0.1021,  0.1347,
          0.0564, -0.1069,  0.0728, -0.0460, -0.0592,  0.0421, -0.1297, -0.1282,
          0.0424,  0.0180, -0.1309,  0.1051, -0.1380,  0.0056, -0.0402,  0.1272,
         -0.0705, -0.0664,  0.0731,  0.0713,  0.1474,  0.1511, -0.0291, -0.0421,
          0.1076, -0.0559,  0.0448, -0.0443, -0.0474,  0.0505,  0.0874, -0.0311,
         -0.0633,  0.0910, -0.1358,  0.0323,  0.0291,  0.0426, -0.1178, -0.1346,
          0.1006, -0.0409, -0.1376, -0.1509,  0.0519, -0.0538, -0.0346, -0.0745,
         -0.1202, -0.1316,  0.0219, -0.1006, -0.0063,  0.1458, -0.0495, -0.0649,
         -0.1389,  0.0019, -0.0613,  0.0269, -0.0370,  0.0917,  0.0414,  0.1087,
         -0.1277,  0.0381, -0.0230, -0.0549, -0.0770,  0.0616, -0.1471,  0.0303,
         -0.0883,  0.0569, -0.1505, -0.0640, -0.0667, -0.0987,  0.0462, -0.1337,
          0.0338, -0.0906, -0.0543,  0.0598, -0.1510, -0.0357, -0.0526,  0.0820,
          0.1354,  0.0371,  0.1244,  0.0362, -0.1064, -0.0675, -0.1303, -0.0971,
         -0.0155,  0.0694, -0.0346, -0.0793, -0.0551,  0.0283,  0.0066, -0.0814,
          0.0389, -0.0217, -0.1177, -0.0677, -0.0925, -0.0726, -0.1062,  0.0257,
          0.0760,  0.1054, -0.0795, -0.0880, -0.1023, -0.0832,  0.0102,  0.1165,
          0.1439,  0.0143,  0.1218, -0.0186, -0.0431,  0.1082,  0.0876, -0.0674,
         -0.0346, -0.0553, -0.0790,  0.1092, -0.1207, -0.0556, -0.0560,  0.1168,
          0.1433, -0.1323,  0.0119,  0.0706, -0.1093, -0.0665,  0.0412, -0.1450,
         -0.1056,  0.1204,  0.1221, -0.0161, -0.1335,  0.0680, -0.0617, -0.1020,
         -0.1009, -0.1185,  0.0939,  0.0472, -0.0102,  0.0229,  0.1171, -0.1428,
         -0.0533,  0.0011, -0.0337,  0.0807,  0.0908, -0.0039,  0.1515, -0.1265,
         -0.0946,  0.0113, -0.1259,  0.1306,  0.1378,  0.0860, -0.1500, -0.0649,
          0.0505, -0.1390, -0.1007, -0.1125,  0.0017,  0.0704, -0.0801,  0.0416,
          0.0569,  0.0811, -0.0628, -0.0654, -0.0812,  0.1348, -0.0637, -0.1089]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.4795e-04,  3.5231e-04, -4.6584e-04,  ...,  2.8056e-04,
          8.4557e-05,  5.3728e-04],
        [-3.8032e-04, -2.8318e-04,  1.9007e-04,  ...,  5.7719e-04,
          6.0908e-04, -2.4290e-05],
        [ 4.0159e-04, -1.6507e-04, -5.9497e-04,  ..., -1.6909e-04,
         -5.4854e-04, -2.4839e-04],
        ...,
        [-1.5730e-04, -2.1983e-04, -4.0523e-04,  ...,  8.9487e-05,
          4.8976e-04,  6.1524e-04],
        [-5.3466e-04,  5.2058e-04,  5.5751e-04,  ..., -1.0371e-04,
         -3.2944e-04, -5.6412e-04],
        [ 1.5282e-04,  1.8105e-04, -2.1137e-04,  ...,  2.5737e-04,
         -5.1450e-04,  5.5595e-04]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 2.9169e-04, -4.9179e-04,  2.2612e-04,  6.1863e-04, -1.4524e-05,
        -1.5803e-04, -5.2509e-04, -3.6213e-04,  5.0356e-04,  2.6351e-04,
         4.7045e-04, -4.3363e-04, -4.5349e-04, -1.4001e-04, -7.2862e-05,
        -1.6946e-04,  3.9075e-04,  5.8983e-04, -1.8100e-04, -1.4333e-04,
         1.2217e-04,  8.8724e-05, -5.9500e-04, -3.1012e-04, -2.0814e-04,
         3.2736e-04, -3.2774e-04, -2.2353e-04,  3.5279e-04, -9.2411e-05,
        -4.2933e-04,  1.3068e-04,  4.3792e-04, -1.6883e-04, -7.9187e-05,
         1.4915e-06, -2.2889e-04,  5.2593e-04,  5.3688e-04, -5.9419e-04,
        -6.0822e-04, -1.0603e-04, -1.3488e-04, -2.6033e-04, -6.3622e-04,
         1.7952e-05, -3.4694e-04,  2.1590e-04, -7.6884e-05, -2.8180e-04,
        -3.5380e-04,  4.5953e-04, -3.1834e-04,  4.8793e-04, -3.0700e-04,
         4.7933e-04, -4.3345e-04, -4.2740e-04, -7.2685e-05,  3.3213e-04,
         2.0979e-04, -4.0336e-04,  3.5062e-04,  1.4733e-04,  3.5571e-04,
        -2.1280e-04,  3.3820e-04, -5.4268e-04, -4.5547e-04, -3.7347e-04,
         1.0955e-05,  4.4269e-04, -3.5647e-04,  1.7566e-05, -4.0915e-04,
        -3.0352e-04, -6.2201e-04,  1.0041e-04,  1.7384e-04, -2.5157e-04,
         4.1450e-05, -5.6902e-04, -2.0256e-04, -5.8008e-04,  6.3968e-04,
         4.8497e-04,  5.8916e-05,  6.2484e-04, -6.0103e-04,  5.5175e-04,
         1.8427e-04,  3.7468e-04, -5.5411e-04, -1.3180e-04,  1.1708e-04,
        -2.3159e-04, -4.7377e-04,  2.4165e-04, -3.0114e-04, -5.1236e-04,
         3.9031e-04,  5.6979e-04, -3.8262e-04,  3.0733e-04,  5.5107e-04,
         1.7014e-04,  6.4572e-04, -1.8667e-04,  7.6110e-05, -4.4203e-04,
         2.9575e-04,  3.5251e-04,  2.1090e-04,  4.7711e-04, -3.2314e-05,
         1.2360e-04,  1.0101e-04, -2.9128e-04, -1.8995e-04,  2.8572e-04,
         2.2886e-04,  2.6951e-04,  5.2029e-04,  5.3383e-05, -6.0972e-04,
         4.2731e-04,  3.6239e-04, -5.8104e-05,  2.2219e-04,  4.9568e-04,
         4.2048e-04, -2.5706e-06,  5.7186e-04, -6.0115e-04,  3.8080e-04,
         3.7476e-04, -6.1606e-04,  4.8039e-04,  1.8065e-04, -1.4778e-04,
        -3.5066e-05, -3.6662e-04,  5.4291e-04, -4.5610e-04, -8.7878e-05,
        -2.1022e-04, -4.2262e-04,  2.0401e-04, -2.5172e-04,  2.0845e-04,
         3.8686e-04, -4.6389e-04, -4.1495e-04,  4.4963e-04, -3.8430e-04,
         6.4069e-04,  9.0555e-05, -3.3395e-04, -4.3258e-04,  3.2887e-04,
         2.2215e-04, -2.6939e-04,  5.3313e-04, -1.0364e-04,  4.1574e-05,
         4.3317e-05, -6.3710e-04, -1.8516e-04,  2.4732e-04, -1.9511e-04,
        -3.8121e-04,  2.4425e-04, -8.3029e-05,  5.6362e-04,  4.4839e-04,
         5.7001e-04, -1.4960e-04, -1.7444e-04,  7.7837e-05,  3.6679e-04,
         1.1349e-04, -3.4460e-04,  2.9415e-04,  1.3466e-04,  6.4503e-04,
         3.0173e-04, -5.0225e-04, -6.0070e-04, -2.5752e-04,  9.9121e-05,
        -1.7641e-04, -1.4693e-04,  4.0695e-04, -5.9669e-04,  1.7335e-04,
        -6.3401e-04,  4.3838e-04, -6.3004e-04, -5.8616e-04, -4.4580e-04,
        -3.6994e-04, -3.4161e-04, -2.8120e-04, -3.8072e-04,  1.3854e-04,
        -3.6859e-04, -3.3048e-04, -1.1097e-04,  4.5406e-04, -2.2458e-05,
        -2.7363e-04, -4.4307e-04, -5.1885e-04,  5.5031e-04, -9.1930e-05,
        -3.5255e-04,  5.7248e-04, -5.5251e-04,  2.7454e-05, -4.1032e-04,
        -8.1995e-05,  5.2925e-04,  3.7179e-04, -4.4154e-04, -2.3967e-04,
         5.3286e-05,  8.6327e-05,  5.7784e-04,  5.3801e-05,  4.6127e-04,
        -1.4087e-04,  4.0739e-04, -5.6460e-04,  1.1892e-04,  1.2995e-04,
         6.0429e-04, -2.9063e-04,  3.1884e-05,  1.6343e-04, -3.9458e-04,
        -5.8890e-04, -3.3474e-04, -2.4240e-04,  1.1406e-04, -6.0009e-04,
         2.8142e-04, -1.1506e-04,  4.3883e-05, -1.6611e-04,  1.4872e-04,
        -1.7439e-04, -4.7797e-05,  1.3955e-04,  1.4088e-04,  2.9377e-04,
         3.9046e-04], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0588, -0.0077, -0.0290,  ...,  0.0363,  0.0457,  0.0255],
        [ 0.0503, -0.0385, -0.0515,  ..., -0.0170,  0.0368, -0.0405],
        [ 0.0020, -0.0608,  0.0212,  ..., -0.0251, -0.0316, -0.0548],
        ...,
        [-0.0501,  0.0212, -0.0388,  ...,  0.0327,  0.0412, -0.0314],
        [ 0.0221,  0.0093, -0.0294,  ..., -0.0613, -0.0577, -0.0503],
        [ 0.0311, -0.0420,  0.0054,  ..., -0.0169,  0.0064, -0.0536]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 0.0440, -0.0256, -0.0513,  ...,  0.0117,  0.0608, -0.0218],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0309, -0.1222, -0.0390,  ...,  0.1020, -0.0954,  0.0579],
        [ 0.0675,  0.0145, -0.0359,  ..., -0.1230,  0.0389,  0.0442],
        [ 0.0630, -0.0055, -0.0629,  ..., -0.0821,  0.0491,  0.0129],
        ...,
        [-0.0718,  0.0615, -0.0361,  ...,  0.0091,  0.0918,  0.1000],
        [-0.0036,  0.0372,  0.0263,  ..., -0.0275,  0.0404,  0.0577],
        [-0.0743,  0.0750, -0.0662,  ..., -0.0050,  0.0962,  0.1239]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0832, -0.0096,  0.1104,  ..., -0.1271,  0.0710, -0.0476],
        [ 0.0684,  0.1202, -0.0905,  ...,  0.0526, -0.1311, -0.1333],
        [ 0.0826, -0.1199,  0.0069,  ..., -0.0406,  0.1327, -0.1418],
        ...,
        [-0.1109, -0.0619,  0.1589,  ...,  0.0636,  0.1558, -0.0593],
        [ 0.1686,  0.0446,  0.0458,  ..., -0.0724, -0.0939,  0.0311],
        [-0.1526,  0.0639,  0.1661,  ..., -0.0796,  0.0921, -0.1297]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0951, -0.1623,  0.2384,  ...,  0.1227, -0.0748,  0.0640],
        [-0.2389,  0.0364, -0.1065,  ..., -0.1626,  0.2192, -0.0814],
        [ 0.1365,  0.2229, -0.2423,  ...,  0.0421,  0.1474, -0.2156],
        ...,
        [-0.1574, -0.2001, -0.1166,  ..., -0.0870,  0.1664,  0.1098],
        [-0.2144, -0.0101,  0.1050,  ..., -0.1857, -0.1199,  0.1056],
        [ 0.0719,  0.1710, -0.1870,  ..., -0.1834,  0.2456, -0.1606]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1107],
        [-0.1547],
        [-0.1605],
        [-0.4077],
        [ 0.2052],
        [-0.3738],
        [ 0.3249],
        [ 0.1810],
        [ 0.1184],
        [ 0.3665],
        [-0.4062],
        [ 0.1600],
        [-0.3866],
        [ 0.1492],
        [-0.3356],
        [ 0.2091],
        [ 0.2939],
        [ 0.1667],
        [-0.0895],
        [-0.4248],
        [ 0.1941],
        [ 0.2055],
        [ 0.2349],
        [-0.2755],
        [ 0.0837],
        [-0.2842],
        [-0.0538],
        [ 0.0142],
        [ 0.3150],
        [ 0.2428],
        [-0.0511],
        [ 0.0438]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups after adding efet as parameters
 [{'params': [Parameter containing:
tensor([[ 0.0924, -0.1444,  0.0060,  0.0521, -0.0576, -0.0125,  0.0198, -0.0005,
          0.0345, -0.0387, -0.1268,  0.1058, -0.1115,  0.0734,  0.1248, -0.1250,
         -0.1387, -0.1240,  0.1473, -0.1025, -0.1163, -0.0138, -0.0569, -0.1477,
         -0.0210, -0.1088,  0.0555,  0.0727, -0.1369,  0.0793,  0.0305, -0.0720,
         -0.1331, -0.1387, -0.1050, -0.0896, -0.0729, -0.1272, -0.0720, -0.0240,
          0.1061, -0.0585,  0.0821,  0.0435, -0.0542,  0.0183, -0.0120, -0.0564,
         -0.0240,  0.0253,  0.0029,  0.0205,  0.0421, -0.0578, -0.0253,  0.0745,
         -0.0566,  0.0216,  0.0978, -0.0350,  0.1524,  0.0622,  0.1021,  0.1347,
          0.0564, -0.1069,  0.0728, -0.0460, -0.0592,  0.0421, -0.1297, -0.1282,
          0.0424,  0.0180, -0.1309,  0.1051, -0.1380,  0.0056, -0.0402,  0.1272,
         -0.0705, -0.0664,  0.0731,  0.0713,  0.1474,  0.1511, -0.0291, -0.0421,
          0.1076, -0.0559,  0.0448, -0.0443, -0.0474,  0.0505,  0.0874, -0.0311,
         -0.0633,  0.0910, -0.1358,  0.0323,  0.0291,  0.0426, -0.1178, -0.1346,
          0.1006, -0.0409, -0.1376, -0.1509,  0.0519, -0.0538, -0.0346, -0.0745,
         -0.1202, -0.1316,  0.0219, -0.1006, -0.0063,  0.1458, -0.0495, -0.0649,
         -0.1389,  0.0019, -0.0613,  0.0269, -0.0370,  0.0917,  0.0414,  0.1087,
         -0.1277,  0.0381, -0.0230, -0.0549, -0.0770,  0.0616, -0.1471,  0.0303,
         -0.0883,  0.0569, -0.1505, -0.0640, -0.0667, -0.0987,  0.0462, -0.1337,
          0.0338, -0.0906, -0.0543,  0.0598, -0.1510, -0.0357, -0.0526,  0.0820,
          0.1354,  0.0371,  0.1244,  0.0362, -0.1064, -0.0675, -0.1303, -0.0971,
         -0.0155,  0.0694, -0.0346, -0.0793, -0.0551,  0.0283,  0.0066, -0.0814,
          0.0389, -0.0217, -0.1177, -0.0677, -0.0925, -0.0726, -0.1062,  0.0257,
          0.0760,  0.1054, -0.0795, -0.0880, -0.1023, -0.0832,  0.0102,  0.1165,
          0.1439,  0.0143,  0.1218, -0.0186, -0.0431,  0.1082,  0.0876, -0.0674,
         -0.0346, -0.0553, -0.0790,  0.1092, -0.1207, -0.0556, -0.0560,  0.1168,
          0.1433, -0.1323,  0.0119,  0.0706, -0.1093, -0.0665,  0.0412, -0.1450,
         -0.1056,  0.1204,  0.1221, -0.0161, -0.1335,  0.0680, -0.0617, -0.1020,
         -0.1009, -0.1185,  0.0939,  0.0472, -0.0102,  0.0229,  0.1171, -0.1428,
         -0.0533,  0.0011, -0.0337,  0.0807,  0.0908, -0.0039,  0.1515, -0.1265,
         -0.0946,  0.0113, -0.1259,  0.1306,  0.1378,  0.0860, -0.1500, -0.0649,
          0.0505, -0.1390, -0.1007, -0.1125,  0.0017,  0.0704, -0.0801,  0.0416,
          0.0569,  0.0811, -0.0628, -0.0654, -0.0812,  0.1348, -0.0637, -0.1089]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.4795e-04,  3.5231e-04, -4.6584e-04,  ...,  2.8056e-04,
          8.4557e-05,  5.3728e-04],
        [-3.8032e-04, -2.8318e-04,  1.9007e-04,  ...,  5.7719e-04,
          6.0908e-04, -2.4290e-05],
        [ 4.0159e-04, -1.6507e-04, -5.9497e-04,  ..., -1.6909e-04,
         -5.4854e-04, -2.4839e-04],
        ...,
        [-1.5730e-04, -2.1983e-04, -4.0523e-04,  ...,  8.9487e-05,
          4.8976e-04,  6.1524e-04],
        [-5.3466e-04,  5.2058e-04,  5.5751e-04,  ..., -1.0371e-04,
         -3.2944e-04, -5.6412e-04],
        [ 1.5282e-04,  1.8105e-04, -2.1137e-04,  ...,  2.5737e-04,
         -5.1450e-04,  5.5595e-04]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 2.9169e-04, -4.9179e-04,  2.2612e-04,  6.1863e-04, -1.4524e-05,
        -1.5803e-04, -5.2509e-04, -3.6213e-04,  5.0356e-04,  2.6351e-04,
         4.7045e-04, -4.3363e-04, -4.5349e-04, -1.4001e-04, -7.2862e-05,
        -1.6946e-04,  3.9075e-04,  5.8983e-04, -1.8100e-04, -1.4333e-04,
         1.2217e-04,  8.8724e-05, -5.9500e-04, -3.1012e-04, -2.0814e-04,
         3.2736e-04, -3.2774e-04, -2.2353e-04,  3.5279e-04, -9.2411e-05,
        -4.2933e-04,  1.3068e-04,  4.3792e-04, -1.6883e-04, -7.9187e-05,
         1.4915e-06, -2.2889e-04,  5.2593e-04,  5.3688e-04, -5.9419e-04,
        -6.0822e-04, -1.0603e-04, -1.3488e-04, -2.6033e-04, -6.3622e-04,
         1.7952e-05, -3.4694e-04,  2.1590e-04, -7.6884e-05, -2.8180e-04,
        -3.5380e-04,  4.5953e-04, -3.1834e-04,  4.8793e-04, -3.0700e-04,
         4.7933e-04, -4.3345e-04, -4.2740e-04, -7.2685e-05,  3.3213e-04,
         2.0979e-04, -4.0336e-04,  3.5062e-04,  1.4733e-04,  3.5571e-04,
        -2.1280e-04,  3.3820e-04, -5.4268e-04, -4.5547e-04, -3.7347e-04,
         1.0955e-05,  4.4269e-04, -3.5647e-04,  1.7566e-05, -4.0915e-04,
        -3.0352e-04, -6.2201e-04,  1.0041e-04,  1.7384e-04, -2.5157e-04,
         4.1450e-05, -5.6902e-04, -2.0256e-04, -5.8008e-04,  6.3968e-04,
         4.8497e-04,  5.8916e-05,  6.2484e-04, -6.0103e-04,  5.5175e-04,
         1.8427e-04,  3.7468e-04, -5.5411e-04, -1.3180e-04,  1.1708e-04,
        -2.3159e-04, -4.7377e-04,  2.4165e-04, -3.0114e-04, -5.1236e-04,
         3.9031e-04,  5.6979e-04, -3.8262e-04,  3.0733e-04,  5.5107e-04,
         1.7014e-04,  6.4572e-04, -1.8667e-04,  7.6110e-05, -4.4203e-04,
         2.9575e-04,  3.5251e-04,  2.1090e-04,  4.7711e-04, -3.2314e-05,
         1.2360e-04,  1.0101e-04, -2.9128e-04, -1.8995e-04,  2.8572e-04,
         2.2886e-04,  2.6951e-04,  5.2029e-04,  5.3383e-05, -6.0972e-04,
         4.2731e-04,  3.6239e-04, -5.8104e-05,  2.2219e-04,  4.9568e-04,
         4.2048e-04, -2.5706e-06,  5.7186e-04, -6.0115e-04,  3.8080e-04,
         3.7476e-04, -6.1606e-04,  4.8039e-04,  1.8065e-04, -1.4778e-04,
        -3.5066e-05, -3.6662e-04,  5.4291e-04, -4.5610e-04, -8.7878e-05,
        -2.1022e-04, -4.2262e-04,  2.0401e-04, -2.5172e-04,  2.0845e-04,
         3.8686e-04, -4.6389e-04, -4.1495e-04,  4.4963e-04, -3.8430e-04,
         6.4069e-04,  9.0555e-05, -3.3395e-04, -4.3258e-04,  3.2887e-04,
         2.2215e-04, -2.6939e-04,  5.3313e-04, -1.0364e-04,  4.1574e-05,
         4.3317e-05, -6.3710e-04, -1.8516e-04,  2.4732e-04, -1.9511e-04,
        -3.8121e-04,  2.4425e-04, -8.3029e-05,  5.6362e-04,  4.4839e-04,
         5.7001e-04, -1.4960e-04, -1.7444e-04,  7.7837e-05,  3.6679e-04,
         1.1349e-04, -3.4460e-04,  2.9415e-04,  1.3466e-04,  6.4503e-04,
         3.0173e-04, -5.0225e-04, -6.0070e-04, -2.5752e-04,  9.9121e-05,
        -1.7641e-04, -1.4693e-04,  4.0695e-04, -5.9669e-04,  1.7335e-04,
        -6.3401e-04,  4.3838e-04, -6.3004e-04, -5.8616e-04, -4.4580e-04,
        -3.6994e-04, -3.4161e-04, -2.8120e-04, -3.8072e-04,  1.3854e-04,
        -3.6859e-04, -3.3048e-04, -1.1097e-04,  4.5406e-04, -2.2458e-05,
        -2.7363e-04, -4.4307e-04, -5.1885e-04,  5.5031e-04, -9.1930e-05,
        -3.5255e-04,  5.7248e-04, -5.5251e-04,  2.7454e-05, -4.1032e-04,
        -8.1995e-05,  5.2925e-04,  3.7179e-04, -4.4154e-04, -2.3967e-04,
         5.3286e-05,  8.6327e-05,  5.7784e-04,  5.3801e-05,  4.6127e-04,
        -1.4087e-04,  4.0739e-04, -5.6460e-04,  1.1892e-04,  1.2995e-04,
         6.0429e-04, -2.9063e-04,  3.1884e-05,  1.6343e-04, -3.9458e-04,
        -5.8890e-04, -3.3474e-04, -2.4240e-04,  1.1406e-04, -6.0009e-04,
         2.8142e-04, -1.1506e-04,  4.3883e-05, -1.6611e-04,  1.4872e-04,
        -1.7439e-04, -4.7797e-05,  1.3955e-04,  1.4088e-04,  2.9377e-04,
         3.9046e-04], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0588, -0.0077, -0.0290,  ...,  0.0363,  0.0457,  0.0255],
        [ 0.0503, -0.0385, -0.0515,  ..., -0.0170,  0.0368, -0.0405],
        [ 0.0020, -0.0608,  0.0212,  ..., -0.0251, -0.0316, -0.0548],
        ...,
        [-0.0501,  0.0212, -0.0388,  ...,  0.0327,  0.0412, -0.0314],
        [ 0.0221,  0.0093, -0.0294,  ..., -0.0613, -0.0577, -0.0503],
        [ 0.0311, -0.0420,  0.0054,  ..., -0.0169,  0.0064, -0.0536]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 0.0440, -0.0256, -0.0513,  ...,  0.0117,  0.0608, -0.0218],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0309, -0.1222, -0.0390,  ...,  0.1020, -0.0954,  0.0579],
        [ 0.0675,  0.0145, -0.0359,  ..., -0.1230,  0.0389,  0.0442],
        [ 0.0630, -0.0055, -0.0629,  ..., -0.0821,  0.0491,  0.0129],
        ...,
        [-0.0718,  0.0615, -0.0361,  ...,  0.0091,  0.0918,  0.1000],
        [-0.0036,  0.0372,  0.0263,  ..., -0.0275,  0.0404,  0.0577],
        [-0.0743,  0.0750, -0.0662,  ..., -0.0050,  0.0962,  0.1239]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0832, -0.0096,  0.1104,  ..., -0.1271,  0.0710, -0.0476],
        [ 0.0684,  0.1202, -0.0905,  ...,  0.0526, -0.1311, -0.1333],
        [ 0.0826, -0.1199,  0.0069,  ..., -0.0406,  0.1327, -0.1418],
        ...,
        [-0.1109, -0.0619,  0.1589,  ...,  0.0636,  0.1558, -0.0593],
        [ 0.1686,  0.0446,  0.0458,  ..., -0.0724, -0.0939,  0.0311],
        [-0.1526,  0.0639,  0.1661,  ..., -0.0796,  0.0921, -0.1297]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0951, -0.1623,  0.2384,  ...,  0.1227, -0.0748,  0.0640],
        [-0.2389,  0.0364, -0.1065,  ..., -0.1626,  0.2192, -0.0814],
        [ 0.1365,  0.2229, -0.2423,  ...,  0.0421,  0.1474, -0.2156],
        ...,
        [-0.1574, -0.2001, -0.1166,  ..., -0.0870,  0.1664,  0.1098],
        [-0.2144, -0.0101,  0.1050,  ..., -0.1857, -0.1199,  0.1056],
        [ 0.0719,  0.1710, -0.1870,  ..., -0.1834,  0.2456, -0.1606]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1107],
        [-0.1547],
        [-0.1605],
        [-0.4077],
        [ 0.2052],
        [-0.3738],
        [ 0.3249],
        [ 0.1810],
        [ 0.1184],
        [ 0.3665],
        [-0.4062],
        [ 0.1600],
        [-0.3866],
        [ 0.1492],
        [-0.3356],
        [ 0.2091],
        [ 0.2939],
        [ 0.1667],
        [-0.0895],
        [-0.4248],
        [ 0.1941],
        [ 0.2055],
        [ 0.2349],
        [-0.2755],
        [ 0.0837],
        [-0.2842],
        [-0.0538],
        [ 0.0142],
        [ 0.3150],
        [ 0.2428],
        [-0.0511],
        [ 0.0438]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]





 shepe: torch.Size([35258310, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(893.2103, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet] tensor([[0.4136],
        [0.0000],
        [0.2095],
        ...,
        [0.0000],
        [0.0000],
        [0.2603]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(3312213.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(893.2103, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053, -0.0083,  0.0003,  ...,  0.0077, -0.0036, -0.0062],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3335.5950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.0774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(82.5585, device='cuda:0')



h[100].sum tensor(24.2618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.9871, device='cuda:0')



h[200].sum tensor(119.4987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(127.9966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0000, 0.0004,  ..., 0.0085, 0.0000, 0.0000],
        [0.0081, 0.0000, 0.0005,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(220731.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0061, 0.0128,  ..., 0.0000, 0.0133, 0.0118],
        [0.0000, 0.0059, 0.0124,  ..., 0.0000, 0.0128, 0.0114],
        [0.0000, 0.0011, 0.0022,  ..., 0.0000, 0.0023, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(4457363.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1002.1333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[8.1720e-02],
        [1.2788e-01],
        [1.8753e-01],
        ...,
        [5.9171e-07],
        [3.7882e-06],
        [1.6468e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(1919445.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.4136],
        [0.0000],
        [0.2095],
        ...,
        [0.0000],
        [0.0000],
        [0.2603]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet].sum tensor(3312213.2500, device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/./TrainingBha2ndneiefet.py", line 85, in <module>
    optimizer.step()  # Update parameters based on gradients.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/optim/adam.py", line 115, in step
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
RuntimeError: CUDA out of memory. Tried to allocate 2.24 GiB (GPU 0; 31.75 GiB total capacity; 27.47 GiB already allocated; 1.98 GiB free; 28.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

real	0m53.278s
user	0m46.168s
sys	0m7.415s
