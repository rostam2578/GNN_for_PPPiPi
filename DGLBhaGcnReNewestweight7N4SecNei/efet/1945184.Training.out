0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.80.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.80.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jan  9 09:56:25 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b9f5e858880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	1m21.695s
user	0m3.773s
sys	0m3.266s




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 6507, 6507, 6507],
        [   1,    2,    3,  ..., 6219, 6794, 6795]]) 

edge_index shape
 torch.Size([2, 1175277])
graph: Graph(num_nodes=6796, num_edges=1175277,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 6507, 6507, 6507], device='cuda:0'), tensor([   1,    2,    3,  ..., 6219, 6794, 6795], device='cuda:0'))

number of nodes: 6796

number of edges: 2350554

node features (random input): tensor([[-0.2993],
        [ 1.7021],
        [-1.0468],
        ...,
        [ 1.6090],
        [ 0.1709],
        [ 0.7952]], device='cuda:0', requires_grad=True) 
node features sum: tensor(53.5809, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[-0.1384],
        [-0.4762],
        [ 0.4082],
        ...,
        [ 3.9284],
        [ 0.0328],
        [-0.8505]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(1108.1903, device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 399

In degrees of node 234: 399





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLBhaGcnReNewestweight7N4SecNei

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0002,  0.1262,  0.0044, -0.0629, -0.1382, -0.1257,  0.0170,  0.0544,
         -0.0142,  0.0878,  0.0608,  0.0868,  0.1356, -0.0393,  0.1191,  0.0592,
         -0.1345, -0.1250,  0.1336,  0.0119, -0.0416, -0.0454,  0.0985, -0.0560,
          0.0728,  0.0439,  0.1085,  0.1324,  0.0417, -0.0455, -0.0912,  0.0619,
         -0.0954, -0.0398,  0.1169, -0.1460, -0.0463, -0.0990, -0.0944, -0.0860,
          0.0166, -0.0661,  0.1285,  0.0348, -0.0186, -0.0584,  0.0821, -0.0798,
         -0.0354, -0.0051,  0.1186,  0.1466, -0.0284,  0.1124,  0.1485,  0.1488,
          0.0496, -0.0010,  0.0602,  0.0209, -0.0608,  0.1246,  0.0942, -0.0827,
         -0.0858,  0.0413, -0.0286, -0.0012, -0.0575,  0.0838, -0.0372,  0.0340,
          0.1427,  0.1363,  0.0538, -0.0963, -0.1268,  0.1354, -0.1328, -0.0626,
         -0.0784, -0.1018, -0.1490, -0.1079,  0.0144, -0.0587, -0.1389, -0.0012,
         -0.0571,  0.0591, -0.1120, -0.0404,  0.1299, -0.0657, -0.0198, -0.0463,
          0.0854, -0.0529, -0.0425,  0.1093, -0.1521,  0.0339, -0.1256, -0.0849,
         -0.1497,  0.0119, -0.0006,  0.0745, -0.0940, -0.0005, -0.0779,  0.1454,
         -0.0352,  0.0934,  0.0749,  0.0509,  0.1194, -0.0422, -0.1482, -0.1140,
         -0.1128, -0.1177,  0.1172, -0.0100, -0.1000, -0.1231,  0.0228,  0.0495,
         -0.1176,  0.0074,  0.0169, -0.0924,  0.0994,  0.0644, -0.1114,  0.1372,
          0.0200,  0.1113,  0.1115, -0.0306, -0.0938,  0.0659, -0.1077,  0.1362,
          0.0320,  0.0569, -0.0664,  0.0645, -0.0697, -0.1498, -0.1417,  0.0079,
         -0.0775,  0.1321,  0.0493, -0.0362, -0.0252, -0.1062,  0.0304,  0.1158,
         -0.1298, -0.0732, -0.0042, -0.1016,  0.1390,  0.0509,  0.0383, -0.1000,
          0.0325,  0.1237, -0.1046, -0.0248,  0.0409, -0.0427, -0.1448, -0.0203,
         -0.0389, -0.0667,  0.0585,  0.0050,  0.0444, -0.0605, -0.1483, -0.0283,
         -0.0456,  0.0086, -0.1474,  0.0553, -0.1071,  0.0480,  0.1233,  0.0678,
         -0.1436,  0.0391, -0.1376, -0.1167, -0.1405, -0.0448, -0.0584, -0.1353,
         -0.0248, -0.0343,  0.1429,  0.1061,  0.0283, -0.1131,  0.0664, -0.0601,
         -0.1089,  0.0134,  0.1427, -0.1450,  0.1368, -0.0174, -0.0324,  0.0196,
          0.1470, -0.1383, -0.0447, -0.0485, -0.1268,  0.1072,  0.1505,  0.1295,
         -0.0559,  0.0628, -0.1115,  0.0381,  0.0711, -0.0030,  0.1491, -0.0921,
          0.0342, -0.0273,  0.1116, -0.0174, -0.0189, -0.0871,  0.0986, -0.0523,
          0.0253,  0.0895,  0.1141, -0.0977,  0.0159, -0.1048,  0.0257,  0.0896,
         -0.0667,  0.0245, -0.1383, -0.0684,  0.0727,  0.0539,  0.0180, -0.0517]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0002,  0.1262,  0.0044, -0.0629, -0.1382, -0.1257,  0.0170,  0.0544,
         -0.0142,  0.0878,  0.0608,  0.0868,  0.1356, -0.0393,  0.1191,  0.0592,
         -0.1345, -0.1250,  0.1336,  0.0119, -0.0416, -0.0454,  0.0985, -0.0560,
          0.0728,  0.0439,  0.1085,  0.1324,  0.0417, -0.0455, -0.0912,  0.0619,
         -0.0954, -0.0398,  0.1169, -0.1460, -0.0463, -0.0990, -0.0944, -0.0860,
          0.0166, -0.0661,  0.1285,  0.0348, -0.0186, -0.0584,  0.0821, -0.0798,
         -0.0354, -0.0051,  0.1186,  0.1466, -0.0284,  0.1124,  0.1485,  0.1488,
          0.0496, -0.0010,  0.0602,  0.0209, -0.0608,  0.1246,  0.0942, -0.0827,
         -0.0858,  0.0413, -0.0286, -0.0012, -0.0575,  0.0838, -0.0372,  0.0340,
          0.1427,  0.1363,  0.0538, -0.0963, -0.1268,  0.1354, -0.1328, -0.0626,
         -0.0784, -0.1018, -0.1490, -0.1079,  0.0144, -0.0587, -0.1389, -0.0012,
         -0.0571,  0.0591, -0.1120, -0.0404,  0.1299, -0.0657, -0.0198, -0.0463,
          0.0854, -0.0529, -0.0425,  0.1093, -0.1521,  0.0339, -0.1256, -0.0849,
         -0.1497,  0.0119, -0.0006,  0.0745, -0.0940, -0.0005, -0.0779,  0.1454,
         -0.0352,  0.0934,  0.0749,  0.0509,  0.1194, -0.0422, -0.1482, -0.1140,
         -0.1128, -0.1177,  0.1172, -0.0100, -0.1000, -0.1231,  0.0228,  0.0495,
         -0.1176,  0.0074,  0.0169, -0.0924,  0.0994,  0.0644, -0.1114,  0.1372,
          0.0200,  0.1113,  0.1115, -0.0306, -0.0938,  0.0659, -0.1077,  0.1362,
          0.0320,  0.0569, -0.0664,  0.0645, -0.0697, -0.1498, -0.1417,  0.0079,
         -0.0775,  0.1321,  0.0493, -0.0362, -0.0252, -0.1062,  0.0304,  0.1158,
         -0.1298, -0.0732, -0.0042, -0.1016,  0.1390,  0.0509,  0.0383, -0.1000,
          0.0325,  0.1237, -0.1046, -0.0248,  0.0409, -0.0427, -0.1448, -0.0203,
         -0.0389, -0.0667,  0.0585,  0.0050,  0.0444, -0.0605, -0.1483, -0.0283,
         -0.0456,  0.0086, -0.1474,  0.0553, -0.1071,  0.0480,  0.1233,  0.0678,
         -0.1436,  0.0391, -0.1376, -0.1167, -0.1405, -0.0448, -0.0584, -0.1353,
         -0.0248, -0.0343,  0.1429,  0.1061,  0.0283, -0.1131,  0.0664, -0.0601,
         -0.1089,  0.0134,  0.1427, -0.1450,  0.1368, -0.0174, -0.0324,  0.0196,
          0.1470, -0.1383, -0.0447, -0.0485, -0.1268,  0.1072,  0.1505,  0.1295,
         -0.0559,  0.0628, -0.1115,  0.0381,  0.0711, -0.0030,  0.1491, -0.0921,
          0.0342, -0.0273,  0.1116, -0.0174, -0.0189, -0.0871,  0.0986, -0.0523,
          0.0253,  0.0895,  0.1141, -0.0977,  0.0159, -0.1048,  0.0257,  0.0896,
         -0.0667,  0.0245, -0.1383, -0.0684,  0.0727,  0.0539,  0.0180, -0.0517]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0694, -0.0344,  0.0180,  ..., -0.1056,  0.0160, -0.0800],
        [ 0.0189,  0.1197, -0.0752,  ...,  0.0521,  0.0375,  0.1107],
        [-0.0620,  0.0217,  0.0234,  ..., -0.1067,  0.1245,  0.0410],
        ...,
        [ 0.0529, -0.0144,  0.0511,  ..., -0.0633, -0.0603,  0.0813],
        [ 0.1193, -0.0313,  0.1179,  ...,  0.0328, -0.0275, -0.0996],
        [-0.0026, -0.0072, -0.0858,  ...,  0.0654, -0.1074,  0.0112]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0694, -0.0344,  0.0180,  ..., -0.1056,  0.0160, -0.0800],
        [ 0.0189,  0.1197, -0.0752,  ...,  0.0521,  0.0375,  0.1107],
        [-0.0620,  0.0217,  0.0234,  ..., -0.1067,  0.1245,  0.0410],
        ...,
        [ 0.0529, -0.0144,  0.0511,  ..., -0.0633, -0.0603,  0.0813],
        [ 0.1193, -0.0313,  0.1179,  ...,  0.0328, -0.0275, -0.0996],
        [-0.0026, -0.0072, -0.0858,  ...,  0.0654, -0.1074,  0.0112]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0463, -0.0623,  0.0338,  ...,  0.1657,  0.1121,  0.0554],
        [-0.0153, -0.0306, -0.0569,  ...,  0.0965,  0.1154,  0.0945],
        [-0.1583, -0.1283,  0.0573,  ...,  0.0460, -0.0837,  0.0951],
        ...,
        [ 0.0449, -0.1012, -0.0255,  ..., -0.1009, -0.0324, -0.0771],
        [-0.1230, -0.0949,  0.0838,  ..., -0.0880,  0.0612, -0.1298],
        [ 0.1484,  0.1230,  0.0980,  ..., -0.0500, -0.0026,  0.1088]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0463, -0.0623,  0.0338,  ...,  0.1657,  0.1121,  0.0554],
        [-0.0153, -0.0306, -0.0569,  ...,  0.0965,  0.1154,  0.0945],
        [-0.1583, -0.1283,  0.0573,  ...,  0.0460, -0.0837,  0.0951],
        ...,
        [ 0.0449, -0.1012, -0.0255,  ..., -0.1009, -0.0324, -0.0771],
        [-0.1230, -0.0949,  0.0838,  ..., -0.0880,  0.0612, -0.1298],
        [ 0.1484,  0.1230,  0.0980,  ..., -0.0500, -0.0026,  0.1088]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.2103, -0.2023,  0.0316,  ..., -0.1759, -0.1755, -0.0868],
        [-0.1090,  0.0735, -0.1734,  ...,  0.0317, -0.0032,  0.1555],
        [-0.0322, -0.0339,  0.1870,  ..., -0.0171, -0.1166, -0.2095],
        ...,
        [ 0.1095,  0.2039, -0.0573,  ..., -0.1960,  0.1371, -0.1015],
        [-0.1283,  0.1810,  0.2113,  ...,  0.2449, -0.1403, -0.0094],
        [ 0.0412, -0.0704, -0.1607,  ...,  0.1052,  0.2320, -0.2373]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2103, -0.2023,  0.0316,  ..., -0.1759, -0.1755, -0.0868],
        [-0.1090,  0.0735, -0.1734,  ...,  0.0317, -0.0032,  0.1555],
        [-0.0322, -0.0339,  0.1870,  ..., -0.0171, -0.1166, -0.2095],
        ...,
        [ 0.1095,  0.2039, -0.0573,  ..., -0.1960,  0.1371, -0.1015],
        [-0.1283,  0.1810,  0.2113,  ...,  0.2449, -0.1403, -0.0094],
        [ 0.0412, -0.0704, -0.1607,  ...,  0.1052,  0.2320, -0.2373]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.0134],
        [-0.1115],
        [ 0.0150],
        [ 0.3175],
        [ 0.3627],
        [-0.1850],
        [ 0.2446],
        [-0.0566],
        [-0.3314],
        [ 0.0696],
        [-0.0089],
        [ 0.0927],
        [-0.3780],
        [ 0.0228],
        [ 0.2026],
        [-0.3637],
        [-0.0251],
        [-0.0794],
        [-0.0060],
        [ 0.1410],
        [-0.3909],
        [ 0.2672],
        [-0.0842],
        [-0.0560],
        [-0.1244],
        [ 0.2079],
        [-0.2625],
        [-0.1238],
        [-0.3882],
        [-0.0097],
        [ 0.2657],
        [ 0.1836]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0134],
        [-0.1115],
        [ 0.0150],
        [ 0.3175],
        [ 0.3627],
        [-0.1850],
        [ 0.2446],
        [-0.0566],
        [-0.3314],
        [ 0.0696],
        [-0.0089],
        [ 0.0927],
        [-0.3780],
        [ 0.0228],
        [ 0.2026],
        [-0.3637],
        [-0.0251],
        [-0.0794],
        [-0.0060],
        [ 0.1410],
        [-0.3909],
        [ 0.2672],
        [-0.0842],
        [-0.0560],
        [-0.1244],
        [ 0.2079],
        [-0.2625],
        [-0.1238],
        [-0.3882],
        [-0.0097],
        [ 0.2657],
        [ 0.1836]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet] tensor([[1.9143e-02],
        [2.2675e-01],
        [1.6661e-01],
        ...,
        [1.5432e+01],
        [1.0765e-03],
        [7.2340e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].sum tensor(2351832.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(48.4095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-4.0775, device='cuda:0')



h[100].sum tensor(-3.2028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.4265, device='cuda:0')



h[200].sum tensor(-2.1003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.2470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(349840.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7209, 0.0000, 0.0000,  ..., 3.1932, 2.9424, 0.0000],
        [0.5952, 0.0000, 0.0000,  ..., 1.1043, 1.0178, 0.0000],
        [0.2033, 0.0000, 0.0000,  ..., 0.3773, 0.3477, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(76879528., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1096835.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1588.0924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2143.7607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.7717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1080.7426],
        [ 698.4617],
        [ 469.7117],
        ...,
        [   0.0000],
        [   0.0000],
        [   0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(3.3510e+08, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.9143e-02],
        [2.2675e-01],
        [1.6661e-01],
        ...,
        [1.5432e+01],
        [1.0765e-03],
        [7.2340e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet].sum tensor(2351832.5000, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[1080.7426],
        [ 698.4617],
        [ 469.7117],
        ...,
        [   0.0000],
        [   0.0000],
        [   0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet] tensor([[1.9143e-02],
        [2.2675e-01],
        [1.6661e-01],
        ...,
        [1.5432e+01],
        [1.0765e-03],
        [7.2340e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].sum tensor(47036648., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099,  0.0008,  0.0021,  ..., -0.0032, -0.0089, -0.0068],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(183.2540, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(69.1260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(74.2455, device='cuda:0')



h[100].sum tensor(-21.5661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-23.1632, device='cuda:0')



h[200].sum tensor(-74.0601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-79.5450, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.5996, 0.0475, 0.1299,  ..., 0.0000, 0.0000, 0.0000],
        [0.4342, 0.0344, 0.0941,  ..., 0.0000, 0.0000, 0.0000],
        [0.1037, 0.0082, 0.0225,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(3042048.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[ 0.0000, 51.3219,  0.0000,  ...,  0.0000,  0.0000, 32.1085],
        [ 0.0000, 37.5578,  0.0000,  ...,  0.0000,  0.0000, 23.4972],
        [ 0.0000, 24.4425,  0.0000,  ...,  0.0000,  0.0000, 15.2923],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(7.7329e+08, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-46173.0430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-8668.8818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20337424., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30977.4531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6848e+03],
        [-2.6596e+03],
        [-2.9246e+03],
        ...,
        [-3.9922e-02],
        [-7.5922e-02],
        [-1.1864e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-1.6276e+09, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.9143e-02],
        [2.2675e-01],
        [1.6661e-01],
        ...,
        [1.5432e+01],
        [1.0765e-03],
        [7.2340e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet].sum tensor(47036648., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[1080.7426],
        [ 698.4617],
        [ 469.7117],
        ...,
        [   0.0000],
        [   0.0000],
        [   0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')



load_model False 
TraEvN 701 
BatchSize 15 
EpochNum 10 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 164 
endmesh 165 






optimizer.param_groups
 [{'params': [Parameter containing:
tensor([[ 0.0751,  0.0059,  0.0163, -0.0705, -0.0532, -0.1141,  0.1302,  0.0756,
          0.1345,  0.0651,  0.0708, -0.0883,  0.0046,  0.0600, -0.0634, -0.0395,
         -0.0657, -0.1278,  0.0874,  0.0383,  0.1292, -0.1097,  0.1513,  0.1081,
          0.0163,  0.0486,  0.0747, -0.0922, -0.0182,  0.0304, -0.0316, -0.0301,
         -0.0033, -0.0900, -0.0212,  0.0847, -0.0563,  0.1356,  0.1243, -0.1516,
         -0.0474,  0.0126, -0.1414, -0.0223,  0.0891,  0.1294,  0.0240, -0.0907,
          0.0539,  0.0431,  0.1294,  0.1353,  0.0752, -0.0197, -0.1389, -0.1388,
         -0.0982,  0.1516,  0.0888, -0.1339, -0.0344,  0.0485, -0.0266,  0.1421,
          0.0978,  0.0119,  0.0330, -0.1028,  0.0120, -0.1504, -0.1462, -0.0049,
          0.1368, -0.1232, -0.1085, -0.0375,  0.0226, -0.1336, -0.0477,  0.1161,
          0.0704, -0.0702,  0.1506, -0.0210,  0.1066,  0.0766,  0.0540, -0.1488,
         -0.0748, -0.0238, -0.1041,  0.1088,  0.0724, -0.0473, -0.0859,  0.0640,
         -0.1102, -0.1001, -0.0539,  0.1470, -0.0234,  0.0932,  0.1017, -0.1187,
          0.0364,  0.1422,  0.0942, -0.1234,  0.1063,  0.0661,  0.1032, -0.1510,
         -0.0429, -0.1339, -0.0020, -0.0201, -0.1129,  0.1085, -0.0393,  0.0252,
          0.0438,  0.0090,  0.0572, -0.1164,  0.0897,  0.0003,  0.0410, -0.0423,
          0.0086, -0.0279,  0.1253, -0.0551,  0.0262, -0.0400, -0.1272,  0.0284,
          0.1474, -0.0860, -0.0011,  0.1395,  0.1265, -0.0566, -0.0584, -0.1503,
         -0.1233, -0.0977,  0.0511, -0.0690, -0.0662,  0.1032, -0.0550,  0.0369,
         -0.0104, -0.0161, -0.1214, -0.0743, -0.0819,  0.0778,  0.0213,  0.0952,
          0.0370, -0.0910, -0.0696, -0.1045, -0.0673, -0.0115, -0.1312,  0.1286,
         -0.0587,  0.0065,  0.0893, -0.1416, -0.0095,  0.0957,  0.0740,  0.1078,
          0.0478, -0.0578, -0.0360,  0.0576,  0.1179,  0.0406,  0.0136, -0.0413,
          0.1288,  0.0767, -0.0248, -0.0655,  0.0298,  0.0089, -0.0665, -0.0796,
         -0.1075,  0.0254,  0.0456, -0.0697,  0.0431,  0.0951,  0.0933,  0.0415,
         -0.0804, -0.1189,  0.0456, -0.0082,  0.1433, -0.0682, -0.0433, -0.0690,
          0.0508,  0.0319,  0.0717, -0.1129,  0.0651,  0.0599,  0.0487,  0.0504,
          0.0431, -0.0958, -0.0458,  0.0069, -0.1109, -0.1471,  0.1295, -0.0113,
          0.1037,  0.0456,  0.1187,  0.0774, -0.0417,  0.1301, -0.1443, -0.0538,
          0.1065,  0.0189, -0.1474, -0.0391, -0.0943,  0.0634,  0.1469, -0.1483,
         -0.1015, -0.0197, -0.0768, -0.0280, -0.0009,  0.0578, -0.0908, -0.0241,
          0.0027,  0.0643,  0.0980,  0.0557,  0.1393, -0.0242, -0.0675, -0.0512]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0951,  0.0809,  0.0172,  ...,  0.1220, -0.0698,  0.0172],
        [ 0.0037,  0.0901, -0.0246,  ..., -0.0223, -0.0855, -0.0240],
        [ 0.0065,  0.0542,  0.0597,  ..., -0.0601,  0.0966,  0.0809],
        ...,
        [-0.0957,  0.0660,  0.1217,  ...,  0.0062, -0.0780,  0.0423],
        [ 0.0476,  0.0332,  0.1143,  ..., -0.0327, -0.0333,  0.1212],
        [-0.0494, -0.0598,  0.0077,  ...,  0.0618,  0.0051,  0.0802]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1459, -0.0074,  0.1738,  ...,  0.1733, -0.0589, -0.1068],
        [-0.1514, -0.1628,  0.0874,  ..., -0.1493, -0.1102, -0.0196],
        [ 0.0573, -0.1439,  0.0442,  ..., -0.1124, -0.0583, -0.0383],
        ...,
        [ 0.0030,  0.1386,  0.1397,  ...,  0.1511, -0.1457,  0.0465],
        [-0.1526,  0.0557,  0.0957,  ...,  0.1008, -0.1502,  0.0215],
        [ 0.0022, -0.0653, -0.1187,  ...,  0.0519,  0.0965,  0.0476]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0611, -0.2147, -0.0855,  ...,  0.0156,  0.1648, -0.1201],
        [-0.1443, -0.1606, -0.1140,  ...,  0.1322,  0.1676, -0.1075],
        [ 0.1493,  0.1585, -0.1392,  ...,  0.2199, -0.2401, -0.0471],
        ...,
        [-0.1606, -0.1784, -0.0134,  ..., -0.0412,  0.0685, -0.0186],
        [ 0.1386,  0.0924,  0.0361,  ..., -0.0391, -0.2289, -0.1255],
        [-0.1093, -0.0967,  0.1811,  ...,  0.1440, -0.0580,  0.2147]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1137],
        [-0.1628],
        [ 0.2288],
        [-0.1628],
        [ 0.1235],
        [ 0.3336],
        [ 0.0855],
        [ 0.4200],
        [-0.1618],
        [-0.2860],
        [ 0.3079],
        [ 0.1790],
        [ 0.0387],
        [-0.0516],
        [-0.2821],
        [-0.2729],
        [ 0.1536],
        [-0.2886],
        [-0.2525],
        [ 0.2618],
        [-0.4170],
        [ 0.0625],
        [ 0.3115],
        [ 0.1588],
        [-0.2466],
        [ 0.1082],
        [-0.0326],
        [ 0.0275],
        [-0.3397],
        [-0.2867],
        [ 0.3683],
        [ 0.3513]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups after adding efet as parameters
 [{'params': [Parameter containing:
tensor([[ 0.0751,  0.0059,  0.0163, -0.0705, -0.0532, -0.1141,  0.1302,  0.0756,
          0.1345,  0.0651,  0.0708, -0.0883,  0.0046,  0.0600, -0.0634, -0.0395,
         -0.0657, -0.1278,  0.0874,  0.0383,  0.1292, -0.1097,  0.1513,  0.1081,
          0.0163,  0.0486,  0.0747, -0.0922, -0.0182,  0.0304, -0.0316, -0.0301,
         -0.0033, -0.0900, -0.0212,  0.0847, -0.0563,  0.1356,  0.1243, -0.1516,
         -0.0474,  0.0126, -0.1414, -0.0223,  0.0891,  0.1294,  0.0240, -0.0907,
          0.0539,  0.0431,  0.1294,  0.1353,  0.0752, -0.0197, -0.1389, -0.1388,
         -0.0982,  0.1516,  0.0888, -0.1339, -0.0344,  0.0485, -0.0266,  0.1421,
          0.0978,  0.0119,  0.0330, -0.1028,  0.0120, -0.1504, -0.1462, -0.0049,
          0.1368, -0.1232, -0.1085, -0.0375,  0.0226, -0.1336, -0.0477,  0.1161,
          0.0704, -0.0702,  0.1506, -0.0210,  0.1066,  0.0766,  0.0540, -0.1488,
         -0.0748, -0.0238, -0.1041,  0.1088,  0.0724, -0.0473, -0.0859,  0.0640,
         -0.1102, -0.1001, -0.0539,  0.1470, -0.0234,  0.0932,  0.1017, -0.1187,
          0.0364,  0.1422,  0.0942, -0.1234,  0.1063,  0.0661,  0.1032, -0.1510,
         -0.0429, -0.1339, -0.0020, -0.0201, -0.1129,  0.1085, -0.0393,  0.0252,
          0.0438,  0.0090,  0.0572, -0.1164,  0.0897,  0.0003,  0.0410, -0.0423,
          0.0086, -0.0279,  0.1253, -0.0551,  0.0262, -0.0400, -0.1272,  0.0284,
          0.1474, -0.0860, -0.0011,  0.1395,  0.1265, -0.0566, -0.0584, -0.1503,
         -0.1233, -0.0977,  0.0511, -0.0690, -0.0662,  0.1032, -0.0550,  0.0369,
         -0.0104, -0.0161, -0.1214, -0.0743, -0.0819,  0.0778,  0.0213,  0.0952,
          0.0370, -0.0910, -0.0696, -0.1045, -0.0673, -0.0115, -0.1312,  0.1286,
         -0.0587,  0.0065,  0.0893, -0.1416, -0.0095,  0.0957,  0.0740,  0.1078,
          0.0478, -0.0578, -0.0360,  0.0576,  0.1179,  0.0406,  0.0136, -0.0413,
          0.1288,  0.0767, -0.0248, -0.0655,  0.0298,  0.0089, -0.0665, -0.0796,
         -0.1075,  0.0254,  0.0456, -0.0697,  0.0431,  0.0951,  0.0933,  0.0415,
         -0.0804, -0.1189,  0.0456, -0.0082,  0.1433, -0.0682, -0.0433, -0.0690,
          0.0508,  0.0319,  0.0717, -0.1129,  0.0651,  0.0599,  0.0487,  0.0504,
          0.0431, -0.0958, -0.0458,  0.0069, -0.1109, -0.1471,  0.1295, -0.0113,
          0.1037,  0.0456,  0.1187,  0.0774, -0.0417,  0.1301, -0.1443, -0.0538,
          0.1065,  0.0189, -0.1474, -0.0391, -0.0943,  0.0634,  0.1469, -0.1483,
         -0.1015, -0.0197, -0.0768, -0.0280, -0.0009,  0.0578, -0.0908, -0.0241,
          0.0027,  0.0643,  0.0980,  0.0557,  0.1393, -0.0242, -0.0675, -0.0512]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0951,  0.0809,  0.0172,  ...,  0.1220, -0.0698,  0.0172],
        [ 0.0037,  0.0901, -0.0246,  ..., -0.0223, -0.0855, -0.0240],
        [ 0.0065,  0.0542,  0.0597,  ..., -0.0601,  0.0966,  0.0809],
        ...,
        [-0.0957,  0.0660,  0.1217,  ...,  0.0062, -0.0780,  0.0423],
        [ 0.0476,  0.0332,  0.1143,  ..., -0.0327, -0.0333,  0.1212],
        [-0.0494, -0.0598,  0.0077,  ...,  0.0618,  0.0051,  0.0802]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1459, -0.0074,  0.1738,  ...,  0.1733, -0.0589, -0.1068],
        [-0.1514, -0.1628,  0.0874,  ..., -0.1493, -0.1102, -0.0196],
        [ 0.0573, -0.1439,  0.0442,  ..., -0.1124, -0.0583, -0.0383],
        ...,
        [ 0.0030,  0.1386,  0.1397,  ...,  0.1511, -0.1457,  0.0465],
        [-0.1526,  0.0557,  0.0957,  ...,  0.1008, -0.1502,  0.0215],
        [ 0.0022, -0.0653, -0.1187,  ...,  0.0519,  0.0965,  0.0476]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0611, -0.2147, -0.0855,  ...,  0.0156,  0.1648, -0.1201],
        [-0.1443, -0.1606, -0.1140,  ...,  0.1322,  0.1676, -0.1075],
        [ 0.1493,  0.1585, -0.1392,  ...,  0.2199, -0.2401, -0.0471],
        ...,
        [-0.1606, -0.1784, -0.0134,  ..., -0.0412,  0.0685, -0.0186],
        [ 0.1386,  0.0924,  0.0361,  ..., -0.0391, -0.2289, -0.1255],
        [-0.1093, -0.0967,  0.1811,  ...,  0.1440, -0.0580,  0.2147]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1137],
        [-0.1628],
        [ 0.2288],
        [-0.1628],
        [ 0.1235],
        [ 0.3336],
        [ 0.0855],
        [ 0.4200],
        [-0.1618],
        [-0.2860],
        [ 0.3079],
        [ 0.1790],
        [ 0.0387],
        [-0.0516],
        [-0.2821],
        [-0.2729],
        [ 0.1536],
        [-0.2886],
        [-0.2525],
        [ 0.2618],
        [-0.4170],
        [ 0.0625],
        [ 0.3115],
        [ 0.1588],
        [-0.2466],
        [ 0.1082],
        [-0.0326],
        [ 0.0275],
        [-0.3397],
        [-0.2867],
        [ 0.3683],
        [ 0.3513]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[-0.1384],
        [-0.4762],
        [ 0.4082],
        ...,
        [ 3.9284],
        [ 0.0328],
        [-0.8505]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(768.6312, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet] tensor([[1.9143e-02],
        [2.2675e-01],
        [1.6661e-01],
        ...,
        [1.5432e+01],
        [1.0765e-03],
        [7.2340e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].sum tensor(35277488., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(768.6312, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(144.0447, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.3357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(57.6995, device='cuda:0')



h[100].sum tensor(-16.9518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.0012, device='cuda:0')



h[200].sum tensor(-58.2141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-61.8180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(2410523.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1878, 0.0000,  ..., 0.0000, 0.0000, 0.1175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(6.1483e+08, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-36587.5820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6869.2349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16169816., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(24546.5742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-133.0742],
        [ -77.0655],
        [ -46.3409],
        ...,
        [   0.0000],
        [   0.0000],
        [   0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1.2977e+09, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.9143e-02],
        [2.2675e-01],
        [1.6661e-01],
        ...,
        [1.5432e+01],
        [1.0765e-03],
        [7.2340e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet].sum tensor(35277488., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2954],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(679.9377, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet] tensor([[3.6645e-04],
        [5.1418e-02],
        [2.7758e-02],
        ...,
        [2.3816e+02],
        [1.1589e-06],
        [5.2330e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].sum tensor(1.0590e+08, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2954],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(679.9377, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 4.8829e-03,  2.8844e-04,  1.1879e-03,  ..., -1.5995e-03,
         -4.4780e-03, -3.3940e-03],
        [ 3.6756e-03,  1.9433e-04,  9.2434e-04,  ..., -1.2120e-03,
         -3.3930e-03, -2.5717e-03],
        ...,
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164.5841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.2138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(50.9734, device='cuda:0')



h[100].sum tensor(-14.7474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.8566, device='cuda:0')



h[200].sum tensor(-50.7964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-54.6169, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.4280, 0.0253, 0.1263,  ..., 0.0000, 0.0000, 0.0000],
        [0.5064, 0.0318, 0.1317,  ..., 0.0000, 0.0000, 0.0000],
        [1.2433, 0.0829, 0.2900,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0046,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(6601950.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.4987e+02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         9.9936e+01],
        [0.0000e+00, 1.3179e+02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         8.7189e+01],
        [0.0000e+00, 1.1581e+02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         7.6145e+01],
        ...,
        [0.0000e+00, 0.0000e+00, 5.1510e-01,  ..., 0.0000e+00, 3.5033e-01,
         1.7618e-01],
        [0.0000e+00, 0.0000e+00, 2.8933e-01,  ..., 0.0000e+00, 1.9678e-01,
         1.0027e-01],
        [0.0000e+00, 0.0000e+00, 8.1475e-01,  ..., 0.0000e+00, 5.5413e-01,
         2.8105e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(5.2177e+09, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-90227.7188, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-21704.0352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.4247e+08, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(71881.3750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[10851.2109],
        [ 7877.7319],
        [ 8208.6758],
        ...,
        [  533.2435],
        [  397.2093],
        [  413.3997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(8.8151e+10, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[3.6645e-04],
        [5.1418e-02],
        [2.7758e-02],
        ...,
        [2.3816e+02],
        [1.1589e-06],
        [5.2330e-01]], device='cuda:0', grad_fn=<MulBackward0>) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet].sum tensor(1.0590e+08, device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/./TrainingBha2ndneiefet.py", line 84, in <module>
    loss.backward()#(retain_graph=True)   # Derive gradients.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

real	1m59.396s
user	0m21.270s
sys	0m11.052s
