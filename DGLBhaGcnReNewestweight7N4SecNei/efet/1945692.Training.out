0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-adc0bb57-5f8a-ba9d-04e2-dbaa9f7c20f1)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        A2:17:1D:79:E5:47:38:13:77:F2:EE:66:EA:5D:DB:A0:A1:CA:F4:E7
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jan  9 17:05:46 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:1D:00.0 Off |                    0 |
| N/A   28C    P0    41W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089926656

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b20b8611880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m27.460s
user	0m3.394s
sys	0m2.217s




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 6507, 6507, 6507],
        [   1,    2,    3,  ..., 6219, 6794, 6795]]) 

edge_index shape
 torch.Size([2, 1175277])
graph: Graph(num_nodes=6796, num_edges=1175277,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 6507, 6507, 6507], device='cuda:0'), tensor([   1,    2,    3,  ..., 6219, 6794, 6795], device='cuda:0'))

number of nodes: 6796

number of edges: 2350554

node features (random input): tensor([[-0.2745],
        [-0.1707],
        [-0.4838],
        ...,
        [-2.3713],
        [ 0.9073],
        [-1.2353]], device='cuda:0', requires_grad=True) 
node features sum: tensor(116.6426, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(2350554., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 399

In degrees of node 234: 399





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLBhaGcnReNewestweight7N4SecNei

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (linear1): Linear(in_features=2350554, out_features=256, bias=True)
  (linear2): Linear(in_features=256, out_features=2350554, bias=True)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 1205878235

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0192,  0.0221,  0.0895, -0.0113, -0.0599,  0.0159,  0.1042,  0.1344,
         -0.1283,  0.0869,  0.1476, -0.0546, -0.0804, -0.1321, -0.0652, -0.1451,
         -0.0593, -0.0110,  0.0149, -0.0418,  0.0393,  0.0877,  0.1238,  0.0915,
          0.0470, -0.0329, -0.1340, -0.1168,  0.0555,  0.0226, -0.1281, -0.1260,
         -0.1416,  0.1446,  0.1289, -0.0121, -0.0637, -0.1439,  0.0636,  0.0085,
         -0.1246,  0.1383, -0.0136,  0.0841, -0.1044, -0.1311,  0.0929,  0.0889,
         -0.1151, -0.1010, -0.1022, -0.1036,  0.1010, -0.0586,  0.1424, -0.1247,
          0.1343,  0.1180,  0.1167,  0.0909, -0.0111, -0.0969, -0.1515, -0.1248,
         -0.0035,  0.0399, -0.0413, -0.0702,  0.0235, -0.1258,  0.0933,  0.1189,
         -0.0735, -0.1101,  0.0189, -0.0082,  0.0771, -0.0528,  0.0406, -0.1162,
          0.0887, -0.0877,  0.1307, -0.1527, -0.0491,  0.1011,  0.0870,  0.1193,
         -0.1097,  0.0498, -0.1516, -0.1506,  0.0934,  0.1484,  0.0136, -0.0436,
          0.0816, -0.1019, -0.1475,  0.1450,  0.1515,  0.0336, -0.1497,  0.0247,
          0.0177, -0.0602,  0.0123, -0.1281,  0.1297, -0.0039, -0.0718, -0.0214,
         -0.1401, -0.0376, -0.0796, -0.0740, -0.0736,  0.0251,  0.0175, -0.0323,
         -0.0043,  0.1162,  0.0529, -0.0981, -0.0597, -0.0883, -0.1145, -0.0580,
         -0.0039, -0.0509,  0.1431,  0.1185, -0.1378, -0.0507, -0.0846,  0.1508,
         -0.1170,  0.1028,  0.1177, -0.0519,  0.1435, -0.1448, -0.0630,  0.0081,
          0.0534, -0.1109, -0.1436, -0.0353, -0.0626,  0.0817,  0.0990, -0.0975,
          0.0267,  0.0313, -0.0562, -0.0951,  0.0941, -0.0038,  0.0155,  0.1139,
         -0.1054,  0.0844, -0.0749, -0.0929, -0.1029, -0.1515, -0.1377, -0.0540,
         -0.0834, -0.0596, -0.1429, -0.0771,  0.0544,  0.1193,  0.0790, -0.0044,
          0.0136, -0.0465, -0.0441,  0.0618, -0.1418,  0.0005,  0.0357, -0.0209,
          0.0738,  0.0571,  0.0737,  0.1046, -0.0493,  0.1360,  0.0731,  0.0344,
         -0.1387,  0.1415,  0.1119,  0.0759, -0.0498,  0.1075,  0.0474,  0.0007,
          0.0559, -0.0268,  0.0304,  0.1307,  0.1464, -0.0666,  0.0375, -0.1378,
          0.1229, -0.0608,  0.1065,  0.1238,  0.0464,  0.0829,  0.1444,  0.1118,
         -0.0764, -0.0558,  0.1015, -0.0930, -0.0546, -0.0424, -0.1081,  0.0309,
          0.0307,  0.1212,  0.0658, -0.1228, -0.1418, -0.0649, -0.0067,  0.0102,
         -0.1465, -0.0592,  0.0225,  0.1446, -0.0670, -0.0174, -0.0153, -0.1414,
         -0.0021,  0.1387,  0.0406,  0.0893, -0.0433,  0.0939,  0.0108,  0.1225,
         -0.0303,  0.0603,  0.0078, -0.0072, -0.0941, -0.1327, -0.0283,  0.0023]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0192,  0.0221,  0.0895, -0.0113, -0.0599,  0.0159,  0.1042,  0.1344,
         -0.1283,  0.0869,  0.1476, -0.0546, -0.0804, -0.1321, -0.0652, -0.1451,
         -0.0593, -0.0110,  0.0149, -0.0418,  0.0393,  0.0877,  0.1238,  0.0915,
          0.0470, -0.0329, -0.1340, -0.1168,  0.0555,  0.0226, -0.1281, -0.1260,
         -0.1416,  0.1446,  0.1289, -0.0121, -0.0637, -0.1439,  0.0636,  0.0085,
         -0.1246,  0.1383, -0.0136,  0.0841, -0.1044, -0.1311,  0.0929,  0.0889,
         -0.1151, -0.1010, -0.1022, -0.1036,  0.1010, -0.0586,  0.1424, -0.1247,
          0.1343,  0.1180,  0.1167,  0.0909, -0.0111, -0.0969, -0.1515, -0.1248,
         -0.0035,  0.0399, -0.0413, -0.0702,  0.0235, -0.1258,  0.0933,  0.1189,
         -0.0735, -0.1101,  0.0189, -0.0082,  0.0771, -0.0528,  0.0406, -0.1162,
          0.0887, -0.0877,  0.1307, -0.1527, -0.0491,  0.1011,  0.0870,  0.1193,
         -0.1097,  0.0498, -0.1516, -0.1506,  0.0934,  0.1484,  0.0136, -0.0436,
          0.0816, -0.1019, -0.1475,  0.1450,  0.1515,  0.0336, -0.1497,  0.0247,
          0.0177, -0.0602,  0.0123, -0.1281,  0.1297, -0.0039, -0.0718, -0.0214,
         -0.1401, -0.0376, -0.0796, -0.0740, -0.0736,  0.0251,  0.0175, -0.0323,
         -0.0043,  0.1162,  0.0529, -0.0981, -0.0597, -0.0883, -0.1145, -0.0580,
         -0.0039, -0.0509,  0.1431,  0.1185, -0.1378, -0.0507, -0.0846,  0.1508,
         -0.1170,  0.1028,  0.1177, -0.0519,  0.1435, -0.1448, -0.0630,  0.0081,
          0.0534, -0.1109, -0.1436, -0.0353, -0.0626,  0.0817,  0.0990, -0.0975,
          0.0267,  0.0313, -0.0562, -0.0951,  0.0941, -0.0038,  0.0155,  0.1139,
         -0.1054,  0.0844, -0.0749, -0.0929, -0.1029, -0.1515, -0.1377, -0.0540,
         -0.0834, -0.0596, -0.1429, -0.0771,  0.0544,  0.1193,  0.0790, -0.0044,
          0.0136, -0.0465, -0.0441,  0.0618, -0.1418,  0.0005,  0.0357, -0.0209,
          0.0738,  0.0571,  0.0737,  0.1046, -0.0493,  0.1360,  0.0731,  0.0344,
         -0.1387,  0.1415,  0.1119,  0.0759, -0.0498,  0.1075,  0.0474,  0.0007,
          0.0559, -0.0268,  0.0304,  0.1307,  0.1464, -0.0666,  0.0375, -0.1378,
          0.1229, -0.0608,  0.1065,  0.1238,  0.0464,  0.0829,  0.1444,  0.1118,
         -0.0764, -0.0558,  0.1015, -0.0930, -0.0546, -0.0424, -0.1081,  0.0309,
          0.0307,  0.1212,  0.0658, -0.1228, -0.1418, -0.0649, -0.0067,  0.0102,
         -0.1465, -0.0592,  0.0225,  0.1446, -0.0670, -0.0174, -0.0153, -0.1414,
         -0.0021,  0.1387,  0.0406,  0.0893, -0.0433,  0.0939,  0.0108,  0.1225,
         -0.0303,  0.0603,  0.0078, -0.0072, -0.0941, -0.1327, -0.0283,  0.0023]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name linear1.weight 
shape:
 torch.Size([256, 2350554]) 
grad:
 True 
date:
 tensor([[ 1.3112e-04,  3.0002e-04,  6.2597e-04,  ...,  1.9780e-04,
          2.0780e-04, -4.1751e-04],
        [-2.9060e-04, -2.0878e-04, -4.1878e-04,  ..., -4.2244e-05,
          3.2310e-04, -3.3334e-05],
        [-1.3659e-04, -1.8023e-04, -3.2459e-04,  ..., -1.4266e-04,
         -9.3386e-06,  5.4370e-04],
        ...,
        [-5.3892e-04, -4.1542e-04, -4.6591e-04,  ..., -5.1137e-05,
          3.8194e-04, -4.3777e-05],
        [ 3.2678e-04,  4.7044e-04,  5.8795e-04,  ..., -4.3190e-04,
         -3.4852e-04, -3.0971e-04],
        [-5.5014e-04, -2.6693e-04, -5.7516e-04,  ..., -4.9053e-04,
         -2.9007e-04,  3.1453e-04]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 1.3112e-04,  3.0002e-04,  6.2597e-04,  ...,  1.9780e-04,
          2.0780e-04, -4.1751e-04],
        [-2.9060e-04, -2.0878e-04, -4.1878e-04,  ..., -4.2244e-05,
          3.2310e-04, -3.3334e-05],
        [-1.3659e-04, -1.8023e-04, -3.2459e-04,  ..., -1.4266e-04,
         -9.3386e-06,  5.4370e-04],
        ...,
        [-5.3892e-04, -4.1542e-04, -4.6591e-04,  ..., -5.1137e-05,
          3.8194e-04, -4.3777e-05],
        [ 3.2678e-04,  4.7044e-04,  5.8795e-04,  ..., -4.3190e-04,
         -3.4852e-04, -3.0971e-04],
        [-5.5014e-04, -2.6693e-04, -5.7516e-04,  ..., -4.9053e-04,
         -2.9007e-04,  3.1453e-04]], device='cuda:0', requires_grad=True)

name linear1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([-3.2135e-05, -4.9176e-04,  1.6732e-04, -1.9478e-04, -3.0445e-04,
        -4.8461e-04, -6.1097e-04, -4.3945e-04,  1.1665e-04,  5.6407e-04,
         1.5030e-04, -4.5336e-04, -5.7910e-04,  1.3155e-04,  4.5764e-04,
         3.9382e-04,  1.4862e-04,  1.0177e-04, -6.1326e-04,  2.2408e-04,
         2.1031e-04,  1.8789e-04, -3.6059e-04, -4.3460e-04,  5.5613e-04,
        -2.9546e-04, -6.1016e-04, -1.4374e-04,  1.4805e-04, -9.5710e-05,
        -1.2516e-04, -1.8292e-04, -6.3033e-04,  2.8691e-04,  6.1955e-04,
         4.3413e-04,  1.5594e-04, -2.5117e-04, -5.7607e-04, -5.9056e-04,
         6.1097e-04,  2.1569e-04,  4.1895e-04, -3.7633e-04, -6.0659e-04,
         6.4564e-04, -2.3364e-04, -2.8961e-05,  2.4011e-04,  1.5100e-04,
         4.8840e-04, -3.2489e-04, -1.8612e-04,  4.2733e-04,  4.5551e-04,
        -9.6893e-05, -3.0639e-04, -6.3191e-04,  5.1076e-04, -2.3929e-04,
         6.7499e-05, -2.5007e-04, -5.5907e-04, -6.4504e-04, -6.1778e-04,
        -1.7369e-04, -3.1373e-04, -1.6182e-04, -4.1931e-04, -5.8130e-04,
        -2.1773e-05, -2.8024e-04,  3.6444e-04,  3.0912e-04,  6.0765e-05,
        -2.2349e-05, -5.2475e-04, -5.2339e-04,  5.1863e-04,  3.9101e-04,
        -1.6443e-04, -5.4424e-04, -3.7021e-04,  4.2148e-04,  1.2664e-04,
         2.7153e-04, -3.5512e-04,  4.1586e-04, -6.3685e-04, -5.1701e-04,
        -4.7094e-04,  6.2042e-04,  6.1553e-04,  1.8525e-04,  4.0796e-04,
         1.9293e-05,  9.8127e-05,  3.9129e-04, -3.8427e-04, -1.8255e-04,
         1.6361e-04, -5.2766e-04,  2.5728e-04, -2.7741e-04,  3.2295e-05,
        -5.6366e-04,  6.3055e-04, -2.5091e-04,  6.1146e-04, -4.3868e-04,
        -2.2682e-04,  9.9765e-05, -2.5987e-04, -3.0558e-05,  5.5774e-04,
         6.1626e-04,  5.7863e-04, -1.8635e-04,  2.8705e-04,  5.3608e-04,
        -1.6681e-04, -2.3108e-04,  1.2873e-04,  1.4884e-04, -3.5527e-04,
        -4.2326e-04,  4.3047e-04, -5.3401e-04,  1.1134e-04, -2.0758e-04,
        -7.6077e-05,  4.1513e-05,  4.4209e-04,  3.7185e-04,  2.1834e-04,
         3.8295e-04,  5.9778e-04,  4.0899e-04,  4.9921e-06,  1.1417e-04,
        -1.0113e-05,  1.6658e-04, -6.2790e-04,  4.8273e-05, -1.6153e-04,
        -4.7230e-05,  4.0904e-04, -4.7454e-04,  2.2283e-05, -5.6565e-04,
        -6.3770e-04,  2.9426e-04,  4.4354e-04, -4.7566e-04,  5.6528e-04,
         1.2676e-04, -1.4494e-04,  1.8604e-04,  1.5067e-04,  7.4513e-05,
        -4.3458e-04,  4.3925e-04, -3.0394e-04, -3.6137e-05,  1.6113e-04,
        -1.3916e-04, -1.4206e-04, -1.4648e-04, -5.6104e-04,  3.8192e-04,
         5.5957e-04, -3.2904e-05, -2.7506e-04,  9.6170e-05, -1.5868e-04,
        -2.7224e-04,  2.3524e-04,  4.9354e-04, -3.2777e-04,  5.4492e-04,
         6.5180e-04,  7.6576e-05,  1.2738e-04, -2.1245e-04,  5.0332e-04,
         4.9347e-05, -4.2754e-05, -3.4236e-04,  1.8174e-04,  1.7958e-04,
        -4.9712e-04,  6.1571e-04,  3.1048e-04, -4.5148e-04,  1.7846e-05,
         3.8208e-04, -2.0985e-04,  3.2375e-04, -5.2055e-04,  3.9264e-04,
        -1.8554e-04,  1.8028e-04, -1.3020e-04, -4.4893e-04, -4.5838e-04,
        -2.8309e-04, -5.3337e-05,  1.6076e-04, -1.9934e-05,  4.9422e-04,
         4.3663e-05, -1.0194e-04,  5.7170e-04, -1.5203e-04,  1.8266e-04,
         3.7064e-04, -2.8423e-04,  5.4576e-04, -4.1196e-04, -1.6444e-04,
         6.2653e-04, -4.8390e-05, -6.4984e-05, -5.9281e-05,  3.8387e-05,
         4.9290e-04,  2.5999e-04, -3.0439e-04, -4.9006e-04,  2.4713e-04,
         4.4389e-04,  4.0053e-04, -4.9086e-04,  4.4383e-04, -4.0041e-04,
         2.9188e-04,  6.2826e-05,  2.8975e-04,  5.4538e-04, -6.1413e-04,
         5.4022e-05,  6.0606e-04,  3.7390e-04, -7.2922e-05,  2.3068e-05,
        -5.5517e-05, -4.9803e-04, -5.7842e-04, -5.6214e-04,  1.9464e-04,
        -2.5341e-04, -5.7534e-04, -5.7168e-04, -8.4880e-06, -2.2494e-05,
        -6.8633e-05], device='cuda:0') 
parameter:
 Parameter containing:
tensor([-3.2135e-05, -4.9176e-04,  1.6732e-04, -1.9478e-04, -3.0445e-04,
        -4.8461e-04, -6.1097e-04, -4.3945e-04,  1.1665e-04,  5.6407e-04,
         1.5030e-04, -4.5336e-04, -5.7910e-04,  1.3155e-04,  4.5764e-04,
         3.9382e-04,  1.4862e-04,  1.0177e-04, -6.1326e-04,  2.2408e-04,
         2.1031e-04,  1.8789e-04, -3.6059e-04, -4.3460e-04,  5.5613e-04,
        -2.9546e-04, -6.1016e-04, -1.4374e-04,  1.4805e-04, -9.5710e-05,
        -1.2516e-04, -1.8292e-04, -6.3033e-04,  2.8691e-04,  6.1955e-04,
         4.3413e-04,  1.5594e-04, -2.5117e-04, -5.7607e-04, -5.9056e-04,
         6.1097e-04,  2.1569e-04,  4.1895e-04, -3.7633e-04, -6.0659e-04,
         6.4564e-04, -2.3364e-04, -2.8961e-05,  2.4011e-04,  1.5100e-04,
         4.8840e-04, -3.2489e-04, -1.8612e-04,  4.2733e-04,  4.5551e-04,
        -9.6893e-05, -3.0639e-04, -6.3191e-04,  5.1076e-04, -2.3929e-04,
         6.7499e-05, -2.5007e-04, -5.5907e-04, -6.4504e-04, -6.1778e-04,
        -1.7369e-04, -3.1373e-04, -1.6182e-04, -4.1931e-04, -5.8130e-04,
        -2.1773e-05, -2.8024e-04,  3.6444e-04,  3.0912e-04,  6.0765e-05,
        -2.2349e-05, -5.2475e-04, -5.2339e-04,  5.1863e-04,  3.9101e-04,
        -1.6443e-04, -5.4424e-04, -3.7021e-04,  4.2148e-04,  1.2664e-04,
         2.7153e-04, -3.5512e-04,  4.1586e-04, -6.3685e-04, -5.1701e-04,
        -4.7094e-04,  6.2042e-04,  6.1553e-04,  1.8525e-04,  4.0796e-04,
         1.9293e-05,  9.8127e-05,  3.9129e-04, -3.8427e-04, -1.8255e-04,
         1.6361e-04, -5.2766e-04,  2.5728e-04, -2.7741e-04,  3.2295e-05,
        -5.6366e-04,  6.3055e-04, -2.5091e-04,  6.1146e-04, -4.3868e-04,
        -2.2682e-04,  9.9765e-05, -2.5987e-04, -3.0558e-05,  5.5774e-04,
         6.1626e-04,  5.7863e-04, -1.8635e-04,  2.8705e-04,  5.3608e-04,
        -1.6681e-04, -2.3108e-04,  1.2873e-04,  1.4884e-04, -3.5527e-04,
        -4.2326e-04,  4.3047e-04, -5.3401e-04,  1.1134e-04, -2.0758e-04,
        -7.6077e-05,  4.1513e-05,  4.4209e-04,  3.7185e-04,  2.1834e-04,
         3.8295e-04,  5.9778e-04,  4.0899e-04,  4.9921e-06,  1.1417e-04,
        -1.0113e-05,  1.6658e-04, -6.2790e-04,  4.8273e-05, -1.6153e-04,
        -4.7230e-05,  4.0904e-04, -4.7454e-04,  2.2283e-05, -5.6565e-04,
        -6.3770e-04,  2.9426e-04,  4.4354e-04, -4.7566e-04,  5.6528e-04,
         1.2676e-04, -1.4494e-04,  1.8604e-04,  1.5067e-04,  7.4513e-05,
        -4.3458e-04,  4.3925e-04, -3.0394e-04, -3.6137e-05,  1.6113e-04,
        -1.3916e-04, -1.4206e-04, -1.4648e-04, -5.6104e-04,  3.8192e-04,
         5.5957e-04, -3.2904e-05, -2.7506e-04,  9.6170e-05, -1.5868e-04,
        -2.7224e-04,  2.3524e-04,  4.9354e-04, -3.2777e-04,  5.4492e-04,
         6.5180e-04,  7.6576e-05,  1.2738e-04, -2.1245e-04,  5.0332e-04,
         4.9347e-05, -4.2754e-05, -3.4236e-04,  1.8174e-04,  1.7958e-04,
        -4.9712e-04,  6.1571e-04,  3.1048e-04, -4.5148e-04,  1.7846e-05,
         3.8208e-04, -2.0985e-04,  3.2375e-04, -5.2055e-04,  3.9264e-04,
        -1.8554e-04,  1.8028e-04, -1.3020e-04, -4.4893e-04, -4.5838e-04,
        -2.8309e-04, -5.3337e-05,  1.6076e-04, -1.9934e-05,  4.9422e-04,
         4.3663e-05, -1.0194e-04,  5.7170e-04, -1.5203e-04,  1.8266e-04,
         3.7064e-04, -2.8423e-04,  5.4576e-04, -4.1196e-04, -1.6444e-04,
         6.2653e-04, -4.8390e-05, -6.4984e-05, -5.9281e-05,  3.8387e-05,
         4.9290e-04,  2.5999e-04, -3.0439e-04, -4.9006e-04,  2.4713e-04,
         4.4389e-04,  4.0053e-04, -4.9086e-04,  4.4383e-04, -4.0041e-04,
         2.9188e-04,  6.2826e-05,  2.8975e-04,  5.4538e-04, -6.1413e-04,
         5.4022e-05,  6.0606e-04,  3.7390e-04, -7.2922e-05,  2.3068e-05,
        -5.5517e-05, -4.9803e-04, -5.7842e-04, -5.6214e-04,  1.9464e-04,
        -2.5341e-04, -5.7534e-04, -5.7168e-04, -8.4880e-06, -2.2494e-05,
        -6.8633e-05], device='cuda:0', requires_grad=True)

name linear2.weight 
shape:
 torch.Size([2350554, 256]) 
grad:
 True 
date:
 tensor([[ 0.0227, -0.0551,  0.0142,  ..., -0.0532, -0.0147,  0.0019],
        [ 0.0284,  0.0205, -0.0274,  ..., -0.0321,  0.0471,  0.0310],
        [ 0.0237,  0.0059, -0.0535,  ...,  0.0187,  0.0525, -0.0606],
        ...,
        [-0.0446,  0.0602, -0.0242,  ...,  0.0302, -0.0309, -0.0274],
        [ 0.0145,  0.0332, -0.0202,  ...,  0.0420,  0.0176, -0.0092],
        [-0.0511,  0.0607, -0.0102,  ..., -0.0602, -0.0305, -0.0354]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0227, -0.0551,  0.0142,  ..., -0.0532, -0.0147,  0.0019],
        [ 0.0284,  0.0205, -0.0274,  ..., -0.0321,  0.0471,  0.0310],
        [ 0.0237,  0.0059, -0.0535,  ...,  0.0187,  0.0525, -0.0606],
        ...,
        [-0.0446,  0.0602, -0.0242,  ...,  0.0302, -0.0309, -0.0274],
        [ 0.0145,  0.0332, -0.0202,  ...,  0.0420,  0.0176, -0.0092],
        [-0.0511,  0.0607, -0.0102,  ..., -0.0602, -0.0305, -0.0354]],
       device='cuda:0', requires_grad=True)

name linear2.bias 
shape:
 torch.Size([2350554]) 
grad:
 True 
date:
 tensor([ 0.0075, -0.0326, -0.0130,  ..., -0.0248, -0.0619, -0.0074],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([ 0.0075, -0.0326, -0.0130,  ..., -0.0248, -0.0619, -0.0074],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0827,  0.1096, -0.0272,  ...,  0.0301,  0.0316, -0.0037],
        [ 0.0165,  0.0896, -0.0113,  ..., -0.0732,  0.1195,  0.0341],
        [-0.1199,  0.0864,  0.0950,  ...,  0.0116,  0.1077,  0.0853],
        ...,
        [ 0.0124,  0.0078,  0.0950,  ...,  0.0869, -0.1147,  0.0377],
        [ 0.0722,  0.0555,  0.1088,  ...,  0.0143,  0.1164,  0.0573],
        [-0.0159, -0.0578,  0.0151,  ...,  0.0649, -0.0559, -0.1047]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0827,  0.1096, -0.0272,  ...,  0.0301,  0.0316, -0.0037],
        [ 0.0165,  0.0896, -0.0113,  ..., -0.0732,  0.1195,  0.0341],
        [-0.1199,  0.0864,  0.0950,  ...,  0.0116,  0.1077,  0.0853],
        ...,
        [ 0.0124,  0.0078,  0.0950,  ...,  0.0869, -0.1147,  0.0377],
        [ 0.0722,  0.0555,  0.1088,  ...,  0.0143,  0.1164,  0.0573],
        [-0.0159, -0.0578,  0.0151,  ...,  0.0649, -0.0559, -0.1047]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.1524,  0.1232, -0.1489,  ..., -0.0402, -0.0303, -0.0917],
        [ 0.1304,  0.0895, -0.0077,  ..., -0.1587, -0.0020, -0.1564],
        [-0.0946,  0.0843,  0.0229,  ..., -0.0069,  0.1335,  0.1626],
        ...,
        [ 0.0993,  0.0810, -0.0588,  ...,  0.0618,  0.0933, -0.1705],
        [-0.0250, -0.1012,  0.0782,  ...,  0.0403, -0.0521,  0.1342],
        [ 0.0899, -0.1604,  0.0787,  ..., -0.0733,  0.1370,  0.1027]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1524,  0.1232, -0.1489,  ..., -0.0402, -0.0303, -0.0917],
        [ 0.1304,  0.0895, -0.0077,  ..., -0.1587, -0.0020, -0.1564],
        [-0.0946,  0.0843,  0.0229,  ..., -0.0069,  0.1335,  0.1626],
        ...,
        [ 0.0993,  0.0810, -0.0588,  ...,  0.0618,  0.0933, -0.1705],
        [-0.0250, -0.1012,  0.0782,  ...,  0.0403, -0.0521,  0.1342],
        [ 0.0899, -0.1604,  0.0787,  ..., -0.0733,  0.1370,  0.1027]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.2009, -0.1769, -0.1921,  ...,  0.0056,  0.0528, -0.0606],
        [-0.1515,  0.1248,  0.1368,  ..., -0.2287,  0.2067,  0.0413],
        [-0.1544,  0.1447, -0.0710,  ...,  0.0352,  0.1844, -0.1991],
        ...,
        [ 0.2046, -0.1983,  0.0156,  ..., -0.0046,  0.1033,  0.0402],
        [-0.0257,  0.1192, -0.1009,  ...,  0.0692,  0.0046,  0.1360],
        [ 0.1881, -0.1012,  0.1507,  ..., -0.2134, -0.1618,  0.1887]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2009, -0.1769, -0.1921,  ...,  0.0056,  0.0528, -0.0606],
        [-0.1515,  0.1248,  0.1368,  ..., -0.2287,  0.2067,  0.0413],
        [-0.1544,  0.1447, -0.0710,  ...,  0.0352,  0.1844, -0.1991],
        ...,
        [ 0.2046, -0.1983,  0.0156,  ..., -0.0046,  0.1033,  0.0402],
        [-0.0257,  0.1192, -0.1009,  ...,  0.0692,  0.0046,  0.1360],
        [ 0.1881, -0.1012,  0.1507,  ..., -0.2134, -0.1618,  0.1887]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.0365],
        [-0.2765],
        [-0.3591],
        [-0.3292],
        [-0.3002],
        [-0.2549],
        [-0.2323],
        [ 0.2788],
        [-0.0075],
        [-0.0310],
        [ 0.2475],
        [ 0.3351],
        [-0.3997],
        [-0.1630],
        [-0.1528],
        [ 0.0829],
        [-0.3765],
        [-0.4198],
        [-0.2395],
        [ 0.2079],
        [ 0.2123],
        [ 0.0916],
        [-0.2207],
        [-0.4153],
        [-0.2509],
        [-0.1409],
        [ 0.1711],
        [-0.3712],
        [-0.2101],
        [-0.2477],
        [ 0.2751],
        [-0.0783]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0365],
        [-0.2765],
        [-0.3591],
        [-0.3292],
        [-0.3002],
        [-0.2549],
        [-0.2323],
        [ 0.2788],
        [-0.0075],
        [-0.0310],
        [ 0.2475],
        [ 0.3351],
        [-0.3997],
        [-0.1630],
        [-0.1528],
        [ 0.0829],
        [-0.3765],
        [-0.4198],
        [-0.2395],
        [ 0.2079],
        [ 0.2123],
        [ 0.0916],
        [-0.2207],
        [-0.4153],
        [-0.2509],
        [-0.1409],
        [ 0.1711],
        [-0.3712],
        [-0.2101],
        [-0.2477],
        [ 0.2751],
        [-0.0783]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)





 shepe: torch.Size([2350554, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet] tensor([[0.4515],
        [0.0827],
        [0.2039],
        ...,
        [0.3302],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(197621.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-9.8653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.7407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.5608, device='cuda:0')



h[100].sum tensor(3.6867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9442, device='cuda:0')



h[200].sum tensor(-2.8284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-3.0259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(28761.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0184, 0.0000, 0.0000,  ..., 0.0138, 0.0000, 0.0116],
        [0.0107, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0067],
        [0.0024, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(553380.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(18187.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.2448, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1214],
        [-1.4352],
        [-1.0022],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-618200., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.4515],
        [0.0827],
        [0.2039],
        ...,
        [0.3302],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet].sum tensor(197621.0312, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-2.1214],
        [-1.4352],
        [-1.0022],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])





 shepe: torch.Size([47011080, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet] tensor([[1.8295e-05],
        [3.2198e-02],
        [0.0000e+00],
        ...,
        [0.0000e+00],
        [0.0000e+00],
        [3.9257e-01]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(4287680., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050, -0.0053, -0.0143,  ...,  0.0183,  0.0005, -0.0178],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-786.7032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.8390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.4191, device='cuda:0')



h[100].sum tensor(-63.7885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-68.5126, device='cuda:0')



h[200].sum tensor(-51.8272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-55.6655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0277, 0.0000, 0.0000,  ..., 0.1013, 0.0027, 0.0000],
        [0.0120, 0.0000, 0.0000,  ..., 0.0440, 0.0012, 0.0000],
        [0.0030, 0.0000, 0.0000,  ..., 0.0110, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(268111.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1952, 0.0228,  ..., 0.2224, 0.3060, 0.0000],
        [0.0000, 0.0906, 0.0106,  ..., 0.1032, 0.1419, 0.0000],
        [0.0000, 0.0832, 0.0097,  ..., 0.0949, 0.1305, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(6363952., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1674.9473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=47011080,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5275e+01],
        [-1.5769e+01],
        [-1.6547e+01],
        ...,
        [-1.9654e-04],
        [-3.2618e-04],
        [-3.9176e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-7289827.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.8295e-05],
        [3.2198e-02],
        [0.0000e+00],
        ...,
        [0.0000e+00],
        [0.0000e+00],
        [3.9257e-01]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([47011080, 1]) 
g.edata[efet].sum tensor(4287680., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-2.1214],
        [-1.4352],
        [-1.0022],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')



load_model False 
TraEvN 401 
BatchSize 15 
EpochNum 10 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 164 
endmesh 165 






optimizer.param_groups
 [{'params': [Parameter containing:
tensor([[ 0.0378, -0.0400, -0.1079, -0.0674,  0.1424,  0.0055,  0.1196, -0.0337,
         -0.0066,  0.1071, -0.0981,  0.0376,  0.0634, -0.0464, -0.0616,  0.0171,
         -0.0031,  0.0664,  0.1113, -0.0345,  0.0973,  0.1507, -0.0818,  0.0652,
         -0.0470,  0.0320, -0.1476, -0.1147, -0.1308, -0.0812, -0.1164, -0.0218,
          0.0592,  0.0275,  0.0504,  0.1426,  0.0311,  0.0489,  0.0760,  0.0993,
         -0.1093,  0.1474, -0.0288,  0.1433,  0.0971, -0.1204,  0.0823, -0.0109,
         -0.0306, -0.0214, -0.0058,  0.0290, -0.1120, -0.0563, -0.1170,  0.1263,
          0.1283,  0.0739, -0.1081,  0.1425,  0.1300, -0.0805, -0.0316,  0.1123,
          0.0484, -0.0643,  0.0388,  0.0638,  0.0600,  0.0597,  0.0544, -0.0342,
          0.1245, -0.1408, -0.1329,  0.1134,  0.0156, -0.0731, -0.0894,  0.0814,
         -0.0453, -0.1337,  0.1128, -0.0868, -0.0396, -0.0042,  0.0192, -0.1034,
          0.0022, -0.1363, -0.0314, -0.1281, -0.1175,  0.1324,  0.0801,  0.0789,
          0.0476,  0.0349, -0.0765, -0.0435, -0.0693,  0.0187, -0.1075,  0.0885,
          0.0337,  0.0157,  0.1473, -0.1030,  0.1228, -0.0401, -0.1306,  0.1196,
          0.1165, -0.0836,  0.0189, -0.1272, -0.0892,  0.0154,  0.0884, -0.0983,
          0.1214,  0.0224,  0.0226,  0.1306,  0.1141,  0.0603,  0.0491, -0.0541,
         -0.0944,  0.0989, -0.1489, -0.1488, -0.0069,  0.1315, -0.0665,  0.1145,
          0.0712, -0.0134, -0.0955, -0.0254,  0.0103,  0.0406, -0.0250,  0.0604,
         -0.0381,  0.1006,  0.0537,  0.1403, -0.0777, -0.0079,  0.0972,  0.0697,
          0.0440,  0.0338, -0.0156,  0.0816,  0.1167, -0.0904,  0.0756,  0.0077,
         -0.0562, -0.0180, -0.0997, -0.1150,  0.0521, -0.0143, -0.0811,  0.0280,
          0.1028, -0.0743, -0.0768, -0.0102, -0.0714, -0.1221,  0.0616, -0.0040,
         -0.1447, -0.1476, -0.0326,  0.0118,  0.1007,  0.0731,  0.0291, -0.1451,
          0.1094,  0.0287, -0.1392,  0.1151, -0.0067, -0.1022, -0.0310, -0.0618,
         -0.1148, -0.1493,  0.1034, -0.1243,  0.1268, -0.0963,  0.0144,  0.1350,
         -0.0563,  0.0269, -0.0322, -0.1410,  0.0583,  0.1304, -0.0930, -0.0661,
          0.0960, -0.0044, -0.1427, -0.0071, -0.0963,  0.0782,  0.0483, -0.0515,
          0.0583,  0.0398, -0.1194, -0.0887, -0.0955,  0.0939, -0.1242, -0.1355,
          0.1263,  0.0529, -0.0241, -0.0795, -0.0564, -0.0735, -0.0057,  0.1109,
          0.0015, -0.0351, -0.1072,  0.1268,  0.1145, -0.1045, -0.1286, -0.0211,
         -0.0638, -0.1012,  0.0326, -0.1231, -0.0964, -0.0702,  0.0541, -0.1324,
         -0.1191, -0.0239,  0.0031,  0.0028,  0.1378,  0.1384,  0.0037, -0.1351]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-3.3192e-04,  1.2886e-04,  2.2174e-04,  ...,  5.1267e-04,
         -3.3139e-04,  3.1444e-04],
        [-2.9132e-04, -5.5794e-04,  3.4189e-05,  ..., -7.0518e-05,
          3.7614e-04, -6.0651e-04],
        [-2.1137e-05, -4.9614e-04, -4.5956e-05,  ...,  3.7820e-05,
         -1.0915e-04, -8.7289e-05],
        ...,
        [ 5.4800e-04, -8.6508e-05,  4.2808e-04,  ..., -3.7484e-04,
          3.5433e-04,  6.3568e-04],
        [-1.6822e-04, -5.7708e-04,  1.3126e-04,  ...,  4.3038e-04,
          6.4098e-04, -4.9321e-04],
        [-4.1604e-04,  6.1768e-04,  2.7556e-04,  ..., -2.1585e-04,
          1.8288e-04,  1.6716e-05]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 1.0092e-04, -6.1213e-04, -2.7423e-04, -4.0416e-04,  3.2190e-04,
        -2.2400e-04, -2.6675e-05, -2.3487e-04,  6.2391e-04,  2.7180e-04,
        -4.6545e-04, -3.3652e-04,  5.0841e-04, -4.9877e-04, -2.8592e-04,
        -2.5548e-04, -3.7244e-04, -5.8355e-05, -5.4721e-04,  7.1467e-05,
         3.8895e-04,  7.9642e-05,  2.6728e-04, -1.4316e-04,  5.7542e-04,
        -8.4301e-05, -3.8892e-04, -1.0545e-04, -2.9194e-04, -5.5738e-04,
         4.3494e-04, -5.4423e-04, -2.8392e-04,  5.0533e-04,  5.8712e-04,
        -4.4662e-04, -6.1221e-04,  3.8680e-04, -4.7492e-04, -6.2990e-04,
        -3.5334e-04, -2.4983e-04,  6.2651e-04, -6.1648e-04, -1.3168e-04,
        -4.2123e-04, -5.6924e-04, -5.8795e-04,  3.2023e-04,  4.0742e-04,
         5.0757e-04,  3.7776e-05, -4.2139e-04, -1.3111e-04, -1.8610e-05,
        -1.5132e-05, -5.0863e-04,  6.0076e-04, -1.2761e-04,  6.3901e-04,
        -3.4145e-04,  2.2675e-04, -3.5498e-04,  5.0508e-04, -5.2560e-04,
        -4.7634e-04, -2.1768e-04, -6.4861e-04, -6.3838e-04, -5.4644e-05,
         4.1287e-04,  4.6744e-04,  1.7457e-04,  3.8357e-04, -4.1073e-04,
         6.4594e-04,  6.3266e-05,  1.1227e-04,  2.9258e-04,  3.6139e-06,
         5.6771e-04, -2.3121e-04,  6.4562e-04, -2.0853e-04, -5.6389e-04,
         5.9363e-04,  4.6262e-04,  6.2074e-04,  6.2089e-04, -1.5325e-04,
         6.0923e-05,  1.8650e-04,  3.8698e-05, -5.9041e-04, -2.4310e-04,
        -5.4086e-04,  5.1957e-04,  1.0822e-04,  2.9819e-04,  4.5020e-04,
         3.1866e-04, -2.3293e-05,  2.2098e-04,  4.6266e-04,  4.0409e-04,
        -5.1969e-04, -4.1486e-04,  3.2505e-04, -8.0096e-05, -1.4410e-04,
        -2.2704e-04, -5.3312e-04,  6.2798e-04,  2.3916e-04,  5.2788e-04,
         8.2256e-05, -6.1140e-05, -1.1697e-04,  3.2659e-04, -3.9374e-04,
         5.1602e-04,  3.7861e-04,  4.5104e-04, -5.5116e-04,  4.1810e-04,
        -6.4419e-04,  5.3380e-04, -4.6120e-04, -1.1844e-04,  5.4657e-04,
        -5.0190e-04, -7.8687e-05,  6.4122e-04,  5.6865e-04, -4.0166e-04,
         5.5583e-04,  4.3500e-04, -3.3140e-04,  1.3849e-04,  3.1139e-04,
        -2.4657e-04, -8.9193e-05, -3.2018e-05, -5.7727e-04,  3.6275e-04,
        -9.1966e-05,  5.0243e-04,  1.2300e-05, -2.1450e-04, -1.4524e-04,
         4.3353e-04, -1.4882e-04, -3.2374e-04, -1.8318e-04,  5.8350e-04,
         3.7960e-04, -2.9067e-04, -9.2353e-05,  1.7007e-04, -1.3625e-04,
         4.3997e-04,  3.2431e-04, -3.3040e-04,  5.5446e-04,  5.0635e-04,
        -1.4580e-05,  2.1310e-04,  1.5864e-04,  3.0970e-04, -4.4123e-04,
        -3.5091e-04, -1.8882e-04, -3.3908e-04, -6.3125e-04, -2.7396e-04,
        -4.2644e-04, -3.1784e-04, -3.8689e-04, -6.0958e-04, -3.2283e-04,
        -6.3350e-04,  1.8684e-05,  5.0416e-04,  5.0478e-04, -2.5289e-04,
        -2.0999e-05,  2.7028e-04,  4.3588e-04,  6.6342e-05, -3.3825e-04,
        -5.6827e-04,  4.0228e-04,  4.2901e-04, -3.5348e-04, -6.0961e-04,
         5.7999e-04,  1.0752e-04, -6.3494e-04,  3.1148e-04,  1.6180e-04,
         2.5658e-04,  4.9053e-04,  5.4967e-04, -4.5912e-04,  4.6348e-04,
         4.6520e-04, -1.4029e-04, -4.0857e-04, -5.7690e-04, -3.8690e-04,
        -4.0095e-04, -5.4043e-04,  2.6672e-04, -4.9483e-04,  4.0675e-04,
        -4.5481e-04, -2.2543e-04,  3.0596e-04,  3.9880e-04, -1.0442e-04,
        -2.3887e-04,  1.9852e-04, -1.3064e-04,  5.1800e-04,  3.2057e-04,
         4.5932e-04, -4.4677e-04, -6.1444e-04,  2.9179e-05,  5.5672e-04,
        -4.7123e-04,  4.8946e-04, -3.5148e-04,  2.8003e-04,  3.0937e-04,
         1.3945e-04,  5.4503e-04, -1.4649e-04, -2.0209e-04,  2.4849e-04,
         2.9889e-04,  6.0424e-05, -4.5271e-04,  3.6308e-04, -5.8291e-04,
         1.8782e-04, -2.0724e-04, -8.2663e-05, -4.9009e-04,  1.6376e-04,
        -6.4153e-04, -1.2738e-04, -2.3934e-04,  9.3010e-05,  4.6852e-04,
         2.6133e-04], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0346, -0.0506,  0.0353,  ..., -0.0095, -0.0351,  0.0416],
        [ 0.0420,  0.0238, -0.0483,  ..., -0.0576,  0.0204, -0.0568],
        [-0.0391, -0.0603,  0.0198,  ...,  0.0411,  0.0522,  0.0360],
        ...,
        [-0.0475,  0.0140, -0.0439,  ..., -0.0504, -0.0213,  0.0282],
        [-0.0211,  0.0602, -0.0015,  ..., -0.0602, -0.0559,  0.0127],
        [ 0.0319, -0.0043,  0.0083,  ..., -0.0275,  0.0186,  0.0272]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 0.0449,  0.0296,  0.0297,  ...,  0.0394, -0.0133, -0.0364],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0700,  0.1074,  0.0886,  ...,  0.0831, -0.0416, -0.0130],
        [-0.0321, -0.0983,  0.0064,  ..., -0.1075, -0.0197,  0.0103],
        [ 0.0518,  0.0556, -0.0246,  ...,  0.0606, -0.0571,  0.0910],
        ...,
        [-0.0604, -0.0050, -0.0540,  ...,  0.0364,  0.0164, -0.0997],
        [-0.0123,  0.0172,  0.0046,  ..., -0.0038, -0.0232, -0.0809],
        [-0.0983, -0.0268,  0.0824,  ...,  0.1166, -0.0273,  0.0376]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1533,  0.0894,  0.0680,  ...,  0.0864, -0.0140,  0.1207],
        [-0.0306, -0.0625,  0.1156,  ..., -0.0518,  0.1244, -0.1320],
        [-0.1288,  0.0256,  0.0943,  ...,  0.0911, -0.0726, -0.1043],
        ...,
        [ 0.0201,  0.0770,  0.1056,  ...,  0.1062,  0.1755, -0.0153],
        [ 0.0302,  0.0259,  0.1464,  ...,  0.0327, -0.1039,  0.0019],
        [ 0.1581, -0.1457,  0.1056,  ...,  0.0036,  0.0133, -0.0750]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2036, -0.1449, -0.0788,  ..., -0.1213, -0.1542,  0.1956],
        [ 0.0133, -0.0300, -0.1157,  ..., -0.0469, -0.2458,  0.0206],
        [ 0.1368,  0.0054, -0.1481,  ..., -0.1439,  0.0334,  0.1270],
        ...,
        [-0.1438,  0.0447,  0.2443,  ...,  0.0665,  0.2209, -0.1679],
        [-0.0273, -0.1437, -0.1518,  ...,  0.0384,  0.1905,  0.2072],
        [ 0.1700,  0.2167, -0.0188,  ...,  0.0052,  0.1878, -0.1237]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4190],
        [ 0.0123],
        [ 0.0353],
        [-0.2025],
        [-0.0500],
        [-0.2547],
        [ 0.0787],
        [-0.3491],
        [-0.1794],
        [ 0.3590],
        [ 0.2600],
        [ 0.2330],
        [-0.2677],
        [ 0.1942],
        [ 0.2706],
        [ 0.2324],
        [-0.3377],
        [-0.2458],
        [ 0.1903],
        [ 0.0560],
        [ 0.1388],
        [ 0.3283],
        [-0.3743],
        [ 0.0290],
        [ 0.3293],
        [ 0.1449],
        [ 0.0426],
        [-0.0901],
        [ 0.1735],
        [-0.3744],
        [-0.1918],
        [ 0.3624]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups after adding efet as parameters
 [{'params': [Parameter containing:
tensor([[ 0.0378, -0.0400, -0.1079, -0.0674,  0.1424,  0.0055,  0.1196, -0.0337,
         -0.0066,  0.1071, -0.0981,  0.0376,  0.0634, -0.0464, -0.0616,  0.0171,
         -0.0031,  0.0664,  0.1113, -0.0345,  0.0973,  0.1507, -0.0818,  0.0652,
         -0.0470,  0.0320, -0.1476, -0.1147, -0.1308, -0.0812, -0.1164, -0.0218,
          0.0592,  0.0275,  0.0504,  0.1426,  0.0311,  0.0489,  0.0760,  0.0993,
         -0.1093,  0.1474, -0.0288,  0.1433,  0.0971, -0.1204,  0.0823, -0.0109,
         -0.0306, -0.0214, -0.0058,  0.0290, -0.1120, -0.0563, -0.1170,  0.1263,
          0.1283,  0.0739, -0.1081,  0.1425,  0.1300, -0.0805, -0.0316,  0.1123,
          0.0484, -0.0643,  0.0388,  0.0638,  0.0600,  0.0597,  0.0544, -0.0342,
          0.1245, -0.1408, -0.1329,  0.1134,  0.0156, -0.0731, -0.0894,  0.0814,
         -0.0453, -0.1337,  0.1128, -0.0868, -0.0396, -0.0042,  0.0192, -0.1034,
          0.0022, -0.1363, -0.0314, -0.1281, -0.1175,  0.1324,  0.0801,  0.0789,
          0.0476,  0.0349, -0.0765, -0.0435, -0.0693,  0.0187, -0.1075,  0.0885,
          0.0337,  0.0157,  0.1473, -0.1030,  0.1228, -0.0401, -0.1306,  0.1196,
          0.1165, -0.0836,  0.0189, -0.1272, -0.0892,  0.0154,  0.0884, -0.0983,
          0.1214,  0.0224,  0.0226,  0.1306,  0.1141,  0.0603,  0.0491, -0.0541,
         -0.0944,  0.0989, -0.1489, -0.1488, -0.0069,  0.1315, -0.0665,  0.1145,
          0.0712, -0.0134, -0.0955, -0.0254,  0.0103,  0.0406, -0.0250,  0.0604,
         -0.0381,  0.1006,  0.0537,  0.1403, -0.0777, -0.0079,  0.0972,  0.0697,
          0.0440,  0.0338, -0.0156,  0.0816,  0.1167, -0.0904,  0.0756,  0.0077,
         -0.0562, -0.0180, -0.0997, -0.1150,  0.0521, -0.0143, -0.0811,  0.0280,
          0.1028, -0.0743, -0.0768, -0.0102, -0.0714, -0.1221,  0.0616, -0.0040,
         -0.1447, -0.1476, -0.0326,  0.0118,  0.1007,  0.0731,  0.0291, -0.1451,
          0.1094,  0.0287, -0.1392,  0.1151, -0.0067, -0.1022, -0.0310, -0.0618,
         -0.1148, -0.1493,  0.1034, -0.1243,  0.1268, -0.0963,  0.0144,  0.1350,
         -0.0563,  0.0269, -0.0322, -0.1410,  0.0583,  0.1304, -0.0930, -0.0661,
          0.0960, -0.0044, -0.1427, -0.0071, -0.0963,  0.0782,  0.0483, -0.0515,
          0.0583,  0.0398, -0.1194, -0.0887, -0.0955,  0.0939, -0.1242, -0.1355,
          0.1263,  0.0529, -0.0241, -0.0795, -0.0564, -0.0735, -0.0057,  0.1109,
          0.0015, -0.0351, -0.1072,  0.1268,  0.1145, -0.1045, -0.1286, -0.0211,
         -0.0638, -0.1012,  0.0326, -0.1231, -0.0964, -0.0702,  0.0541, -0.1324,
         -0.1191, -0.0239,  0.0031,  0.0028,  0.1378,  0.1384,  0.0037, -0.1351]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-3.3192e-04,  1.2886e-04,  2.2174e-04,  ...,  5.1267e-04,
         -3.3139e-04,  3.1444e-04],
        [-2.9132e-04, -5.5794e-04,  3.4189e-05,  ..., -7.0518e-05,
          3.7614e-04, -6.0651e-04],
        [-2.1137e-05, -4.9614e-04, -4.5956e-05,  ...,  3.7820e-05,
         -1.0915e-04, -8.7289e-05],
        ...,
        [ 5.4800e-04, -8.6508e-05,  4.2808e-04,  ..., -3.7484e-04,
          3.5433e-04,  6.3568e-04],
        [-1.6822e-04, -5.7708e-04,  1.3126e-04,  ...,  4.3038e-04,
          6.4098e-04, -4.9321e-04],
        [-4.1604e-04,  6.1768e-04,  2.7556e-04,  ..., -2.1585e-04,
          1.8288e-04,  1.6716e-05]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 1.0092e-04, -6.1213e-04, -2.7423e-04, -4.0416e-04,  3.2190e-04,
        -2.2400e-04, -2.6675e-05, -2.3487e-04,  6.2391e-04,  2.7180e-04,
        -4.6545e-04, -3.3652e-04,  5.0841e-04, -4.9877e-04, -2.8592e-04,
        -2.5548e-04, -3.7244e-04, -5.8355e-05, -5.4721e-04,  7.1467e-05,
         3.8895e-04,  7.9642e-05,  2.6728e-04, -1.4316e-04,  5.7542e-04,
        -8.4301e-05, -3.8892e-04, -1.0545e-04, -2.9194e-04, -5.5738e-04,
         4.3494e-04, -5.4423e-04, -2.8392e-04,  5.0533e-04,  5.8712e-04,
        -4.4662e-04, -6.1221e-04,  3.8680e-04, -4.7492e-04, -6.2990e-04,
        -3.5334e-04, -2.4983e-04,  6.2651e-04, -6.1648e-04, -1.3168e-04,
        -4.2123e-04, -5.6924e-04, -5.8795e-04,  3.2023e-04,  4.0742e-04,
         5.0757e-04,  3.7776e-05, -4.2139e-04, -1.3111e-04, -1.8610e-05,
        -1.5132e-05, -5.0863e-04,  6.0076e-04, -1.2761e-04,  6.3901e-04,
        -3.4145e-04,  2.2675e-04, -3.5498e-04,  5.0508e-04, -5.2560e-04,
        -4.7634e-04, -2.1768e-04, -6.4861e-04, -6.3838e-04, -5.4644e-05,
         4.1287e-04,  4.6744e-04,  1.7457e-04,  3.8357e-04, -4.1073e-04,
         6.4594e-04,  6.3266e-05,  1.1227e-04,  2.9258e-04,  3.6139e-06,
         5.6771e-04, -2.3121e-04,  6.4562e-04, -2.0853e-04, -5.6389e-04,
         5.9363e-04,  4.6262e-04,  6.2074e-04,  6.2089e-04, -1.5325e-04,
         6.0923e-05,  1.8650e-04,  3.8698e-05, -5.9041e-04, -2.4310e-04,
        -5.4086e-04,  5.1957e-04,  1.0822e-04,  2.9819e-04,  4.5020e-04,
         3.1866e-04, -2.3293e-05,  2.2098e-04,  4.6266e-04,  4.0409e-04,
        -5.1969e-04, -4.1486e-04,  3.2505e-04, -8.0096e-05, -1.4410e-04,
        -2.2704e-04, -5.3312e-04,  6.2798e-04,  2.3916e-04,  5.2788e-04,
         8.2256e-05, -6.1140e-05, -1.1697e-04,  3.2659e-04, -3.9374e-04,
         5.1602e-04,  3.7861e-04,  4.5104e-04, -5.5116e-04,  4.1810e-04,
        -6.4419e-04,  5.3380e-04, -4.6120e-04, -1.1844e-04,  5.4657e-04,
        -5.0190e-04, -7.8687e-05,  6.4122e-04,  5.6865e-04, -4.0166e-04,
         5.5583e-04,  4.3500e-04, -3.3140e-04,  1.3849e-04,  3.1139e-04,
        -2.4657e-04, -8.9193e-05, -3.2018e-05, -5.7727e-04,  3.6275e-04,
        -9.1966e-05,  5.0243e-04,  1.2300e-05, -2.1450e-04, -1.4524e-04,
         4.3353e-04, -1.4882e-04, -3.2374e-04, -1.8318e-04,  5.8350e-04,
         3.7960e-04, -2.9067e-04, -9.2353e-05,  1.7007e-04, -1.3625e-04,
         4.3997e-04,  3.2431e-04, -3.3040e-04,  5.5446e-04,  5.0635e-04,
        -1.4580e-05,  2.1310e-04,  1.5864e-04,  3.0970e-04, -4.4123e-04,
        -3.5091e-04, -1.8882e-04, -3.3908e-04, -6.3125e-04, -2.7396e-04,
        -4.2644e-04, -3.1784e-04, -3.8689e-04, -6.0958e-04, -3.2283e-04,
        -6.3350e-04,  1.8684e-05,  5.0416e-04,  5.0478e-04, -2.5289e-04,
        -2.0999e-05,  2.7028e-04,  4.3588e-04,  6.6342e-05, -3.3825e-04,
        -5.6827e-04,  4.0228e-04,  4.2901e-04, -3.5348e-04, -6.0961e-04,
         5.7999e-04,  1.0752e-04, -6.3494e-04,  3.1148e-04,  1.6180e-04,
         2.5658e-04,  4.9053e-04,  5.4967e-04, -4.5912e-04,  4.6348e-04,
         4.6520e-04, -1.4029e-04, -4.0857e-04, -5.7690e-04, -3.8690e-04,
        -4.0095e-04, -5.4043e-04,  2.6672e-04, -4.9483e-04,  4.0675e-04,
        -4.5481e-04, -2.2543e-04,  3.0596e-04,  3.9880e-04, -1.0442e-04,
        -2.3887e-04,  1.9852e-04, -1.3064e-04,  5.1800e-04,  3.2057e-04,
         4.5932e-04, -4.4677e-04, -6.1444e-04,  2.9179e-05,  5.5672e-04,
        -4.7123e-04,  4.8946e-04, -3.5148e-04,  2.8003e-04,  3.0937e-04,
         1.3945e-04,  5.4503e-04, -1.4649e-04, -2.0209e-04,  2.4849e-04,
         2.9889e-04,  6.0424e-05, -4.5271e-04,  3.6308e-04, -5.8291e-04,
         1.8782e-04, -2.0724e-04, -8.2663e-05, -4.9009e-04,  1.6376e-04,
        -6.4153e-04, -1.2738e-04, -2.3934e-04,  9.3010e-05,  4.6852e-04,
         2.6133e-04], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0346, -0.0506,  0.0353,  ..., -0.0095, -0.0351,  0.0416],
        [ 0.0420,  0.0238, -0.0483,  ..., -0.0576,  0.0204, -0.0568],
        [-0.0391, -0.0603,  0.0198,  ...,  0.0411,  0.0522,  0.0360],
        ...,
        [-0.0475,  0.0140, -0.0439,  ..., -0.0504, -0.0213,  0.0282],
        [-0.0211,  0.0602, -0.0015,  ..., -0.0602, -0.0559,  0.0127],
        [ 0.0319, -0.0043,  0.0083,  ..., -0.0275,  0.0186,  0.0272]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([ 0.0449,  0.0296,  0.0297,  ...,  0.0394, -0.0133, -0.0364],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0700,  0.1074,  0.0886,  ...,  0.0831, -0.0416, -0.0130],
        [-0.0321, -0.0983,  0.0064,  ..., -0.1075, -0.0197,  0.0103],
        [ 0.0518,  0.0556, -0.0246,  ...,  0.0606, -0.0571,  0.0910],
        ...,
        [-0.0604, -0.0050, -0.0540,  ...,  0.0364,  0.0164, -0.0997],
        [-0.0123,  0.0172,  0.0046,  ..., -0.0038, -0.0232, -0.0809],
        [-0.0983, -0.0268,  0.0824,  ...,  0.1166, -0.0273,  0.0376]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1533,  0.0894,  0.0680,  ...,  0.0864, -0.0140,  0.1207],
        [-0.0306, -0.0625,  0.1156,  ..., -0.0518,  0.1244, -0.1320],
        [-0.1288,  0.0256,  0.0943,  ...,  0.0911, -0.0726, -0.1043],
        ...,
        [ 0.0201,  0.0770,  0.1056,  ...,  0.1062,  0.1755, -0.0153],
        [ 0.0302,  0.0259,  0.1464,  ...,  0.0327, -0.1039,  0.0019],
        [ 0.1581, -0.1457,  0.1056,  ...,  0.0036,  0.0133, -0.0750]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2036, -0.1449, -0.0788,  ..., -0.1213, -0.1542,  0.1956],
        [ 0.0133, -0.0300, -0.1157,  ..., -0.0469, -0.2458,  0.0206],
        [ 0.1368,  0.0054, -0.1481,  ..., -0.1439,  0.0334,  0.1270],
        ...,
        [-0.1438,  0.0447,  0.2443,  ...,  0.0665,  0.2209, -0.1679],
        [-0.0273, -0.1437, -0.1518,  ...,  0.0384,  0.1905,  0.2072],
        [ 0.1700,  0.2167, -0.0188,  ...,  0.0052,  0.1878, -0.1237]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4190],
        [ 0.0123],
        [ 0.0353],
        [-0.2025],
        [-0.0500],
        [-0.2547],
        [ 0.0787],
        [-0.3491],
        [-0.1794],
        [ 0.3590],
        [ 0.2600],
        [ 0.2330],
        [-0.2677],
        [ 0.1942],
        [ 0.2706],
        [ 0.2324],
        [-0.3377],
        [-0.2458],
        [ 0.1903],
        [ 0.0560],
        [ 0.1388],
        [ 0.3283],
        [-0.3743],
        [ 0.0290],
        [ 0.3293],
        [ 0.1449],
        [ 0.0426],
        [-0.0901],
        [ 0.1735],
        [-0.3744],
        [-0.1918],
        [ 0.3624]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]





 shepe: torch.Size([35258310, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7212],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.9717, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet] tensor([[1.8302e-05],
        [3.2198e-02],
        [0.0000e+00],
        ...,
        [0.0000e+00],
        [0.0000e+00],
        [3.9257e-01]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(3215760.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7212],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.9717, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065, -0.0069, -0.0186,  ...,  0.0239,  0.0006, -0.0233],
        [ 0.0118, -0.0125, -0.0337,  ...,  0.0432,  0.0011, -0.0422],
        [ 0.0055, -0.0058, -0.0156,  ...,  0.0201,  0.0005, -0.0196],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-586.2061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6553, device='cuda:0')



h[100].sum tensor(-47.5315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-50.6355, device='cuda:0')



h[200].sum tensor(-38.6187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-41.1406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0460, 0.0000, 0.0000,  ..., 0.1684, 0.0045, 0.0000],
        [0.0275, 0.0000, 0.0000,  ..., 0.1005, 0.0027, 0.0000],
        [0.0335, 0.0000, 0.0000,  ..., 0.1224, 0.0033, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(187966.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0073e-01, 2.3487e-02,  ..., 2.2871e-01, 3.1465e-01,
         0.0000e+00],
        [0.0000e+00, 1.1071e-01, 1.2955e-02,  ..., 1.2615e-01, 1.7355e-01,
         0.0000e+00],
        [0.0000e+00, 1.1736e-01, 1.3733e-02,  ..., 1.3372e-01, 1.8397e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 6.0932e-04, 7.1297e-05,  ..., 6.9425e-04, 9.5513e-04,
         0.0000e+00],
        [0.0000e+00, 1.0787e-03, 1.2622e-04,  ..., 1.2291e-03, 1.6909e-03,
         0.0000e+00],
        [0.0000e+00, 9.6984e-04, 1.1348e-04,  ..., 1.1050e-03, 1.5203e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(4326430., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1174.2676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=35258310,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7290],
        [-4.5751],
        [-4.3494],
        ...,
        [-0.0508],
        [-0.0752],
        [-0.0823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-4871272., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.8302e-05],
        [3.2198e-02],
        [0.0000e+00],
        ...,
        [0.0000e+00],
        [0.0000e+00],
        [3.9257e-01]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([35258310, 1]) 
g.edata[efet].sum tensor(3215760.2500, device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/./TrainingBha2ndneiefet.py", line 85, in <module>
    optimizer.step()  # Update parameters based on gradients.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/optim/adam.py", line 115, in step
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
RuntimeError: CUDA out of memory. Tried to allocate 2.24 GiB (GPU 0; 31.75 GiB total capacity; 27.47 GiB already allocated; 1.98 GiB free; 28.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

real	1m21.047s
user	0m47.177s
sys	0m12.215s
