0: gpu035.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-8ac19b97-e996-d56f-26ed-caea68e1fcfc)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        70:B2:A7:DF:ED:82:78:26:9F:D8:28:A0:1D:52:CD:B5:3B:DF:C3:17
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jan  9 17:01:05 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |
| N/A   44C    P0    40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089926656

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ac4e9653880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m2.955s
user	0m1.742s
sys	0m0.493s




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 6507, 6507, 6507],
        [   1,    2,    3,  ..., 6219, 6794, 6795]]) 

edge_index shape
 torch.Size([2, 1175277])
graph: Graph(num_nodes=6796, num_edges=1175277,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 6507, 6507, 6507], device='cuda:0'), tensor([   1,    2,    3,  ..., 6219, 6794, 6795], device='cuda:0'))

number of nodes: 6796

number of edges: 2350554

node features (random input): tensor([[-0.0609],
        [-0.4931],
        [-0.1207],
        ...,
        [ 0.3537],
        [ 0.3707],
        [-0.0387]], device='cuda:0', requires_grad=True) 
node features sum: tensor(70.0518, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(2350554., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 399

In degrees of node 234: 399





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLBhaGcnReNewestweight7N4SecNei

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (linear1): Linear(in_features=2350554, out_features=256, bias=True)
  (linear2): Linear(in_features=256, out_features=2350554, bias=True)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 1205878235

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[ 0.0176, -0.0897,  0.1161, -0.0209,  0.0133, -0.0347,  0.0855,  0.1054,
          0.0189,  0.1244, -0.1012,  0.0962,  0.1053,  0.0814,  0.0065, -0.0377,
         -0.1247,  0.0820,  0.0528, -0.0163, -0.0451,  0.1364,  0.0223, -0.0235,
          0.1029,  0.0383,  0.0320, -0.0200,  0.0723, -0.0836, -0.1393,  0.0374,
         -0.0853, -0.0528, -0.0256,  0.1305,  0.0434, -0.0624, -0.1049,  0.1017,
         -0.1002, -0.1398,  0.0030,  0.0584, -0.1025, -0.0708, -0.1337, -0.0120,
          0.0774, -0.0434,  0.1316, -0.0642, -0.0724, -0.0876, -0.0352,  0.0926,
         -0.0276,  0.0416, -0.1334, -0.0573, -0.0080, -0.0229,  0.1477,  0.1250,
          0.0803, -0.0545,  0.0910, -0.1521, -0.0817, -0.1264,  0.0228,  0.1117,
         -0.1320, -0.1444, -0.0341,  0.0534, -0.0761,  0.0140, -0.0549, -0.0095,
         -0.0641, -0.0218, -0.0225,  0.0433,  0.1267,  0.0392, -0.0696, -0.0682,
         -0.1173,  0.0941, -0.0930, -0.1422,  0.0636, -0.1133, -0.0776,  0.0959,
          0.0859, -0.0588,  0.0819,  0.1139,  0.0237, -0.1427,  0.1323,  0.0978,
         -0.1386,  0.0217,  0.0205, -0.0591, -0.0769, -0.0201,  0.1490,  0.0688,
          0.0270, -0.0185, -0.1032, -0.1368,  0.1216, -0.1287, -0.1441, -0.0477,
         -0.0424,  0.0210, -0.0375,  0.1419, -0.0880,  0.0681,  0.0199,  0.1503,
          0.0476, -0.1515, -0.0569, -0.0987,  0.0073, -0.0269, -0.0436, -0.1102,
          0.1108,  0.1137, -0.0656,  0.1187, -0.1034,  0.1289, -0.0536, -0.0279,
         -0.0596, -0.1055,  0.1162, -0.0132,  0.0850, -0.0842, -0.0923,  0.1372,
         -0.0186,  0.0563, -0.1521,  0.1460, -0.0828, -0.0155,  0.0453, -0.1201,
          0.0916,  0.1007, -0.0456,  0.0699, -0.0179,  0.0248,  0.0409, -0.1252,
          0.1365, -0.0509,  0.0969, -0.1456,  0.1414, -0.0588,  0.0253,  0.0959,
          0.1020,  0.0474,  0.1264,  0.0893,  0.0450,  0.0643, -0.0601,  0.0155,
         -0.1508,  0.0490,  0.0883,  0.0521,  0.0380, -0.1221, -0.0242, -0.0491,
          0.0993,  0.1253,  0.0822, -0.1139,  0.0106,  0.0418, -0.0297, -0.0482,
         -0.0108,  0.0152,  0.0310, -0.0294,  0.0540,  0.1371, -0.1028, -0.1025,
         -0.0769, -0.1028,  0.0820,  0.1096, -0.0379, -0.1349,  0.1409, -0.1318,
         -0.0392, -0.0613,  0.0605,  0.1123, -0.1411, -0.1289, -0.1436,  0.1114,
          0.0528,  0.1175,  0.0419, -0.1225,  0.1064, -0.0836, -0.0704, -0.0260,
         -0.0044,  0.0170, -0.0326,  0.0613,  0.0793, -0.1503, -0.0019, -0.0658,
          0.1272, -0.0045,  0.0871, -0.0457,  0.1309, -0.1461,  0.0115, -0.1461,
         -0.0122,  0.0152,  0.1216,  0.0928, -0.1494,  0.0857, -0.0138,  0.0451]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0176, -0.0897,  0.1161, -0.0209,  0.0133, -0.0347,  0.0855,  0.1054,
          0.0189,  0.1244, -0.1012,  0.0962,  0.1053,  0.0814,  0.0065, -0.0377,
         -0.1247,  0.0820,  0.0528, -0.0163, -0.0451,  0.1364,  0.0223, -0.0235,
          0.1029,  0.0383,  0.0320, -0.0200,  0.0723, -0.0836, -0.1393,  0.0374,
         -0.0853, -0.0528, -0.0256,  0.1305,  0.0434, -0.0624, -0.1049,  0.1017,
         -0.1002, -0.1398,  0.0030,  0.0584, -0.1025, -0.0708, -0.1337, -0.0120,
          0.0774, -0.0434,  0.1316, -0.0642, -0.0724, -0.0876, -0.0352,  0.0926,
         -0.0276,  0.0416, -0.1334, -0.0573, -0.0080, -0.0229,  0.1477,  0.1250,
          0.0803, -0.0545,  0.0910, -0.1521, -0.0817, -0.1264,  0.0228,  0.1117,
         -0.1320, -0.1444, -0.0341,  0.0534, -0.0761,  0.0140, -0.0549, -0.0095,
         -0.0641, -0.0218, -0.0225,  0.0433,  0.1267,  0.0392, -0.0696, -0.0682,
         -0.1173,  0.0941, -0.0930, -0.1422,  0.0636, -0.1133, -0.0776,  0.0959,
          0.0859, -0.0588,  0.0819,  0.1139,  0.0237, -0.1427,  0.1323,  0.0978,
         -0.1386,  0.0217,  0.0205, -0.0591, -0.0769, -0.0201,  0.1490,  0.0688,
          0.0270, -0.0185, -0.1032, -0.1368,  0.1216, -0.1287, -0.1441, -0.0477,
         -0.0424,  0.0210, -0.0375,  0.1419, -0.0880,  0.0681,  0.0199,  0.1503,
          0.0476, -0.1515, -0.0569, -0.0987,  0.0073, -0.0269, -0.0436, -0.1102,
          0.1108,  0.1137, -0.0656,  0.1187, -0.1034,  0.1289, -0.0536, -0.0279,
         -0.0596, -0.1055,  0.1162, -0.0132,  0.0850, -0.0842, -0.0923,  0.1372,
         -0.0186,  0.0563, -0.1521,  0.1460, -0.0828, -0.0155,  0.0453, -0.1201,
          0.0916,  0.1007, -0.0456,  0.0699, -0.0179,  0.0248,  0.0409, -0.1252,
          0.1365, -0.0509,  0.0969, -0.1456,  0.1414, -0.0588,  0.0253,  0.0959,
          0.1020,  0.0474,  0.1264,  0.0893,  0.0450,  0.0643, -0.0601,  0.0155,
         -0.1508,  0.0490,  0.0883,  0.0521,  0.0380, -0.1221, -0.0242, -0.0491,
          0.0993,  0.1253,  0.0822, -0.1139,  0.0106,  0.0418, -0.0297, -0.0482,
         -0.0108,  0.0152,  0.0310, -0.0294,  0.0540,  0.1371, -0.1028, -0.1025,
         -0.0769, -0.1028,  0.0820,  0.1096, -0.0379, -0.1349,  0.1409, -0.1318,
         -0.0392, -0.0613,  0.0605,  0.1123, -0.1411, -0.1289, -0.1436,  0.1114,
          0.0528,  0.1175,  0.0419, -0.1225,  0.1064, -0.0836, -0.0704, -0.0260,
         -0.0044,  0.0170, -0.0326,  0.0613,  0.0793, -0.1503, -0.0019, -0.0658,
          0.1272, -0.0045,  0.0871, -0.0457,  0.1309, -0.1461,  0.0115, -0.1461,
         -0.0122,  0.0152,  0.1216,  0.0928, -0.1494,  0.0857, -0.0138,  0.0451]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name linear1.weight 
shape:
 torch.Size([256, 2350554]) 
grad:
 True 
date:
 tensor([[ 5.7571e-04, -1.3199e-04,  6.5106e-04,  ...,  3.1541e-04,
          3.4891e-04, -3.4477e-04],
        [ 1.6255e-04, -3.1048e-04, -5.9479e-05,  ...,  4.7574e-04,
          1.6856e-04, -2.3193e-04],
        [ 6.2535e-04,  1.7350e-04, -1.2583e-04,  ..., -5.2097e-04,
         -4.8311e-04, -2.6531e-04],
        ...,
        [ 1.7338e-04,  5.3353e-04,  6.4421e-04,  ..., -1.8718e-04,
          1.2335e-04,  3.2501e-05],
        [ 3.7097e-04,  4.2743e-04, -2.4157e-04,  ..., -1.7136e-04,
         -3.0166e-04,  6.1370e-04],
        [ 5.8359e-04, -5.6879e-04, -1.7535e-04,  ...,  5.2460e-04,
          7.8146e-05, -5.3632e-04]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 5.7571e-04, -1.3199e-04,  6.5106e-04,  ...,  3.1541e-04,
          3.4891e-04, -3.4477e-04],
        [ 1.6255e-04, -3.1048e-04, -5.9479e-05,  ...,  4.7574e-04,
          1.6856e-04, -2.3193e-04],
        [ 6.2535e-04,  1.7350e-04, -1.2583e-04,  ..., -5.2097e-04,
         -4.8311e-04, -2.6531e-04],
        ...,
        [ 1.7338e-04,  5.3353e-04,  6.4421e-04,  ..., -1.8718e-04,
          1.2335e-04,  3.2501e-05],
        [ 3.7097e-04,  4.2743e-04, -2.4157e-04,  ..., -1.7136e-04,
         -3.0166e-04,  6.1370e-04],
        [ 5.8359e-04, -5.6879e-04, -1.7535e-04,  ...,  5.2460e-04,
          7.8146e-05, -5.3632e-04]], device='cuda:0', requires_grad=True)

name linear1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([ 2.7181e-04, -5.6372e-04, -6.0229e-04,  2.5138e-04,  5.7980e-04,
        -1.0450e-04,  3.0701e-04,  2.7878e-04, -5.4645e-04,  2.0969e-04,
         5.7215e-04,  1.4322e-04,  5.6939e-04, -2.6091e-05, -3.4887e-04,
        -4.1896e-04, -6.4403e-04,  4.2774e-04, -2.2743e-04,  2.4068e-04,
         5.3312e-04,  3.9211e-05, -3.9553e-04, -9.4104e-05,  3.1909e-04,
        -6.1615e-05, -1.6395e-04,  2.0017e-04,  3.1903e-04,  5.3581e-04,
         4.7807e-04, -3.1328e-04,  2.2515e-05,  9.9770e-05, -1.6280e-04,
         5.0525e-04, -3.3042e-04, -5.5997e-04,  1.0413e-04,  5.7228e-05,
         2.4124e-04,  3.3239e-04, -3.0108e-04,  2.6933e-04,  4.3672e-05,
         5.7418e-04, -2.4367e-04,  4.8197e-04,  1.9426e-05, -1.8905e-05,
        -7.8927e-05,  9.7382e-06,  7.5972e-05, -3.1269e-04, -6.9750e-05,
         2.5993e-04, -4.2392e-04,  3.5986e-06, -4.5713e-04,  6.2834e-04,
         5.8731e-05, -1.9954e-04, -4.9186e-04, -5.2720e-04, -4.6645e-07,
        -2.8909e-04,  3.5359e-04,  1.4764e-04,  2.9700e-04, -5.7279e-04,
        -4.4673e-04, -1.3805e-04, -2.6225e-04,  4.4443e-04,  3.4900e-04,
         6.4696e-04, -3.9801e-04,  1.4740e-04, -6.2354e-04,  5.3038e-04,
        -3.9740e-04, -4.7951e-04,  3.2591e-04, -1.6509e-05, -6.4611e-05,
        -1.4484e-05, -3.8408e-04, -2.0289e-04, -5.8885e-04,  3.1863e-04,
        -4.0084e-04, -6.2770e-04, -5.7566e-04,  4.6332e-04, -9.7253e-06,
        -2.3434e-05, -1.2752e-04, -3.3402e-04,  1.2894e-04,  5.0775e-04,
         6.0064e-04, -2.2692e-04,  5.0196e-04, -1.6370e-04, -1.5781e-04,
         4.9066e-04, -5.9209e-04, -3.1712e-04,  5.1189e-04, -4.9800e-04,
         4.3795e-04, -3.7842e-06, -4.9742e-04, -1.4820e-04, -1.3788e-04,
        -6.3378e-05, -3.2004e-05, -3.8525e-04, -3.6708e-04,  3.1229e-04,
        -5.9482e-04,  4.5548e-04,  5.6060e-04, -4.7550e-04,  6.0219e-04,
        -3.6271e-04,  1.0120e-04, -5.4460e-04, -2.7614e-04, -4.6847e-04,
        -1.4592e-04, -5.9312e-04, -4.8695e-04, -2.8024e-04,  5.1408e-05,
         6.3231e-04,  5.3752e-04,  1.3627e-04, -6.2246e-05, -4.6597e-05,
        -1.1406e-04,  1.7029e-04,  6.5938e-05,  1.3249e-05,  1.9158e-04,
        -1.4448e-04,  3.7121e-04,  3.5731e-04,  4.6258e-04, -4.9381e-04,
        -1.5648e-04,  2.0382e-04,  2.0896e-04,  1.6369e-04,  6.4815e-05,
        -6.1118e-04,  1.7915e-04, -1.3500e-05,  1.9780e-04, -8.6028e-05,
         3.8680e-04,  1.6709e-07,  6.9466e-05, -1.1420e-04,  2.0500e-04,
        -2.2727e-04,  9.4992e-05,  5.6437e-04,  3.7573e-04, -5.8737e-04,
        -3.8107e-04,  1.5212e-04,  6.1388e-04,  5.6484e-04, -4.2388e-04,
        -2.5372e-05,  7.2472e-05,  4.5092e-04,  4.0728e-04,  1.4791e-04,
         2.8911e-04,  1.1728e-04,  3.9663e-04,  5.9188e-04, -1.7987e-04,
        -3.0380e-05, -1.2197e-04,  6.2593e-04, -6.0701e-05, -1.6339e-04,
        -2.7462e-04, -6.7817e-05,  5.0250e-04,  2.9210e-04,  3.6499e-04,
         5.3499e-04, -3.0993e-04, -1.7150e-04, -4.6426e-04, -5.1356e-04,
         8.2330e-05,  6.9946e-05,  2.9353e-04,  6.2015e-04, -2.2223e-04,
         5.3667e-04,  2.6095e-04,  5.8309e-05,  6.7722e-05,  2.2469e-04,
        -1.0444e-04,  2.3473e-05, -2.1189e-04, -3.7548e-04,  4.5225e-04,
         2.1981e-04, -1.2979e-04,  2.5938e-04, -3.7511e-04, -1.9509e-04,
        -7.5623e-05, -8.7863e-05, -3.7139e-04, -5.6993e-04,  3.6725e-04,
        -2.2523e-04,  4.7472e-04,  1.0955e-04, -3.7831e-04,  2.0792e-04,
         5.1471e-04,  1.8361e-04, -1.3893e-04, -1.7848e-04,  5.9994e-04,
        -1.6764e-04,  9.2902e-06,  5.6167e-04, -1.1887e-04,  5.3869e-04,
         6.9435e-05, -6.4932e-04,  1.9796e-04,  2.4604e-04,  4.5409e-08,
        -6.2213e-04,  1.9888e-04,  5.9707e-04,  4.4792e-04, -9.8408e-05,
         2.0097e-04, -2.5067e-05,  4.6780e-04,  5.7518e-04, -2.0528e-05,
        -3.2188e-04], device='cuda:0') 
parameter:
 Parameter containing:
tensor([ 2.7181e-04, -5.6372e-04, -6.0229e-04,  2.5138e-04,  5.7980e-04,
        -1.0450e-04,  3.0701e-04,  2.7878e-04, -5.4645e-04,  2.0969e-04,
         5.7215e-04,  1.4322e-04,  5.6939e-04, -2.6091e-05, -3.4887e-04,
        -4.1896e-04, -6.4403e-04,  4.2774e-04, -2.2743e-04,  2.4068e-04,
         5.3312e-04,  3.9211e-05, -3.9553e-04, -9.4104e-05,  3.1909e-04,
        -6.1615e-05, -1.6395e-04,  2.0017e-04,  3.1903e-04,  5.3581e-04,
         4.7807e-04, -3.1328e-04,  2.2515e-05,  9.9770e-05, -1.6280e-04,
         5.0525e-04, -3.3042e-04, -5.5997e-04,  1.0413e-04,  5.7228e-05,
         2.4124e-04,  3.3239e-04, -3.0108e-04,  2.6933e-04,  4.3672e-05,
         5.7418e-04, -2.4367e-04,  4.8197e-04,  1.9426e-05, -1.8905e-05,
        -7.8927e-05,  9.7382e-06,  7.5972e-05, -3.1269e-04, -6.9750e-05,
         2.5993e-04, -4.2392e-04,  3.5986e-06, -4.5713e-04,  6.2834e-04,
         5.8731e-05, -1.9954e-04, -4.9186e-04, -5.2720e-04, -4.6645e-07,
        -2.8909e-04,  3.5359e-04,  1.4764e-04,  2.9700e-04, -5.7279e-04,
        -4.4673e-04, -1.3805e-04, -2.6225e-04,  4.4443e-04,  3.4900e-04,
         6.4696e-04, -3.9801e-04,  1.4740e-04, -6.2354e-04,  5.3038e-04,
        -3.9740e-04, -4.7951e-04,  3.2591e-04, -1.6509e-05, -6.4611e-05,
        -1.4484e-05, -3.8408e-04, -2.0289e-04, -5.8885e-04,  3.1863e-04,
        -4.0084e-04, -6.2770e-04, -5.7566e-04,  4.6332e-04, -9.7253e-06,
        -2.3434e-05, -1.2752e-04, -3.3402e-04,  1.2894e-04,  5.0775e-04,
         6.0064e-04, -2.2692e-04,  5.0196e-04, -1.6370e-04, -1.5781e-04,
         4.9066e-04, -5.9209e-04, -3.1712e-04,  5.1189e-04, -4.9800e-04,
         4.3795e-04, -3.7842e-06, -4.9742e-04, -1.4820e-04, -1.3788e-04,
        -6.3378e-05, -3.2004e-05, -3.8525e-04, -3.6708e-04,  3.1229e-04,
        -5.9482e-04,  4.5548e-04,  5.6060e-04, -4.7550e-04,  6.0219e-04,
        -3.6271e-04,  1.0120e-04, -5.4460e-04, -2.7614e-04, -4.6847e-04,
        -1.4592e-04, -5.9312e-04, -4.8695e-04, -2.8024e-04,  5.1408e-05,
         6.3231e-04,  5.3752e-04,  1.3627e-04, -6.2246e-05, -4.6597e-05,
        -1.1406e-04,  1.7029e-04,  6.5938e-05,  1.3249e-05,  1.9158e-04,
        -1.4448e-04,  3.7121e-04,  3.5731e-04,  4.6258e-04, -4.9381e-04,
        -1.5648e-04,  2.0382e-04,  2.0896e-04,  1.6369e-04,  6.4815e-05,
        -6.1118e-04,  1.7915e-04, -1.3500e-05,  1.9780e-04, -8.6028e-05,
         3.8680e-04,  1.6709e-07,  6.9466e-05, -1.1420e-04,  2.0500e-04,
        -2.2727e-04,  9.4992e-05,  5.6437e-04,  3.7573e-04, -5.8737e-04,
        -3.8107e-04,  1.5212e-04,  6.1388e-04,  5.6484e-04, -4.2388e-04,
        -2.5372e-05,  7.2472e-05,  4.5092e-04,  4.0728e-04,  1.4791e-04,
         2.8911e-04,  1.1728e-04,  3.9663e-04,  5.9188e-04, -1.7987e-04,
        -3.0380e-05, -1.2197e-04,  6.2593e-04, -6.0701e-05, -1.6339e-04,
        -2.7462e-04, -6.7817e-05,  5.0250e-04,  2.9210e-04,  3.6499e-04,
         5.3499e-04, -3.0993e-04, -1.7150e-04, -4.6426e-04, -5.1356e-04,
         8.2330e-05,  6.9946e-05,  2.9353e-04,  6.2015e-04, -2.2223e-04,
         5.3667e-04,  2.6095e-04,  5.8309e-05,  6.7722e-05,  2.2469e-04,
        -1.0444e-04,  2.3473e-05, -2.1189e-04, -3.7548e-04,  4.5225e-04,
         2.1981e-04, -1.2979e-04,  2.5938e-04, -3.7511e-04, -1.9509e-04,
        -7.5623e-05, -8.7863e-05, -3.7139e-04, -5.6993e-04,  3.6725e-04,
        -2.2523e-04,  4.7472e-04,  1.0955e-04, -3.7831e-04,  2.0792e-04,
         5.1471e-04,  1.8361e-04, -1.3893e-04, -1.7848e-04,  5.9994e-04,
        -1.6764e-04,  9.2902e-06,  5.6167e-04, -1.1887e-04,  5.3869e-04,
         6.9435e-05, -6.4932e-04,  1.9796e-04,  2.4604e-04,  4.5409e-08,
        -6.2213e-04,  1.9888e-04,  5.9707e-04,  4.4792e-04, -9.8408e-05,
         2.0097e-04, -2.5067e-05,  4.6780e-04,  5.7518e-04, -2.0528e-05,
        -3.2188e-04], device='cuda:0', requires_grad=True)

name linear2.weight 
shape:
 torch.Size([2350554, 256]) 
grad:
 True 
date:
 tensor([[ 0.0616, -0.0600,  0.0213,  ...,  0.0547, -0.0235,  0.0192],
        [ 0.0065, -0.0398,  0.0602,  ..., -0.0156, -0.0410,  0.0445],
        [ 0.0146,  0.0610,  0.0409,  ...,  0.0018, -0.0456,  0.0063],
        ...,
        [ 0.0551, -0.0021, -0.0297,  ...,  0.0087,  0.0618, -0.0591],
        [-0.0625, -0.0190, -0.0087,  ..., -0.0196, -0.0289,  0.0461],
        [ 0.0103,  0.0428,  0.0526,  ..., -0.0251,  0.0055,  0.0255]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0616, -0.0600,  0.0213,  ...,  0.0547, -0.0235,  0.0192],
        [ 0.0065, -0.0398,  0.0602,  ..., -0.0156, -0.0410,  0.0445],
        [ 0.0146,  0.0610,  0.0409,  ...,  0.0018, -0.0456,  0.0063],
        ...,
        [ 0.0551, -0.0021, -0.0297,  ...,  0.0087,  0.0618, -0.0591],
        [-0.0625, -0.0190, -0.0087,  ..., -0.0196, -0.0289,  0.0461],
        [ 0.0103,  0.0428,  0.0526,  ..., -0.0251,  0.0055,  0.0255]],
       device='cuda:0', requires_grad=True)

name linear2.bias 
shape:
 torch.Size([2350554]) 
grad:
 True 
date:
 tensor([ 0.0425,  0.0284,  0.0482,  ..., -0.0017,  0.0516, -0.0312],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([ 0.0425,  0.0284,  0.0482,  ..., -0.0017,  0.0516, -0.0312],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0407, -0.0435,  0.0992,  ...,  0.0696, -0.0604,  0.0620],
        [ 0.0055,  0.0591, -0.0946,  ...,  0.0950,  0.1189, -0.0030],
        [-0.0700, -0.0809,  0.0126,  ...,  0.0069, -0.0163, -0.0276],
        ...,
        [ 0.1007,  0.0288,  0.1200,  ...,  0.0539,  0.0208, -0.0095],
        [-0.0271, -0.0987,  0.0927,  ...,  0.0492,  0.0358,  0.1169],
        [ 0.0222,  0.1187, -0.0112,  ..., -0.0840, -0.0955,  0.0670]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0407, -0.0435,  0.0992,  ...,  0.0696, -0.0604,  0.0620],
        [ 0.0055,  0.0591, -0.0946,  ...,  0.0950,  0.1189, -0.0030],
        [-0.0700, -0.0809,  0.0126,  ...,  0.0069, -0.0163, -0.0276],
        ...,
        [ 0.1007,  0.0288,  0.1200,  ...,  0.0539,  0.0208, -0.0095],
        [-0.0271, -0.0987,  0.0927,  ...,  0.0492,  0.0358,  0.1169],
        [ 0.0222,  0.1187, -0.0112,  ..., -0.0840, -0.0955,  0.0670]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.0607, -0.0448,  0.0391,  ..., -0.1696,  0.1322, -0.0388],
        [ 0.0411, -0.0755,  0.0662,  ..., -0.0098, -0.0199, -0.1084],
        [ 0.0421, -0.0305, -0.1383,  ..., -0.0642, -0.0148,  0.0808],
        ...,
        [ 0.1048, -0.0564,  0.0113,  ..., -0.1740, -0.0580, -0.1241],
        [ 0.1672,  0.0825,  0.0776,  ...,  0.1646, -0.1158,  0.1235],
        [ 0.1214, -0.1186, -0.1533,  ...,  0.1486, -0.1350,  0.1546]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0607, -0.0448,  0.0391,  ..., -0.1696,  0.1322, -0.0388],
        [ 0.0411, -0.0755,  0.0662,  ..., -0.0098, -0.0199, -0.1084],
        [ 0.0421, -0.0305, -0.1383,  ..., -0.0642, -0.0148,  0.0808],
        ...,
        [ 0.1048, -0.0564,  0.0113,  ..., -0.1740, -0.0580, -0.1241],
        [ 0.1672,  0.0825,  0.0776,  ...,  0.1646, -0.1158,  0.1235],
        [ 0.1214, -0.1186, -0.1533,  ...,  0.1486, -0.1350,  0.1546]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.0201, -0.0978,  0.1322,  ..., -0.0926,  0.1855, -0.1690],
        [-0.2157,  0.2034, -0.0392,  ..., -0.1099,  0.0061, -0.0951],
        [-0.2439, -0.2467, -0.1851,  ..., -0.0004,  0.0005,  0.0731],
        ...,
        [ 0.0637,  0.0408,  0.1235,  ...,  0.1885, -0.2237,  0.1599],
        [ 0.0772,  0.1411,  0.0333,  ...,  0.1547,  0.2332,  0.1148],
        [ 0.0294, -0.0041,  0.1095,  ..., -0.1692,  0.0513, -0.1327]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0201, -0.0978,  0.1322,  ..., -0.0926,  0.1855, -0.1690],
        [-0.2157,  0.2034, -0.0392,  ..., -0.1099,  0.0061, -0.0951],
        [-0.2439, -0.2467, -0.1851,  ..., -0.0004,  0.0005,  0.0731],
        ...,
        [ 0.0637,  0.0408,  0.1235,  ...,  0.1885, -0.2237,  0.1599],
        [ 0.0772,  0.1411,  0.0333,  ...,  0.1547,  0.2332,  0.1148],
        [ 0.0294, -0.0041,  0.1095,  ..., -0.1692,  0.0513, -0.1327]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.2350],
        [-0.4249],
        [ 0.2298],
        [ 0.2234],
        [ 0.0298],
        [ 0.3935],
        [-0.0016],
        [-0.2723],
        [ 0.1058],
        [ 0.2019],
        [-0.2147],
        [ 0.3159],
        [-0.1919],
        [ 0.3203],
        [ 0.4023],
        [ 0.3520],
        [-0.0817],
        [ 0.4208],
        [-0.3546],
        [-0.1076],
        [-0.2065],
        [ 0.1409],
        [ 0.1886],
        [ 0.0480],
        [ 0.4019],
        [-0.0968],
        [ 0.2848],
        [ 0.1974],
        [ 0.4015],
        [ 0.1948],
        [-0.2428],
        [-0.1013]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.2350],
        [-0.4249],
        [ 0.2298],
        [ 0.2234],
        [ 0.0298],
        [ 0.3935],
        [-0.0016],
        [-0.2723],
        [ 0.1058],
        [ 0.2019],
        [-0.2147],
        [ 0.3159],
        [-0.1919],
        [ 0.3203],
        [ 0.4023],
        [ 0.3520],
        [-0.0817],
        [ 0.4208],
        [-0.3546],
        [-0.1076],
        [-0.2065],
        [ 0.1409],
        [ 0.1886],
        [ 0.0480],
        [ 0.4019],
        [-0.0968],
        [ 0.2848],
        [ 0.1974],
        [ 0.4015],
        [ 0.1948],
        [-0.2428],
        [-0.1013]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)





 shepe: torch.Size([2350554, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(1, 256), dtype=torch.float32), 'h2': Scheme(shape=(1, 128), dtype=torch.float32), 'h3': Scheme(shape=(1, 64), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1, 1), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2350554, 1, 1]) 
g.edata[efet] tensor([[[0.3332]],

        [[0.0000]],

        [[0.1955]],

        ...,

        [[0.0343]],

        [[0.0000]],

        [[0.0000]]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(216111.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-113.5060, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-4.3689, device='cuda:0')



h[100].sum tensor(2.5337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7106, device='cuda:0')



h[200].sum tensor(-10.0096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.7088, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.]],

        ...,

        [[0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.]],

        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 1, 256]) 
h.sum tensor(29315.5996, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[[0.0317, 0.0000, 0.0043,  ..., 0.0069, 0.0000, 0.0072]],

        [[0.0143, 0.0000, 0.0020,  ..., 0.0031, 0.0000, 0.0032]],

        [[0.0056, 0.0000, 0.0008,  ..., 0.0012, 0.0000, 0.0013]],

        ...,

        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],

        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],

        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 1, 128]) 
h2.sum tensor(686033.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(686033.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(606.5962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(1, 256), dtype=torch.float32), 'h2': Scheme(shape=(1, 128), dtype=torch.float32), 'h3': Scheme(shape=(1, 64), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1, 1), dtype=torch.float32)})



 output, 
h5 tensor([[[-1.0132]],

        [[-0.7200]],

        [[-0.5023]],

        ...,

        [[ 0.0000]],

        [[ 0.0000]],

        [[ 0.0000]]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1, 1]) 
h5.sum tensor(-340936.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[[0.3332]],

        [[0.0000]],

        [[0.1955]],

        ...,

        [[0.0343]],

        [[0.0000]],

        [[0.0000]]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([2350554, 1, 1]) 
g.edata[efet].sum tensor(216111.9219, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[[-1.0132]],

        [[-0.7200]],

        [[-0.5023]],

        ...,

        [[ 0.0000]],

        [[ 0.0000]],

        [[ 0.0000]]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1, 1])





 shepe: torch.Size([47011080, 1]) 






Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/./TrainingBha2ndneiefet.py", line 5, in <module>
    from ModelBha2ndneiefet import *
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/ModelBha2ndneiefet.py", line 210, in <module>
    result2 = net(batcheddglgraph, batten)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/ModelBha2ndneiefet.py", line 67, in forward
    g.edata['efet'] = F.relu(he).reshape(2350554, 1, -1) 
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/view.py", line 209, in __setitem__
    self._graph._set_e_repr(self._etid, self._edges, {key: val})
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/heterograph.py", line 4220, in _set_e_repr
    raise DGLError('Expect number of features to match number of edges.'
dgl._ffi.base.DGLError: Expect number of features to match number of edges. Got 2350554 and 47011080 instead.

real	0m41.389s
user	0m34.871s
sys	0m6.849s
