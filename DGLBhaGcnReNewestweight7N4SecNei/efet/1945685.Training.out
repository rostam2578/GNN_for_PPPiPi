0: gpu035.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-8ac19b97-e996-d56f-26ed-caea68e1fcfc)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        70:B2:A7:DF:ED:82:78:26:9F:D8:28:A0:1D:52:CD:B5:3B:DF:C3:17
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jan  9 16:53:15 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |
| N/A   43C    P0    40W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089926656

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b818582e880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m2.972s
user	0m1.714s
sys	0m0.528s




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 6507, 6507, 6507],
        [   1,    2,    3,  ..., 6219, 6794, 6795]]) 

edge_index shape
 torch.Size([2, 1175277])
graph: Graph(num_nodes=6796, num_edges=1175277,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 6507, 6507, 6507], device='cuda:0'), tensor([   1,    2,    3,  ..., 6219, 6794, 6795], device='cuda:0'))

number of nodes: 6796

number of edges: 2350554

node features (random input): tensor([[-0.0065],
        [-0.9612],
        [-0.0697],
        ...,
        [-0.1423],
        [-0.1217],
        [-1.8957]], device='cuda:0', requires_grad=True) 
node features sum: tensor(119.2086, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(2350554., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 399

In degrees of node 234: 399





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLBhaGcnReNewestweight7N4SecNei

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (linear1): Linear(in_features=2350554, out_features=256, bias=True)
  (linear2): Linear(in_features=256, out_features=2350554, bias=True)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 1205878235

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-8.1546e-02, -1.2418e-01, -6.6094e-02, -1.3605e-01, -4.8233e-02,
         -2.5701e-03, -5.8740e-02, -6.6236e-03, -9.3412e-02,  1.0637e-01,
         -1.2018e-01,  6.8030e-03, -5.2403e-02,  2.8502e-02, -6.0034e-02,
          6.8175e-02, -7.6305e-02, -1.0222e-01, -7.0157e-03,  5.0893e-02,
         -4.3409e-02, -4.1388e-02, -9.3698e-02, -1.2469e-01, -1.7334e-02,
          1.2289e-01,  3.1867e-02,  1.3849e-02, -1.3534e-01,  1.9727e-02,
          4.1829e-02,  9.9927e-02, -2.9404e-02,  1.3073e-01, -3.3865e-02,
          3.5657e-02,  6.0878e-02,  1.4280e-01,  1.1530e-01,  8.1306e-02,
         -1.1891e-01,  1.2819e-01, -8.1256e-02, -7.5409e-02, -6.2880e-02,
         -1.4575e-01, -8.2221e-02, -1.3727e-01, -1.1104e-02, -1.6699e-02,
         -1.3062e-03, -1.4281e-01,  3.8986e-02, -5.5384e-02, -1.4243e-01,
          3.7507e-02,  2.0116e-02, -1.2078e-01, -5.5218e-02,  3.7154e-02,
          7.6266e-04, -5.1943e-02,  1.3725e-01,  8.7655e-02, -8.3489e-02,
         -1.2993e-01,  2.9897e-02, -5.4752e-02, -7.1476e-02,  1.0963e-02,
         -7.3863e-02, -3.9691e-02,  1.3646e-01,  1.4805e-01, -5.2242e-02,
          1.2162e-01,  7.8671e-02, -9.4250e-02,  1.1994e-02,  1.5269e-01,
          1.8771e-02,  6.2370e-02, -5.2984e-02,  1.7434e-02,  6.2552e-02,
         -3.6485e-02, -4.3344e-02, -1.2265e-01, -1.2659e-01, -1.3342e-01,
          6.8107e-02,  6.3623e-02, -9.7375e-02, -1.0005e-01, -6.6706e-02,
         -1.6148e-02, -1.7191e-02, -5.1951e-02, -3.8731e-02, -8.3846e-02,
          1.1100e-01, -1.1181e-01,  9.7907e-02, -1.6753e-02,  7.3264e-02,
         -6.8003e-02, -1.2202e-01, -1.0269e-02, -1.2876e-01, -2.3553e-02,
          8.2556e-02,  1.0513e-01,  6.7639e-02,  1.6029e-02,  1.0436e-01,
          2.9546e-02, -3.1435e-02,  9.5102e-02,  1.3599e-02,  1.4921e-01,
          1.4147e-01,  3.4792e-02, -9.1204e-03,  6.6714e-02, -8.6716e-02,
          1.4137e-01, -8.0295e-02,  2.4527e-02, -8.4028e-02,  4.5814e-02,
         -1.2850e-01,  5.3587e-02, -7.2868e-02,  1.0915e-01,  3.7067e-02,
          1.2569e-01, -4.8834e-02,  1.2899e-01,  6.9019e-02, -5.4602e-02,
          6.4098e-02,  7.3731e-02,  5.8491e-02, -4.7669e-02, -1.7333e-02,
          3.3096e-02, -4.1848e-02,  7.3819e-02,  3.7425e-02, -3.4976e-02,
         -2.1772e-02,  2.9608e-02,  2.5545e-02,  1.1229e-01, -1.1153e-01,
          7.1240e-02,  2.9740e-02, -2.1266e-02,  1.1682e-01, -1.2716e-01,
          4.7449e-02, -9.5313e-02,  1.4153e-01, -1.2862e-01, -1.4056e-01,
         -1.4746e-02,  1.4894e-01,  1.2312e-01, -8.0928e-03,  1.4700e-01,
         -1.2943e-01, -1.4827e-01, -5.6411e-03,  6.5772e-02,  1.0797e-01,
         -6.9792e-02, -5.0870e-02,  1.6580e-02, -1.1741e-01,  3.6056e-02,
          1.6709e-02,  7.8250e-02,  1.4604e-01,  3.9040e-02,  1.1944e-01,
         -4.6301e-03,  7.3146e-02, -5.6241e-02,  8.9996e-02,  7.7241e-03,
         -1.2393e-01,  4.8905e-02, -4.0985e-02,  3.0181e-02, -1.1853e-01,
          1.3246e-04, -1.1553e-01, -3.6210e-03, -1.1778e-01,  2.9736e-02,
          9.2955e-02, -2.4527e-02,  9.6966e-02,  2.5561e-02, -9.1344e-02,
         -8.2481e-02, -6.0932e-02,  6.2940e-03,  9.1317e-02,  1.2716e-01,
         -5.0583e-02, -2.3857e-02,  6.7458e-02,  6.9541e-02, -6.0510e-03,
         -9.7478e-03,  1.0043e-01,  5.2974e-02, -1.4486e-01, -6.4604e-02,
         -3.7151e-02,  6.3826e-02,  5.1661e-02,  4.3868e-02, -7.0204e-02,
         -2.1963e-02, -7.1865e-02,  1.5061e-01, -1.2761e-01, -5.2111e-02,
         -1.0228e-01,  4.4213e-02,  1.4063e-01, -4.1641e-02, -7.1400e-02,
          1.1128e-01,  4.5459e-02, -2.8969e-02, -1.1388e-01,  7.2164e-02,
          1.2950e-01, -9.7842e-02,  1.4440e-01,  1.1366e-02, -7.1935e-02,
         -4.9241e-02,  1.4832e-01,  1.4879e-01,  8.7233e-02, -1.0872e-01,
         -1.9030e-02,  1.0435e-02, -7.0389e-02,  1.7152e-02, -7.0693e-02,
          4.3765e-02]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-8.1546e-02, -1.2418e-01, -6.6094e-02, -1.3605e-01, -4.8233e-02,
         -2.5701e-03, -5.8740e-02, -6.6236e-03, -9.3412e-02,  1.0637e-01,
         -1.2018e-01,  6.8030e-03, -5.2403e-02,  2.8502e-02, -6.0034e-02,
          6.8175e-02, -7.6305e-02, -1.0222e-01, -7.0157e-03,  5.0893e-02,
         -4.3409e-02, -4.1388e-02, -9.3698e-02, -1.2469e-01, -1.7334e-02,
          1.2289e-01,  3.1867e-02,  1.3849e-02, -1.3534e-01,  1.9727e-02,
          4.1829e-02,  9.9927e-02, -2.9404e-02,  1.3073e-01, -3.3865e-02,
          3.5657e-02,  6.0878e-02,  1.4280e-01,  1.1530e-01,  8.1306e-02,
         -1.1891e-01,  1.2819e-01, -8.1256e-02, -7.5409e-02, -6.2880e-02,
         -1.4575e-01, -8.2221e-02, -1.3727e-01, -1.1104e-02, -1.6699e-02,
         -1.3062e-03, -1.4281e-01,  3.8986e-02, -5.5384e-02, -1.4243e-01,
          3.7507e-02,  2.0116e-02, -1.2078e-01, -5.5218e-02,  3.7154e-02,
          7.6266e-04, -5.1943e-02,  1.3725e-01,  8.7655e-02, -8.3489e-02,
         -1.2993e-01,  2.9897e-02, -5.4752e-02, -7.1476e-02,  1.0963e-02,
         -7.3863e-02, -3.9691e-02,  1.3646e-01,  1.4805e-01, -5.2242e-02,
          1.2162e-01,  7.8671e-02, -9.4250e-02,  1.1994e-02,  1.5269e-01,
          1.8771e-02,  6.2370e-02, -5.2984e-02,  1.7434e-02,  6.2552e-02,
         -3.6485e-02, -4.3344e-02, -1.2265e-01, -1.2659e-01, -1.3342e-01,
          6.8107e-02,  6.3623e-02, -9.7375e-02, -1.0005e-01, -6.6706e-02,
         -1.6148e-02, -1.7191e-02, -5.1951e-02, -3.8731e-02, -8.3846e-02,
          1.1100e-01, -1.1181e-01,  9.7907e-02, -1.6753e-02,  7.3264e-02,
         -6.8003e-02, -1.2202e-01, -1.0269e-02, -1.2876e-01, -2.3553e-02,
          8.2556e-02,  1.0513e-01,  6.7639e-02,  1.6029e-02,  1.0436e-01,
          2.9546e-02, -3.1435e-02,  9.5102e-02,  1.3599e-02,  1.4921e-01,
          1.4147e-01,  3.4792e-02, -9.1204e-03,  6.6714e-02, -8.6716e-02,
          1.4137e-01, -8.0295e-02,  2.4527e-02, -8.4028e-02,  4.5814e-02,
         -1.2850e-01,  5.3587e-02, -7.2868e-02,  1.0915e-01,  3.7067e-02,
          1.2569e-01, -4.8834e-02,  1.2899e-01,  6.9019e-02, -5.4602e-02,
          6.4098e-02,  7.3731e-02,  5.8491e-02, -4.7669e-02, -1.7333e-02,
          3.3096e-02, -4.1848e-02,  7.3819e-02,  3.7425e-02, -3.4976e-02,
         -2.1772e-02,  2.9608e-02,  2.5545e-02,  1.1229e-01, -1.1153e-01,
          7.1240e-02,  2.9740e-02, -2.1266e-02,  1.1682e-01, -1.2716e-01,
          4.7449e-02, -9.5313e-02,  1.4153e-01, -1.2862e-01, -1.4056e-01,
         -1.4746e-02,  1.4894e-01,  1.2312e-01, -8.0928e-03,  1.4700e-01,
         -1.2943e-01, -1.4827e-01, -5.6411e-03,  6.5772e-02,  1.0797e-01,
         -6.9792e-02, -5.0870e-02,  1.6580e-02, -1.1741e-01,  3.6056e-02,
          1.6709e-02,  7.8250e-02,  1.4604e-01,  3.9040e-02,  1.1944e-01,
         -4.6301e-03,  7.3146e-02, -5.6241e-02,  8.9996e-02,  7.7241e-03,
         -1.2393e-01,  4.8905e-02, -4.0985e-02,  3.0181e-02, -1.1853e-01,
          1.3246e-04, -1.1553e-01, -3.6210e-03, -1.1778e-01,  2.9736e-02,
          9.2955e-02, -2.4527e-02,  9.6966e-02,  2.5561e-02, -9.1344e-02,
         -8.2481e-02, -6.0932e-02,  6.2940e-03,  9.1317e-02,  1.2716e-01,
         -5.0583e-02, -2.3857e-02,  6.7458e-02,  6.9541e-02, -6.0510e-03,
         -9.7478e-03,  1.0043e-01,  5.2974e-02, -1.4486e-01, -6.4604e-02,
         -3.7151e-02,  6.3826e-02,  5.1661e-02,  4.3868e-02, -7.0204e-02,
         -2.1963e-02, -7.1865e-02,  1.5061e-01, -1.2761e-01, -5.2111e-02,
         -1.0228e-01,  4.4213e-02,  1.4063e-01, -4.1641e-02, -7.1400e-02,
          1.1128e-01,  4.5459e-02, -2.8969e-02, -1.1388e-01,  7.2164e-02,
          1.2950e-01, -9.7842e-02,  1.4440e-01,  1.1366e-02, -7.1935e-02,
         -4.9241e-02,  1.4832e-01,  1.4879e-01,  8.7233e-02, -1.0872e-01,
         -1.9030e-02,  1.0435e-02, -7.0389e-02,  1.7152e-02, -7.0693e-02,
          4.3765e-02]], device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name linear1.weight 
shape:
 torch.Size([256, 2350554]) 
grad:
 True 
date:
 tensor([[ 1.5997e-04,  5.8218e-04, -1.9775e-04,  ...,  4.4106e-04,
          1.6418e-04,  2.8543e-04],
        [-1.8040e-04,  4.4362e-04, -5.8892e-04,  ...,  6.3641e-04,
          5.6115e-04, -7.0880e-05],
        [-1.2007e-04,  2.6924e-04,  4.1678e-04,  ...,  4.7299e-05,
         -6.1190e-04,  1.5586e-04],
        ...,
        [ 5.1407e-04, -1.1840e-04, -2.2063e-04,  ...,  2.8217e-04,
         -1.6114e-04,  2.3189e-04],
        [ 9.5357e-05,  6.0539e-04, -2.0796e-04,  ..., -3.7084e-04,
         -7.4545e-05,  6.3165e-04],
        [ 3.8650e-04, -5.5602e-04,  2.7329e-04,  ..., -5.7540e-04,
         -5.8340e-04,  3.3595e-04]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 1.5997e-04,  5.8218e-04, -1.9775e-04,  ...,  4.4106e-04,
          1.6418e-04,  2.8543e-04],
        [-1.8040e-04,  4.4362e-04, -5.8892e-04,  ...,  6.3641e-04,
          5.6115e-04, -7.0880e-05],
        [-1.2007e-04,  2.6924e-04,  4.1678e-04,  ...,  4.7299e-05,
         -6.1190e-04,  1.5586e-04],
        ...,
        [ 5.1407e-04, -1.1840e-04, -2.2063e-04,  ...,  2.8217e-04,
         -1.6114e-04,  2.3189e-04],
        [ 9.5357e-05,  6.0539e-04, -2.0796e-04,  ..., -3.7084e-04,
         -7.4545e-05,  6.3165e-04],
        [ 3.8650e-04, -5.5602e-04,  2.7329e-04,  ..., -5.7540e-04,
         -5.8340e-04,  3.3595e-04]], device='cuda:0', requires_grad=True)

name linear1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([ 6.5165e-05,  4.9432e-04, -3.2730e-04, -5.8110e-04,  2.3227e-04,
         6.3205e-04,  8.2262e-05, -3.7837e-04,  6.3796e-04,  2.2650e-04,
         1.6209e-04,  4.5566e-04,  2.3995e-04, -4.4330e-04, -5.8732e-04,
        -4.4714e-04,  5.4840e-04,  4.3037e-04, -1.2541e-04,  5.6819e-04,
        -5.5980e-05,  5.7384e-04,  2.9596e-04,  2.4040e-04,  3.5489e-04,
        -4.0104e-04, -3.0112e-05, -5.9595e-04,  2.4661e-04, -3.7794e-04,
        -4.6683e-04,  1.4552e-04,  3.6091e-05, -6.4259e-04,  3.7132e-04,
        -1.7409e-04, -4.4514e-04, -2.2598e-04, -6.4083e-04, -1.6839e-04,
         2.1027e-04,  3.7688e-04, -1.2584e-04, -2.8412e-05,  4.0227e-04,
        -2.9406e-04,  4.4223e-04, -3.7559e-04, -4.2815e-04,  3.2093e-04,
         4.7385e-04, -5.1967e-04, -1.7271e-04,  3.0878e-04,  3.8492e-04,
        -2.8102e-04,  5.4029e-04, -5.6785e-04,  8.3584e-05,  1.1281e-04,
         1.8586e-05, -2.3595e-04,  6.3328e-04,  3.8634e-04, -9.5697e-05,
        -1.9185e-04, -3.8586e-05,  4.6796e-04,  4.9635e-04,  3.6716e-04,
         1.7467e-04,  5.5275e-04,  3.1394e-04, -4.0134e-04,  5.7399e-04,
        -5.4571e-04, -3.7603e-04, -1.1188e-04, -5.9675e-04,  4.0451e-04,
         3.6447e-04,  2.1710e-04, -3.8155e-04,  2.6472e-05, -1.5210e-04,
        -4.8397e-04, -3.3723e-05, -5.7924e-04,  3.9240e-04,  4.9775e-04,
         6.2648e-04, -2.3275e-04, -4.9314e-04, -5.8509e-04, -3.9564e-04,
        -6.8016e-05, -1.1814e-04, -3.7629e-04,  3.0165e-05,  1.7286e-04,
        -2.6661e-05, -4.8284e-04, -2.5145e-04, -1.1307e-04,  5.1603e-04,
        -2.6810e-04,  4.3371e-05, -4.0270e-04, -2.3704e-04,  1.2751e-04,
         1.1011e-04,  2.4377e-04,  8.6951e-05, -1.3879e-04, -4.5195e-04,
        -4.9983e-04, -1.8643e-04,  4.5378e-04, -1.0592e-04, -2.9975e-05,
        -1.8824e-04, -5.3613e-04,  1.7474e-04,  1.2698e-04,  2.6477e-05,
         3.8768e-04, -2.7795e-04, -3.8054e-04, -3.0203e-04,  4.3194e-05,
         6.0749e-04,  3.4924e-04,  2.3291e-04,  5.2351e-04,  6.2080e-04,
         4.6807e-05,  2.3516e-04, -6.5136e-04,  3.4402e-04,  1.8825e-04,
        -2.0942e-04, -6.2683e-04, -4.4279e-04, -1.0565e-04,  6.0974e-04,
        -4.4699e-04, -1.3235e-04, -1.3592e-04, -4.2527e-05,  1.4109e-04,
        -3.2402e-04,  6.1463e-04, -1.7397e-04, -2.6702e-04, -4.1266e-04,
         2.1551e-04, -4.1614e-04,  2.2385e-04,  9.6902e-05, -1.8409e-04,
        -2.5478e-04,  5.3472e-04, -1.0743e-04, -2.9644e-04, -4.8108e-04,
        -1.6808e-04, -6.2769e-04, -5.8022e-04,  4.3134e-04,  1.9964e-04,
        -3.8008e-04,  8.0605e-05,  2.7186e-04,  3.0138e-04,  1.5876e-04,
         2.9549e-04,  4.6804e-04,  7.6329e-05, -1.1293e-04,  5.5526e-04,
        -3.4260e-04,  2.9750e-04, -1.6256e-04, -6.1158e-04,  5.9997e-05,
         6.9590e-05,  1.8389e-04, -5.5555e-04,  6.8189e-05, -4.1034e-04,
         1.4620e-04, -2.4245e-05, -9.9271e-05, -4.8102e-04, -5.6314e-04,
        -2.9482e-04, -4.6454e-04,  1.5983e-04, -3.3553e-04,  1.8280e-04,
         1.7721e-04, -2.6501e-04, -3.6999e-04, -2.9064e-04, -3.2507e-04,
         2.5894e-04,  2.0367e-05,  5.4248e-04, -6.2565e-04,  1.9436e-04,
         1.4304e-04, -2.2146e-04,  2.6271e-04, -5.2348e-05,  1.7816e-04,
        -9.5026e-05, -5.8695e-04, -4.9527e-04,  2.1358e-04, -1.4795e-04,
         1.2994e-05, -3.0033e-04,  6.0338e-04,  5.8032e-04,  3.4608e-04,
        -1.9684e-04,  3.5961e-04,  6.0143e-04, -1.1234e-04,  3.0206e-04,
         3.7786e-04,  6.1233e-04,  6.0186e-04,  2.7851e-04, -5.5919e-04,
         1.3522e-04, -3.9803e-04,  3.9740e-04, -6.2061e-04, -3.7637e-04,
        -3.8525e-04,  4.4808e-04, -5.6519e-04, -1.3579e-04, -5.8210e-04,
        -2.5786e-04, -4.0540e-04, -3.1422e-04,  1.1910e-04,  1.7289e-04,
        -3.7076e-04, -6.1051e-04, -1.1810e-04,  3.4784e-04, -5.7090e-04,
         4.2337e-04], device='cuda:0') 
parameter:
 Parameter containing:
tensor([ 6.5165e-05,  4.9432e-04, -3.2730e-04, -5.8110e-04,  2.3227e-04,
         6.3205e-04,  8.2262e-05, -3.7837e-04,  6.3796e-04,  2.2650e-04,
         1.6209e-04,  4.5566e-04,  2.3995e-04, -4.4330e-04, -5.8732e-04,
        -4.4714e-04,  5.4840e-04,  4.3037e-04, -1.2541e-04,  5.6819e-04,
        -5.5980e-05,  5.7384e-04,  2.9596e-04,  2.4040e-04,  3.5489e-04,
        -4.0104e-04, -3.0112e-05, -5.9595e-04,  2.4661e-04, -3.7794e-04,
        -4.6683e-04,  1.4552e-04,  3.6091e-05, -6.4259e-04,  3.7132e-04,
        -1.7409e-04, -4.4514e-04, -2.2598e-04, -6.4083e-04, -1.6839e-04,
         2.1027e-04,  3.7688e-04, -1.2584e-04, -2.8412e-05,  4.0227e-04,
        -2.9406e-04,  4.4223e-04, -3.7559e-04, -4.2815e-04,  3.2093e-04,
         4.7385e-04, -5.1967e-04, -1.7271e-04,  3.0878e-04,  3.8492e-04,
        -2.8102e-04,  5.4029e-04, -5.6785e-04,  8.3584e-05,  1.1281e-04,
         1.8586e-05, -2.3595e-04,  6.3328e-04,  3.8634e-04, -9.5697e-05,
        -1.9185e-04, -3.8586e-05,  4.6796e-04,  4.9635e-04,  3.6716e-04,
         1.7467e-04,  5.5275e-04,  3.1394e-04, -4.0134e-04,  5.7399e-04,
        -5.4571e-04, -3.7603e-04, -1.1188e-04, -5.9675e-04,  4.0451e-04,
         3.6447e-04,  2.1710e-04, -3.8155e-04,  2.6472e-05, -1.5210e-04,
        -4.8397e-04, -3.3723e-05, -5.7924e-04,  3.9240e-04,  4.9775e-04,
         6.2648e-04, -2.3275e-04, -4.9314e-04, -5.8509e-04, -3.9564e-04,
        -6.8016e-05, -1.1814e-04, -3.7629e-04,  3.0165e-05,  1.7286e-04,
        -2.6661e-05, -4.8284e-04, -2.5145e-04, -1.1307e-04,  5.1603e-04,
        -2.6810e-04,  4.3371e-05, -4.0270e-04, -2.3704e-04,  1.2751e-04,
         1.1011e-04,  2.4377e-04,  8.6951e-05, -1.3879e-04, -4.5195e-04,
        -4.9983e-04, -1.8643e-04,  4.5378e-04, -1.0592e-04, -2.9975e-05,
        -1.8824e-04, -5.3613e-04,  1.7474e-04,  1.2698e-04,  2.6477e-05,
         3.8768e-04, -2.7795e-04, -3.8054e-04, -3.0203e-04,  4.3194e-05,
         6.0749e-04,  3.4924e-04,  2.3291e-04,  5.2351e-04,  6.2080e-04,
         4.6807e-05,  2.3516e-04, -6.5136e-04,  3.4402e-04,  1.8825e-04,
        -2.0942e-04, -6.2683e-04, -4.4279e-04, -1.0565e-04,  6.0974e-04,
        -4.4699e-04, -1.3235e-04, -1.3592e-04, -4.2527e-05,  1.4109e-04,
        -3.2402e-04,  6.1463e-04, -1.7397e-04, -2.6702e-04, -4.1266e-04,
         2.1551e-04, -4.1614e-04,  2.2385e-04,  9.6902e-05, -1.8409e-04,
        -2.5478e-04,  5.3472e-04, -1.0743e-04, -2.9644e-04, -4.8108e-04,
        -1.6808e-04, -6.2769e-04, -5.8022e-04,  4.3134e-04,  1.9964e-04,
        -3.8008e-04,  8.0605e-05,  2.7186e-04,  3.0138e-04,  1.5876e-04,
         2.9549e-04,  4.6804e-04,  7.6329e-05, -1.1293e-04,  5.5526e-04,
        -3.4260e-04,  2.9750e-04, -1.6256e-04, -6.1158e-04,  5.9997e-05,
         6.9590e-05,  1.8389e-04, -5.5555e-04,  6.8189e-05, -4.1034e-04,
         1.4620e-04, -2.4245e-05, -9.9271e-05, -4.8102e-04, -5.6314e-04,
        -2.9482e-04, -4.6454e-04,  1.5983e-04, -3.3553e-04,  1.8280e-04,
         1.7721e-04, -2.6501e-04, -3.6999e-04, -2.9064e-04, -3.2507e-04,
         2.5894e-04,  2.0367e-05,  5.4248e-04, -6.2565e-04,  1.9436e-04,
         1.4304e-04, -2.2146e-04,  2.6271e-04, -5.2348e-05,  1.7816e-04,
        -9.5026e-05, -5.8695e-04, -4.9527e-04,  2.1358e-04, -1.4795e-04,
         1.2994e-05, -3.0033e-04,  6.0338e-04,  5.8032e-04,  3.4608e-04,
        -1.9684e-04,  3.5961e-04,  6.0143e-04, -1.1234e-04,  3.0206e-04,
         3.7786e-04,  6.1233e-04,  6.0186e-04,  2.7851e-04, -5.5919e-04,
         1.3522e-04, -3.9803e-04,  3.9740e-04, -6.2061e-04, -3.7637e-04,
        -3.8525e-04,  4.4808e-04, -5.6519e-04, -1.3579e-04, -5.8210e-04,
        -2.5786e-04, -4.0540e-04, -3.1422e-04,  1.1910e-04,  1.7289e-04,
        -3.7076e-04, -6.1051e-04, -1.1810e-04,  3.4784e-04, -5.7090e-04,
         4.2337e-04], device='cuda:0', requires_grad=True)

name linear2.weight 
shape:
 torch.Size([2350554, 256]) 
grad:
 True 
date:
 tensor([[-0.0606,  0.0108, -0.0581,  ...,  0.0169,  0.0276, -0.0412],
        [ 0.0264, -0.0376, -0.0001,  ..., -0.0172, -0.0601, -0.0280],
        [-0.0537, -0.0430, -0.0599,  ..., -0.0418, -0.0212,  0.0323],
        ...,
        [ 0.0501, -0.0357,  0.0552,  ...,  0.0163, -0.0320, -0.0442],
        [ 0.0518,  0.0352,  0.0321,  ...,  0.0138,  0.0401, -0.0163],
        [ 0.0548,  0.0058, -0.0172,  ...,  0.0201,  0.0406,  0.0251]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0606,  0.0108, -0.0581,  ...,  0.0169,  0.0276, -0.0412],
        [ 0.0264, -0.0376, -0.0001,  ..., -0.0172, -0.0601, -0.0280],
        [-0.0537, -0.0430, -0.0599,  ..., -0.0418, -0.0212,  0.0323],
        ...,
        [ 0.0501, -0.0357,  0.0552,  ...,  0.0163, -0.0320, -0.0442],
        [ 0.0518,  0.0352,  0.0321,  ...,  0.0138,  0.0401, -0.0163],
        [ 0.0548,  0.0058, -0.0172,  ...,  0.0201,  0.0406,  0.0251]],
       device='cuda:0', requires_grad=True)

name linear2.bias 
shape:
 torch.Size([2350554]) 
grad:
 True 
date:
 tensor([-0.0613, -0.0076,  0.0620,  ..., -0.0215,  0.0569,  0.0314],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([-0.0613, -0.0076,  0.0620,  ..., -0.0215,  0.0569,  0.0314],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0704,  0.0084, -0.0032,  ..., -0.0291, -0.0882, -0.0937],
        [ 0.0516,  0.0601,  0.0230,  ...,  0.0498,  0.0320, -0.0206],
        [ 0.0557, -0.1087, -0.0566,  ...,  0.0930,  0.0352, -0.0210],
        ...,
        [-0.0174, -0.0327, -0.0487,  ..., -0.0677, -0.1209, -0.1175],
        [ 0.0824,  0.0226,  0.0566,  ..., -0.0751,  0.0647,  0.0149],
        [ 0.0432,  0.0735,  0.0036,  ..., -0.1036, -0.0034,  0.0920]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0704,  0.0084, -0.0032,  ..., -0.0291, -0.0882, -0.0937],
        [ 0.0516,  0.0601,  0.0230,  ...,  0.0498,  0.0320, -0.0206],
        [ 0.0557, -0.1087, -0.0566,  ...,  0.0930,  0.0352, -0.0210],
        ...,
        [-0.0174, -0.0327, -0.0487,  ..., -0.0677, -0.1209, -0.1175],
        [ 0.0824,  0.0226,  0.0566,  ..., -0.0751,  0.0647,  0.0149],
        [ 0.0432,  0.0735,  0.0036,  ..., -0.1036, -0.0034,  0.0920]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.1045, -0.0889,  0.0095,  ..., -0.0304, -0.0005,  0.1495],
        [ 0.0428,  0.1092,  0.0624,  ...,  0.1711,  0.1109,  0.1245],
        [ 0.0399, -0.1312,  0.0266,  ...,  0.0878,  0.1403, -0.0063],
        ...,
        [-0.0049,  0.0451,  0.0779,  ...,  0.0292,  0.0682,  0.0610],
        [-0.1141,  0.1577,  0.0678,  ...,  0.1596, -0.1013,  0.1565],
        [ 0.0155,  0.0746,  0.0477,  ..., -0.0663, -0.0610, -0.0804]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1045, -0.0889,  0.0095,  ..., -0.0304, -0.0005,  0.1495],
        [ 0.0428,  0.1092,  0.0624,  ...,  0.1711,  0.1109,  0.1245],
        [ 0.0399, -0.1312,  0.0266,  ...,  0.0878,  0.1403, -0.0063],
        ...,
        [-0.0049,  0.0451,  0.0779,  ...,  0.0292,  0.0682,  0.0610],
        [-0.1141,  0.1577,  0.0678,  ...,  0.1596, -0.1013,  0.1565],
        [ 0.0155,  0.0746,  0.0477,  ..., -0.0663, -0.0610, -0.0804]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.1650,  0.0016,  0.0187,  ..., -0.2236, -0.2117, -0.0743],
        [-0.0437, -0.0108,  0.0591,  ...,  0.0116,  0.1145,  0.2438],
        [-0.0605, -0.0175,  0.0737,  ..., -0.1278,  0.0501,  0.2499],
        ...,
        [ 0.1695,  0.1084, -0.2366,  ...,  0.0490,  0.1372, -0.1793],
        [-0.0818, -0.0318, -0.1667,  ...,  0.1084,  0.1604, -0.1096],
        [-0.0028, -0.1916, -0.2241,  ..., -0.0879,  0.0982,  0.2025]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1650,  0.0016,  0.0187,  ..., -0.2236, -0.2117, -0.0743],
        [-0.0437, -0.0108,  0.0591,  ...,  0.0116,  0.1145,  0.2438],
        [-0.0605, -0.0175,  0.0737,  ..., -0.1278,  0.0501,  0.2499],
        ...,
        [ 0.1695,  0.1084, -0.2366,  ...,  0.0490,  0.1372, -0.1793],
        [-0.0818, -0.0318, -0.1667,  ...,  0.1084,  0.1604, -0.1096],
        [-0.0028, -0.1916, -0.2241,  ..., -0.0879,  0.0982,  0.2025]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.1052],
        [ 0.0744],
        [ 0.0398],
        [ 0.2427],
        [ 0.2349],
        [-0.2175],
        [ 0.4242],
        [-0.3306],
        [ 0.3405],
        [-0.1292],
        [-0.0314],
        [ 0.3941],
        [ 0.3353],
        [-0.0518],
        [ 0.3916],
        [-0.2405],
        [-0.0198],
        [-0.3559],
        [-0.0957],
        [ 0.3615],
        [ 0.2896],
        [-0.2462],
        [ 0.1048],
        [ 0.1915],
        [-0.1233],
        [ 0.1146],
        [ 0.1938],
        [ 0.0911],
        [-0.2603],
        [-0.1992],
        [ 0.2405],
        [ 0.3983]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1052],
        [ 0.0744],
        [ 0.0398],
        [ 0.2427],
        [ 0.2349],
        [-0.2175],
        [ 0.4242],
        [-0.3306],
        [ 0.3405],
        [-0.1292],
        [-0.0314],
        [ 0.3941],
        [ 0.3353],
        [-0.0518],
        [ 0.3916],
        [-0.2405],
        [-0.0198],
        [-0.3559],
        [-0.0957],
        [ 0.3615],
        [ 0.2896],
        [-0.2462],
        [ 0.1048],
        [ 0.1915],
        [-0.1233],
        [ 0.1146],
        [ 0.1938],
        [ 0.0911],
        [-0.2603],
        [-0.1992],
        [ 0.2405],
        [ 0.3983]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)





 shepe: torch.Size([2350554, 1]) 









input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].sum tensor(226357.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-69.6443, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.9883, device='cuda:0')



h[100].sum tensor(9.9085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6006, device='cuda:0')



h[200].sum tensor(-8.6962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-9.3037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(29782.4746, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0105, 0.0000, 0.0000,  ..., 0.0042, 0.0362, 0.0238],
        [0.0014, 0.0000, 0.0000,  ..., 0.0006, 0.0049, 0.0032],
        [0.0032, 0.0000, 0.0000,  ..., 0.0013, 0.0111, 0.0073],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(704743., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10556.6729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(159.0784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2032.2322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(30.6237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.3464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=2350554,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4223],
        [-1.6688],
        [-1.3109],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-846177.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReshapeAliasBackward0>) 
g.edata[efet].shape torch.Size([2350554, 1]) 
g.edata[efet].sum tensor(226357.9375, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-2.4223],
        [-1.6688],
        [-1.3109],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])





 shepe: torch.Size([47011080, 1]) 






Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/./TrainingBha2ndneiefet.py", line 5, in <module>
    from ModelBha2ndneiefet import *
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/ModelBha2ndneiefet.py", line 210, in <module>
    result2 = net(batcheddglgraph, batten)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4SecNei/ModelBha2ndneiefet.py", line 64, in forward
    he = (g.edata['efet']).reshape(1, 2350554)
RuntimeError: shape '[1, 2350554]' is invalid for input of size 47011080

real	0m42.483s
user	0m34.919s
sys	0m6.879s
