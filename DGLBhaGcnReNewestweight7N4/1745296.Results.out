0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 12:13:32 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   21C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2abedc8168e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.746s
user	0m2.529s
sys	0m1.150s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[12:13:56] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.6223],
        [-1.3683],
        [-1.2499],
        ...,
        [-1.0049],
        [ 0.2469],
        [ 0.5552]], device='cuda:0', requires_grad=True) 
node features sum: tensor(28.2234, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[ 0.0450, -0.0430,  0.1383, -0.0433,  0.0074,  0.0153,  0.0216, -0.1357,
         -0.0387,  0.0773, -0.0550, -0.1382,  0.0547, -0.0343, -0.0883,  0.1163,
          0.0155,  0.0023,  0.0235,  0.0633, -0.0785, -0.1077, -0.1240,  0.0693,
          0.0631, -0.0699,  0.0931, -0.0574,  0.0142,  0.0155,  0.1314,  0.0970,
         -0.0607,  0.0387, -0.1451,  0.0156,  0.1057, -0.1311, -0.0497,  0.0159,
          0.1414,  0.0218,  0.1127, -0.0662, -0.0333, -0.0083, -0.1379,  0.0733,
          0.0505,  0.0402, -0.0240, -0.0836,  0.1140,  0.0876, -0.0108, -0.1202,
          0.1359,  0.0336,  0.0270, -0.1287,  0.1085,  0.1124,  0.1246, -0.0323,
          0.0588, -0.0685, -0.0209,  0.0176,  0.1192, -0.0530, -0.0021,  0.0767,
         -0.0276, -0.1246,  0.0462,  0.1078,  0.1455, -0.0416,  0.0368,  0.1367,
          0.1298,  0.1202, -0.1158, -0.0473,  0.0904,  0.0593, -0.0257,  0.1289,
          0.0218,  0.0409,  0.0501,  0.0414, -0.0600, -0.1482, -0.0243, -0.0915,
          0.0380, -0.0139, -0.0952,  0.1156,  0.0039,  0.1312,  0.0898,  0.0373,
          0.0861, -0.1396, -0.0132, -0.0710,  0.1440,  0.0302,  0.0822, -0.0305,
         -0.0894,  0.1181, -0.0458, -0.0484,  0.0596,  0.1185, -0.0135, -0.0347,
          0.1523,  0.1161, -0.0694,  0.0969, -0.0585,  0.1490,  0.1198,  0.1437,
         -0.0503,  0.0053,  0.1231, -0.0328, -0.0512,  0.0602, -0.0297, -0.0486,
         -0.0873, -0.0700,  0.1189,  0.0056, -0.0065, -0.1262, -0.0623,  0.1266,
          0.0975, -0.0738, -0.0621, -0.1351, -0.0421, -0.1209, -0.0472, -0.0177,
         -0.0420,  0.0404,  0.0343,  0.0965, -0.1141, -0.1253,  0.1065,  0.0729,
          0.0934,  0.0497,  0.1228,  0.1270, -0.0934, -0.0521,  0.1236,  0.1273,
         -0.0805,  0.0152,  0.0316,  0.0710,  0.1263, -0.1291, -0.0222, -0.1361,
          0.0700, -0.0042,  0.0216, -0.1516,  0.1415,  0.1501,  0.1275,  0.0883,
         -0.0540,  0.1360, -0.0555, -0.0190, -0.1169,  0.0433,  0.0667, -0.1436,
         -0.0023,  0.1095, -0.0213, -0.1427, -0.1050, -0.0853, -0.0399,  0.1147,
         -0.0093,  0.0448,  0.0949,  0.0743, -0.1029, -0.0063,  0.0853,  0.0089,
         -0.1107, -0.1296,  0.0521, -0.0289, -0.1011, -0.1384, -0.1300, -0.0088,
          0.0528,  0.0923,  0.0479,  0.0194,  0.0584,  0.1345,  0.0181, -0.1092,
          0.0772, -0.0523,  0.1020,  0.0189,  0.0396, -0.1111, -0.0577,  0.0858,
          0.0412,  0.0046, -0.0389,  0.1146,  0.0538, -0.1146,  0.0528,  0.0763,
          0.1414,  0.1328,  0.0608,  0.0274,  0.0245,  0.0652,  0.1067, -0.0911,
          0.1071, -0.0984,  0.1384, -0.0811,  0.1064, -0.1029, -0.0183,  0.0577]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0450, -0.0430,  0.1383, -0.0433,  0.0074,  0.0153,  0.0216, -0.1357,
         -0.0387,  0.0773, -0.0550, -0.1382,  0.0547, -0.0343, -0.0883,  0.1163,
          0.0155,  0.0023,  0.0235,  0.0633, -0.0785, -0.1077, -0.1240,  0.0693,
          0.0631, -0.0699,  0.0931, -0.0574,  0.0142,  0.0155,  0.1314,  0.0970,
         -0.0607,  0.0387, -0.1451,  0.0156,  0.1057, -0.1311, -0.0497,  0.0159,
          0.1414,  0.0218,  0.1127, -0.0662, -0.0333, -0.0083, -0.1379,  0.0733,
          0.0505,  0.0402, -0.0240, -0.0836,  0.1140,  0.0876, -0.0108, -0.1202,
          0.1359,  0.0336,  0.0270, -0.1287,  0.1085,  0.1124,  0.1246, -0.0323,
          0.0588, -0.0685, -0.0209,  0.0176,  0.1192, -0.0530, -0.0021,  0.0767,
         -0.0276, -0.1246,  0.0462,  0.1078,  0.1455, -0.0416,  0.0368,  0.1367,
          0.1298,  0.1202, -0.1158, -0.0473,  0.0904,  0.0593, -0.0257,  0.1289,
          0.0218,  0.0409,  0.0501,  0.0414, -0.0600, -0.1482, -0.0243, -0.0915,
          0.0380, -0.0139, -0.0952,  0.1156,  0.0039,  0.1312,  0.0898,  0.0373,
          0.0861, -0.1396, -0.0132, -0.0710,  0.1440,  0.0302,  0.0822, -0.0305,
         -0.0894,  0.1181, -0.0458, -0.0484,  0.0596,  0.1185, -0.0135, -0.0347,
          0.1523,  0.1161, -0.0694,  0.0969, -0.0585,  0.1490,  0.1198,  0.1437,
         -0.0503,  0.0053,  0.1231, -0.0328, -0.0512,  0.0602, -0.0297, -0.0486,
         -0.0873, -0.0700,  0.1189,  0.0056, -0.0065, -0.1262, -0.0623,  0.1266,
          0.0975, -0.0738, -0.0621, -0.1351, -0.0421, -0.1209, -0.0472, -0.0177,
         -0.0420,  0.0404,  0.0343,  0.0965, -0.1141, -0.1253,  0.1065,  0.0729,
          0.0934,  0.0497,  0.1228,  0.1270, -0.0934, -0.0521,  0.1236,  0.1273,
         -0.0805,  0.0152,  0.0316,  0.0710,  0.1263, -0.1291, -0.0222, -0.1361,
          0.0700, -0.0042,  0.0216, -0.1516,  0.1415,  0.1501,  0.1275,  0.0883,
         -0.0540,  0.1360, -0.0555, -0.0190, -0.1169,  0.0433,  0.0667, -0.1436,
         -0.0023,  0.1095, -0.0213, -0.1427, -0.1050, -0.0853, -0.0399,  0.1147,
         -0.0093,  0.0448,  0.0949,  0.0743, -0.1029, -0.0063,  0.0853,  0.0089,
         -0.1107, -0.1296,  0.0521, -0.0289, -0.1011, -0.1384, -0.1300, -0.0088,
          0.0528,  0.0923,  0.0479,  0.0194,  0.0584,  0.1345,  0.0181, -0.1092,
          0.0772, -0.0523,  0.1020,  0.0189,  0.0396, -0.1111, -0.0577,  0.0858,
          0.0412,  0.0046, -0.0389,  0.1146,  0.0538, -0.1146,  0.0528,  0.0763,
          0.1414,  0.1328,  0.0608,  0.0274,  0.0245,  0.0652,  0.1067, -0.0911,
          0.1071, -0.0984,  0.1384, -0.0811,  0.1064, -0.1029, -0.0183,  0.0577]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[ 0.0959,  0.0173, -0.0903,  ...,  0.0676,  0.0482, -0.0909],
        [-0.0677, -0.0269, -0.0567,  ...,  0.0870,  0.0272, -0.0197],
        [ 0.0978,  0.0413, -0.0497,  ...,  0.1122, -0.0961, -0.0681],
        ...,
        [ 0.1202, -0.1064,  0.0132,  ...,  0.0096,  0.0790, -0.0553],
        [ 0.0985, -0.0859, -0.0664,  ..., -0.0084,  0.0182, -0.1150],
        [ 0.0207, -0.0379, -0.0495,  ..., -0.1208, -0.0649, -0.0866]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0959,  0.0173, -0.0903,  ...,  0.0676,  0.0482, -0.0909],
        [-0.0677, -0.0269, -0.0567,  ...,  0.0870,  0.0272, -0.0197],
        [ 0.0978,  0.0413, -0.0497,  ...,  0.1122, -0.0961, -0.0681],
        ...,
        [ 0.1202, -0.1064,  0.0132,  ...,  0.0096,  0.0790, -0.0553],
        [ 0.0985, -0.0859, -0.0664,  ..., -0.0084,  0.0182, -0.1150],
        [ 0.0207, -0.0379, -0.0495,  ..., -0.1208, -0.0649, -0.0866]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0827, -0.0401, -0.0982,  ..., -0.1225, -0.1216,  0.0791],
        [-0.1097, -0.0496,  0.1675,  ...,  0.1335, -0.0788,  0.1674],
        [-0.0221,  0.0136, -0.0695,  ...,  0.0548, -0.1454, -0.1653],
        ...,
        [-0.1610, -0.1700,  0.1528,  ...,  0.1763,  0.0153, -0.1532],
        [ 0.0591,  0.0983,  0.0811,  ...,  0.1449, -0.1702,  0.0778],
        [ 0.0679,  0.0611, -0.1466,  ...,  0.1303,  0.0248, -0.1241]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0827, -0.0401, -0.0982,  ..., -0.1225, -0.1216,  0.0791],
        [-0.1097, -0.0496,  0.1675,  ...,  0.1335, -0.0788,  0.1674],
        [-0.0221,  0.0136, -0.0695,  ...,  0.0548, -0.1454, -0.1653],
        ...,
        [-0.1610, -0.1700,  0.1528,  ...,  0.1763,  0.0153, -0.1532],
        [ 0.0591,  0.0983,  0.0811,  ...,  0.1449, -0.1702,  0.0778],
        [ 0.0679,  0.0611, -0.1466,  ...,  0.1303,  0.0248, -0.1241]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.1367,  0.0599,  0.0547,  ...,  0.1171,  0.2331,  0.0700],
        [ 0.0107, -0.1869, -0.2249,  ...,  0.0814, -0.0764, -0.2084],
        [ 0.2480,  0.0980, -0.1068,  ..., -0.2192,  0.1421,  0.0363],
        ...,
        [ 0.2072,  0.0670,  0.0274,  ...,  0.0079,  0.0135,  0.1160],
        [-0.2165,  0.0220,  0.1713,  ..., -0.1322, -0.2086, -0.0857],
        [-0.0854,  0.0733, -0.2163,  ...,  0.0200, -0.1228, -0.0287]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1367,  0.0599,  0.0547,  ...,  0.1171,  0.2331,  0.0700],
        [ 0.0107, -0.1869, -0.2249,  ...,  0.0814, -0.0764, -0.2084],
        [ 0.2480,  0.0980, -0.1068,  ..., -0.2192,  0.1421,  0.0363],
        ...,
        [ 0.2072,  0.0670,  0.0274,  ...,  0.0079,  0.0135,  0.1160],
        [-0.2165,  0.0220,  0.1713,  ..., -0.1322, -0.2086, -0.0857],
        [-0.0854,  0.0733, -0.2163,  ...,  0.0200, -0.1228, -0.0287]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.3254],
        [ 0.4163],
        [ 0.0784],
        [ 0.1159],
        [ 0.1832],
        [ 0.0906],
        [-0.4217],
        [-0.0320],
        [ 0.2071],
        [ 0.0295],
        [-0.0957],
        [ 0.2267],
        [ 0.0750],
        [-0.3388],
        [ 0.1945],
        [-0.0694],
        [ 0.2419],
        [-0.3131],
        [ 0.0859],
        [-0.3053],
        [ 0.1564],
        [-0.3411],
        [-0.0993],
        [ 0.4000],
        [-0.2462],
        [-0.1908],
        [ 0.4151],
        [-0.0449],
        [-0.0780],
        [-0.1035],
        [-0.0365],
        [-0.3233]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.3254],
        [ 0.4163],
        [ 0.0784],
        [ 0.1159],
        [ 0.1832],
        [ 0.0906],
        [-0.4217],
        [-0.0320],
        [ 0.2071],
        [ 0.0295],
        [-0.0957],
        [ 0.2267],
        [ 0.0750],
        [-0.3388],
        [ 0.1945],
        [-0.0694],
        [ 0.2419],
        [-0.3131],
        [ 0.0859],
        [-0.3053],
        [ 0.1564],
        [-0.3411],
        [-0.0993],
        [ 0.4000],
        [-0.2462],
        [-0.1908],
        [ 0.4151],
        [-0.0449],
        [-0.0780],
        [-0.1035],
        [-0.0365],
        [-0.3233]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(261.7118, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(5.3783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(5.4986, device='cuda:0')



h[100].sum tensor(0.7203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0.7364, device='cuda:0')



h[200].sum tensor(-1.6968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(11710.3105, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.0032, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(64886.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.2358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(548.5356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(34.8631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(998.8553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(63.5369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2177],
        [-0.1534],
        [-0.1032],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-10073.2520, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.2177],
        [-0.1534],
        [-0.1032],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0124,  0.0005, -0.0011,  ...,  0.0162, -0.0151, -0.0172],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-1832.3857, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-81.7644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-83.8171, device='cuda:0')



h[100].sum tensor(96.8471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.2783, device='cuda:0')



h[200].sum tensor(-91.5183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-93.8158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0611, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0503, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(93616.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0907, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0399],
        [0.0778, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0342],
        [0.0624, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0275],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(468518.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4490.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.6977, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-403.1546, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4238.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(296.9071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[6.9514e-02],
        [7.5201e-02],
        [8.2963e-02],
        ...,
        [9.0483e-07],
        [1.5064e-06],
        [2.1553e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(4824.5767, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.2177],
        [-0.1534],
        [-0.1032],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 39, in <module>
    result1 = net(batcheddglgraph.to('cuda'), TraTen[10000:10010].reshape(10 * 6796, 1).to('cuda'))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 47, in forward
    h = self.conv1(g, in_fet)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 437, in forward
    rst = th.matmul(rst, weight)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)

real	0m25.839s
user	0m18.418s
sys	0m4.868s
