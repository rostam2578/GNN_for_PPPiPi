0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 14:26:16 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b07e63658e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.028s
user	0m2.631s
sys	0m1.164s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[14:26:39] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.5297],
        [ 2.2957],
        [ 1.5911],
        ...,
        [ 0.2588],
        [ 0.0826],
        [ 0.4463]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-8.7869, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.1517, -0.0660,  0.1516,  0.1003,  0.0830, -0.0988,  0.0325, -0.1494,
         -0.1009,  0.1340,  0.0920,  0.0149, -0.0566, -0.1264,  0.1070,  0.0286,
         -0.0812, -0.0805,  0.0981, -0.0102, -0.0073,  0.0786,  0.0841, -0.0239,
         -0.0247, -0.0369, -0.0143,  0.0231,  0.0398,  0.1345,  0.0947, -0.0310,
          0.0515, -0.0388,  0.1270,  0.0448, -0.0929,  0.1257, -0.0460, -0.0411,
         -0.0367, -0.1235, -0.0209,  0.1490,  0.0177, -0.0049,  0.1101,  0.1233,
          0.0009, -0.1519, -0.1322,  0.1120,  0.0683, -0.0031, -0.0289,  0.1035,
          0.1291, -0.0138,  0.0273,  0.1314, -0.0869, -0.0795,  0.0070, -0.1504,
         -0.0051,  0.1080,  0.1293,  0.1408, -0.0695, -0.1138, -0.0492, -0.0801,
          0.1190, -0.0100, -0.0672, -0.1468, -0.0129,  0.0077,  0.0133,  0.1156,
          0.0963,  0.0589,  0.1382, -0.1221, -0.0610, -0.0382, -0.0541, -0.1378,
         -0.1078,  0.0808, -0.1159, -0.1463,  0.1049, -0.1497,  0.1355, -0.0407,
          0.1511,  0.1099,  0.0121, -0.0443, -0.1151,  0.0864, -0.1103, -0.1350,
         -0.0783,  0.0988, -0.0105,  0.1379,  0.1234, -0.0711,  0.1216, -0.0751,
         -0.0352, -0.0548,  0.1201,  0.0407,  0.0077, -0.1190, -0.0166, -0.0413,
          0.0234, -0.0961, -0.0354,  0.1020,  0.1413, -0.0366, -0.1346, -0.1293,
         -0.1189, -0.1062,  0.0273, -0.0763,  0.0895,  0.1299, -0.0473, -0.0700,
          0.0882,  0.0827, -0.1491, -0.0458, -0.1308, -0.0339, -0.1173, -0.1268,
         -0.0115, -0.1028, -0.1288,  0.0036, -0.1039, -0.0176,  0.1191,  0.0646,
         -0.1190,  0.0799,  0.0746, -0.0102,  0.0455,  0.1353,  0.1045,  0.0746,
         -0.0959, -0.0612,  0.0652,  0.0277, -0.0031, -0.1185, -0.1118, -0.0420,
          0.1026, -0.0339, -0.0420, -0.1133,  0.0591,  0.0134, -0.0272, -0.0149,
         -0.1335, -0.1342,  0.0347,  0.0489,  0.0741,  0.0696,  0.0953,  0.1341,
         -0.0828, -0.0972, -0.1055, -0.1011, -0.1040,  0.0672,  0.1024, -0.0315,
         -0.1145, -0.0166, -0.0898,  0.0150, -0.0673,  0.0430,  0.1293,  0.1304,
         -0.1414,  0.1225, -0.0003,  0.0060, -0.0912, -0.0312, -0.0336,  0.0196,
          0.1006,  0.0817, -0.0692,  0.0522,  0.0779,  0.0609, -0.1040,  0.1422,
         -0.0017, -0.0531,  0.0225, -0.1484, -0.1111,  0.1436,  0.0371,  0.1190,
         -0.0816,  0.0889,  0.0051, -0.0120,  0.1103,  0.0988,  0.0563,  0.0944,
          0.0530,  0.0750,  0.0566,  0.1461,  0.0299, -0.0179, -0.0326, -0.0040,
          0.0761,  0.1416, -0.1347, -0.0213,  0.0845,  0.0502,  0.0220,  0.1142,
         -0.1335,  0.0219,  0.0098,  0.1300, -0.0365,  0.0638, -0.1218, -0.0766]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1517, -0.0660,  0.1516,  0.1003,  0.0830, -0.0988,  0.0325, -0.1494,
         -0.1009,  0.1340,  0.0920,  0.0149, -0.0566, -0.1264,  0.1070,  0.0286,
         -0.0812, -0.0805,  0.0981, -0.0102, -0.0073,  0.0786,  0.0841, -0.0239,
         -0.0247, -0.0369, -0.0143,  0.0231,  0.0398,  0.1345,  0.0947, -0.0310,
          0.0515, -0.0388,  0.1270,  0.0448, -0.0929,  0.1257, -0.0460, -0.0411,
         -0.0367, -0.1235, -0.0209,  0.1490,  0.0177, -0.0049,  0.1101,  0.1233,
          0.0009, -0.1519, -0.1322,  0.1120,  0.0683, -0.0031, -0.0289,  0.1035,
          0.1291, -0.0138,  0.0273,  0.1314, -0.0869, -0.0795,  0.0070, -0.1504,
         -0.0051,  0.1080,  0.1293,  0.1408, -0.0695, -0.1138, -0.0492, -0.0801,
          0.1190, -0.0100, -0.0672, -0.1468, -0.0129,  0.0077,  0.0133,  0.1156,
          0.0963,  0.0589,  0.1382, -0.1221, -0.0610, -0.0382, -0.0541, -0.1378,
         -0.1078,  0.0808, -0.1159, -0.1463,  0.1049, -0.1497,  0.1355, -0.0407,
          0.1511,  0.1099,  0.0121, -0.0443, -0.1151,  0.0864, -0.1103, -0.1350,
         -0.0783,  0.0988, -0.0105,  0.1379,  0.1234, -0.0711,  0.1216, -0.0751,
         -0.0352, -0.0548,  0.1201,  0.0407,  0.0077, -0.1190, -0.0166, -0.0413,
          0.0234, -0.0961, -0.0354,  0.1020,  0.1413, -0.0366, -0.1346, -0.1293,
         -0.1189, -0.1062,  0.0273, -0.0763,  0.0895,  0.1299, -0.0473, -0.0700,
          0.0882,  0.0827, -0.1491, -0.0458, -0.1308, -0.0339, -0.1173, -0.1268,
         -0.0115, -0.1028, -0.1288,  0.0036, -0.1039, -0.0176,  0.1191,  0.0646,
         -0.1190,  0.0799,  0.0746, -0.0102,  0.0455,  0.1353,  0.1045,  0.0746,
         -0.0959, -0.0612,  0.0652,  0.0277, -0.0031, -0.1185, -0.1118, -0.0420,
          0.1026, -0.0339, -0.0420, -0.1133,  0.0591,  0.0134, -0.0272, -0.0149,
         -0.1335, -0.1342,  0.0347,  0.0489,  0.0741,  0.0696,  0.0953,  0.1341,
         -0.0828, -0.0972, -0.1055, -0.1011, -0.1040,  0.0672,  0.1024, -0.0315,
         -0.1145, -0.0166, -0.0898,  0.0150, -0.0673,  0.0430,  0.1293,  0.1304,
         -0.1414,  0.1225, -0.0003,  0.0060, -0.0912, -0.0312, -0.0336,  0.0196,
          0.1006,  0.0817, -0.0692,  0.0522,  0.0779,  0.0609, -0.1040,  0.1422,
         -0.0017, -0.0531,  0.0225, -0.1484, -0.1111,  0.1436,  0.0371,  0.1190,
         -0.0816,  0.0889,  0.0051, -0.0120,  0.1103,  0.0988,  0.0563,  0.0944,
          0.0530,  0.0750,  0.0566,  0.1461,  0.0299, -0.0179, -0.0326, -0.0040,
          0.0761,  0.1416, -0.1347, -0.0213,  0.0845,  0.0502,  0.0220,  0.1142,
         -0.1335,  0.0219,  0.0098,  0.1300, -0.0365,  0.0638, -0.1218, -0.0766]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0221, -0.0761, -0.0886,  ...,  0.0447,  0.1093, -0.0415],
        [ 0.0673,  0.0680,  0.1157,  ..., -0.0387, -0.1162, -0.1060],
        [ 0.1212,  0.0377,  0.0246,  ..., -0.0543, -0.0109,  0.1082],
        ...,
        [ 0.0463,  0.0925, -0.1203,  ..., -0.0164,  0.0876,  0.0929],
        [ 0.0089,  0.0410,  0.0994,  ...,  0.0868,  0.0201, -0.0580],
        [ 0.0512,  0.0895,  0.0950,  ...,  0.0214, -0.1004, -0.0889]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0221, -0.0761, -0.0886,  ...,  0.0447,  0.1093, -0.0415],
        [ 0.0673,  0.0680,  0.1157,  ..., -0.0387, -0.1162, -0.1060],
        [ 0.1212,  0.0377,  0.0246,  ..., -0.0543, -0.0109,  0.1082],
        ...,
        [ 0.0463,  0.0925, -0.1203,  ..., -0.0164,  0.0876,  0.0929],
        [ 0.0089,  0.0410,  0.0994,  ...,  0.0868,  0.0201, -0.0580],
        [ 0.0512,  0.0895,  0.0950,  ...,  0.0214, -0.1004, -0.0889]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.0625, -0.0501,  0.1509,  ...,  0.0953,  0.0179, -0.0296],
        [ 0.1506, -0.1201, -0.1179,  ...,  0.1469,  0.1139,  0.1406],
        [ 0.0480,  0.0500, -0.0875,  ..., -0.0516,  0.1628, -0.0291],
        ...,
        [ 0.0837,  0.0823, -0.1465,  ..., -0.0833, -0.0944, -0.0723],
        [ 0.0010, -0.1306, -0.0574,  ...,  0.1724,  0.0969,  0.1345],
        [ 0.0527,  0.0090,  0.0139,  ...,  0.0234, -0.0699, -0.0365]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0625, -0.0501,  0.1509,  ...,  0.0953,  0.0179, -0.0296],
        [ 0.1506, -0.1201, -0.1179,  ...,  0.1469,  0.1139,  0.1406],
        [ 0.0480,  0.0500, -0.0875,  ..., -0.0516,  0.1628, -0.0291],
        ...,
        [ 0.0837,  0.0823, -0.1465,  ..., -0.0833, -0.0944, -0.0723],
        [ 0.0010, -0.1306, -0.0574,  ...,  0.1724,  0.0969,  0.1345],
        [ 0.0527,  0.0090,  0.0139,  ...,  0.0234, -0.0699, -0.0365]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.0771,  0.0394,  0.1074,  ...,  0.0491,  0.1787,  0.1342],
        [ 0.2331, -0.0415, -0.0943,  ..., -0.0460,  0.1882,  0.1555],
        [ 0.0081, -0.0178,  0.2304,  ..., -0.1830, -0.0212, -0.0121],
        ...,
        [ 0.2245, -0.0907,  0.0670,  ..., -0.2039, -0.0276, -0.1833],
        [ 0.2257, -0.0971, -0.1815,  ...,  0.1903,  0.2376, -0.0477],
        [-0.0317,  0.0899,  0.1202,  ...,  0.0675, -0.0015,  0.2397]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0771,  0.0394,  0.1074,  ...,  0.0491,  0.1787,  0.1342],
        [ 0.2331, -0.0415, -0.0943,  ..., -0.0460,  0.1882,  0.1555],
        [ 0.0081, -0.0178,  0.2304,  ..., -0.1830, -0.0212, -0.0121],
        ...,
        [ 0.2245, -0.0907,  0.0670,  ..., -0.2039, -0.0276, -0.1833],
        [ 0.2257, -0.0971, -0.1815,  ...,  0.1903,  0.2376, -0.0477],
        [-0.0317,  0.0899,  0.1202,  ...,  0.0675, -0.0015,  0.2397]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.2238],
        [-0.2295],
        [ 0.0279],
        [-0.0798],
        [ 0.1182],
        [ 0.4259],
        [-0.0202],
        [-0.1993],
        [ 0.4166],
        [-0.3938],
        [-0.3203],
        [ 0.1175],
        [ 0.0524],
        [ 0.2264],
        [ 0.0948],
        [ 0.1508],
        [-0.3808],
        [ 0.3423],
        [-0.2286],
        [ 0.0366],
        [-0.0455],
        [-0.2471],
        [-0.0796],
        [ 0.3391],
        [-0.4002],
        [ 0.0215],
        [ 0.3560],
        [-0.0428],
        [ 0.2387],
        [ 0.1956],
        [-0.2375],
        [-0.2024]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2238],
        [-0.2295],
        [ 0.0279],
        [-0.0798],
        [ 0.1182],
        [ 0.4259],
        [-0.0202],
        [-0.1993],
        [ 0.4166],
        [-0.3938],
        [-0.3203],
        [ 0.1175],
        [ 0.0524],
        [ 0.2264],
        [ 0.0948],
        [ 0.1508],
        [-0.3808],
        [ 0.3423],
        [-0.2286],
        [ 0.0366],
        [-0.0455],
        [-0.2471],
        [-0.0796],
        [ 0.3391],
        [-0.4002],
        [ 0.0215],
        [ 0.3560],
        [-0.0428],
        [ 0.2387],
        [ 0.1956],
        [-0.2375],
        [-0.2024]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(102.4631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.6404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(7.8115, device='cuda:0')



h[100].sum tensor(-2.5162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.5725, device='cuda:0')



h[200].sum tensor(-1.1523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1781, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(11017.5439, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0130,  ..., 0.0020, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0061,  ..., 0.0009, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0017,  ..., 0.0003, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(63176.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.6517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1327.1439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(84.3614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.1253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2835],
        [-0.1998],
        [-0.1344],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-13117.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.2835],
        [-0.1998],
        [-0.1344],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0049, -0.0155, -0.0032,  ..., -0.0038, -0.0118, -0.0202],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-637.0861, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.3859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-33.1989, device='cuda:0')



h[100].sum tensor(-60.6420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-62.1644, device='cuda:0')



h[200].sum tensor(-130.2634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.5335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(98386.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0513, 0.1542],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.1321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.1061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(566848.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-769.9645, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1131.9551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.5919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[3.4735e-01],
        [3.7585e-01],
        [4.1475e-01],
        ...,
        [4.5237e-06],
        [7.5250e-06],
        [1.0757e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(24115.1445, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.2835],
        [-0.1998],
        [-0.1344],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 41, in <module>
    result1 = net(batcheddglgraph, TraTen[10000:10010].reshape(10 * 6796, 1))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 47, in forward
    h = self.conv1(g, in_fet)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 437, in forward
    rst = th.matmul(rst, weight)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)

real	0m24.800s
user	0m18.026s
sys	0m4.862s
