0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 14:39:16 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   22C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b40ce7f18e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.134s
user	0m2.626s
sys	0m1.194s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[14:39:41] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.1102],
        [-0.0234],
        [ 1.4394],
        ...,
        [-0.1558],
        [-2.0519],
        [-0.7713]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-32.6936, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0188, -0.1008, -0.0054,  0.1079,  0.1383,  0.0099, -0.0509,  0.0173,
          0.0584, -0.0618,  0.1452,  0.0071, -0.0028, -0.1059, -0.0354,  0.1100,
         -0.1382,  0.0771,  0.0740, -0.0220,  0.1187,  0.0726, -0.0919, -0.1522,
          0.0181,  0.1293, -0.0683,  0.1142,  0.1127, -0.0894, -0.1217,  0.1393,
          0.0747, -0.1223,  0.0652,  0.0208,  0.0139,  0.1457,  0.0746,  0.1092,
          0.1468, -0.0376,  0.0213,  0.1272,  0.0174,  0.0363,  0.0623, -0.0955,
         -0.1528,  0.1265, -0.0872, -0.0999, -0.0164, -0.0076, -0.0715,  0.1221,
          0.0830,  0.0002, -0.0117,  0.0685, -0.0630,  0.0967,  0.1028,  0.1474,
          0.0299,  0.0305,  0.0391, -0.1259,  0.0866, -0.0846,  0.0120, -0.1462,
         -0.0362, -0.1006,  0.1308, -0.0853, -0.0699, -0.0306,  0.0482, -0.1083,
          0.1082, -0.1186,  0.0614, -0.0558, -0.0411,  0.0721, -0.1528,  0.0644,
         -0.1451, -0.1023, -0.1428,  0.0380, -0.0774,  0.0270, -0.0646, -0.0734,
         -0.0870, -0.0577,  0.1251, -0.0731,  0.0024,  0.0515, -0.1242,  0.1505,
          0.0612, -0.0232, -0.0873, -0.1028,  0.0586,  0.0743,  0.1120,  0.0864,
          0.0115, -0.0378, -0.0660,  0.0638, -0.0762, -0.1312, -0.0458, -0.0120,
          0.1137, -0.1161,  0.1518,  0.0590,  0.0678,  0.1391, -0.0713,  0.0081,
          0.0947, -0.0286, -0.1458,  0.0759, -0.1277,  0.0766,  0.0350,  0.0827,
         -0.1024, -0.0766,  0.1367,  0.0010, -0.0254, -0.0630,  0.1173,  0.0357,
          0.1208,  0.0763,  0.1218,  0.0877, -0.1082,  0.0206,  0.1290,  0.0308,
          0.0722,  0.1002, -0.0301,  0.0249,  0.1045, -0.0265,  0.0278,  0.0512,
          0.0828, -0.1073,  0.0866, -0.0724, -0.0919, -0.1478, -0.0719,  0.0322,
          0.0809,  0.0129, -0.0908,  0.0453, -0.0959,  0.0995,  0.1202, -0.0423,
          0.0739, -0.0143, -0.0526, -0.0711,  0.0995, -0.1168, -0.0973, -0.1396,
         -0.1334,  0.0054,  0.1122, -0.0483, -0.0856, -0.0716, -0.0762,  0.0289,
         -0.1169,  0.0148, -0.0357,  0.0644,  0.0639,  0.0878,  0.1224,  0.1307,
         -0.0174,  0.0976,  0.1465,  0.1451,  0.1503,  0.1170,  0.0672, -0.0624,
         -0.0633, -0.1527, -0.0259,  0.0381,  0.1097,  0.0986,  0.0354,  0.0803,
         -0.0345,  0.1143,  0.0726,  0.0607, -0.0923, -0.0183,  0.0906, -0.1058,
         -0.0450,  0.1435, -0.0827,  0.1169, -0.1128,  0.0455,  0.0345,  0.1345,
         -0.1481, -0.0611, -0.0057,  0.0701, -0.0299,  0.0960,  0.0761, -0.1175,
         -0.0216,  0.0384,  0.1093,  0.1333,  0.0889, -0.0998,  0.1446,  0.0182,
          0.0054,  0.1129,  0.1349, -0.0310,  0.0765,  0.0907,  0.0374,  0.1267]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0188, -0.1008, -0.0054,  0.1079,  0.1383,  0.0099, -0.0509,  0.0173,
          0.0584, -0.0618,  0.1452,  0.0071, -0.0028, -0.1059, -0.0354,  0.1100,
         -0.1382,  0.0771,  0.0740, -0.0220,  0.1187,  0.0726, -0.0919, -0.1522,
          0.0181,  0.1293, -0.0683,  0.1142,  0.1127, -0.0894, -0.1217,  0.1393,
          0.0747, -0.1223,  0.0652,  0.0208,  0.0139,  0.1457,  0.0746,  0.1092,
          0.1468, -0.0376,  0.0213,  0.1272,  0.0174,  0.0363,  0.0623, -0.0955,
         -0.1528,  0.1265, -0.0872, -0.0999, -0.0164, -0.0076, -0.0715,  0.1221,
          0.0830,  0.0002, -0.0117,  0.0685, -0.0630,  0.0967,  0.1028,  0.1474,
          0.0299,  0.0305,  0.0391, -0.1259,  0.0866, -0.0846,  0.0120, -0.1462,
         -0.0362, -0.1006,  0.1308, -0.0853, -0.0699, -0.0306,  0.0482, -0.1083,
          0.1082, -0.1186,  0.0614, -0.0558, -0.0411,  0.0721, -0.1528,  0.0644,
         -0.1451, -0.1023, -0.1428,  0.0380, -0.0774,  0.0270, -0.0646, -0.0734,
         -0.0870, -0.0577,  0.1251, -0.0731,  0.0024,  0.0515, -0.1242,  0.1505,
          0.0612, -0.0232, -0.0873, -0.1028,  0.0586,  0.0743,  0.1120,  0.0864,
          0.0115, -0.0378, -0.0660,  0.0638, -0.0762, -0.1312, -0.0458, -0.0120,
          0.1137, -0.1161,  0.1518,  0.0590,  0.0678,  0.1391, -0.0713,  0.0081,
          0.0947, -0.0286, -0.1458,  0.0759, -0.1277,  0.0766,  0.0350,  0.0827,
         -0.1024, -0.0766,  0.1367,  0.0010, -0.0254, -0.0630,  0.1173,  0.0357,
          0.1208,  0.0763,  0.1218,  0.0877, -0.1082,  0.0206,  0.1290,  0.0308,
          0.0722,  0.1002, -0.0301,  0.0249,  0.1045, -0.0265,  0.0278,  0.0512,
          0.0828, -0.1073,  0.0866, -0.0724, -0.0919, -0.1478, -0.0719,  0.0322,
          0.0809,  0.0129, -0.0908,  0.0453, -0.0959,  0.0995,  0.1202, -0.0423,
          0.0739, -0.0143, -0.0526, -0.0711,  0.0995, -0.1168, -0.0973, -0.1396,
         -0.1334,  0.0054,  0.1122, -0.0483, -0.0856, -0.0716, -0.0762,  0.0289,
         -0.1169,  0.0148, -0.0357,  0.0644,  0.0639,  0.0878,  0.1224,  0.1307,
         -0.0174,  0.0976,  0.1465,  0.1451,  0.1503,  0.1170,  0.0672, -0.0624,
         -0.0633, -0.1527, -0.0259,  0.0381,  0.1097,  0.0986,  0.0354,  0.0803,
         -0.0345,  0.1143,  0.0726,  0.0607, -0.0923, -0.0183,  0.0906, -0.1058,
         -0.0450,  0.1435, -0.0827,  0.1169, -0.1128,  0.0455,  0.0345,  0.1345,
         -0.1481, -0.0611, -0.0057,  0.0701, -0.0299,  0.0960,  0.0761, -0.1175,
         -0.0216,  0.0384,  0.1093,  0.1333,  0.0889, -0.0998,  0.1446,  0.0182,
          0.0054,  0.1129,  0.1349, -0.0310,  0.0765,  0.0907,  0.0374,  0.1267]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[ 0.0255,  0.0055,  0.0058,  ...,  0.0169,  0.0337,  0.0608],
        [ 0.0207, -0.0456, -0.0650,  ...,  0.0223,  0.0895, -0.1118],
        [-0.0735, -0.0239,  0.0721,  ..., -0.1169,  0.0754, -0.0133],
        ...,
        [-0.0688, -0.1209, -0.0332,  ...,  0.0342,  0.0955,  0.0889],
        [-0.1164,  0.0358,  0.0517,  ...,  0.0864,  0.1038,  0.1133],
        [-0.1053, -0.0453,  0.1103,  ..., -0.0975, -0.1008,  0.0157]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0255,  0.0055,  0.0058,  ...,  0.0169,  0.0337,  0.0608],
        [ 0.0207, -0.0456, -0.0650,  ...,  0.0223,  0.0895, -0.1118],
        [-0.0735, -0.0239,  0.0721,  ..., -0.1169,  0.0754, -0.0133],
        ...,
        [-0.0688, -0.1209, -0.0332,  ...,  0.0342,  0.0955,  0.0889],
        [-0.1164,  0.0358,  0.0517,  ...,  0.0864,  0.1038,  0.1133],
        [-0.1053, -0.0453,  0.1103,  ..., -0.0975, -0.1008,  0.0157]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0129, -0.1279, -0.0726,  ...,  0.0597, -0.0725,  0.0684],
        [ 0.0965, -0.0162,  0.0279,  ...,  0.1540, -0.1040,  0.0957],
        [ 0.0167, -0.1630,  0.1690,  ...,  0.0663, -0.1080, -0.0641],
        ...,
        [ 0.0075, -0.0927,  0.1691,  ..., -0.0487,  0.1304,  0.0825],
        [ 0.1014, -0.1510, -0.0097,  ..., -0.1673, -0.1211,  0.0226],
        [ 0.0806, -0.1356, -0.1389,  ..., -0.1012, -0.1277,  0.0344]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0129, -0.1279, -0.0726,  ...,  0.0597, -0.0725,  0.0684],
        [ 0.0965, -0.0162,  0.0279,  ...,  0.1540, -0.1040,  0.0957],
        [ 0.0167, -0.1630,  0.1690,  ...,  0.0663, -0.1080, -0.0641],
        ...,
        [ 0.0075, -0.0927,  0.1691,  ..., -0.0487,  0.1304,  0.0825],
        [ 0.1014, -0.1510, -0.0097,  ..., -0.1673, -0.1211,  0.0226],
        [ 0.0806, -0.1356, -0.1389,  ..., -0.1012, -0.1277,  0.0344]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.0454,  0.1056, -0.1364,  ..., -0.1292,  0.0941,  0.1492],
        [ 0.2285,  0.2433, -0.0050,  ..., -0.2274,  0.2274,  0.1763],
        [-0.1042, -0.1041, -0.0297,  ..., -0.2279,  0.1002, -0.1842],
        ...,
        [ 0.2390, -0.0776,  0.2180,  ..., -0.0867,  0.1625, -0.0215],
        [ 0.1858,  0.0927,  0.1075,  ...,  0.0013, -0.2411,  0.1988],
        [-0.2324,  0.1607,  0.2476,  ...,  0.1523,  0.0217,  0.2391]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0454,  0.1056, -0.1364,  ..., -0.1292,  0.0941,  0.1492],
        [ 0.2285,  0.2433, -0.0050,  ..., -0.2274,  0.2274,  0.1763],
        [-0.1042, -0.1041, -0.0297,  ..., -0.2279,  0.1002, -0.1842],
        ...,
        [ 0.2390, -0.0776,  0.2180,  ..., -0.0867,  0.1625, -0.0215],
        [ 0.1858,  0.0927,  0.1075,  ...,  0.0013, -0.2411,  0.1988],
        [-0.2324,  0.1607,  0.2476,  ...,  0.1523,  0.0217,  0.2391]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.3399],
        [ 0.4015],
        [-0.1974],
        [-0.3312],
        [ 0.1708],
        [-0.2756],
        [-0.3607],
        [ 0.0654],
        [ 0.0022],
        [ 0.1150],
        [ 0.2000],
        [ 0.2357],
        [-0.0050],
        [-0.3979],
        [-0.0895],
        [-0.0635],
        [ 0.3943],
        [ 0.2672],
        [ 0.1605],
        [-0.1425],
        [ 0.1871],
        [ 0.0571],
        [-0.3848],
        [ 0.0916],
        [ 0.3553],
        [-0.3243],
        [ 0.3755],
        [ 0.4261],
        [ 0.1558],
        [ 0.1597],
        [ 0.1603],
        [-0.1226]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.3399],
        [ 0.4015],
        [-0.1974],
        [-0.3312],
        [ 0.1708],
        [-0.2756],
        [-0.3607],
        [ 0.0654],
        [ 0.0022],
        [ 0.1150],
        [ 0.2000],
        [ 0.2357],
        [-0.0050],
        [-0.3979],
        [-0.0895],
        [-0.0635],
        [ 0.3943],
        [ 0.2672],
        [ 0.1605],
        [-0.1425],
        [ 0.1871],
        [ 0.0571],
        [-0.3848],
        [ 0.0916],
        [ 0.3553],
        [-0.3243],
        [ 0.3755],
        [ 0.4261],
        [ 0.1558],
        [ 0.1597],
        [ 0.1603],
        [-0.1226]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-150.6862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-4.0857, device='cuda:0')



h[100].sum tensor(-5.1156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.2301, device='cuda:0')



h[200].sum tensor(9.9285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(8998.7568, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(63655.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-207.7954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104.9151, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6.6701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(263.6537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(16.7478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0501],
        [-0.0353],
        [-0.0237],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-2316.0537, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.0501],
        [-0.0353],
        [-0.0237],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043,  0.0071,  0.0131,  ..., -0.0152, -0.0157, -0.0046],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-105.6502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.1639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8710, device='cuda:0')



h[100].sum tensor(-79.7756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.7783, device='cuda:0')



h[200].sum tensor(-124.7585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.8904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0162, 0.0269, 0.0495,  ..., 0.0000, 0.0000, 0.0000],
        [0.0133, 0.0221, 0.0408,  ..., 0.0000, 0.0000, 0.0000],
        [0.0031, 0.0052, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(99973.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3385, 0.1578,  ..., 0.0000, 0.0000, 0.5371],
        [0.0000, 0.2900, 0.1352,  ..., 0.0000, 0.0000, 0.4602],
        [0.0000, 0.2329, 0.1086,  ..., 0.0000, 0.0000, 0.3695],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(466970.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-747.0826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13685.2607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(958.5262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-427.7265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.2084e+00],
        [1.3075e+00],
        [1.4428e+00],
        ...,
        [1.5739e-05],
        [2.6174e-05],
        [3.7425e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(83888.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0501],
        [-0.0353],
        [-0.0237],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0105, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-45.3505, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-70.9221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3556.9729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3069.5366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2334.8601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0253,  0.0047,  0.0066,  ..., -0.0270,  0.0105, -0.0199],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0321,  0.0059,  0.0084,  ..., -0.0344,  0.0133, -0.0254],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(-201.3323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(69.8674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0105, device='cuda:0')



h[100].sum tensor(-54.1613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-45.3505, device='cuda:0')



h[200].sum tensor(48.7914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-70.9221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0206, 0.0038, 0.0054,  ..., 0.0000, 0.0085, 0.0000],
        [0.1042, 0.0193, 0.0272,  ..., 0.0000, 0.0432, 0.0000],
        [0.0392, 0.0073, 0.0102,  ..., 0.0000, 0.0162, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(60039.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1289,  ..., 0.0652, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2471,  ..., 0.1250, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.2060,  ..., 0.1042, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(301245.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.6050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19191.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(49.9839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4077.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.4577, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3293],
        [-0.4102],
        [-0.4483],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-57436.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)

result1 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape torch.Size([67960, 1]) 
result0 tensor([[-0.3293],
        [-0.4102],
        [-0.4483],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result0.shape torch.Size([67960, 1])
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 66, in <module>
    plotevent(sitonsquare((result1.reshape(10, 6796)[EvBTr - 10000] > thr[0]) & ((trvalgnnpppipi[EvBTr]).cpu() > 0)), 335, f'event number {EvBTr} passed through network after training \n threshold is {thr[0]}')
AttributeError: 'numpy.ndarray' object has no attribute 'cpu'

real	0m26.204s
user	0m18.674s
sys	0m5.066s
