0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 14:19:24 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b7509caf8e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.400s
user	0m2.660s
sys	0m1.211s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[14:19:48] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.4424],
        [-0.0338],
        [-1.4811],
        ...,
        [-0.1924],
        [ 0.3990],
        [ 0.8652]], device='cuda:0', requires_grad=True) 
node features sum: tensor(56.9686, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[ 0.1014,  0.0990,  0.1505,  0.0943, -0.0644, -0.0071,  0.0031,  0.0266,
         -0.0666,  0.0225, -0.0190, -0.0049, -0.1467,  0.1057, -0.0215,  0.0909,
         -0.0628,  0.0024,  0.0814,  0.0464,  0.0180,  0.1082,  0.0485,  0.1500,
         -0.1314, -0.0608,  0.0597, -0.0219,  0.1192,  0.0525, -0.0572,  0.1348,
         -0.1497, -0.0445, -0.0133,  0.1467,  0.0350, -0.1112, -0.0531, -0.1502,
         -0.0957, -0.1211,  0.1368,  0.1298, -0.1025,  0.1067,  0.0273,  0.0654,
          0.0620, -0.0876,  0.0945,  0.0147, -0.0581,  0.0012, -0.1430, -0.1064,
          0.0936, -0.0058,  0.0938, -0.1064,  0.0994,  0.0531,  0.1151, -0.0299,
         -0.0184, -0.1022, -0.0083,  0.0060, -0.0596, -0.0814,  0.1355,  0.1071,
          0.0494,  0.0377,  0.0353,  0.1067,  0.0319,  0.1206,  0.0874,  0.1190,
         -0.1437,  0.1027, -0.1096, -0.0012, -0.0869, -0.1422, -0.0083,  0.0488,
          0.1206,  0.0147, -0.1093,  0.1516,  0.1059, -0.0038, -0.1451,  0.0771,
          0.0084, -0.0077, -0.0265,  0.0666,  0.0220,  0.0564,  0.1126,  0.1213,
         -0.0208,  0.1086,  0.0515, -0.1304, -0.0892, -0.1485, -0.0800, -0.0650,
          0.0087, -0.1258, -0.1322,  0.0205,  0.0174, -0.1311, -0.0513,  0.0329,
         -0.0929,  0.0469,  0.0225, -0.1129,  0.0661, -0.1411, -0.0549,  0.0929,
          0.1442, -0.0103,  0.0810,  0.1498,  0.1157, -0.0338, -0.0508, -0.0228,
         -0.1350, -0.0462, -0.0283,  0.0622, -0.0089,  0.0712,  0.1021,  0.0673,
          0.0304, -0.1034, -0.1352,  0.0491, -0.1029,  0.0059,  0.0899, -0.0635,
         -0.0525, -0.1328,  0.0743,  0.0824,  0.0868,  0.0902, -0.0876, -0.0372,
          0.1121, -0.1390, -0.0595,  0.0854,  0.0906,  0.0163,  0.0209,  0.0150,
          0.0193, -0.0682, -0.0194,  0.0097,  0.0761,  0.0134,  0.0368, -0.0413,
         -0.0012,  0.1457,  0.1016,  0.0506, -0.1139,  0.0103,  0.1186,  0.1187,
         -0.0621, -0.1150,  0.1351,  0.0335, -0.0503, -0.0370, -0.1526,  0.1475,
          0.1283,  0.0595,  0.0699,  0.0925,  0.0801,  0.1159,  0.0127, -0.0958,
          0.1077, -0.0780,  0.1432,  0.0501, -0.1260, -0.0311, -0.1405, -0.1343,
          0.1525, -0.1490,  0.0844,  0.1166, -0.0733, -0.0125,  0.0702, -0.1478,
          0.0033, -0.1228, -0.0784,  0.0913, -0.1492, -0.0136,  0.0706, -0.1516,
          0.1226, -0.0032,  0.0034, -0.0377,  0.1425, -0.1095, -0.1207, -0.1165,
         -0.0260,  0.0088, -0.0729, -0.1482, -0.1502,  0.1084, -0.0269, -0.1153,
         -0.0450, -0.0633, -0.1257, -0.0651,  0.0515, -0.0455,  0.1319, -0.0053,
          0.0077,  0.1242,  0.1475,  0.1436,  0.0116,  0.1345, -0.0401,  0.0286]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1014,  0.0990,  0.1505,  0.0943, -0.0644, -0.0071,  0.0031,  0.0266,
         -0.0666,  0.0225, -0.0190, -0.0049, -0.1467,  0.1057, -0.0215,  0.0909,
         -0.0628,  0.0024,  0.0814,  0.0464,  0.0180,  0.1082,  0.0485,  0.1500,
         -0.1314, -0.0608,  0.0597, -0.0219,  0.1192,  0.0525, -0.0572,  0.1348,
         -0.1497, -0.0445, -0.0133,  0.1467,  0.0350, -0.1112, -0.0531, -0.1502,
         -0.0957, -0.1211,  0.1368,  0.1298, -0.1025,  0.1067,  0.0273,  0.0654,
          0.0620, -0.0876,  0.0945,  0.0147, -0.0581,  0.0012, -0.1430, -0.1064,
          0.0936, -0.0058,  0.0938, -0.1064,  0.0994,  0.0531,  0.1151, -0.0299,
         -0.0184, -0.1022, -0.0083,  0.0060, -0.0596, -0.0814,  0.1355,  0.1071,
          0.0494,  0.0377,  0.0353,  0.1067,  0.0319,  0.1206,  0.0874,  0.1190,
         -0.1437,  0.1027, -0.1096, -0.0012, -0.0869, -0.1422, -0.0083,  0.0488,
          0.1206,  0.0147, -0.1093,  0.1516,  0.1059, -0.0038, -0.1451,  0.0771,
          0.0084, -0.0077, -0.0265,  0.0666,  0.0220,  0.0564,  0.1126,  0.1213,
         -0.0208,  0.1086,  0.0515, -0.1304, -0.0892, -0.1485, -0.0800, -0.0650,
          0.0087, -0.1258, -0.1322,  0.0205,  0.0174, -0.1311, -0.0513,  0.0329,
         -0.0929,  0.0469,  0.0225, -0.1129,  0.0661, -0.1411, -0.0549,  0.0929,
          0.1442, -0.0103,  0.0810,  0.1498,  0.1157, -0.0338, -0.0508, -0.0228,
         -0.1350, -0.0462, -0.0283,  0.0622, -0.0089,  0.0712,  0.1021,  0.0673,
          0.0304, -0.1034, -0.1352,  0.0491, -0.1029,  0.0059,  0.0899, -0.0635,
         -0.0525, -0.1328,  0.0743,  0.0824,  0.0868,  0.0902, -0.0876, -0.0372,
          0.1121, -0.1390, -0.0595,  0.0854,  0.0906,  0.0163,  0.0209,  0.0150,
          0.0193, -0.0682, -0.0194,  0.0097,  0.0761,  0.0134,  0.0368, -0.0413,
         -0.0012,  0.1457,  0.1016,  0.0506, -0.1139,  0.0103,  0.1186,  0.1187,
         -0.0621, -0.1150,  0.1351,  0.0335, -0.0503, -0.0370, -0.1526,  0.1475,
          0.1283,  0.0595,  0.0699,  0.0925,  0.0801,  0.1159,  0.0127, -0.0958,
          0.1077, -0.0780,  0.1432,  0.0501, -0.1260, -0.0311, -0.1405, -0.1343,
          0.1525, -0.1490,  0.0844,  0.1166, -0.0733, -0.0125,  0.0702, -0.1478,
          0.0033, -0.1228, -0.0784,  0.0913, -0.1492, -0.0136,  0.0706, -0.1516,
          0.1226, -0.0032,  0.0034, -0.0377,  0.1425, -0.1095, -0.1207, -0.1165,
         -0.0260,  0.0088, -0.0729, -0.1482, -0.1502,  0.1084, -0.0269, -0.1153,
         -0.0450, -0.0633, -0.1257, -0.0651,  0.0515, -0.0455,  0.1319, -0.0053,
          0.0077,  0.1242,  0.1475,  0.1436,  0.0116,  0.1345, -0.0401,  0.0286]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0811,  0.0647,  0.1222,  ...,  0.0658, -0.0218,  0.0393],
        [ 0.0879,  0.0198,  0.0769,  ..., -0.0848, -0.0869,  0.0536],
        [-0.0010, -0.0112, -0.1090,  ...,  0.0092,  0.0468, -0.0393],
        ...,
        [-0.0636, -0.0233, -0.0971,  ..., -0.0485,  0.0389,  0.0199],
        [-0.0592,  0.0539, -0.0183,  ...,  0.0889,  0.1112,  0.0148],
        [-0.0552,  0.0578, -0.1198,  ..., -0.0930, -0.0326,  0.0917]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0811,  0.0647,  0.1222,  ...,  0.0658, -0.0218,  0.0393],
        [ 0.0879,  0.0198,  0.0769,  ..., -0.0848, -0.0869,  0.0536],
        [-0.0010, -0.0112, -0.1090,  ...,  0.0092,  0.0468, -0.0393],
        ...,
        [-0.0636, -0.0233, -0.0971,  ..., -0.0485,  0.0389,  0.0199],
        [-0.0592,  0.0539, -0.0183,  ...,  0.0889,  0.1112,  0.0148],
        [-0.0552,  0.0578, -0.1198,  ..., -0.0930, -0.0326,  0.0917]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0652,  0.1577, -0.1756,  ...,  0.0784,  0.0440,  0.0165],
        [ 0.1434, -0.0826, -0.0987,  ..., -0.1620, -0.1219, -0.1529],
        [-0.1152, -0.0984, -0.1709,  ...,  0.0873,  0.1586,  0.0739],
        ...,
        [ 0.0105, -0.1457,  0.0525,  ..., -0.0175,  0.0619,  0.0420],
        [ 0.1471, -0.1327, -0.1717,  ...,  0.0806,  0.1409, -0.0729],
        [-0.1530, -0.1729,  0.0037,  ...,  0.0216,  0.0602,  0.1504]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0652,  0.1577, -0.1756,  ...,  0.0784,  0.0440,  0.0165],
        [ 0.1434, -0.0826, -0.0987,  ..., -0.1620, -0.1219, -0.1529],
        [-0.1152, -0.0984, -0.1709,  ...,  0.0873,  0.1586,  0.0739],
        ...,
        [ 0.0105, -0.1457,  0.0525,  ..., -0.0175,  0.0619,  0.0420],
        [ 0.1471, -0.1327, -0.1717,  ...,  0.0806,  0.1409, -0.0729],
        [-0.1530, -0.1729,  0.0037,  ...,  0.0216,  0.0602,  0.1504]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.1603, -0.0252,  0.0515,  ..., -0.1050, -0.0081, -0.0285],
        [ 0.0161, -0.1388, -0.0172,  ...,  0.1321, -0.0632, -0.1071],
        [-0.1630, -0.2483,  0.0562,  ..., -0.2307, -0.1743,  0.2404],
        ...,
        [-0.0697,  0.1813, -0.1690,  ..., -0.1819,  0.2468,  0.0574],
        [ 0.2288,  0.0645, -0.0624,  ..., -0.0292,  0.0256,  0.1107],
        [-0.2380,  0.0034, -0.0038,  ..., -0.2386, -0.0319,  0.0019]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1603, -0.0252,  0.0515,  ..., -0.1050, -0.0081, -0.0285],
        [ 0.0161, -0.1388, -0.0172,  ...,  0.1321, -0.0632, -0.1071],
        [-0.1630, -0.2483,  0.0562,  ..., -0.2307, -0.1743,  0.2404],
        ...,
        [-0.0697,  0.1813, -0.1690,  ..., -0.1819,  0.2468,  0.0574],
        [ 0.2288,  0.0645, -0.0624,  ..., -0.0292,  0.0256,  0.1107],
        [-0.2380,  0.0034, -0.0038,  ..., -0.2386, -0.0319,  0.0019]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.2817],
        [ 0.3081],
        [ 0.4008],
        [ 0.0828],
        [-0.3000],
        [-0.1583],
        [ 0.3255],
        [-0.0024],
        [-0.0657],
        [-0.0517],
        [ 0.1359],
        [ 0.3003],
        [-0.2883],
        [-0.3840],
        [ 0.1975],
        [-0.3244],
        [ 0.1558],
        [-0.0765],
        [ 0.2133],
        [ 0.1176],
        [ 0.0618],
        [-0.2671],
        [ 0.4145],
        [ 0.2706],
        [ 0.3555],
        [ 0.3789],
        [-0.3921],
        [-0.4040],
        [-0.3873],
        [ 0.2853],
        [-0.0418],
        [-0.0904]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2817],
        [ 0.3081],
        [ 0.4008],
        [ 0.0828],
        [-0.3000],
        [-0.1583],
        [ 0.3255],
        [-0.0024],
        [-0.0657],
        [-0.0517],
        [ 0.1359],
        [ 0.3003],
        [-0.2883],
        [-0.3840],
        [ 0.1975],
        [-0.3244],
        [ 0.1558],
        [-0.0765],
        [ 0.2133],
        [ 0.1176],
        [ 0.0618],
        [-0.2671],
        [ 0.4145],
        [ 0.2706],
        [ 0.3555],
        [ 0.3789],
        [-0.3921],
        [-0.4040],
        [-0.3873],
        [ 0.2853],
        [-0.0418],
        [-0.0904]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(65.5496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.2805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(4.3763, device='cuda:0')



h[100].sum tensor(-8.1687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.3515, device='cuda:0')



h[200].sum tensor(6.3501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.4923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10817.1689, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0056, 0.0140,  ..., 0.0048, 0.0000, 0.0060],
        [0.0000, 0.0026, 0.0066,  ..., 0.0022, 0.0000, 0.0028],
        [0.0000, 0.0007, 0.0019,  ..., 0.0006, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(63583.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.5426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-66.8130, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-72.9831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0412],
        [-0.0291],
        [-0.0196],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-1905.5425, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.0412],
        [-0.0291],
        [-0.0196],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0159,  0.0017, -0.0111,  ...,  0.0123, -0.0036,  0.0184],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(339.1193, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(104.8489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(107.4811, device='cuda:0')



h[100].sum tensor(136.6581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.0887, device='cuda:0')



h[200].sum tensor(146.2755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(149.9476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0603, 0.0066, 0.0000,  ..., 0.0464, 0.0000, 0.0695],
        [0.0496, 0.0054, 0.0000,  ..., 0.0381, 0.0000, 0.0572],
        [0.0116, 0.0013, 0.0000,  ..., 0.0089, 0.0000, 0.0134],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(102766.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0316, 0.0400, 0.0829,  ..., 0.0000, 0.0000, 0.0000],
        [0.0271, 0.0343, 0.0710,  ..., 0.0000, 0.0000, 0.0000],
        [0.0217, 0.0275, 0.0570,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(508058.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1564.4116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(109.6505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2403.0510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(168.2857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.6684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.3881e-01],
        [-9.0768e-01],
        [-1.0017e+00],
        ...,
        [-1.0932e-05],
        [-1.8174e-05],
        [-2.5982e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-58240.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0412],
        [-0.0291],
        [-0.0196],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 41, in <module>
    result1 = net(batcheddglgraph, TraTen[10000:10010].reshape(10 * 6796, 1).to(device))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 37, in forward
    g.ndata['nfet'] = in_fet
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/view.py", line 81, in __setitem__
    self._graph._set_n_repr(self._ntid, self._nodes, {key : val})
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/heterograph.py", line 4113, in _set_n_repr
    raise DGLError('Cannot assign node feature "{}" on device {} to a graph on'
dgl._ffi.base.DGLError: Cannot assign node feature "nfet" on device cpu to a graph on device cuda:0. Call DGLGraph.to() to copy the graph to the same device.

real	0m25.405s
user	0m18.248s
sys	0m4.952s
