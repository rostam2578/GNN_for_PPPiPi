0: gpu023.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-68b3c133-0ec7-b77e-bda3-8284c63b6924)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9D:43:29:92:F3:5A:7B:48:7C:2B:1E:8D:D9:B6:B6:8B:84:B4:6F:8F
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 09:25:27 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   32C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089926656

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b865c68d8e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m43.375s
user	0m3.331s
sys	0m2.811s
[09:26:13] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.0199],
        [-0.2518],
        [-0.2333],
        ...,
        [ 0.4490],
        [ 0.7091],
        [-1.7079]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-58.7170, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[ 0.1093, -0.1307,  0.0758, -0.0214, -0.0277, -0.0853,  0.1485, -0.0477,
         -0.0529,  0.0077,  0.0346,  0.0091,  0.0351, -0.0077,  0.1296, -0.0134,
         -0.0157, -0.0393,  0.0867,  0.1155, -0.1441,  0.0878,  0.0662, -0.1436,
          0.1186, -0.0771, -0.1479,  0.0846,  0.0161, -0.0472,  0.0447,  0.1519,
         -0.0211,  0.1461,  0.0826, -0.1285,  0.0091,  0.1351,  0.1034,  0.1390,
          0.0127,  0.0391,  0.0426,  0.1026, -0.0239, -0.0479,  0.1117, -0.0738,
          0.0160,  0.1107,  0.0780,  0.1000, -0.0554, -0.0965, -0.0448,  0.1324,
          0.0969,  0.0547, -0.1083,  0.0394,  0.0926, -0.1307,  0.0597,  0.0938,
          0.0828, -0.0242,  0.0484,  0.1300, -0.0858, -0.1439, -0.0658,  0.0118,
          0.0248, -0.1270,  0.1333, -0.0129,  0.0329,  0.0807, -0.1275,  0.1163,
          0.0722, -0.1021, -0.1454,  0.0457, -0.0592,  0.1046, -0.0367,  0.1260,
          0.1134, -0.0018,  0.1351, -0.0013, -0.1032, -0.0856, -0.0079,  0.0720,
          0.1490, -0.0590, -0.0164,  0.1314, -0.0627, -0.0659,  0.0699, -0.1119,
         -0.0518, -0.0092,  0.1258,  0.0759,  0.0389, -0.0272, -0.0740, -0.0568,
         -0.1415,  0.0714, -0.0250,  0.0514,  0.0215,  0.1175, -0.1125, -0.0505,
         -0.1041, -0.0276, -0.0179,  0.0096,  0.0007, -0.1193,  0.1001, -0.0083,
         -0.0931,  0.1327, -0.0729, -0.0889,  0.0804,  0.0489, -0.0237, -0.1420,
         -0.1458, -0.0691,  0.0181,  0.1164,  0.1158, -0.0451,  0.1096,  0.0493,
         -0.0524,  0.0489,  0.1480,  0.0514, -0.0949,  0.1401,  0.0690, -0.1237,
         -0.0779, -0.0900,  0.0508, -0.1047,  0.0682, -0.0214, -0.0593,  0.0643,
          0.0095,  0.1017, -0.1009,  0.0370,  0.0449,  0.0952,  0.0471, -0.0107,
         -0.0125,  0.1332,  0.0773,  0.1038, -0.1166, -0.1272, -0.1291, -0.1454,
         -0.1512,  0.1243, -0.1520, -0.0207,  0.0107, -0.1433,  0.1158, -0.0673,
         -0.0137, -0.0728,  0.0508, -0.0075,  0.0524,  0.1205, -0.0655,  0.0277,
         -0.0993,  0.1143, -0.0789, -0.0437,  0.1139, -0.0607,  0.0095, -0.0435,
         -0.1452,  0.0542, -0.0423,  0.1179, -0.0699,  0.1262,  0.0984,  0.0163,
          0.0187, -0.0718,  0.0599,  0.0031, -0.1244, -0.1432,  0.1338, -0.0436,
          0.1086, -0.0998,  0.0234, -0.0566,  0.0890,  0.0157, -0.0999,  0.0060,
          0.0144, -0.1505, -0.0623,  0.0262,  0.0280, -0.0823,  0.0368, -0.1104,
         -0.0891,  0.0441, -0.1340,  0.1315, -0.0660,  0.1375,  0.0049,  0.0178,
          0.1411, -0.0718,  0.1016,  0.1037, -0.0296,  0.0959,  0.1017, -0.1066,
         -0.0414, -0.0669,  0.1041, -0.0220,  0.1031, -0.0371,  0.1373, -0.0851]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1093, -0.1307,  0.0758, -0.0214, -0.0277, -0.0853,  0.1485, -0.0477,
         -0.0529,  0.0077,  0.0346,  0.0091,  0.0351, -0.0077,  0.1296, -0.0134,
         -0.0157, -0.0393,  0.0867,  0.1155, -0.1441,  0.0878,  0.0662, -0.1436,
          0.1186, -0.0771, -0.1479,  0.0846,  0.0161, -0.0472,  0.0447,  0.1519,
         -0.0211,  0.1461,  0.0826, -0.1285,  0.0091,  0.1351,  0.1034,  0.1390,
          0.0127,  0.0391,  0.0426,  0.1026, -0.0239, -0.0479,  0.1117, -0.0738,
          0.0160,  0.1107,  0.0780,  0.1000, -0.0554, -0.0965, -0.0448,  0.1324,
          0.0969,  0.0547, -0.1083,  0.0394,  0.0926, -0.1307,  0.0597,  0.0938,
          0.0828, -0.0242,  0.0484,  0.1300, -0.0858, -0.1439, -0.0658,  0.0118,
          0.0248, -0.1270,  0.1333, -0.0129,  0.0329,  0.0807, -0.1275,  0.1163,
          0.0722, -0.1021, -0.1454,  0.0457, -0.0592,  0.1046, -0.0367,  0.1260,
          0.1134, -0.0018,  0.1351, -0.0013, -0.1032, -0.0856, -0.0079,  0.0720,
          0.1490, -0.0590, -0.0164,  0.1314, -0.0627, -0.0659,  0.0699, -0.1119,
         -0.0518, -0.0092,  0.1258,  0.0759,  0.0389, -0.0272, -0.0740, -0.0568,
         -0.1415,  0.0714, -0.0250,  0.0514,  0.0215,  0.1175, -0.1125, -0.0505,
         -0.1041, -0.0276, -0.0179,  0.0096,  0.0007, -0.1193,  0.1001, -0.0083,
         -0.0931,  0.1327, -0.0729, -0.0889,  0.0804,  0.0489, -0.0237, -0.1420,
         -0.1458, -0.0691,  0.0181,  0.1164,  0.1158, -0.0451,  0.1096,  0.0493,
         -0.0524,  0.0489,  0.1480,  0.0514, -0.0949,  0.1401,  0.0690, -0.1237,
         -0.0779, -0.0900,  0.0508, -0.1047,  0.0682, -0.0214, -0.0593,  0.0643,
          0.0095,  0.1017, -0.1009,  0.0370,  0.0449,  0.0952,  0.0471, -0.0107,
         -0.0125,  0.1332,  0.0773,  0.1038, -0.1166, -0.1272, -0.1291, -0.1454,
         -0.1512,  0.1243, -0.1520, -0.0207,  0.0107, -0.1433,  0.1158, -0.0673,
         -0.0137, -0.0728,  0.0508, -0.0075,  0.0524,  0.1205, -0.0655,  0.0277,
         -0.0993,  0.1143, -0.0789, -0.0437,  0.1139, -0.0607,  0.0095, -0.0435,
         -0.1452,  0.0542, -0.0423,  0.1179, -0.0699,  0.1262,  0.0984,  0.0163,
          0.0187, -0.0718,  0.0599,  0.0031, -0.1244, -0.1432,  0.1338, -0.0436,
          0.1086, -0.0998,  0.0234, -0.0566,  0.0890,  0.0157, -0.0999,  0.0060,
          0.0144, -0.1505, -0.0623,  0.0262,  0.0280, -0.0823,  0.0368, -0.1104,
         -0.0891,  0.0441, -0.1340,  0.1315, -0.0660,  0.1375,  0.0049,  0.0178,
          0.1411, -0.0718,  0.1016,  0.1037, -0.0296,  0.0959,  0.1017, -0.1066,
         -0.0414, -0.0669,  0.1041, -0.0220,  0.1031, -0.0371,  0.1373, -0.0851]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0633, -0.0348, -0.0975,  ...,  0.1190,  0.0735, -0.1237],
        [ 0.0838, -0.0059,  0.0071,  ..., -0.0849, -0.0904, -0.0914],
        [-0.0893, -0.1233,  0.0180,  ..., -0.0944, -0.1016,  0.1094],
        ...,
        [ 0.1089, -0.0464,  0.0881,  ..., -0.1123,  0.0690, -0.0989],
        [ 0.0661, -0.0581,  0.1244,  ...,  0.1058,  0.0827, -0.0709],
        [-0.0120,  0.0630,  0.1124,  ...,  0.0592,  0.0593,  0.0443]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0633, -0.0348, -0.0975,  ...,  0.1190,  0.0735, -0.1237],
        [ 0.0838, -0.0059,  0.0071,  ..., -0.0849, -0.0904, -0.0914],
        [-0.0893, -0.1233,  0.0180,  ..., -0.0944, -0.1016,  0.1094],
        ...,
        [ 0.1089, -0.0464,  0.0881,  ..., -0.1123,  0.0690, -0.0989],
        [ 0.0661, -0.0581,  0.1244,  ...,  0.1058,  0.0827, -0.0709],
        [-0.0120,  0.0630,  0.1124,  ...,  0.0592,  0.0593,  0.0443]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.0634,  0.0500, -0.1509,  ...,  0.1561,  0.0061, -0.0190],
        [-0.1123, -0.0547, -0.1296,  ..., -0.1496, -0.0083, -0.0791],
        [ 0.0171,  0.1210, -0.1151,  ...,  0.0985,  0.1477,  0.1531],
        ...,
        [ 0.0703,  0.1189, -0.0808,  ..., -0.0257, -0.1116, -0.0647],
        [ 0.1001,  0.0711, -0.0729,  ..., -0.1204, -0.0660, -0.0726],
        [-0.1753, -0.1015, -0.1173,  ..., -0.0116,  0.0804,  0.0975]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0634,  0.0500, -0.1509,  ...,  0.1561,  0.0061, -0.0190],
        [-0.1123, -0.0547, -0.1296,  ..., -0.1496, -0.0083, -0.0791],
        [ 0.0171,  0.1210, -0.1151,  ...,  0.0985,  0.1477,  0.1531],
        ...,
        [ 0.0703,  0.1189, -0.0808,  ..., -0.0257, -0.1116, -0.0647],
        [ 0.1001,  0.0711, -0.0729,  ..., -0.1204, -0.0660, -0.0726],
        [-0.1753, -0.1015, -0.1173,  ..., -0.0116,  0.0804,  0.0975]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 2.3583e-01, -2.4021e-05,  2.3001e-01,  ..., -2.3059e-01,
         -2.4768e-01, -2.2720e-01],
        [-5.7583e-02, -1.8498e-01, -6.4538e-02,  ..., -1.1611e-01,
          5.7019e-02, -2.7185e-02],
        [-7.6537e-03,  1.4646e-01,  2.8190e-02,  ...,  1.9752e-01,
         -5.0790e-02,  2.2181e-01],
        ...,
        [-2.4915e-02,  1.6899e-01, -1.1586e-01,  ...,  6.1408e-02,
          8.7233e-02,  1.2131e-01],
        [-1.4281e-01, -1.6520e-01, -5.9309e-02,  ...,  1.0933e-01,
          6.6997e-02,  2.4116e-01],
        [-1.7791e-01, -1.9037e-01,  6.4217e-02,  ...,  4.0869e-02,
         -1.0904e-01, -3.9887e-02]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 2.3583e-01, -2.4021e-05,  2.3001e-01,  ..., -2.3059e-01,
         -2.4768e-01, -2.2720e-01],
        [-5.7583e-02, -1.8498e-01, -6.4538e-02,  ..., -1.1611e-01,
          5.7019e-02, -2.7185e-02],
        [-7.6537e-03,  1.4646e-01,  2.8190e-02,  ...,  1.9752e-01,
         -5.0790e-02,  2.2181e-01],
        ...,
        [-2.4915e-02,  1.6899e-01, -1.1586e-01,  ...,  6.1408e-02,
          8.7233e-02,  1.2131e-01],
        [-1.4281e-01, -1.6520e-01, -5.9309e-02,  ...,  1.0933e-01,
          6.6997e-02,  2.4116e-01],
        [-1.7791e-01, -1.9037e-01,  6.4217e-02,  ...,  4.0869e-02,
         -1.0904e-01, -3.9887e-02]], device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.3471],
        [ 0.3294],
        [ 0.2396],
        [-0.1308],
        [-0.3466],
        [-0.1222],
        [-0.3878],
        [ 0.2048],
        [-0.1431],
        [ 0.0553],
        [ 0.1253],
        [-0.0469],
        [ 0.3013],
        [ 0.3829],
        [ 0.2706],
        [ 0.3859],
        [-0.3019],
        [ 0.1273],
        [-0.3464],
        [ 0.1747],
        [-0.1277],
        [ 0.1186],
        [ 0.2636],
        [-0.0559],
        [ 0.4011],
        [-0.4086],
        [ 0.2157],
        [-0.0126],
        [ 0.2338],
        [ 0.2252],
        [-0.2429],
        [ 0.0740]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.3471],
        [ 0.3294],
        [ 0.2396],
        [-0.1308],
        [-0.3466],
        [-0.1222],
        [-0.3878],
        [ 0.2048],
        [-0.1431],
        [ 0.0553],
        [ 0.1253],
        [-0.0469],
        [ 0.3013],
        [ 0.3829],
        [ 0.2706],
        [ 0.3859],
        [-0.3019],
        [ 0.1273],
        [-0.3464],
        [ 0.1747],
        [-0.1277],
        [ 0.1186],
        [ 0.2636],
        [-0.0559],
        [ 0.4011],
        [-0.4086],
        [ 0.2157],
        [-0.0126],
        [ 0.2338],
        [ 0.2252],
        [-0.2429],
        [ 0.0740]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-112.1198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.9983, device='cuda:0')



h[100].sum tensor(7.4327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5991, device='cuda:0')



h[200].sum tensor(-3.4680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-3.5457, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(9561.9170, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0023, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0023, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0006, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(58225.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(488.3570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.0505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(389.6496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(24.7745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-26.9997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0955],
        [-0.0673],
        [-0.0453],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-4419.3027, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.0955],
        [-0.0673],
        [-0.0453],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0202,  0.0039, -0.0115,  ...,  0.0082, -0.0184,  0.0014],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-497.7766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-132.5777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-135.9059, device='cuda:0')



h[100].sum tensor(-111.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.4176, device='cuda:0')



h[200].sum tensor(52.6849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(54.0075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0148, 0.0000,  ..., 0.0310, 0.0000, 0.0051],
        [0.0000, 0.0122, 0.0000,  ..., 0.0255, 0.0000, 0.0042],
        [0.0000, 0.0029, 0.0000,  ..., 0.0060, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(99153.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2644, 0.0000, 0.0590,  ..., 0.0633, 0.0974, 0.0000],
        [0.2266, 0.0000, 0.0506,  ..., 0.0542, 0.0835, 0.0000],
        [0.1819, 0.0000, 0.0406,  ..., 0.0435, 0.0670, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(495907.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13089.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(916.8929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-328.1243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6146.1431, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.5259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.0122e-01],
        [-8.6696e-01],
        [-9.5666e-01],
        ...,
        [-1.0438e-05],
        [-1.7355e-05],
        [-2.4808e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-55619.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0955],
        [-0.0673],
        [-0.0453],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./TrainingBha.py", line 52, in <module>
    checkpoint_load(torch.load(F"{checkpoint_dir_path}/checkpoint_dir/{TraEvN}{6}{startmesh}saved_checkpoint.tar"))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/serialization.py", line 594, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar'

real	1m47.345s
user	0m20.243s
sys	0m9.400s
