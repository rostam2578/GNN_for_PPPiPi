0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 11:43:10 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b7d9660b8e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.307s
user	0m2.713s
sys	0m1.221s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[11:43:34] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.8425],
        [-0.0201],
        [ 1.7804],
        ...,
        [ 0.6848],
        [ 0.5322],
        [ 1.3021]], device='cuda:0', requires_grad=True) 
node features sum: tensor(39.1500, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0560, -0.0938,  0.0722, -0.0968,  0.1464, -0.1517,  0.1027, -0.1086,
         -0.1091, -0.1142,  0.1392,  0.0209, -0.0126,  0.1205,  0.0406,  0.0334,
         -0.0721, -0.1270, -0.1347, -0.1085, -0.0232,  0.0833,  0.0385,  0.1036,
          0.0411,  0.0931,  0.0283,  0.1253, -0.0334, -0.1108,  0.0163, -0.1433,
          0.1333, -0.1453,  0.0147, -0.1204, -0.1495,  0.0942, -0.0034, -0.1346,
          0.1349, -0.1038,  0.0654, -0.0063, -0.0399, -0.1471, -0.0662,  0.0188,
          0.1077, -0.0832,  0.1114, -0.0135,  0.0367,  0.1374, -0.0557, -0.0059,
         -0.1180,  0.1061, -0.0225,  0.0713, -0.1426,  0.1100, -0.0074,  0.0310,
          0.1206,  0.0394, -0.0026, -0.0360, -0.0634,  0.0319, -0.0224,  0.0627,
          0.0477, -0.0835,  0.1130,  0.0858,  0.1143, -0.0335, -0.1025, -0.0232,
         -0.1237,  0.0518, -0.1353, -0.1430,  0.0278, -0.0698, -0.1280,  0.0149,
         -0.0767, -0.1498,  0.1282, -0.0127, -0.0669,  0.1052,  0.0488,  0.1375,
         -0.1285, -0.0348,  0.1098,  0.1147,  0.1173,  0.0912,  0.0957, -0.0564,
         -0.1416, -0.0951,  0.1503, -0.1411,  0.0677,  0.0667,  0.0446,  0.0815,
          0.1121,  0.0624,  0.0666, -0.0016, -0.1440, -0.0340, -0.0863, -0.0458,
         -0.0531,  0.1169,  0.1357,  0.0217,  0.1467, -0.1505,  0.0668,  0.1267,
          0.0179,  0.0940,  0.0171,  0.0865, -0.1346, -0.0611, -0.1038, -0.0260,
          0.0790,  0.0574, -0.0610,  0.0373, -0.0666,  0.0217,  0.1123,  0.1472,
         -0.0894, -0.0360,  0.1445, -0.0163, -0.0945,  0.1242, -0.0899, -0.0734,
         -0.1388,  0.1269, -0.1203, -0.0810,  0.0884, -0.1466, -0.0203,  0.0211,
         -0.0015, -0.1404, -0.0177, -0.0023,  0.0623,  0.0924,  0.1406,  0.1120,
         -0.0373, -0.1147,  0.1155,  0.0980, -0.0305, -0.1118, -0.0669,  0.1497,
         -0.0098,  0.0800, -0.0557,  0.0422,  0.1221, -0.1354, -0.1268, -0.1405,
         -0.1208,  0.0274, -0.0088,  0.0207, -0.0109,  0.0287, -0.1342,  0.0801,
         -0.1327,  0.0229,  0.0711, -0.0407,  0.0557,  0.0664, -0.1013,  0.0395,
         -0.1379,  0.0057, -0.0874,  0.1381, -0.0236,  0.0399, -0.1147, -0.1165,
         -0.1271,  0.1078, -0.0298, -0.1286,  0.1292, -0.0506, -0.1504,  0.1092,
          0.0593, -0.0949,  0.1250, -0.0762, -0.0137, -0.1271,  0.1513, -0.1275,
          0.0176, -0.0487,  0.0084, -0.0298, -0.1313,  0.0842, -0.0646,  0.1038,
          0.1488, -0.0627, -0.1259,  0.0748,  0.0179,  0.0408,  0.0349, -0.0509,
          0.0813, -0.0707,  0.0325,  0.0940, -0.0524, -0.1292, -0.1523, -0.1344,
          0.0684, -0.1026, -0.1454, -0.1155, -0.0494,  0.1004,  0.0580,  0.1440]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0560, -0.0938,  0.0722, -0.0968,  0.1464, -0.1517,  0.1027, -0.1086,
         -0.1091, -0.1142,  0.1392,  0.0209, -0.0126,  0.1205,  0.0406,  0.0334,
         -0.0721, -0.1270, -0.1347, -0.1085, -0.0232,  0.0833,  0.0385,  0.1036,
          0.0411,  0.0931,  0.0283,  0.1253, -0.0334, -0.1108,  0.0163, -0.1433,
          0.1333, -0.1453,  0.0147, -0.1204, -0.1495,  0.0942, -0.0034, -0.1346,
          0.1349, -0.1038,  0.0654, -0.0063, -0.0399, -0.1471, -0.0662,  0.0188,
          0.1077, -0.0832,  0.1114, -0.0135,  0.0367,  0.1374, -0.0557, -0.0059,
         -0.1180,  0.1061, -0.0225,  0.0713, -0.1426,  0.1100, -0.0074,  0.0310,
          0.1206,  0.0394, -0.0026, -0.0360, -0.0634,  0.0319, -0.0224,  0.0627,
          0.0477, -0.0835,  0.1130,  0.0858,  0.1143, -0.0335, -0.1025, -0.0232,
         -0.1237,  0.0518, -0.1353, -0.1430,  0.0278, -0.0698, -0.1280,  0.0149,
         -0.0767, -0.1498,  0.1282, -0.0127, -0.0669,  0.1052,  0.0488,  0.1375,
         -0.1285, -0.0348,  0.1098,  0.1147,  0.1173,  0.0912,  0.0957, -0.0564,
         -0.1416, -0.0951,  0.1503, -0.1411,  0.0677,  0.0667,  0.0446,  0.0815,
          0.1121,  0.0624,  0.0666, -0.0016, -0.1440, -0.0340, -0.0863, -0.0458,
         -0.0531,  0.1169,  0.1357,  0.0217,  0.1467, -0.1505,  0.0668,  0.1267,
          0.0179,  0.0940,  0.0171,  0.0865, -0.1346, -0.0611, -0.1038, -0.0260,
          0.0790,  0.0574, -0.0610,  0.0373, -0.0666,  0.0217,  0.1123,  0.1472,
         -0.0894, -0.0360,  0.1445, -0.0163, -0.0945,  0.1242, -0.0899, -0.0734,
         -0.1388,  0.1269, -0.1203, -0.0810,  0.0884, -0.1466, -0.0203,  0.0211,
         -0.0015, -0.1404, -0.0177, -0.0023,  0.0623,  0.0924,  0.1406,  0.1120,
         -0.0373, -0.1147,  0.1155,  0.0980, -0.0305, -0.1118, -0.0669,  0.1497,
         -0.0098,  0.0800, -0.0557,  0.0422,  0.1221, -0.1354, -0.1268, -0.1405,
         -0.1208,  0.0274, -0.0088,  0.0207, -0.0109,  0.0287, -0.1342,  0.0801,
         -0.1327,  0.0229,  0.0711, -0.0407,  0.0557,  0.0664, -0.1013,  0.0395,
         -0.1379,  0.0057, -0.0874,  0.1381, -0.0236,  0.0399, -0.1147, -0.1165,
         -0.1271,  0.1078, -0.0298, -0.1286,  0.1292, -0.0506, -0.1504,  0.1092,
          0.0593, -0.0949,  0.1250, -0.0762, -0.0137, -0.1271,  0.1513, -0.1275,
          0.0176, -0.0487,  0.0084, -0.0298, -0.1313,  0.0842, -0.0646,  0.1038,
          0.1488, -0.0627, -0.1259,  0.0748,  0.0179,  0.0408,  0.0349, -0.0509,
          0.0813, -0.0707,  0.0325,  0.0940, -0.0524, -0.1292, -0.1523, -0.1344,
          0.0684, -0.1026, -0.1454, -0.1155, -0.0494,  0.1004,  0.0580,  0.1440]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0610, -0.0304,  0.0679,  ...,  0.0551,  0.1038, -0.1003],
        [-0.0480, -0.0202,  0.0764,  ..., -0.0110,  0.1052,  0.1198],
        [-0.1107,  0.1239, -0.0437,  ...,  0.1112, -0.0160,  0.0505],
        ...,
        [-0.0162, -0.0754,  0.0129,  ...,  0.0791, -0.0514,  0.0341],
        [ 0.0030,  0.1171, -0.0194,  ...,  0.0532, -0.1057,  0.0693],
        [ 0.0836, -0.1040, -0.0240,  ...,  0.0600,  0.1224, -0.1245]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0610, -0.0304,  0.0679,  ...,  0.0551,  0.1038, -0.1003],
        [-0.0480, -0.0202,  0.0764,  ..., -0.0110,  0.1052,  0.1198],
        [-0.1107,  0.1239, -0.0437,  ...,  0.1112, -0.0160,  0.0505],
        ...,
        [-0.0162, -0.0754,  0.0129,  ...,  0.0791, -0.0514,  0.0341],
        [ 0.0030,  0.1171, -0.0194,  ...,  0.0532, -0.1057,  0.0693],
        [ 0.0836, -0.1040, -0.0240,  ...,  0.0600,  0.1224, -0.1245]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0649,  0.1007, -0.0144,  ...,  0.0807, -0.0812,  0.1217],
        [ 0.0636,  0.0004, -0.1596,  ...,  0.0404,  0.0132, -0.1734],
        [-0.1090,  0.0812, -0.0972,  ...,  0.1744,  0.1750,  0.0355],
        ...,
        [ 0.0449,  0.0709,  0.0759,  ...,  0.0920, -0.0495, -0.0534],
        [-0.0334, -0.0349,  0.1205,  ..., -0.1517, -0.0956, -0.1087],
        [ 0.1229,  0.0269,  0.0359,  ...,  0.1548, -0.1610,  0.0945]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0649,  0.1007, -0.0144,  ...,  0.0807, -0.0812,  0.1217],
        [ 0.0636,  0.0004, -0.1596,  ...,  0.0404,  0.0132, -0.1734],
        [-0.1090,  0.0812, -0.0972,  ...,  0.1744,  0.1750,  0.0355],
        ...,
        [ 0.0449,  0.0709,  0.0759,  ...,  0.0920, -0.0495, -0.0534],
        [-0.0334, -0.0349,  0.1205,  ..., -0.1517, -0.0956, -0.1087],
        [ 0.1229,  0.0269,  0.0359,  ...,  0.1548, -0.1610,  0.0945]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.2087, -0.1857,  0.1377,  ...,  0.1698, -0.1412, -0.0006],
        [-0.1605,  0.2358, -0.0340,  ..., -0.1124,  0.2402, -0.2201],
        [ 0.0036, -0.2474, -0.0723,  ..., -0.2149, -0.1286,  0.2471],
        ...,
        [-0.0266,  0.2385,  0.0872,  ..., -0.1009, -0.1769,  0.1784],
        [ 0.0829, -0.0159, -0.1416,  ...,  0.1007,  0.0817, -0.0595],
        [-0.1557, -0.2108, -0.0572,  ..., -0.2228,  0.0209, -0.0499]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2087, -0.1857,  0.1377,  ...,  0.1698, -0.1412, -0.0006],
        [-0.1605,  0.2358, -0.0340,  ..., -0.1124,  0.2402, -0.2201],
        [ 0.0036, -0.2474, -0.0723,  ..., -0.2149, -0.1286,  0.2471],
        ...,
        [-0.0266,  0.2385,  0.0872,  ..., -0.1009, -0.1769,  0.1784],
        [ 0.0829, -0.0159, -0.1416,  ...,  0.1007,  0.0817, -0.0595],
        [-0.1557, -0.2108, -0.0572,  ..., -0.2228,  0.0209, -0.0499]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.2226],
        [-0.3909],
        [-0.0962],
        [-0.3133],
        [ 0.1351],
        [ 0.3841],
        [ 0.2687],
        [ 0.2852],
        [ 0.3329],
        [-0.3036],
        [ 0.4126],
        [ 0.4118],
        [ 0.1853],
        [-0.3308],
        [ 0.2742],
        [ 0.2766],
        [ 0.0439],
        [ 0.3976],
        [-0.2172],
        [ 0.1243],
        [-0.3082],
        [ 0.1458],
        [ 0.0326],
        [-0.3012],
        [ 0.2294],
        [ 0.0822],
        [-0.3791],
        [ 0.2708],
        [ 0.3669],
        [-0.3472],
        [-0.2823],
        [ 0.1583]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.2226],
        [-0.3909],
        [-0.0962],
        [-0.3133],
        [ 0.1351],
        [ 0.3841],
        [ 0.2687],
        [ 0.2852],
        [ 0.3329],
        [-0.3036],
        [ 0.4126],
        [ 0.4118],
        [ 0.1853],
        [-0.3308],
        [ 0.2742],
        [ 0.2766],
        [ 0.0439],
        [ 0.3976],
        [-0.2172],
        [ 0.1243],
        [-0.3082],
        [ 0.1458],
        [ 0.0326],
        [-0.3012],
        [ 0.2294],
        [ 0.0822],
        [-0.3791],
        [ 0.2708],
        [ 0.3669],
        [-0.3472],
        [-0.2823],
        [ 0.1583]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-19.2151, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1041, device='cuda:0')



h[100].sum tensor(-11.8596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1250, device='cuda:0')



h[200].sum tensor(5.7930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.9227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10708.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0020, 0.0056,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0026,  ..., 0.0087, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0007,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(87422.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.4830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1188.1804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(75.5414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(672.7518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.7617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2264],
        [-0.1594],
        [-0.1073],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-10473.4619, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.2264],
        [-0.1594],
        [-0.1073],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0117, -0.0188, -0.0002,  ...,  0.0168, -0.0049,  0.0125],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-1437.1863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-76.9827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-78.9153, device='cuda:0')



h[100].sum tensor(30.6236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.3923, device='cuda:0')



h[200].sum tensor(-82.7742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-84.8522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0471],
        [0.0000, 0.0000, 0.0000,  ..., 0.0523, 0.0000, 0.0387],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(94077.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.2963,  ..., 0.2504, 0.0000, 0.1497],
        [0.0000, 0.0000, 0.2538,  ..., 0.2145, 0.0000, 0.1283],
        [0.0000, 0.0000, 0.2038,  ..., 0.1723, 0.0000, 0.1030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(470367.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-138.5972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-584.3229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.3404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2494e+00],
        [-1.3519e+00],
        [-1.4918e+00],
        ...,
        [-1.6278e-05],
        [-2.7063e-05],
        [-3.8685e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-86734.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.2264],
        [-0.1594],
        [-0.1073],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 



Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 38, in <module>
    result1 = net(batcheddglgraph, TraTen[10000:10010].reshape(10 * 6796, 1).to('cpu'))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 37, in forward
    g.ndata['nfet'] = in_fet
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/view.py", line 81, in __setitem__
    self._graph._set_n_repr(self._ntid, self._nodes, {key : val})
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/heterograph.py", line 4113, in _set_n_repr
    raise DGLError('Cannot assign node feature "{}" on device {} to a graph on'
dgl._ffi.base.DGLError: Cannot assign node feature "nfet" on device cpu to a graph on device cuda:0. Call DGLGraph.to() to copy the graph to the same device.

real	0m26.139s
user	0m18.460s
sys	0m5.012s
