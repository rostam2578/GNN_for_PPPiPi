0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 11:58:09 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ad11ae898e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.597s
user	0m2.379s
sys	0m0.962s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[11:58:32] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.8426],
        [ 0.9391],
        [ 1.5550],
        ...,
        [ 0.0900],
        [ 2.1021],
        [ 1.3246]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-46.8744, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.1068,  0.0084,  0.0843,  0.1482,  0.1261,  0.0310,  0.0773,  0.0288,
         -0.0380,  0.0363,  0.1278, -0.1420,  0.0879,  0.0826, -0.0865,  0.0106,
         -0.1418, -0.1022,  0.1396, -0.0369,  0.0968, -0.0050, -0.1262,  0.0792,
          0.0829, -0.0395,  0.0639, -0.0343, -0.0433,  0.0678,  0.0434, -0.1451,
          0.0087,  0.0995, -0.1312,  0.1484, -0.0415,  0.1049,  0.0978,  0.0489,
          0.0206,  0.0301,  0.0496,  0.0403,  0.0441, -0.0600, -0.1190,  0.0937,
          0.1480,  0.0476,  0.0349,  0.0067,  0.0617, -0.0240,  0.0915,  0.0068,
         -0.0368,  0.1409,  0.0704, -0.1216, -0.0765, -0.1118, -0.0899, -0.0309,
          0.0426,  0.0247,  0.1032, -0.1250, -0.0734,  0.1019,  0.0204,  0.0729,
         -0.0401, -0.1199,  0.0006, -0.1381,  0.0824,  0.0475,  0.0290,  0.0773,
          0.1381,  0.0869,  0.0056,  0.0551, -0.0036,  0.1462, -0.0593, -0.0345,
          0.0460, -0.0998,  0.0754, -0.0993,  0.1274, -0.1310,  0.0855, -0.0265,
         -0.0255, -0.0686,  0.0471,  0.1165, -0.1315, -0.0527, -0.0841,  0.1198,
          0.0558,  0.1065, -0.1320,  0.1100, -0.0419,  0.0682,  0.0986,  0.1206,
          0.1479,  0.1132, -0.0519, -0.0520, -0.1409, -0.1232,  0.0403,  0.0649,
          0.0052, -0.1296, -0.0432, -0.0006, -0.0896,  0.1376, -0.0346,  0.0810,
          0.1428, -0.0751, -0.0614, -0.1492, -0.0410,  0.0045, -0.1281, -0.0184,
         -0.0767, -0.1399,  0.0418, -0.1460,  0.0483,  0.1109,  0.0731,  0.0411,
          0.1109,  0.0162, -0.0610, -0.0223,  0.0614, -0.0207, -0.1479,  0.0986,
          0.1347,  0.1282, -0.1066,  0.0283,  0.0744,  0.0065, -0.1329,  0.0718,
         -0.1473, -0.1267,  0.0788,  0.0482,  0.0118,  0.1374,  0.0507, -0.1435,
         -0.1341,  0.0603,  0.0838, -0.0300,  0.1214, -0.1422,  0.0236,  0.0876,
          0.0211, -0.1015, -0.1506, -0.0619,  0.0194,  0.1086, -0.0198, -0.1011,
          0.0251,  0.0672,  0.0207, -0.0219, -0.0091,  0.1394, -0.1233, -0.0252,
         -0.0427, -0.0760, -0.0912, -0.0137,  0.1380, -0.0675,  0.0460, -0.0104,
          0.0718, -0.0922, -0.1135, -0.0877,  0.1244, -0.0428, -0.0417, -0.0738,
          0.0435, -0.0143, -0.1495, -0.0788,  0.1425, -0.0099,  0.0800, -0.0160,
          0.0657, -0.0829, -0.0407, -0.0530, -0.0913,  0.0118, -0.0323,  0.0314,
         -0.0527,  0.0804,  0.1520, -0.1016, -0.0976,  0.1465,  0.0512,  0.1385,
          0.1516, -0.0505, -0.0630,  0.1522,  0.1079, -0.0999, -0.0107, -0.1285,
          0.1110,  0.1231,  0.1142, -0.0788, -0.0367, -0.0524, -0.0315, -0.0053,
          0.0651,  0.0081, -0.1371,  0.1115, -0.1227, -0.0439, -0.0450, -0.0240]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1068,  0.0084,  0.0843,  0.1482,  0.1261,  0.0310,  0.0773,  0.0288,
         -0.0380,  0.0363,  0.1278, -0.1420,  0.0879,  0.0826, -0.0865,  0.0106,
         -0.1418, -0.1022,  0.1396, -0.0369,  0.0968, -0.0050, -0.1262,  0.0792,
          0.0829, -0.0395,  0.0639, -0.0343, -0.0433,  0.0678,  0.0434, -0.1451,
          0.0087,  0.0995, -0.1312,  0.1484, -0.0415,  0.1049,  0.0978,  0.0489,
          0.0206,  0.0301,  0.0496,  0.0403,  0.0441, -0.0600, -0.1190,  0.0937,
          0.1480,  0.0476,  0.0349,  0.0067,  0.0617, -0.0240,  0.0915,  0.0068,
         -0.0368,  0.1409,  0.0704, -0.1216, -0.0765, -0.1118, -0.0899, -0.0309,
          0.0426,  0.0247,  0.1032, -0.1250, -0.0734,  0.1019,  0.0204,  0.0729,
         -0.0401, -0.1199,  0.0006, -0.1381,  0.0824,  0.0475,  0.0290,  0.0773,
          0.1381,  0.0869,  0.0056,  0.0551, -0.0036,  0.1462, -0.0593, -0.0345,
          0.0460, -0.0998,  0.0754, -0.0993,  0.1274, -0.1310,  0.0855, -0.0265,
         -0.0255, -0.0686,  0.0471,  0.1165, -0.1315, -0.0527, -0.0841,  0.1198,
          0.0558,  0.1065, -0.1320,  0.1100, -0.0419,  0.0682,  0.0986,  0.1206,
          0.1479,  0.1132, -0.0519, -0.0520, -0.1409, -0.1232,  0.0403,  0.0649,
          0.0052, -0.1296, -0.0432, -0.0006, -0.0896,  0.1376, -0.0346,  0.0810,
          0.1428, -0.0751, -0.0614, -0.1492, -0.0410,  0.0045, -0.1281, -0.0184,
         -0.0767, -0.1399,  0.0418, -0.1460,  0.0483,  0.1109,  0.0731,  0.0411,
          0.1109,  0.0162, -0.0610, -0.0223,  0.0614, -0.0207, -0.1479,  0.0986,
          0.1347,  0.1282, -0.1066,  0.0283,  0.0744,  0.0065, -0.1329,  0.0718,
         -0.1473, -0.1267,  0.0788,  0.0482,  0.0118,  0.1374,  0.0507, -0.1435,
         -0.1341,  0.0603,  0.0838, -0.0300,  0.1214, -0.1422,  0.0236,  0.0876,
          0.0211, -0.1015, -0.1506, -0.0619,  0.0194,  0.1086, -0.0198, -0.1011,
          0.0251,  0.0672,  0.0207, -0.0219, -0.0091,  0.1394, -0.1233, -0.0252,
         -0.0427, -0.0760, -0.0912, -0.0137,  0.1380, -0.0675,  0.0460, -0.0104,
          0.0718, -0.0922, -0.1135, -0.0877,  0.1244, -0.0428, -0.0417, -0.0738,
          0.0435, -0.0143, -0.1495, -0.0788,  0.1425, -0.0099,  0.0800, -0.0160,
          0.0657, -0.0829, -0.0407, -0.0530, -0.0913,  0.0118, -0.0323,  0.0314,
         -0.0527,  0.0804,  0.1520, -0.1016, -0.0976,  0.1465,  0.0512,  0.1385,
          0.1516, -0.0505, -0.0630,  0.1522,  0.1079, -0.0999, -0.0107, -0.1285,
          0.1110,  0.1231,  0.1142, -0.0788, -0.0367, -0.0524, -0.0315, -0.0053,
          0.0651,  0.0081, -0.1371,  0.1115, -0.1227, -0.0439, -0.0450, -0.0240]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[ 0.0177, -0.0900, -0.1062,  ..., -0.0093,  0.0192, -0.0836],
        [ 0.1165, -0.1057, -0.0404,  ...,  0.0060, -0.0681,  0.0950],
        [-0.1035,  0.0633, -0.0666,  ..., -0.0892, -0.0551,  0.0375],
        ...,
        [ 0.1131,  0.0767,  0.0613,  ...,  0.0975, -0.0783,  0.1000],
        [-0.1247,  0.0009,  0.0551,  ..., -0.0164, -0.0892, -0.0481],
        [-0.0631, -0.0257,  0.0945,  ...,  0.0725, -0.0999,  0.0335]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0177, -0.0900, -0.1062,  ..., -0.0093,  0.0192, -0.0836],
        [ 0.1165, -0.1057, -0.0404,  ...,  0.0060, -0.0681,  0.0950],
        [-0.1035,  0.0633, -0.0666,  ..., -0.0892, -0.0551,  0.0375],
        ...,
        [ 0.1131,  0.0767,  0.0613,  ...,  0.0975, -0.0783,  0.1000],
        [-0.1247,  0.0009,  0.0551,  ..., -0.0164, -0.0892, -0.0481],
        [-0.0631, -0.0257,  0.0945,  ...,  0.0725, -0.0999,  0.0335]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.1589, -0.1237,  0.1729,  ..., -0.0547, -0.1400,  0.1170],
        [ 0.0757, -0.1187,  0.0673,  ...,  0.0338,  0.1214, -0.0329],
        [ 0.0699, -0.0332, -0.0014,  ...,  0.0500,  0.1533,  0.1358],
        ...,
        [ 0.0145, -0.0351,  0.0084,  ..., -0.0422, -0.1414, -0.1062],
        [ 0.1475,  0.1462,  0.1530,  ..., -0.0342,  0.0657, -0.1110],
        [ 0.0922, -0.0988, -0.0461,  ..., -0.1243, -0.0061,  0.1636]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1589, -0.1237,  0.1729,  ..., -0.0547, -0.1400,  0.1170],
        [ 0.0757, -0.1187,  0.0673,  ...,  0.0338,  0.1214, -0.0329],
        [ 0.0699, -0.0332, -0.0014,  ...,  0.0500,  0.1533,  0.1358],
        ...,
        [ 0.0145, -0.0351,  0.0084,  ..., -0.0422, -0.1414, -0.1062],
        [ 0.1475,  0.1462,  0.1530,  ..., -0.0342,  0.0657, -0.1110],
        [ 0.0922, -0.0988, -0.0461,  ..., -0.1243, -0.0061,  0.1636]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.1100,  0.1894, -0.1024,  ...,  0.0500, -0.1391,  0.0251],
        [ 0.0299,  0.2063,  0.2458,  ...,  0.0826, -0.2349,  0.0909],
        [ 0.0136, -0.0128, -0.1208,  ..., -0.1495,  0.1021, -0.0743],
        ...,
        [-0.1422,  0.1976,  0.1261,  ..., -0.0877, -0.0185,  0.1642],
        [ 0.2445,  0.2149, -0.0759,  ...,  0.0453,  0.1402,  0.1329],
        [ 0.1083, -0.2224,  0.1531,  ...,  0.2082,  0.0641,  0.2387]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1100,  0.1894, -0.1024,  ...,  0.0500, -0.1391,  0.0251],
        [ 0.0299,  0.2063,  0.2458,  ...,  0.0826, -0.2349,  0.0909],
        [ 0.0136, -0.0128, -0.1208,  ..., -0.1495,  0.1021, -0.0743],
        ...,
        [-0.1422,  0.1976,  0.1261,  ..., -0.0877, -0.0185,  0.1642],
        [ 0.2445,  0.2149, -0.0759,  ...,  0.0453,  0.1402,  0.1329],
        [ 0.1083, -0.2224,  0.1531,  ...,  0.2082,  0.0641,  0.2387]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.1831],
        [-0.3751],
        [ 0.0885],
        [ 0.1638],
        [-0.0938],
        [ 0.0163],
        [-0.3003],
        [-0.1671],
        [ 0.1555],
        [-0.3878],
        [-0.0885],
        [ 0.4192],
        [ 0.1676],
        [-0.0818],
        [-0.0051],
        [ 0.0160],
        [ 0.2879],
        [-0.2485],
        [-0.2220],
        [ 0.1425],
        [ 0.0531],
        [ 0.2183],
        [ 0.2275],
        [-0.0552],
        [-0.3894],
        [ 0.0462],
        [-0.3052],
        [-0.1741],
        [-0.4203],
        [-0.0305],
        [-0.0114],
        [-0.1035]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1831],
        [-0.3751],
        [ 0.0885],
        [ 0.1638],
        [-0.0938],
        [ 0.0163],
        [-0.3003],
        [-0.1671],
        [ 0.1555],
        [-0.3878],
        [-0.0885],
        [ 0.4192],
        [ 0.1676],
        [-0.0818],
        [-0.0051],
        [ 0.0160],
        [ 0.2879],
        [-0.2485],
        [-0.2220],
        [ 0.1425],
        [ 0.0531],
        [ 0.2183],
        [ 0.2275],
        [-0.0552],
        [-0.3894],
        [ 0.0462],
        [-0.3052],
        [-0.1741],
        [-0.4203],
        [-0.0305],
        [-0.0114],
        [-0.1035]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(48.1582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4253, device='cuda:0')



h[100].sum tensor(7.6046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7748, device='cuda:0')



h[200].sum tensor(4.9773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.0887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(9632.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0070, 0.0000, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        [0.0033, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0009, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(43148.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(713.0300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(45.3364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.0586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1786.1382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(113.5350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1483],
        [-0.1044],
        [-0.0703],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-6860.8682, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.1483],
        [-0.1044],
        [-0.0703],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0159,  0.0095, -0.0198,  ..., -0.0115, -0.0096,  0.0206],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(1850.1841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-104.8597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-107.4921, device='cuda:0')



h[100].sum tensor(16.0270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4293, device='cuda:0')



h[200].sum tensor(56.9245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(58.3535, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0358, 0.0000,  ..., 0.0000, 0.0000, 0.0777],
        [0.0000, 0.0294, 0.0000,  ..., 0.0000, 0.0000, 0.0639],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0150],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(110999.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3399, 0.3231],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2912, 0.2769],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2338, 0.2223],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(702480.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-846.6853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4686.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(328.3042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2534.8430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(177.5182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9950e+00],
        [-5.4047e+00],
        [-5.9638e+00],
        ...,
        [-6.5074e-05],
        [-1.0820e-04],
        [-1.5467e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-346727.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.1483],
        [-0.1044],
        [-0.0703],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 



Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 38, in <module>
    result1 = net(batcheddglgraph, TraTen[10000:10010].reshape(10 * 6796, 1).to('cuda'))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 47, in forward
    h = self.conv1(g, in_fet)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 437, in forward
    rst = th.matmul(rst, weight)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)

real	0m24.561s
user	0m17.868s
sys	0m4.648s
