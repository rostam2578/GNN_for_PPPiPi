0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 11:26:51 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   20C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b7c6c8398e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.179s
user	0m2.608s
sys	0m1.203s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[11:27:15] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.1504],
        [-0.4669],
        [ 0.5465],
        ...,
        [-0.2901],
        [-3.2568],
        [ 0.6823]], device='cuda:0', requires_grad=True) 
node features sum: tensor(91.8002, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.1429,  0.0368,  0.0569, -0.0358, -0.0795, -0.0976, -0.1213,  0.0905,
          0.1099,  0.0921,  0.1454,  0.0660, -0.1348, -0.1087,  0.0227,  0.0877,
         -0.0760, -0.1374,  0.0747, -0.0145, -0.0419,  0.0245, -0.0714,  0.1104,
          0.0363, -0.1289,  0.0046, -0.0822,  0.0343,  0.1500,  0.0834, -0.0460,
         -0.0066, -0.0444, -0.0696, -0.1323, -0.0033,  0.0887,  0.0475,  0.1212,
         -0.0939, -0.0409,  0.1451,  0.0668, -0.0144,  0.0890, -0.0088, -0.0797,
          0.1408,  0.0249,  0.1492,  0.1452, -0.0044, -0.0018, -0.1252,  0.1079,
          0.1388, -0.0929,  0.0129, -0.0454, -0.1508, -0.0453, -0.0683, -0.1149,
         -0.0625,  0.0115, -0.0558,  0.0448,  0.1461, -0.0148, -0.0476, -0.0232,
         -0.0761, -0.1230, -0.1494,  0.0157,  0.0295,  0.0896, -0.1249, -0.1146,
          0.1482,  0.0256,  0.1356,  0.1413,  0.0966,  0.0712,  0.0163, -0.0240,
          0.0505, -0.0774,  0.0643, -0.0471,  0.1214,  0.0970, -0.0800,  0.0870,
         -0.0004, -0.0951,  0.0376, -0.0985,  0.0516,  0.1145,  0.0240,  0.1085,
         -0.0334,  0.0220, -0.0904,  0.0373, -0.1321, -0.1009, -0.0713,  0.1159,
         -0.0542, -0.0859, -0.1291, -0.0379, -0.1388,  0.0904, -0.0174,  0.0566,
         -0.1328, -0.0761, -0.1159,  0.0891, -0.1003,  0.0923, -0.0369, -0.1199,
         -0.1126, -0.0353, -0.0141, -0.0548, -0.0352, -0.0526, -0.1435,  0.1131,
         -0.0269, -0.1233, -0.1236,  0.1153,  0.1087, -0.1203,  0.1055,  0.1451,
          0.0431, -0.1426,  0.0560,  0.0026, -0.0356,  0.1008,  0.0681,  0.0658,
         -0.0794,  0.1424,  0.0254, -0.0975, -0.1495, -0.0962,  0.1377,  0.0299,
          0.1223,  0.0576, -0.0931,  0.0133, -0.1459, -0.0499, -0.0264,  0.1053,
         -0.1412,  0.0860, -0.0083,  0.0793, -0.0885,  0.0993, -0.0099,  0.1438,
         -0.1411, -0.0934,  0.0998,  0.1498, -0.0174, -0.1028, -0.0423, -0.0197,
          0.0941,  0.0077, -0.0431, -0.1344,  0.0654,  0.0233, -0.1388,  0.1218,
          0.0897,  0.1503, -0.0189,  0.0048,  0.1037, -0.1321, -0.0949,  0.1194,
          0.0130,  0.1042, -0.0054, -0.1330,  0.0968,  0.0911, -0.0232, -0.0824,
         -0.1297,  0.0950,  0.0275, -0.0605, -0.0505,  0.1337,  0.1433, -0.0513,
         -0.0464,  0.0803, -0.0240,  0.0373, -0.0620,  0.1347, -0.1331, -0.1404,
         -0.1339, -0.0536,  0.1065, -0.0665,  0.1519, -0.0853, -0.1285, -0.0943,
         -0.0397,  0.1389, -0.1118,  0.1384,  0.1127, -0.0250,  0.0124,  0.1165,
         -0.1285,  0.0164, -0.1453,  0.0802, -0.0444, -0.0565,  0.0557,  0.0195,
         -0.1152,  0.0989,  0.1224, -0.1514, -0.0881, -0.0126,  0.0354,  0.0665]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1429,  0.0368,  0.0569, -0.0358, -0.0795, -0.0976, -0.1213,  0.0905,
          0.1099,  0.0921,  0.1454,  0.0660, -0.1348, -0.1087,  0.0227,  0.0877,
         -0.0760, -0.1374,  0.0747, -0.0145, -0.0419,  0.0245, -0.0714,  0.1104,
          0.0363, -0.1289,  0.0046, -0.0822,  0.0343,  0.1500,  0.0834, -0.0460,
         -0.0066, -0.0444, -0.0696, -0.1323, -0.0033,  0.0887,  0.0475,  0.1212,
         -0.0939, -0.0409,  0.1451,  0.0668, -0.0144,  0.0890, -0.0088, -0.0797,
          0.1408,  0.0249,  0.1492,  0.1452, -0.0044, -0.0018, -0.1252,  0.1079,
          0.1388, -0.0929,  0.0129, -0.0454, -0.1508, -0.0453, -0.0683, -0.1149,
         -0.0625,  0.0115, -0.0558,  0.0448,  0.1461, -0.0148, -0.0476, -0.0232,
         -0.0761, -0.1230, -0.1494,  0.0157,  0.0295,  0.0896, -0.1249, -0.1146,
          0.1482,  0.0256,  0.1356,  0.1413,  0.0966,  0.0712,  0.0163, -0.0240,
          0.0505, -0.0774,  0.0643, -0.0471,  0.1214,  0.0970, -0.0800,  0.0870,
         -0.0004, -0.0951,  0.0376, -0.0985,  0.0516,  0.1145,  0.0240,  0.1085,
         -0.0334,  0.0220, -0.0904,  0.0373, -0.1321, -0.1009, -0.0713,  0.1159,
         -0.0542, -0.0859, -0.1291, -0.0379, -0.1388,  0.0904, -0.0174,  0.0566,
         -0.1328, -0.0761, -0.1159,  0.0891, -0.1003,  0.0923, -0.0369, -0.1199,
         -0.1126, -0.0353, -0.0141, -0.0548, -0.0352, -0.0526, -0.1435,  0.1131,
         -0.0269, -0.1233, -0.1236,  0.1153,  0.1087, -0.1203,  0.1055,  0.1451,
          0.0431, -0.1426,  0.0560,  0.0026, -0.0356,  0.1008,  0.0681,  0.0658,
         -0.0794,  0.1424,  0.0254, -0.0975, -0.1495, -0.0962,  0.1377,  0.0299,
          0.1223,  0.0576, -0.0931,  0.0133, -0.1459, -0.0499, -0.0264,  0.1053,
         -0.1412,  0.0860, -0.0083,  0.0793, -0.0885,  0.0993, -0.0099,  0.1438,
         -0.1411, -0.0934,  0.0998,  0.1498, -0.0174, -0.1028, -0.0423, -0.0197,
          0.0941,  0.0077, -0.0431, -0.1344,  0.0654,  0.0233, -0.1388,  0.1218,
          0.0897,  0.1503, -0.0189,  0.0048,  0.1037, -0.1321, -0.0949,  0.1194,
          0.0130,  0.1042, -0.0054, -0.1330,  0.0968,  0.0911, -0.0232, -0.0824,
         -0.1297,  0.0950,  0.0275, -0.0605, -0.0505,  0.1337,  0.1433, -0.0513,
         -0.0464,  0.0803, -0.0240,  0.0373, -0.0620,  0.1347, -0.1331, -0.1404,
         -0.1339, -0.0536,  0.1065, -0.0665,  0.1519, -0.0853, -0.1285, -0.0943,
         -0.0397,  0.1389, -0.1118,  0.1384,  0.1127, -0.0250,  0.0124,  0.1165,
         -0.1285,  0.0164, -0.1453,  0.0802, -0.0444, -0.0565,  0.0557,  0.0195,
         -0.1152,  0.0989,  0.1224, -0.1514, -0.0881, -0.0126,  0.0354,  0.0665]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0555, -0.0563,  0.0789,  ..., -0.0359, -0.0315,  0.0772],
        [ 0.0310,  0.0142, -0.0710,  ...,  0.0649,  0.0812, -0.1134],
        [ 0.1114,  0.0061,  0.1051,  ...,  0.0832, -0.1036, -0.0625],
        ...,
        [-0.0986,  0.0371,  0.1129,  ..., -0.1098,  0.0007,  0.0058],
        [-0.0228, -0.0444, -0.1197,  ..., -0.0908, -0.0421,  0.0753],
        [ 0.0999, -0.0694, -0.0339,  ..., -0.0384,  0.0095,  0.0569]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0555, -0.0563,  0.0789,  ..., -0.0359, -0.0315,  0.0772],
        [ 0.0310,  0.0142, -0.0710,  ...,  0.0649,  0.0812, -0.1134],
        [ 0.1114,  0.0061,  0.1051,  ...,  0.0832, -0.1036, -0.0625],
        ...,
        [-0.0986,  0.0371,  0.1129,  ..., -0.1098,  0.0007,  0.0058],
        [-0.0228, -0.0444, -0.1197,  ..., -0.0908, -0.0421,  0.0753],
        [ 0.0999, -0.0694, -0.0339,  ..., -0.0384,  0.0095,  0.0569]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.1550, -0.0933, -0.1012,  ..., -0.0762, -0.1463,  0.0056],
        [-0.1167,  0.0563,  0.1312,  ...,  0.1003, -0.1648,  0.0420],
        [-0.1456,  0.0896,  0.0358,  ..., -0.0519, -0.1155,  0.0763],
        ...,
        [-0.1438, -0.0694, -0.0337,  ..., -0.0256,  0.1284, -0.0628],
        [-0.1734, -0.1296,  0.1411,  ...,  0.1616,  0.0790,  0.0154],
        [ 0.0914, -0.0326,  0.1670,  ...,  0.0534,  0.0535,  0.1368]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1550, -0.0933, -0.1012,  ..., -0.0762, -0.1463,  0.0056],
        [-0.1167,  0.0563,  0.1312,  ...,  0.1003, -0.1648,  0.0420],
        [-0.1456,  0.0896,  0.0358,  ..., -0.0519, -0.1155,  0.0763],
        ...,
        [-0.1438, -0.0694, -0.0337,  ..., -0.0256,  0.1284, -0.0628],
        [-0.1734, -0.1296,  0.1411,  ...,  0.1616,  0.0790,  0.0154],
        [ 0.0914, -0.0326,  0.1670,  ...,  0.0534,  0.0535,  0.1368]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.2185, -0.1594,  0.2151,  ...,  0.1681, -0.1173,  0.0154],
        [-0.1838,  0.1451,  0.2170,  ...,  0.0233, -0.2319,  0.0706],
        [-0.0283, -0.2486,  0.1372,  ..., -0.2106,  0.1290, -0.0235],
        ...,
        [ 0.1508,  0.0865, -0.1963,  ...,  0.2467,  0.2032,  0.2271],
        [-0.2079,  0.0848,  0.0921,  ..., -0.1417, -0.2461,  0.1075],
        [ 0.1736, -0.2119, -0.0882,  ..., -0.0667, -0.0141,  0.0660]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2185, -0.1594,  0.2151,  ...,  0.1681, -0.1173,  0.0154],
        [-0.1838,  0.1451,  0.2170,  ...,  0.0233, -0.2319,  0.0706],
        [-0.0283, -0.2486,  0.1372,  ..., -0.2106,  0.1290, -0.0235],
        ...,
        [ 0.1508,  0.0865, -0.1963,  ...,  0.2467,  0.2032,  0.2271],
        [-0.2079,  0.0848,  0.0921,  ..., -0.1417, -0.2461,  0.1075],
        [ 0.1736, -0.2119, -0.0882,  ..., -0.0667, -0.0141,  0.0660]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.0584],
        [-0.0088],
        [-0.2413],
        [-0.4172],
        [-0.2111],
        [ 0.3306],
        [-0.0073],
        [-0.0940],
        [ 0.1721],
        [ 0.2206],
        [-0.3702],
        [ 0.0229],
        [-0.4123],
        [-0.0761],
        [-0.2990],
        [-0.3256],
        [ 0.3670],
        [ 0.2390],
        [ 0.0743],
        [-0.2406],
        [ 0.3439],
        [-0.2174],
        [ 0.2052],
        [-0.3807],
        [-0.0349],
        [-0.2104],
        [-0.3856],
        [-0.3625],
        [-0.0080],
        [ 0.4142],
        [-0.1719],
        [ 0.3478]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0584],
        [-0.0088],
        [-0.2413],
        [-0.4172],
        [-0.2111],
        [ 0.3306],
        [-0.0073],
        [-0.0940],
        [ 0.1721],
        [ 0.2206],
        [-0.3702],
        [ 0.0229],
        [-0.4123],
        [-0.0761],
        [-0.2990],
        [-0.3256],
        [ 0.3670],
        [ 0.2390],
        [ 0.0743],
        [-0.2406],
        [ 0.3439],
        [-0.2174],
        [ 0.2052],
        [-0.3807],
        [-0.0349],
        [-0.2104],
        [-0.3856],
        [-0.3625],
        [-0.0080],
        [ 0.4142],
        [-0.1719],
        [ 0.3478]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(47.0686, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1582, device='cuda:0')



h[100].sum tensor(-2.2992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.3507, device='cuda:0')



h[200].sum tensor(9.9866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10175.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0132, 0.0165, 0.0000,  ..., 0.0079, 0.0047, 0.0000],
        [0.0062, 0.0077, 0.0000,  ..., 0.0037, 0.0022, 0.0000],
        [0.0017, 0.0022, 0.0000,  ..., 0.0010, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(63470.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1339.8855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(85.1954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5.8254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-79.2210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4610],
        [-0.3247],
        [-0.2185],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-21328.4980, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.4610],
        [-0.3247],
        [-0.2185],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0131, -0.0171, -0.0202,  ...,  0.0171, -0.0068,  0.0041],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-2081.0339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-86.0211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-88.1806, device='cuda:0')



h[100].sum tensor(-5.8438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.9905, device='cuda:0')



h[200].sum tensor(59.9535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(61.4586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0156],
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.0128],
        [0.0000, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(86812.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0122, 0.2648, 0.3279,  ..., 0.1857, 0.0000, 0.0617],
        [0.0105, 0.2269, 0.2809,  ..., 0.1591, 0.0000, 0.0528],
        [0.0084, 0.1822, 0.2256,  ..., 0.1278, 0.0000, 0.0424],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(430688.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(605.0376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(42.5016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-848.0253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.8616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.0523e-01],
        [-8.7126e-01],
        [-9.6139e-01],
        ...,
        [-1.0487e-05],
        [-1.7440e-05],
        [-2.4937e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-55900.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.4610],
        [-0.3247],
        [-0.2185],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 



Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 37, in <module>
    result1 = net(TraTen[10000:10010].reshape(10 * 6796, 1).to('cpu'))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() missing 1 required positional argument: 'in_fet'

real	0m25.863s
user	0m18.252s
sys	0m5.082s
