0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 13:34:59 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ad4230928e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.679s
user	0m2.435s
sys	0m0.966s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[13:35:22] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.6582],
        [-0.2203],
        [ 0.9061],
        ...,
        [-1.1718],
        [-0.9313],
        [ 0.9686]], device='cuda:0', requires_grad=True) 
node features sum: tensor(11.7826, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-8.5654e-02, -1.0897e-03, -6.6718e-02, -5.8155e-02,  8.3243e-02,
         -3.4687e-02,  4.0439e-02,  3.5820e-02,  8.2620e-02,  6.9652e-02,
          1.1043e-01, -1.5769e-02, -4.9434e-02, -4.8076e-02, -1.7173e-02,
          9.3181e-03,  5.3633e-02, -1.3186e-01, -1.1381e-03, -1.0061e-01,
          5.6177e-02, -8.0258e-02, -4.5874e-02,  9.9345e-02,  1.5259e-01,
         -2.0029e-02, -7.5542e-02, -1.0119e-01, -4.0394e-02, -9.3058e-02,
          3.5884e-02,  6.9860e-02, -7.8971e-02,  6.9620e-02, -1.1767e-01,
          8.1700e-02, -1.7873e-02,  1.4372e-01,  9.5411e-02, -1.1120e-01,
         -1.3779e-01, -4.5628e-03, -1.4157e-02,  5.1112e-02,  7.2507e-02,
          8.7999e-02, -5.1256e-03, -4.3411e-02, -1.0775e-01,  6.1278e-02,
         -7.3914e-02, -9.2727e-02,  2.1088e-03,  8.2344e-02, -1.4253e-01,
         -3.2236e-02, -4.1947e-02,  5.6156e-02, -1.4025e-01, -4.5354e-02,
         -5.4797e-02, -2.4294e-03, -6.4864e-02,  4.5504e-02, -6.7239e-02,
         -7.4289e-03,  1.0118e-01,  1.4501e-01,  1.0709e-01,  1.0430e-01,
         -1.4506e-01,  5.3540e-02, -5.9886e-02,  9.2757e-02, -1.4107e-01,
          5.0435e-02,  5.2585e-02,  8.8696e-02,  9.2789e-02,  1.4610e-01,
         -1.0300e-01,  3.2217e-02, -1.4587e-01, -7.3271e-02, -1.2820e-01,
          1.4768e-01, -7.5055e-02, -1.3448e-01,  6.2609e-02, -6.5837e-02,
         -7.6734e-02,  4.9981e-02, -1.2189e-01,  5.2039e-02,  3.2688e-02,
          6.7936e-02, -6.8889e-02, -1.6436e-02,  2.3144e-02, -1.2373e-01,
         -4.5335e-02,  5.0614e-02,  1.2873e-01,  9.5619e-02,  3.2884e-02,
          3.1755e-02, -1.7538e-03, -7.6107e-02,  1.3859e-01,  1.3980e-01,
         -7.8133e-02, -8.7489e-02,  3.1382e-02,  9.2760e-03,  1.0487e-01,
          1.1932e-01, -4.6320e-02, -1.0866e-01,  1.1756e-01,  9.0514e-02,
          8.6654e-02, -7.9912e-02,  2.3737e-02,  1.1194e-01, -2.1219e-02,
          1.5214e-01, -6.3267e-02, -1.2617e-02, -3.3372e-02,  6.3551e-02,
         -1.0843e-01, -1.4087e-01, -6.2727e-02, -1.5241e-01, -4.0836e-02,
         -5.4152e-02, -1.0131e-01,  4.5993e-02, -3.6366e-03,  4.8147e-02,
         -2.5184e-03,  3.3381e-02, -1.4995e-01,  4.1847e-02,  4.6181e-02,
          1.0046e-01, -8.4921e-03,  5.1638e-05, -2.4213e-02,  7.4746e-02,
          4.8643e-02, -6.0001e-02,  1.1419e-01, -5.7444e-02,  1.2000e-02,
          3.0368e-02,  6.1276e-02, -1.4290e-01, -1.3341e-01, -1.4643e-01,
          1.2038e-01, -2.9789e-02,  6.3275e-02, -1.2307e-01, -8.7102e-02,
         -7.0561e-02,  4.5850e-02, -1.3683e-01, -7.3152e-02,  5.4778e-02,
         -9.1708e-03,  9.4501e-02, -1.5183e-01, -2.8473e-02, -1.1930e-01,
          8.2659e-02,  6.3525e-02, -9.0966e-02,  3.0678e-02, -3.1858e-03,
         -6.0503e-03, -9.1522e-02,  5.8211e-02,  3.0034e-02, -1.1178e-01,
          1.2406e-01,  1.1335e-01, -7.9041e-02,  9.1987e-02, -3.1387e-02,
          1.5223e-01,  1.0356e-01, -8.9796e-02, -4.5469e-02,  6.1088e-03,
         -3.1968e-02, -1.0195e-01,  8.0404e-02, -9.4162e-02,  2.5958e-02,
         -1.4482e-02,  6.3356e-02,  8.0266e-02, -1.1430e-01, -3.9446e-02,
         -1.1582e-01, -4.0355e-02,  8.5069e-02,  1.0340e-01, -1.2480e-01,
         -1.3948e-01, -1.0825e-01,  1.6173e-02,  1.3592e-01,  9.6147e-02,
         -1.2211e-01, -1.1667e-01, -1.5058e-01,  8.1293e-02,  6.9155e-02,
         -9.5645e-03,  7.8030e-02, -9.3087e-02, -1.3295e-01, -1.4247e-01,
          1.4023e-01,  7.7300e-02, -1.3720e-01, -8.4636e-02, -9.9875e-03,
         -1.2237e-01,  1.3649e-02,  1.0690e-01,  1.4166e-01, -1.2255e-01,
          1.3800e-01, -1.1677e-01, -1.4164e-01, -1.2273e-01,  1.0694e-01,
          1.3002e-01, -1.2797e-01,  1.2004e-01,  4.0428e-02,  1.3238e-01,
          8.1291e-02, -1.2956e-01, -3.4516e-03, -3.3720e-02,  6.3543e-02,
         -5.4344e-02,  1.0109e-01, -1.4204e-01,  4.4986e-02, -9.6766e-02,
          1.4549e-01]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-8.5654e-02, -1.0897e-03, -6.6718e-02, -5.8155e-02,  8.3243e-02,
         -3.4687e-02,  4.0439e-02,  3.5820e-02,  8.2620e-02,  6.9652e-02,
          1.1043e-01, -1.5769e-02, -4.9434e-02, -4.8076e-02, -1.7173e-02,
          9.3181e-03,  5.3633e-02, -1.3186e-01, -1.1381e-03, -1.0061e-01,
          5.6177e-02, -8.0258e-02, -4.5874e-02,  9.9345e-02,  1.5259e-01,
         -2.0029e-02, -7.5542e-02, -1.0119e-01, -4.0394e-02, -9.3058e-02,
          3.5884e-02,  6.9860e-02, -7.8971e-02,  6.9620e-02, -1.1767e-01,
          8.1700e-02, -1.7873e-02,  1.4372e-01,  9.5411e-02, -1.1120e-01,
         -1.3779e-01, -4.5628e-03, -1.4157e-02,  5.1112e-02,  7.2507e-02,
          8.7999e-02, -5.1256e-03, -4.3411e-02, -1.0775e-01,  6.1278e-02,
         -7.3914e-02, -9.2727e-02,  2.1088e-03,  8.2344e-02, -1.4253e-01,
         -3.2236e-02, -4.1947e-02,  5.6156e-02, -1.4025e-01, -4.5354e-02,
         -5.4797e-02, -2.4294e-03, -6.4864e-02,  4.5504e-02, -6.7239e-02,
         -7.4289e-03,  1.0118e-01,  1.4501e-01,  1.0709e-01,  1.0430e-01,
         -1.4506e-01,  5.3540e-02, -5.9886e-02,  9.2757e-02, -1.4107e-01,
          5.0435e-02,  5.2585e-02,  8.8696e-02,  9.2789e-02,  1.4610e-01,
         -1.0300e-01,  3.2217e-02, -1.4587e-01, -7.3271e-02, -1.2820e-01,
          1.4768e-01, -7.5055e-02, -1.3448e-01,  6.2609e-02, -6.5837e-02,
         -7.6734e-02,  4.9981e-02, -1.2189e-01,  5.2039e-02,  3.2688e-02,
          6.7936e-02, -6.8889e-02, -1.6436e-02,  2.3144e-02, -1.2373e-01,
         -4.5335e-02,  5.0614e-02,  1.2873e-01,  9.5619e-02,  3.2884e-02,
          3.1755e-02, -1.7538e-03, -7.6107e-02,  1.3859e-01,  1.3980e-01,
         -7.8133e-02, -8.7489e-02,  3.1382e-02,  9.2760e-03,  1.0487e-01,
          1.1932e-01, -4.6320e-02, -1.0866e-01,  1.1756e-01,  9.0514e-02,
          8.6654e-02, -7.9912e-02,  2.3737e-02,  1.1194e-01, -2.1219e-02,
          1.5214e-01, -6.3267e-02, -1.2617e-02, -3.3372e-02,  6.3551e-02,
         -1.0843e-01, -1.4087e-01, -6.2727e-02, -1.5241e-01, -4.0836e-02,
         -5.4152e-02, -1.0131e-01,  4.5993e-02, -3.6366e-03,  4.8147e-02,
         -2.5184e-03,  3.3381e-02, -1.4995e-01,  4.1847e-02,  4.6181e-02,
          1.0046e-01, -8.4921e-03,  5.1638e-05, -2.4213e-02,  7.4746e-02,
          4.8643e-02, -6.0001e-02,  1.1419e-01, -5.7444e-02,  1.2000e-02,
          3.0368e-02,  6.1276e-02, -1.4290e-01, -1.3341e-01, -1.4643e-01,
          1.2038e-01, -2.9789e-02,  6.3275e-02, -1.2307e-01, -8.7102e-02,
         -7.0561e-02,  4.5850e-02, -1.3683e-01, -7.3152e-02,  5.4778e-02,
         -9.1708e-03,  9.4501e-02, -1.5183e-01, -2.8473e-02, -1.1930e-01,
          8.2659e-02,  6.3525e-02, -9.0966e-02,  3.0678e-02, -3.1858e-03,
         -6.0503e-03, -9.1522e-02,  5.8211e-02,  3.0034e-02, -1.1178e-01,
          1.2406e-01,  1.1335e-01, -7.9041e-02,  9.1987e-02, -3.1387e-02,
          1.5223e-01,  1.0356e-01, -8.9796e-02, -4.5469e-02,  6.1088e-03,
         -3.1968e-02, -1.0195e-01,  8.0404e-02, -9.4162e-02,  2.5958e-02,
         -1.4482e-02,  6.3356e-02,  8.0266e-02, -1.1430e-01, -3.9446e-02,
         -1.1582e-01, -4.0355e-02,  8.5069e-02,  1.0340e-01, -1.2480e-01,
         -1.3948e-01, -1.0825e-01,  1.6173e-02,  1.3592e-01,  9.6147e-02,
         -1.2211e-01, -1.1667e-01, -1.5058e-01,  8.1293e-02,  6.9155e-02,
         -9.5645e-03,  7.8030e-02, -9.3087e-02, -1.3295e-01, -1.4247e-01,
          1.4023e-01,  7.7300e-02, -1.3720e-01, -8.4636e-02, -9.9875e-03,
         -1.2237e-01,  1.3649e-02,  1.0690e-01,  1.4166e-01, -1.2255e-01,
          1.3800e-01, -1.1677e-01, -1.4164e-01, -1.2273e-01,  1.0694e-01,
          1.3002e-01, -1.2797e-01,  1.2004e-01,  4.0428e-02,  1.3238e-01,
          8.1291e-02, -1.2956e-01, -3.4516e-03, -3.3720e-02,  6.3543e-02,
         -5.4344e-02,  1.0109e-01, -1.4204e-01,  4.4986e-02, -9.6766e-02,
          1.4549e-01]], device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0271,  0.0895, -0.0347,  ..., -0.0118,  0.0047, -0.0329],
        [ 0.0815, -0.0846,  0.0435,  ..., -0.0520,  0.0349,  0.0514],
        [ 0.0622, -0.0051,  0.0411,  ..., -0.0403,  0.1053,  0.0737],
        ...,
        [-0.0305,  0.0013, -0.1166,  ..., -0.0488, -0.0323, -0.0291],
        [ 0.0135, -0.1182,  0.0437,  ..., -0.0447, -0.0900, -0.0673],
        [-0.1196, -0.0354, -0.0604,  ...,  0.0045, -0.0482, -0.0550]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0271,  0.0895, -0.0347,  ..., -0.0118,  0.0047, -0.0329],
        [ 0.0815, -0.0846,  0.0435,  ..., -0.0520,  0.0349,  0.0514],
        [ 0.0622, -0.0051,  0.0411,  ..., -0.0403,  0.1053,  0.0737],
        ...,
        [-0.0305,  0.0013, -0.1166,  ..., -0.0488, -0.0323, -0.0291],
        [ 0.0135, -0.1182,  0.0437,  ..., -0.0447, -0.0900, -0.0673],
        [-0.1196, -0.0354, -0.0604,  ...,  0.0045, -0.0482, -0.0550]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.1074, -0.1318, -0.0139,  ...,  0.0345, -0.1030,  0.0534],
        [ 0.1013,  0.0340,  0.1226,  ..., -0.0445, -0.1304, -0.0038],
        [-0.0684,  0.0962, -0.1024,  ..., -0.0193,  0.1011, -0.0332],
        ...,
        [ 0.1114,  0.0533, -0.0462,  ...,  0.0221,  0.1169,  0.0122],
        [-0.0620,  0.0723, -0.1742,  ...,  0.0601, -0.0578,  0.1609],
        [-0.1505,  0.0084,  0.1261,  ...,  0.0747,  0.1180, -0.1014]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1074, -0.1318, -0.0139,  ...,  0.0345, -0.1030,  0.0534],
        [ 0.1013,  0.0340,  0.1226,  ..., -0.0445, -0.1304, -0.0038],
        [-0.0684,  0.0962, -0.1024,  ..., -0.0193,  0.1011, -0.0332],
        ...,
        [ 0.1114,  0.0533, -0.0462,  ...,  0.0221,  0.1169,  0.0122],
        [-0.0620,  0.0723, -0.1742,  ...,  0.0601, -0.0578,  0.1609],
        [-0.1505,  0.0084,  0.1261,  ...,  0.0747,  0.1180, -0.1014]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.0630,  0.1055,  0.1821,  ..., -0.1223,  0.0159,  0.1263],
        [-0.0806,  0.0739,  0.1412,  ...,  0.2060,  0.1372, -0.1773],
        [-0.0375, -0.1611,  0.1483,  ..., -0.0424, -0.1539, -0.0421],
        ...,
        [-0.1262,  0.1016, -0.0658,  ...,  0.2152, -0.1198,  0.0522],
        [ 0.0931, -0.2310, -0.2500,  ..., -0.1853,  0.0737,  0.0507],
        [-0.2289, -0.1629,  0.1404,  ..., -0.1713,  0.0948,  0.2463]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0630,  0.1055,  0.1821,  ..., -0.1223,  0.0159,  0.1263],
        [-0.0806,  0.0739,  0.1412,  ...,  0.2060,  0.1372, -0.1773],
        [-0.0375, -0.1611,  0.1483,  ..., -0.0424, -0.1539, -0.0421],
        ...,
        [-0.1262,  0.1016, -0.0658,  ...,  0.2152, -0.1198,  0.0522],
        [ 0.0931, -0.2310, -0.2500,  ..., -0.1853,  0.0737,  0.0507],
        [-0.2289, -0.1629,  0.1404,  ..., -0.1713,  0.0948,  0.2463]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.2579],
        [ 0.1695],
        [ 0.0820],
        [-0.1907],
        [-0.2630],
        [-0.3544],
        [-0.0819],
        [-0.4083],
        [ 0.2640],
        [ 0.0917],
        [-0.1918],
        [ 0.3700],
        [ 0.0277],
        [-0.0824],
        [ 0.3028],
        [-0.1527],
        [-0.4230],
        [ 0.1073],
        [-0.3797],
        [-0.4213],
        [ 0.1577],
        [-0.1345],
        [ 0.0758],
        [ 0.0073],
        [-0.2433],
        [ 0.3256],
        [-0.0591],
        [ 0.0674],
        [-0.0914],
        [ 0.3724],
        [ 0.3026],
        [-0.3924]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2579],
        [ 0.1695],
        [ 0.0820],
        [-0.1907],
        [-0.2630],
        [-0.3544],
        [-0.0819],
        [-0.4083],
        [ 0.2640],
        [ 0.0917],
        [-0.1918],
        [ 0.3700],
        [ 0.0277],
        [-0.0824],
        [ 0.3028],
        [-0.1527],
        [-0.4230],
        [ 0.1073],
        [-0.3797],
        [-0.4213],
        [ 0.1577],
        [-0.1345],
        [ 0.0758],
        [ 0.0073],
        [-0.2433],
        [ 0.3256],
        [-0.0591],
        [ 0.0674],
        [-0.0914],
        [ 0.3724],
        [ 0.3026],
        [-0.3924]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-18.1531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.8458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.8647, device='cuda:0')



h[100].sum tensor(2.9811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0478, device='cuda:0')



h[200].sum tensor(11.3881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10267.9121, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0000, 0.0204,  ..., 0.0107, 0.0000, 0.0000],
        [0.0015, 0.0000, 0.0095,  ..., 0.0050, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0027,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(64109.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(316.5856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(20.1302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-9.0727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1593.7177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(101.3511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2016],
        [-0.1420],
        [-0.0956],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-9326.8555, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.2016],
        [-0.1420],
        [-0.0956],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0126, -0.0109, -0.0071,  ...,  0.0027, -0.0068, -0.0173],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-467.3529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-83.0383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-85.1229, device='cuda:0')



h[100].sum tensor(112.8719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(115.7055, device='cuda:0')



h[200].sum tensor(-99.6912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-102.1939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(94946.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1051, 0.1739, 0.0000,  ..., 0.3621, 0.2643, 0.0000],
        [0.0900, 0.1490, 0.0000,  ..., 0.3103, 0.2265, 0.0000],
        [0.0723, 0.1196, 0.0000,  ..., 0.2491, 0.1819, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(570332.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5201.4048, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(364.4279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-24.9337, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-603.7140, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.6938e+00],
        [1.8327e+00],
        [2.0223e+00],
        ...,
        [2.2064e-05],
        [3.6682e-05],
        [5.2429e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(117579.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.2016],
        [-0.1420],
        [-0.0956],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-47.2052, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(64.1649, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-56.6720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(4170.6118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-12.8373, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2769.1274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0259,  0.0015, -0.0042,  ...,  0.0051, -0.0262,  0.0024],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0329,  0.0019, -0.0054,  ...,  0.0064, -0.0334,  0.0031],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(-1080.2458, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-71.6330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-47.2052, device='cuda:0')



h[100].sum tensor(39.7921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(64.1649, device='cuda:0')



h[200].sum tensor(-5.8321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-56.6720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0020],
        [0.0000, 0.0063, 0.0000,  ..., 0.0209, 0.0000, 0.0100],
        [0.0000, 0.0024, 0.0000,  ..., 0.0078, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(47689.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0554, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0885, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(185903.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(35.7518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5052.5522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-96.3032, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-508.3759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0550],
        [0.0685],
        [0.0749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(9599.3701, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)
result1 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) result0 tensor([[0.0550],
        [0.0685],
        [0.0749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 61, in <module>
    plotevent(sitonsquare(result0[EvBTr - 10000]), 333, f'event number {EvBTr} passed through network before training \n color indicates time')
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/DataLoadBha.py", line 49, in sitonsquare
    sqevent[i, j + 144] = event[(wiresum[i]) + j + w]
IndexError: index 1 is out of bounds for dimension 0 with size 1

real	0m24.796s
user	0m17.945s
sys	0m4.960s
