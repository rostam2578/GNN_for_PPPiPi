0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 14:41:48 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0    33W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ba8aacf28e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.580s
user	0m2.319s
sys	0m1.047s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[14:42:11] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.7044],
        [ 1.9531],
        [ 2.2888],
        ...,
        [-1.1486],
        [-0.3700],
        [-0.8979]], device='cuda:0', requires_grad=True) 
node features sum: tensor(99.1164, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[ 0.0212,  0.0985, -0.0111, -0.0810,  0.1136, -0.0546, -0.1159, -0.1196,
          0.1131, -0.1015,  0.1457, -0.0412, -0.0948, -0.0618, -0.0681, -0.0562,
          0.1430, -0.1003, -0.0435, -0.0351, -0.1395,  0.0643, -0.0523, -0.0170,
         -0.0413, -0.0494, -0.0465,  0.1155, -0.0111,  0.0364,  0.0259,  0.0861,
          0.0108, -0.0603, -0.1396,  0.1100, -0.1134, -0.0016,  0.0702,  0.0556,
          0.0118,  0.0230,  0.0963,  0.1345, -0.1378, -0.1090, -0.1044, -0.0278,
          0.0015,  0.0729,  0.0537,  0.0542, -0.0594, -0.0029,  0.0931, -0.1358,
         -0.1167,  0.1354,  0.0959,  0.0045,  0.0963,  0.0884,  0.0836,  0.0328,
          0.0375,  0.1140, -0.0745,  0.1466,  0.0921, -0.1418,  0.1337, -0.1519,
         -0.1166,  0.1114, -0.0319,  0.0902,  0.0025, -0.0084,  0.0683, -0.0904,
         -0.0315,  0.1347, -0.0697, -0.0907,  0.0201,  0.1472,  0.0017,  0.1232,
          0.0813,  0.1464,  0.0479,  0.0523, -0.0425,  0.0815, -0.1449,  0.0285,
          0.0991, -0.0626, -0.0006, -0.1158, -0.0856,  0.0653, -0.1522, -0.0730,
          0.1247, -0.1459, -0.0121,  0.1459,  0.0146, -0.0098,  0.0259,  0.0430,
          0.1025, -0.1088,  0.0498,  0.1087,  0.0086,  0.0614,  0.0012,  0.1410,
         -0.0626,  0.0613, -0.0721,  0.0031,  0.1243,  0.0894,  0.0063,  0.0081,
          0.0018, -0.0360, -0.1499, -0.0220, -0.0453, -0.0626,  0.1114, -0.0299,
          0.0011,  0.0580,  0.1028,  0.0551, -0.0046,  0.1411, -0.0679,  0.0537,
          0.1243, -0.1394,  0.0068,  0.1117,  0.0031, -0.0500,  0.0734, -0.0857,
          0.0798,  0.1352, -0.1222,  0.0838,  0.1246, -0.0977,  0.0952,  0.0547,
          0.1087, -0.0493,  0.0250, -0.1120,  0.0476,  0.0344, -0.1473,  0.0772,
          0.1226, -0.0892, -0.1121, -0.0057,  0.0907, -0.1185, -0.0332, -0.0071,
          0.0420, -0.0737, -0.0805, -0.1003, -0.1022,  0.0010, -0.0603,  0.0702,
          0.0853, -0.0862, -0.1474, -0.1012, -0.0277,  0.0706, -0.1283, -0.0063,
          0.0761,  0.0407, -0.0481,  0.0768,  0.1093,  0.1214,  0.0950, -0.0261,
          0.1167,  0.1271,  0.0899, -0.0713, -0.1285,  0.1086, -0.0468, -0.0275,
          0.1224,  0.0481, -0.1093,  0.1275,  0.0103, -0.0098,  0.0221, -0.1479,
          0.0927,  0.0706, -0.1156,  0.1478, -0.1105,  0.0683, -0.0492,  0.0559,
         -0.0834, -0.0807, -0.1060, -0.0650,  0.0821,  0.0436,  0.0663, -0.0706,
         -0.1463,  0.1450,  0.0685,  0.0726, -0.1115, -0.0817,  0.0757, -0.0005,
         -0.1386, -0.0853,  0.1127, -0.0373,  0.0360,  0.0117, -0.1113,  0.1146,
         -0.0152,  0.0094, -0.0876,  0.1483, -0.1408, -0.1221, -0.1198, -0.0693]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0212,  0.0985, -0.0111, -0.0810,  0.1136, -0.0546, -0.1159, -0.1196,
          0.1131, -0.1015,  0.1457, -0.0412, -0.0948, -0.0618, -0.0681, -0.0562,
          0.1430, -0.1003, -0.0435, -0.0351, -0.1395,  0.0643, -0.0523, -0.0170,
         -0.0413, -0.0494, -0.0465,  0.1155, -0.0111,  0.0364,  0.0259,  0.0861,
          0.0108, -0.0603, -0.1396,  0.1100, -0.1134, -0.0016,  0.0702,  0.0556,
          0.0118,  0.0230,  0.0963,  0.1345, -0.1378, -0.1090, -0.1044, -0.0278,
          0.0015,  0.0729,  0.0537,  0.0542, -0.0594, -0.0029,  0.0931, -0.1358,
         -0.1167,  0.1354,  0.0959,  0.0045,  0.0963,  0.0884,  0.0836,  0.0328,
          0.0375,  0.1140, -0.0745,  0.1466,  0.0921, -0.1418,  0.1337, -0.1519,
         -0.1166,  0.1114, -0.0319,  0.0902,  0.0025, -0.0084,  0.0683, -0.0904,
         -0.0315,  0.1347, -0.0697, -0.0907,  0.0201,  0.1472,  0.0017,  0.1232,
          0.0813,  0.1464,  0.0479,  0.0523, -0.0425,  0.0815, -0.1449,  0.0285,
          0.0991, -0.0626, -0.0006, -0.1158, -0.0856,  0.0653, -0.1522, -0.0730,
          0.1247, -0.1459, -0.0121,  0.1459,  0.0146, -0.0098,  0.0259,  0.0430,
          0.1025, -0.1088,  0.0498,  0.1087,  0.0086,  0.0614,  0.0012,  0.1410,
         -0.0626,  0.0613, -0.0721,  0.0031,  0.1243,  0.0894,  0.0063,  0.0081,
          0.0018, -0.0360, -0.1499, -0.0220, -0.0453, -0.0626,  0.1114, -0.0299,
          0.0011,  0.0580,  0.1028,  0.0551, -0.0046,  0.1411, -0.0679,  0.0537,
          0.1243, -0.1394,  0.0068,  0.1117,  0.0031, -0.0500,  0.0734, -0.0857,
          0.0798,  0.1352, -0.1222,  0.0838,  0.1246, -0.0977,  0.0952,  0.0547,
          0.1087, -0.0493,  0.0250, -0.1120,  0.0476,  0.0344, -0.1473,  0.0772,
          0.1226, -0.0892, -0.1121, -0.0057,  0.0907, -0.1185, -0.0332, -0.0071,
          0.0420, -0.0737, -0.0805, -0.1003, -0.1022,  0.0010, -0.0603,  0.0702,
          0.0853, -0.0862, -0.1474, -0.1012, -0.0277,  0.0706, -0.1283, -0.0063,
          0.0761,  0.0407, -0.0481,  0.0768,  0.1093,  0.1214,  0.0950, -0.0261,
          0.1167,  0.1271,  0.0899, -0.0713, -0.1285,  0.1086, -0.0468, -0.0275,
          0.1224,  0.0481, -0.1093,  0.1275,  0.0103, -0.0098,  0.0221, -0.1479,
          0.0927,  0.0706, -0.1156,  0.1478, -0.1105,  0.0683, -0.0492,  0.0559,
         -0.0834, -0.0807, -0.1060, -0.0650,  0.0821,  0.0436,  0.0663, -0.0706,
         -0.1463,  0.1450,  0.0685,  0.0726, -0.1115, -0.0817,  0.0757, -0.0005,
         -0.1386, -0.0853,  0.1127, -0.0373,  0.0360,  0.0117, -0.1113,  0.1146,
         -0.0152,  0.0094, -0.0876,  0.1483, -0.1408, -0.1221, -0.1198, -0.0693]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.1058, -0.0912,  0.0595,  ..., -0.1176, -0.0404, -0.0440],
        [ 0.0748,  0.0812,  0.1066,  ..., -0.0068, -0.0003, -0.0675],
        [-0.0222, -0.0183,  0.0620,  ..., -0.0673, -0.0083,  0.0657],
        ...,
        [-0.0098, -0.0852, -0.0488,  ...,  0.0944,  0.0226, -0.1029],
        [-0.0391,  0.1004,  0.0038,  ...,  0.0700, -0.0924,  0.0981],
        [-0.0161,  0.1235, -0.0315,  ...,  0.0365, -0.0077,  0.0368]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1058, -0.0912,  0.0595,  ..., -0.1176, -0.0404, -0.0440],
        [ 0.0748,  0.0812,  0.1066,  ..., -0.0068, -0.0003, -0.0675],
        [-0.0222, -0.0183,  0.0620,  ..., -0.0673, -0.0083,  0.0657],
        ...,
        [-0.0098, -0.0852, -0.0488,  ...,  0.0944,  0.0226, -0.1029],
        [-0.0391,  0.1004,  0.0038,  ...,  0.0700, -0.0924,  0.0981],
        [-0.0161,  0.1235, -0.0315,  ...,  0.0365, -0.0077,  0.0368]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0496, -0.0507,  0.0397,  ...,  0.0468,  0.0958, -0.1385],
        [-0.0176, -0.1372, -0.0672,  ...,  0.0665, -0.0786,  0.1010],
        [-0.0244, -0.0769,  0.1168,  ..., -0.1282,  0.0716,  0.0621],
        ...,
        [ 0.1716,  0.0050, -0.0056,  ..., -0.1555, -0.0583,  0.1423],
        [ 0.0360, -0.1141, -0.1445,  ..., -0.0161,  0.0781,  0.1595],
        [ 0.0745,  0.1573, -0.0404,  ..., -0.0115, -0.1097,  0.0266]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0496, -0.0507,  0.0397,  ...,  0.0468,  0.0958, -0.1385],
        [-0.0176, -0.1372, -0.0672,  ...,  0.0665, -0.0786,  0.1010],
        [-0.0244, -0.0769,  0.1168,  ..., -0.1282,  0.0716,  0.0621],
        ...,
        [ 0.1716,  0.0050, -0.0056,  ..., -0.1555, -0.0583,  0.1423],
        [ 0.0360, -0.1141, -0.1445,  ..., -0.0161,  0.0781,  0.1595],
        [ 0.0745,  0.1573, -0.0404,  ..., -0.0115, -0.1097,  0.0266]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.0892,  0.1125, -0.2049,  ...,  0.1981,  0.2405, -0.0551],
        [-0.0220,  0.0790,  0.0762,  ...,  0.0998,  0.1135, -0.0767],
        [ 0.0063, -0.1359, -0.0052,  ...,  0.2134,  0.1588,  0.0276],
        ...,
        [-0.1673,  0.0521, -0.0466,  ...,  0.2477, -0.0789,  0.0916],
        [ 0.1183, -0.0203, -0.1730,  ...,  0.2351, -0.0123,  0.0645],
        [ 0.0957,  0.0020, -0.0659,  ...,  0.0185, -0.2179, -0.1995]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0892,  0.1125, -0.2049,  ...,  0.1981,  0.2405, -0.0551],
        [-0.0220,  0.0790,  0.0762,  ...,  0.0998,  0.1135, -0.0767],
        [ 0.0063, -0.1359, -0.0052,  ...,  0.2134,  0.1588,  0.0276],
        ...,
        [-0.1673,  0.0521, -0.0466,  ...,  0.2477, -0.0789,  0.0916],
        [ 0.1183, -0.0203, -0.1730,  ...,  0.2351, -0.0123,  0.0645],
        [ 0.0957,  0.0020, -0.0659,  ...,  0.0185, -0.2179, -0.1995]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.3624],
        [ 0.2758],
        [-0.2055],
        [-0.0644],
        [ 0.2846],
        [ 0.0159],
        [ 0.0782],
        [-0.0125],
        [-0.0689],
        [ 0.3367],
        [-0.0564],
        [ 0.3979],
        [-0.1390],
        [-0.3823],
        [-0.1839],
        [ 0.0673],
        [-0.4169],
        [ 0.3768],
        [-0.3923],
        [ 0.0751],
        [ 0.1683],
        [ 0.1422],
        [ 0.3563],
        [-0.3940],
        [ 0.2540],
        [ 0.2083],
        [ 0.2175],
        [-0.0413],
        [-0.0515],
        [ 0.0416],
        [ 0.2462],
        [-0.2924]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.3624],
        [ 0.2758],
        [-0.2055],
        [-0.0644],
        [ 0.2846],
        [ 0.0159],
        [ 0.0782],
        [-0.0125],
        [-0.0689],
        [ 0.3367],
        [-0.0564],
        [ 0.3979],
        [-0.1390],
        [-0.3823],
        [-0.1839],
        [ 0.0673],
        [-0.4169],
        [ 0.3768],
        [-0.3923],
        [ 0.0751],
        [ 0.1683],
        [ 0.1422],
        [ 0.3563],
        [-0.3940],
        [ 0.2540],
        [ 0.2083],
        [ 0.2175],
        [-0.0413],
        [-0.0515],
        [ 0.0416],
        [ 0.2462],
        [-0.2924]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-95.4118, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.1479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(6.2855, device='cuda:0')



h[100].sum tensor(-1.9479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.9915, device='cuda:0')



h[200].sum tensor(9.7163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(9408.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.4989e-03, 0.0000e+00,  ..., 0.0000e+00, 1.5544e-04,
         4.0144e-03],
        [0.0000e+00, 4.4494e-03, 0.0000e+00,  ..., 0.0000e+00, 7.2982e-05,
         1.8807e-03],
        [0.0000e+00, 1.2538e-03, 0.0000e+00,  ..., 0.0000e+00, 2.0617e-05,
         5.2982e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(56618.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-21.3536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-77.2989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1042.1874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(66.2628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.3759],
        [0.2648],
        [0.1782],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(17390.0977, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[0.3759],
        [0.2648],
        [0.1782],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0063,  0.0091,  ...,  0.0178,  0.0114, -0.0092],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-2277.0095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-41.8701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-42.9212, device='cuda:0')



h[100].sum tensor(-122.3376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-125.4088, device='cuda:0')



h[200].sum tensor(72.9465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(74.7777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0237, 0.0343,  ..., 0.0673, 0.0429, 0.0000],
        [0.0000, 0.0195, 0.0282,  ..., 0.0554, 0.0353, 0.0000],
        [0.0000, 0.0046, 0.0066,  ..., 0.0130, 0.0083, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(93542.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3375, 0.0564,  ..., 0.0000, 0.2779, 0.0000],
        [0.0000, 0.2892, 0.0484,  ..., 0.0000, 0.2381, 0.0000],
        [0.0000, 0.2322, 0.0388,  ..., 0.0000, 0.1912, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(449504.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-313.5976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-917.1530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10416.0146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(729.6271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.1781e+00],
        [1.2748e+00],
        [1.4067e+00],
        ...,
        [1.5354e-05],
        [2.5523e-05],
        [3.6474e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(81783.2266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.3759],
        [0.2648],
        [0.1782],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.8021, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-69.5459, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.4683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1949.7358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4537.9258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1054.6792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0162,  0.0060,  0.0279,  ...,  0.0152,  0.0165,  0.0006],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0207,  0.0076,  0.0355,  ...,  0.0194,  0.0209,  0.0008],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(934.7516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-44.9339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.8021, device='cuda:0')



h[100].sum tensor(76.5875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-69.5459, device='cuda:0')



h[200].sum tensor(43.1204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.4683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0228,  ..., 0.0124, 0.0134, 0.0005],
        [0.0000, 0.0247, 0.1150,  ..., 0.0628, 0.0680, 0.0026],
        [0.0000, 0.0093, 0.0433,  ..., 0.0236, 0.0256, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(59845.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0793, 0.0000, 0.1817,  ..., 0.0000, 0.0672, 0.0000],
        [0.1520, 0.0000, 0.3484,  ..., 0.0000, 0.1288, 0.0000],
        [0.1267, 0.0000, 0.2905,  ..., 0.0000, 0.1074, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(295812.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5983.2690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.5363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2348.7705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-826.7289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(305.6537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4186],
        [-0.5214],
        [-0.5699],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-73005.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)

result1 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape torch.Size([67960, 1]) 
result0 tensor([[-0.4186],
        [-0.5214],
        [-0.5699],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result0.shape torch.Size([67960, 1])
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 67, in <module>
    plotevent(sitonsquare((result1.reshape(10, 6796)[EvBTr - 10000] > thr[1]) & ((trvalgnnpppipi[EvBTr]) > 0)), 336, f'event number {EvBTr} passed through network after training \n threshold is {thr[1]}')
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/_tensor.py", line 678, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

real	0m25.028s
user	0m18.217s
sys	0m4.780s
