0: gpu026.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-65d728cc-cad9-b47c-6c9c-ab0f28c9368d)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        E9:B8:F8:81:15:18:E1:E0:5F:F6:3C:FD:48:A3:6D:5C:01:89:82:53
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 14:16:23 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |
| N/A   27C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089926656

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b715e9ce8e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m23.535s
user	0m2.965s
sys	0m1.962s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[14:17:47] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.7101],
        [ 0.2987],
        [-0.1876],
        ...,
        [ 0.2010],
        [-1.4729],
        [-0.7067]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-58.7364, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0530, -0.1023, -0.0877,  0.0966,  0.0303, -0.0183, -0.1267,  0.0310,
         -0.0481,  0.0901,  0.0903, -0.0950,  0.1280,  0.0280,  0.0746, -0.1449,
         -0.1080, -0.1057, -0.1081, -0.0904, -0.0927,  0.1030, -0.1458, -0.0003,
         -0.1448, -0.1051,  0.0429,  0.0370,  0.1354,  0.1344, -0.1288,  0.0661,
         -0.1054, -0.0815,  0.1141, -0.0300,  0.0785, -0.0606, -0.1101, -0.0776,
         -0.0295, -0.0734,  0.0443,  0.0187, -0.1229,  0.1274, -0.0801,  0.1447,
          0.1097, -0.0532, -0.0049,  0.0023,  0.0222,  0.0743, -0.0159, -0.0985,
         -0.1325,  0.1512,  0.0358, -0.1139,  0.1032,  0.0774,  0.0019,  0.0429,
         -0.1392,  0.0007, -0.0206, -0.0874,  0.0492,  0.0717, -0.1102, -0.1488,
         -0.0024,  0.1486, -0.0574,  0.1222, -0.0726,  0.0543,  0.1384,  0.0145,
         -0.1265,  0.0107,  0.0500,  0.0382, -0.0640,  0.1353,  0.0835, -0.0896,
          0.1460, -0.0987, -0.1211, -0.0364, -0.1466,  0.1444, -0.0950, -0.1520,
         -0.1285, -0.1166,  0.1342,  0.0959, -0.0877,  0.0901, -0.0071, -0.0852,
          0.1000, -0.1321, -0.0799,  0.1005, -0.0652,  0.0532, -0.0930,  0.0871,
         -0.1400,  0.1215,  0.0851,  0.0568,  0.0280, -0.1247, -0.0529, -0.0283,
          0.0499,  0.0239,  0.0322, -0.0028, -0.0331,  0.0347, -0.0283, -0.0955,
          0.0450,  0.0499,  0.0235, -0.0391, -0.1374, -0.0066,  0.0175, -0.0570,
         -0.0168,  0.0698,  0.0020, -0.0838, -0.0949,  0.0886, -0.0324,  0.0610,
         -0.1258,  0.0944,  0.0588,  0.0652,  0.0702,  0.0118,  0.1516,  0.1334,
          0.1124, -0.0977,  0.0398, -0.0054, -0.1089, -0.1190,  0.0225, -0.1152,
          0.0057, -0.0841, -0.1516, -0.1393,  0.0096,  0.1061, -0.1135, -0.0421,
          0.0771, -0.1227,  0.0132, -0.1201, -0.1291, -0.1108,  0.1009, -0.1329,
         -0.0234,  0.0434,  0.0121,  0.0323, -0.1146, -0.0576, -0.1112,  0.0759,
         -0.1230, -0.0507, -0.1069,  0.0320, -0.0054,  0.0791,  0.0678, -0.1273,
          0.0018,  0.1448,  0.1128, -0.0116,  0.0870, -0.0891, -0.0122, -0.1135,
          0.0618,  0.0642, -0.1308, -0.1470,  0.0029, -0.1130, -0.1175, -0.0883,
          0.0103, -0.0562,  0.0065,  0.1357,  0.0068,  0.1137, -0.0538,  0.0750,
         -0.0488, -0.0535, -0.1101, -0.0791, -0.1500, -0.0548, -0.0411, -0.0903,
         -0.1379, -0.0071,  0.0263,  0.1287, -0.0317,  0.0167, -0.0701, -0.0703,
         -0.0462, -0.0238, -0.0862,  0.0666, -0.1121,  0.0935,  0.0083,  0.0121,
         -0.0358, -0.1122, -0.0450,  0.1233,  0.1138, -0.1182, -0.1210, -0.0330,
         -0.0946, -0.0408, -0.0604, -0.1173, -0.1499, -0.1070,  0.1019, -0.0815]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0530, -0.1023, -0.0877,  0.0966,  0.0303, -0.0183, -0.1267,  0.0310,
         -0.0481,  0.0901,  0.0903, -0.0950,  0.1280,  0.0280,  0.0746, -0.1449,
         -0.1080, -0.1057, -0.1081, -0.0904, -0.0927,  0.1030, -0.1458, -0.0003,
         -0.1448, -0.1051,  0.0429,  0.0370,  0.1354,  0.1344, -0.1288,  0.0661,
         -0.1054, -0.0815,  0.1141, -0.0300,  0.0785, -0.0606, -0.1101, -0.0776,
         -0.0295, -0.0734,  0.0443,  0.0187, -0.1229,  0.1274, -0.0801,  0.1447,
          0.1097, -0.0532, -0.0049,  0.0023,  0.0222,  0.0743, -0.0159, -0.0985,
         -0.1325,  0.1512,  0.0358, -0.1139,  0.1032,  0.0774,  0.0019,  0.0429,
         -0.1392,  0.0007, -0.0206, -0.0874,  0.0492,  0.0717, -0.1102, -0.1488,
         -0.0024,  0.1486, -0.0574,  0.1222, -0.0726,  0.0543,  0.1384,  0.0145,
         -0.1265,  0.0107,  0.0500,  0.0382, -0.0640,  0.1353,  0.0835, -0.0896,
          0.1460, -0.0987, -0.1211, -0.0364, -0.1466,  0.1444, -0.0950, -0.1520,
         -0.1285, -0.1166,  0.1342,  0.0959, -0.0877,  0.0901, -0.0071, -0.0852,
          0.1000, -0.1321, -0.0799,  0.1005, -0.0652,  0.0532, -0.0930,  0.0871,
         -0.1400,  0.1215,  0.0851,  0.0568,  0.0280, -0.1247, -0.0529, -0.0283,
          0.0499,  0.0239,  0.0322, -0.0028, -0.0331,  0.0347, -0.0283, -0.0955,
          0.0450,  0.0499,  0.0235, -0.0391, -0.1374, -0.0066,  0.0175, -0.0570,
         -0.0168,  0.0698,  0.0020, -0.0838, -0.0949,  0.0886, -0.0324,  0.0610,
         -0.1258,  0.0944,  0.0588,  0.0652,  0.0702,  0.0118,  0.1516,  0.1334,
          0.1124, -0.0977,  0.0398, -0.0054, -0.1089, -0.1190,  0.0225, -0.1152,
          0.0057, -0.0841, -0.1516, -0.1393,  0.0096,  0.1061, -0.1135, -0.0421,
          0.0771, -0.1227,  0.0132, -0.1201, -0.1291, -0.1108,  0.1009, -0.1329,
         -0.0234,  0.0434,  0.0121,  0.0323, -0.1146, -0.0576, -0.1112,  0.0759,
         -0.1230, -0.0507, -0.1069,  0.0320, -0.0054,  0.0791,  0.0678, -0.1273,
          0.0018,  0.1448,  0.1128, -0.0116,  0.0870, -0.0891, -0.0122, -0.1135,
          0.0618,  0.0642, -0.1308, -0.1470,  0.0029, -0.1130, -0.1175, -0.0883,
          0.0103, -0.0562,  0.0065,  0.1357,  0.0068,  0.1137, -0.0538,  0.0750,
         -0.0488, -0.0535, -0.1101, -0.0791, -0.1500, -0.0548, -0.0411, -0.0903,
         -0.1379, -0.0071,  0.0263,  0.1287, -0.0317,  0.0167, -0.0701, -0.0703,
         -0.0462, -0.0238, -0.0862,  0.0666, -0.1121,  0.0935,  0.0083,  0.0121,
         -0.0358, -0.1122, -0.0450,  0.1233,  0.1138, -0.1182, -0.1210, -0.0330,
         -0.0946, -0.0408, -0.0604, -0.1173, -0.1499, -0.1070,  0.1019, -0.0815]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[ 0.0538, -0.0429,  0.0608,  ..., -0.1121, -0.0555, -0.0708],
        [ 0.1093, -0.0963, -0.0224,  ...,  0.0251,  0.1241,  0.0071],
        [-0.0946, -0.0887,  0.0578,  ...,  0.0281, -0.0047,  0.0308],
        ...,
        [ 0.0551, -0.0051, -0.0565,  ..., -0.0072, -0.1187, -0.0386],
        [-0.0459, -0.1146,  0.0427,  ...,  0.0453, -0.0093, -0.1224],
        [ 0.0230, -0.1071,  0.0867,  ...,  0.0123,  0.1160,  0.0222]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0538, -0.0429,  0.0608,  ..., -0.1121, -0.0555, -0.0708],
        [ 0.1093, -0.0963, -0.0224,  ...,  0.0251,  0.1241,  0.0071],
        [-0.0946, -0.0887,  0.0578,  ...,  0.0281, -0.0047,  0.0308],
        ...,
        [ 0.0551, -0.0051, -0.0565,  ..., -0.0072, -0.1187, -0.0386],
        [-0.0459, -0.1146,  0.0427,  ...,  0.0453, -0.0093, -0.1224],
        [ 0.0230, -0.1071,  0.0867,  ...,  0.0123,  0.1160,  0.0222]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.0166,  0.0988,  0.0474,  ..., -0.0883, -0.0534,  0.1221],
        [-0.1404, -0.1318,  0.0246,  ...,  0.1011,  0.1764,  0.1656],
        [ 0.0465,  0.0644, -0.0142,  ...,  0.0869, -0.0698, -0.0054],
        ...,
        [ 0.1621,  0.0397, -0.0972,  ...,  0.1726, -0.1450,  0.0118],
        [-0.0551,  0.1436,  0.1445,  ...,  0.1070,  0.0668, -0.0758],
        [ 0.1180, -0.0294, -0.1347,  ...,  0.0109,  0.1110,  0.0786]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0166,  0.0988,  0.0474,  ..., -0.0883, -0.0534,  0.1221],
        [-0.1404, -0.1318,  0.0246,  ...,  0.1011,  0.1764,  0.1656],
        [ 0.0465,  0.0644, -0.0142,  ...,  0.0869, -0.0698, -0.0054],
        ...,
        [ 0.1621,  0.0397, -0.0972,  ...,  0.1726, -0.1450,  0.0118],
        [-0.0551,  0.1436,  0.1445,  ...,  0.1070,  0.0668, -0.0758],
        [ 0.1180, -0.0294, -0.1347,  ...,  0.0109,  0.1110,  0.0786]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.0483, -0.1791,  0.1354,  ..., -0.2132,  0.1533, -0.1104],
        [ 0.1260,  0.1064,  0.1751,  ..., -0.1044,  0.0808, -0.1782],
        [-0.0643, -0.0982, -0.1134,  ...,  0.0974, -0.0732, -0.1748],
        ...,
        [-0.2166,  0.0531,  0.0323,  ...,  0.2302, -0.1759,  0.1459],
        [ 0.0293,  0.1779, -0.2244,  ..., -0.1633,  0.0419, -0.0337],
        [ 0.0205,  0.1821, -0.2414,  ..., -0.1188,  0.1623, -0.1905]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0483, -0.1791,  0.1354,  ..., -0.2132,  0.1533, -0.1104],
        [ 0.1260,  0.1064,  0.1751,  ..., -0.1044,  0.0808, -0.1782],
        [-0.0643, -0.0982, -0.1134,  ...,  0.0974, -0.0732, -0.1748],
        ...,
        [-0.2166,  0.0531,  0.0323,  ...,  0.2302, -0.1759,  0.1459],
        [ 0.0293,  0.1779, -0.2244,  ..., -0.1633,  0.0419, -0.0337],
        [ 0.0205,  0.1821, -0.2414,  ..., -0.1188,  0.1623, -0.1905]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.0678],
        [ 0.1487],
        [ 0.1838],
        [ 0.1166],
        [ 0.4037],
        [-0.2528],
        [ 0.3955],
        [-0.2396],
        [ 0.0172],
        [-0.0973],
        [ 0.3380],
        [-0.2942],
        [-0.0945],
        [-0.0213],
        [ 0.1792],
        [ 0.2902],
        [ 0.0055],
        [ 0.2147],
        [-0.1096],
        [-0.2926],
        [-0.3568],
        [ 0.1337],
        [ 0.4232],
        [-0.0598],
        [ 0.1386],
        [-0.1105],
        [ 0.2800],
        [-0.4242],
        [ 0.3239],
        [-0.3490],
        [-0.2984],
        [ 0.0950]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0678],
        [ 0.1487],
        [ 0.1838],
        [ 0.1166],
        [ 0.4037],
        [-0.2528],
        [ 0.3955],
        [-0.2396],
        [ 0.0172],
        [-0.0973],
        [ 0.3380],
        [-0.2942],
        [-0.0945],
        [-0.0213],
        [ 0.1792],
        [ 0.2902],
        [ 0.0055],
        [ 0.2147],
        [-0.1096],
        [-0.2926],
        [-0.3568],
        [ 0.1337],
        [ 0.4232],
        [-0.0598],
        [ 0.1386],
        [-0.1105],
        [ 0.2800],
        [-0.4242],
        [ 0.3239],
        [-0.3490],
        [-0.2984],
        [ 0.0950]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-1.2081, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4323, device='cuda:0')



h[100].sum tensor(11.4303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6861, device='cuda:0')



h[200].sum tensor(-7.4779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.6452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10399.5332, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0175, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0048,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(58489.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0434, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(395.5383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(25.1489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-8.7348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4289],
        [-0.3021],
        [-0.2033],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-19843.6387, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.4289],
        [-0.3021],
        [-0.2033],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0119, -0.0135,  0.0211,  ..., -0.0079, -0.0002, -0.0134],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(661.5828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-78.3299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-80.2963, device='cuda:0')



h[100].sum tensor(59.1497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(60.6346, device='cuda:0')



h[200].sum tensor(-38.6753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-39.6462, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0797,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0655,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0153,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(102947.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3137, 0.3162, 0.0000,  ..., 0.0000, 0.3185, 0.2487],
        [0.2688, 0.2710, 0.0000,  ..., 0.0000, 0.2729, 0.2131],
        [0.2158, 0.2176, 0.0000,  ..., 0.0000, 0.2191, 0.1711],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(672771.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(15527.4111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1087.6663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14793.0537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1036.2257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1597.9738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(111.9350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.9453e+00],
        [2.1049e+00],
        [2.3227e+00],
        ...,
        [2.5343e-05],
        [4.2136e-05],
        [6.0231e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(135038.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.4289],
        [-0.3021],
        [-0.2033],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 41, in <module>
    result1 = net(batcheddglgraph, TraTen[10000:10010].reshape(10 * 6796, 1).to(device))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 37, in forward
    g.ndata['nfet'] = in_fet
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/view.py", line 81, in __setitem__
    self._graph._set_n_repr(self._ntid, self._nodes, {key : val})
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/heterograph.py", line 4113, in _set_n_repr
    raise DGLError('Cannot assign node feature "{}" on device {} to a graph on'
dgl._ffi.base.DGLError: Cannot assign node feature "nfet" on device cpu to a graph on device cuda:0. Call DGLGraph.to() to copy the graph to the same device.

real	1m24.839s
user	0m18.678s
sys	0m9.041s
