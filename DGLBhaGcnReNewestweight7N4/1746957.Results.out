0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 12:26:49 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b9f74f288e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m39.743s
user	0m3.673s
sys	0m3.362s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[12:28:12] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-1.5743],
        [-1.4407],
        [ 1.1875],
        ...,
        [ 1.1245],
        [-0.0367],
        [-0.1124]], device='cuda:0', requires_grad=True) 
node features sum: tensor(103.9437, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.1307,  0.0248, -0.1525,  0.1492,  0.1062, -0.0154, -0.1335,  0.0215,
         -0.1167, -0.0730, -0.0200,  0.1114, -0.0985,  0.0525,  0.0157, -0.1147,
          0.1307, -0.0787,  0.0789,  0.1336,  0.0395,  0.0669, -0.1174, -0.0357,
         -0.1102, -0.0444, -0.0927,  0.0669, -0.0863,  0.0041,  0.0645, -0.0226,
          0.1300, -0.0035, -0.1321, -0.0294, -0.1511,  0.1403, -0.0663,  0.1478,
         -0.1078, -0.1159, -0.1433,  0.0159, -0.1054,  0.1040,  0.1134,  0.0453,
         -0.0850, -0.0952,  0.0806,  0.0600,  0.0295, -0.0634,  0.0557,  0.0906,
          0.1219, -0.1333,  0.0662,  0.0601,  0.0541,  0.0575,  0.0270,  0.1072,
         -0.0586, -0.1022, -0.0435,  0.0166,  0.0871,  0.0513, -0.0656,  0.1308,
         -0.0134, -0.1044, -0.0841,  0.1106,  0.0612, -0.1060,  0.1284, -0.1102,
          0.1323,  0.0690, -0.1180,  0.0056,  0.0832,  0.0519, -0.1378, -0.0481,
         -0.0587, -0.1390, -0.1009,  0.0094, -0.1022, -0.0681, -0.0987, -0.0268,
          0.1308, -0.0286,  0.0879, -0.1045,  0.1277,  0.0844,  0.1475, -0.0039,
          0.0693, -0.0504, -0.0303, -0.1488, -0.1321, -0.0462, -0.0353, -0.1020,
         -0.1000,  0.0572, -0.0786,  0.0377, -0.0423,  0.1207, -0.1376, -0.0737,
         -0.0703, -0.0884, -0.1031,  0.1314,  0.1291,  0.0051, -0.0478,  0.0273,
         -0.0889,  0.0709, -0.0239,  0.0788,  0.0382,  0.0397, -0.1046,  0.0010,
          0.0302,  0.0838, -0.0039, -0.0236, -0.1321, -0.0365, -0.0240, -0.1440,
         -0.1331, -0.0577, -0.0366,  0.1384,  0.1246,  0.0973, -0.0891,  0.0282,
         -0.1174,  0.0087,  0.0508,  0.1057, -0.1047,  0.0397, -0.1122, -0.1524,
         -0.0288, -0.1000, -0.1090, -0.1179, -0.0871, -0.0650,  0.1290, -0.1180,
          0.0800,  0.1492,  0.0151, -0.1056,  0.0512,  0.1523, -0.1043,  0.0389,
          0.0372,  0.0108, -0.0944,  0.0857,  0.1278, -0.0962,  0.0291, -0.0194,
          0.1182,  0.0774, -0.1512,  0.0330, -0.0825, -0.0293,  0.0927,  0.0479,
         -0.0862,  0.0269, -0.0191, -0.0912, -0.0673,  0.0032, -0.0460, -0.0411,
         -0.0990,  0.1506,  0.0662,  0.1218, -0.0955, -0.0747, -0.0241,  0.0897,
         -0.0624, -0.0166,  0.1365,  0.1320,  0.0078,  0.0859, -0.1280, -0.0582,
         -0.0126,  0.0511,  0.1457,  0.1265,  0.1461,  0.0533,  0.0877,  0.0266,
          0.1157, -0.0822,  0.1248,  0.1080, -0.0533, -0.0534,  0.0330, -0.0567,
         -0.0577, -0.0524, -0.1291, -0.0587, -0.0792, -0.0618, -0.1395, -0.1515,
          0.1095,  0.0915, -0.0715,  0.1337,  0.0135,  0.0255, -0.1144,  0.0613,
          0.0938,  0.0023,  0.1321, -0.1507,  0.0668, -0.1475, -0.0752, -0.0233]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1307,  0.0248, -0.1525,  0.1492,  0.1062, -0.0154, -0.1335,  0.0215,
         -0.1167, -0.0730, -0.0200,  0.1114, -0.0985,  0.0525,  0.0157, -0.1147,
          0.1307, -0.0787,  0.0789,  0.1336,  0.0395,  0.0669, -0.1174, -0.0357,
         -0.1102, -0.0444, -0.0927,  0.0669, -0.0863,  0.0041,  0.0645, -0.0226,
          0.1300, -0.0035, -0.1321, -0.0294, -0.1511,  0.1403, -0.0663,  0.1478,
         -0.1078, -0.1159, -0.1433,  0.0159, -0.1054,  0.1040,  0.1134,  0.0453,
         -0.0850, -0.0952,  0.0806,  0.0600,  0.0295, -0.0634,  0.0557,  0.0906,
          0.1219, -0.1333,  0.0662,  0.0601,  0.0541,  0.0575,  0.0270,  0.1072,
         -0.0586, -0.1022, -0.0435,  0.0166,  0.0871,  0.0513, -0.0656,  0.1308,
         -0.0134, -0.1044, -0.0841,  0.1106,  0.0612, -0.1060,  0.1284, -0.1102,
          0.1323,  0.0690, -0.1180,  0.0056,  0.0832,  0.0519, -0.1378, -0.0481,
         -0.0587, -0.1390, -0.1009,  0.0094, -0.1022, -0.0681, -0.0987, -0.0268,
          0.1308, -0.0286,  0.0879, -0.1045,  0.1277,  0.0844,  0.1475, -0.0039,
          0.0693, -0.0504, -0.0303, -0.1488, -0.1321, -0.0462, -0.0353, -0.1020,
         -0.1000,  0.0572, -0.0786,  0.0377, -0.0423,  0.1207, -0.1376, -0.0737,
         -0.0703, -0.0884, -0.1031,  0.1314,  0.1291,  0.0051, -0.0478,  0.0273,
         -0.0889,  0.0709, -0.0239,  0.0788,  0.0382,  0.0397, -0.1046,  0.0010,
          0.0302,  0.0838, -0.0039, -0.0236, -0.1321, -0.0365, -0.0240, -0.1440,
         -0.1331, -0.0577, -0.0366,  0.1384,  0.1246,  0.0973, -0.0891,  0.0282,
         -0.1174,  0.0087,  0.0508,  0.1057, -0.1047,  0.0397, -0.1122, -0.1524,
         -0.0288, -0.1000, -0.1090, -0.1179, -0.0871, -0.0650,  0.1290, -0.1180,
          0.0800,  0.1492,  0.0151, -0.1056,  0.0512,  0.1523, -0.1043,  0.0389,
          0.0372,  0.0108, -0.0944,  0.0857,  0.1278, -0.0962,  0.0291, -0.0194,
          0.1182,  0.0774, -0.1512,  0.0330, -0.0825, -0.0293,  0.0927,  0.0479,
         -0.0862,  0.0269, -0.0191, -0.0912, -0.0673,  0.0032, -0.0460, -0.0411,
         -0.0990,  0.1506,  0.0662,  0.1218, -0.0955, -0.0747, -0.0241,  0.0897,
         -0.0624, -0.0166,  0.1365,  0.1320,  0.0078,  0.0859, -0.1280, -0.0582,
         -0.0126,  0.0511,  0.1457,  0.1265,  0.1461,  0.0533,  0.0877,  0.0266,
          0.1157, -0.0822,  0.1248,  0.1080, -0.0533, -0.0534,  0.0330, -0.0567,
         -0.0577, -0.0524, -0.1291, -0.0587, -0.0792, -0.0618, -0.1395, -0.1515,
          0.1095,  0.0915, -0.0715,  0.1337,  0.0135,  0.0255, -0.1144,  0.0613,
          0.0938,  0.0023,  0.1321, -0.1507,  0.0668, -0.1475, -0.0752, -0.0233]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0721, -0.0275,  0.1232,  ...,  0.1125,  0.0679, -0.0414],
        [-0.0575, -0.0133, -0.0697,  ..., -0.1100, -0.0242, -0.0936],
        [ 0.1200, -0.0846, -0.0165,  ..., -0.0993, -0.0352, -0.0197],
        ...,
        [-0.0740,  0.0349, -0.0751,  ..., -0.1242, -0.0864, -0.0932],
        [ 0.0960,  0.0560, -0.1032,  ...,  0.1091,  0.0289,  0.0886],
        [ 0.1106, -0.0210,  0.0708,  ..., -0.1157, -0.1112,  0.0738]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0721, -0.0275,  0.1232,  ...,  0.1125,  0.0679, -0.0414],
        [-0.0575, -0.0133, -0.0697,  ..., -0.1100, -0.0242, -0.0936],
        [ 0.1200, -0.0846, -0.0165,  ..., -0.0993, -0.0352, -0.0197],
        ...,
        [-0.0740,  0.0349, -0.0751,  ..., -0.1242, -0.0864, -0.0932],
        [ 0.0960,  0.0560, -0.1032,  ...,  0.1091,  0.0289,  0.0886],
        [ 0.1106, -0.0210,  0.0708,  ..., -0.1157, -0.1112,  0.0738]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.1106, -0.0021,  0.1123,  ...,  0.1589, -0.1504, -0.0096],
        [-0.0225,  0.1569, -0.0957,  ...,  0.0716,  0.1626,  0.0637],
        [-0.1312,  0.0864,  0.0632,  ..., -0.1054,  0.0672, -0.1688],
        ...,
        [-0.1405, -0.1025, -0.0758,  ..., -0.0008,  0.0814,  0.1492],
        [ 0.0455,  0.1663,  0.0181,  ..., -0.1410,  0.0336,  0.1269],
        [ 0.0808,  0.0408,  0.0923,  ..., -0.0381, -0.0304, -0.0788]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1106, -0.0021,  0.1123,  ...,  0.1589, -0.1504, -0.0096],
        [-0.0225,  0.1569, -0.0957,  ...,  0.0716,  0.1626,  0.0637],
        [-0.1312,  0.0864,  0.0632,  ..., -0.1054,  0.0672, -0.1688],
        ...,
        [-0.1405, -0.1025, -0.0758,  ..., -0.0008,  0.0814,  0.1492],
        [ 0.0455,  0.1663,  0.0181,  ..., -0.1410,  0.0336,  0.1269],
        [ 0.0808,  0.0408,  0.0923,  ..., -0.0381, -0.0304, -0.0788]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.0614,  0.1655,  0.2140,  ...,  0.0889,  0.1839, -0.2469],
        [ 0.0468,  0.0362, -0.0054,  ...,  0.1821, -0.1848, -0.1765],
        [-0.2193,  0.1715, -0.0851,  ..., -0.1486, -0.0442, -0.0590],
        ...,
        [-0.0013, -0.1265,  0.1190,  ..., -0.0429, -0.2399, -0.0330],
        [ 0.1314, -0.1804, -0.1292,  ...,  0.0296,  0.1839,  0.1307],
        [-0.0345, -0.1419,  0.0206,  ..., -0.2157, -0.1320, -0.0448]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0614,  0.1655,  0.2140,  ...,  0.0889,  0.1839, -0.2469],
        [ 0.0468,  0.0362, -0.0054,  ...,  0.1821, -0.1848, -0.1765],
        [-0.2193,  0.1715, -0.0851,  ..., -0.1486, -0.0442, -0.0590],
        ...,
        [-0.0013, -0.1265,  0.1190,  ..., -0.0429, -0.2399, -0.0330],
        [ 0.1314, -0.1804, -0.1292,  ...,  0.0296,  0.1839,  0.1307],
        [-0.0345, -0.1419,  0.0206,  ..., -0.2157, -0.1320, -0.0448]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.0456],
        [-0.3743],
        [ 0.0962],
        [-0.0054],
        [-0.2017],
        [ 0.2080],
        [ 0.2718],
        [ 0.0029],
        [ 0.3543],
        [ 0.2892],
        [-0.2430],
        [ 0.0074],
        [ 0.1354],
        [-0.1609],
        [-0.1099],
        [-0.0963],
        [ 0.2796],
        [ 0.1309],
        [ 0.1588],
        [ 0.2324],
        [-0.0440],
        [ 0.3519],
        [-0.2320],
        [ 0.4168],
        [ 0.0163],
        [-0.3548],
        [-0.1926],
        [-0.2589],
        [ 0.3366],
        [-0.2614],
        [ 0.0737],
        [-0.1191]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0456],
        [-0.3743],
        [ 0.0962],
        [-0.0054],
        [-0.2017],
        [ 0.2080],
        [ 0.2718],
        [ 0.0029],
        [ 0.3543],
        [ 0.2892],
        [-0.2430],
        [ 0.0074],
        [ 0.1354],
        [-0.1609],
        [-0.1099],
        [-0.0963],
        [ 0.2796],
        [ 0.1309],
        [ 0.1588],
        [ 0.2324],
        [-0.0440],
        [ 0.3519],
        [-0.2320],
        [ 0.4168],
        [ 0.0163],
        [-0.3548],
        [-0.1926],
        [-0.2589],
        [ 0.3366],
        [-0.2614],
        [ 0.0737],
        [-0.1191]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(209.1011, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(8.8793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(9.0781, device='cuda:0')



h[100].sum tensor(10.1283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3550, device='cuda:0')



h[200].sum tensor(-9.8039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.0233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(11554.2920, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0025, 0.0197,  ..., 0.0043, 0.0079, 0.0000],
        [0.0000, 0.0012, 0.0092,  ..., 0.0020, 0.0037, 0.0000],
        [0.0000, 0.0003, 0.0026,  ..., 0.0006, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(58973.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-165.6622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-24.5921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-8.7173, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1140],
        [0.0803],
        [0.0541],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(5277.6074, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[0.1140],
        [0.0803],
        [0.0541],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0006,  0.0084,  ...,  0.0105, -0.0103, -0.0217],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-30.7455, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(70.1346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(71.8952, device='cuda:0')



h[100].sum tensor(66.0553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(67.7136, device='cuda:0')



h[200].sum tensor(48.0915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(49.2988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0403, 0.0025, 0.0316,  ..., 0.0397, 0.0000, 0.0000],
        [0.0332, 0.0020, 0.0260,  ..., 0.0326, 0.0000, 0.0000],
        [0.0078, 0.0005, 0.0061,  ..., 0.0076, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(102534.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0372, 0.1587, 0.2018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0319, 0.1360, 0.1729,  ..., 0.0000, 0.0000, 0.0000],
        [0.0256, 0.1092, 0.1389,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(491314.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1841.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(129.1365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1313.5215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.2405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6846e+00],
        [-2.9049e+00],
        [-3.2054e+00],
        ...,
        [-3.4981e-05],
        [-5.8159e-05],
        [-8.3134e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-186368.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.1140],
        [0.0803],
        [0.0541],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.8698, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5508, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2284.9644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3761.7083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.0526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0222,  0.0100,  0.0086,  ..., -0.0044,  0.0293,  0.0029],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0282,  0.0127,  0.0110,  ..., -0.0056,  0.0373,  0.0037],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(48.8399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(61.3970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.8698, device='cuda:0')



h[100].sum tensor(-50.8676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5508, device='cuda:0')



h[200].sum tensor(49.8982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0082, 0.0070,  ..., 0.0000, 0.0239, 0.0024],
        [0.0916, 0.0412, 0.0356,  ..., 0.0000, 0.1210, 0.0120],
        [0.0345, 0.0155, 0.0134,  ..., 0.0000, 0.0455, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(56430.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0538, 0.0238,  ..., 0.0000, 0.0445, 0.0987],
        [0.0000, 0.1030, 0.0455,  ..., 0.0000, 0.0854, 0.1891],
        [0.0000, 0.0859, 0.0380,  ..., 0.0000, 0.0712, 0.1577],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(292941.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-26.8141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1302.5671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-455.4972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.3138],
        [1.6364],
        [1.7886],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(229137.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 59, in <module>
    plotevent(traingnnpppipi[EvBTr], 331, f'event number {EvBTr} with noise \n color indicates time')
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 47, in plotevent
    fig.colorbar(ax.matshow(event, aspect=2, vmin=0, vmax=1, extent=[0, 288, 0, 43], origin='lower')\
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/axes/_axes.py", line 7931, in matshow
    im = self.imshow(Z, **kw)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/__init__.py", line 1361, in inner
    return func(ax, *map(sanitize_sequence, args), **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/axes/_axes.py", line 5609, in imshow
    im.set_data(X)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/image.py", line 709, in set_data
    raise TypeError("Invalid shape {} for image data"
TypeError: Invalid shape (6796,) for image data

real	1m12.929s
user	0m19.928s
sys	0m11.395s
