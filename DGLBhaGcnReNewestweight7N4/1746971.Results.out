0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 13:24:38 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   22C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ab5978408e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.265s
user	0m2.696s
sys	0m1.242s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[13:25:03] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.3646],
        [ 0.9478],
        [-0.4969],
        ...,
        [-1.1041],
        [-1.0076],
        [ 0.4766]], device='cuda:0', requires_grad=True) 
node features sum: tensor(2.1763, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-2.9659e-02,  7.6819e-02, -1.3400e-01, -9.2219e-02, -4.5372e-02,
         -8.7757e-03, -4.7168e-02, -4.5282e-02, -4.5280e-02, -5.8252e-02,
         -5.3966e-02,  3.0540e-02,  3.7925e-02, -2.7923e-02, -1.3808e-01,
          5.5494e-02, -9.5306e-02,  1.4089e-01,  1.6702e-02,  1.2443e-01,
          5.7287e-02, -1.4473e-01, -4.7615e-02, -6.2072e-02, -1.0995e-02,
         -2.5037e-02,  2.0576e-02,  8.6259e-02, -1.0036e-01, -7.1050e-03,
         -2.2427e-02, -2.2849e-02,  1.0534e-02,  1.1683e-01, -3.8835e-02,
          1.4719e-01, -1.8838e-02, -1.2921e-01, -1.0200e-01, -4.4683e-02,
         -1.0942e-01, -3.4871e-02,  1.2659e-01,  4.0358e-02,  3.9074e-02,
         -5.1435e-02, -2.6141e-02,  1.4157e-01,  2.3966e-02, -1.3232e-01,
          1.1805e-01,  6.3187e-02,  1.2025e-01,  9.3883e-02,  5.7622e-02,
          7.1045e-02, -1.3362e-01, -1.0768e-01, -1.5007e-01, -4.3903e-02,
          3.9602e-02,  1.1586e-01,  1.1335e-01,  6.4671e-02,  1.1659e-01,
         -2.8701e-02, -1.7889e-02,  1.4157e-01,  4.0818e-02,  5.6982e-02,
         -4.4979e-02, -5.1990e-02, -3.9126e-02, -6.7707e-02,  1.4470e-01,
         -3.3776e-02, -3.2846e-02,  1.1252e-01,  5.7349e-04,  1.2300e-02,
          5.3999e-02, -1.4965e-01, -9.2208e-02, -3.0675e-03,  4.8361e-02,
         -1.1533e-04, -2.1317e-02, -1.0769e-01,  4.9591e-02,  1.3808e-01,
         -1.2336e-01,  9.2501e-02,  1.1057e-01,  1.3246e-01, -1.4850e-01,
         -8.6625e-02, -1.3744e-01,  1.4057e-01, -1.9863e-02, -1.0035e-01,
         -1.4674e-01,  1.4563e-01, -1.4969e-01, -1.0977e-02,  1.4748e-01,
          1.3351e-01, -2.0423e-02,  1.8319e-02,  2.1764e-03, -1.1958e-01,
         -3.8048e-04,  8.9108e-02, -7.4886e-02,  1.1776e-01, -4.6885e-02,
         -4.5569e-02,  1.0364e-01,  8.2976e-02,  4.9946e-02, -9.3659e-02,
         -3.3275e-02, -1.3209e-01,  1.4653e-01,  4.5043e-02,  3.2259e-02,
         -1.0945e-01,  5.5349e-02, -9.3196e-02,  5.1350e-02,  9.7439e-02,
         -1.4320e-02,  1.4537e-01, -2.9693e-02, -1.2666e-01, -1.4797e-01,
         -6.6205e-02, -2.0427e-02,  1.4499e-01,  1.0231e-01,  1.0273e-04,
         -5.8834e-02, -6.8863e-02,  1.0242e-01, -8.8072e-03,  5.9410e-04,
         -1.4859e-01,  4.8467e-02,  5.1398e-02,  6.7590e-02, -9.6966e-02,
          4.1830e-03,  6.6081e-02,  2.8846e-02,  1.1295e-01,  5.2200e-02,
         -3.5701e-02, -2.0206e-02,  8.9137e-02,  9.0965e-02,  4.1715e-02,
         -2.1743e-02,  1.3166e-01,  1.0104e-04, -7.0953e-02, -1.9696e-02,
         -8.7868e-02,  4.0572e-02,  1.0518e-01, -2.6739e-02,  5.4915e-02,
          2.0840e-02, -2.3645e-02, -8.5692e-02, -8.1487e-02,  1.4340e-01,
         -1.4501e-01, -1.0563e-01,  9.4429e-02,  1.4619e-01,  3.0541e-02,
         -1.1296e-01,  1.0607e-01,  9.1633e-02,  4.7079e-03, -1.2226e-01,
          2.2467e-02,  9.5109e-02, -1.1509e-01, -4.5631e-02, -1.7413e-02,
         -5.5677e-02,  6.8115e-02, -9.6085e-02,  1.2119e-01,  7.1745e-02,
          7.9724e-02, -3.6677e-02,  3.8153e-02, -1.5163e-01,  5.1254e-02,
          1.5248e-01, -1.2599e-01, -1.2898e-01,  4.7107e-02, -1.0614e-01,
         -1.2727e-01,  1.3497e-01,  8.2705e-03, -6.3733e-03, -3.1990e-02,
         -1.4977e-01,  1.3300e-01, -1.1309e-01, -2.4016e-02, -1.2235e-01,
         -1.3319e-01,  2.1518e-02, -5.3582e-03, -4.8795e-02,  5.4103e-02,
          5.3221e-03, -5.3661e-02,  6.4269e-02,  1.3888e-01,  3.2528e-02,
         -1.0690e-02, -1.3416e-01, -4.2698e-02, -1.2797e-02, -1.3956e-01,
          1.4406e-01, -6.5786e-02, -2.5837e-02,  6.9500e-03,  8.6449e-02,
          1.4361e-01,  7.2802e-02, -1.1264e-01,  2.2987e-02, -6.5396e-02,
         -1.5006e-01,  1.0393e-01,  1.0673e-01, -8.1082e-02, -7.5340e-02,
          1.0487e-02, -9.1728e-02, -6.4551e-02,  1.3956e-03,  1.0956e-01,
         -1.1191e-01, -3.6893e-03,  4.3893e-02,  1.2157e-01, -1.2690e-01,
          5.5574e-02]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-2.9659e-02,  7.6819e-02, -1.3400e-01, -9.2219e-02, -4.5372e-02,
         -8.7757e-03, -4.7168e-02, -4.5282e-02, -4.5280e-02, -5.8252e-02,
         -5.3966e-02,  3.0540e-02,  3.7925e-02, -2.7923e-02, -1.3808e-01,
          5.5494e-02, -9.5306e-02,  1.4089e-01,  1.6702e-02,  1.2443e-01,
          5.7287e-02, -1.4473e-01, -4.7615e-02, -6.2072e-02, -1.0995e-02,
         -2.5037e-02,  2.0576e-02,  8.6259e-02, -1.0036e-01, -7.1050e-03,
         -2.2427e-02, -2.2849e-02,  1.0534e-02,  1.1683e-01, -3.8835e-02,
          1.4719e-01, -1.8838e-02, -1.2921e-01, -1.0200e-01, -4.4683e-02,
         -1.0942e-01, -3.4871e-02,  1.2659e-01,  4.0358e-02,  3.9074e-02,
         -5.1435e-02, -2.6141e-02,  1.4157e-01,  2.3966e-02, -1.3232e-01,
          1.1805e-01,  6.3187e-02,  1.2025e-01,  9.3883e-02,  5.7622e-02,
          7.1045e-02, -1.3362e-01, -1.0768e-01, -1.5007e-01, -4.3903e-02,
          3.9602e-02,  1.1586e-01,  1.1335e-01,  6.4671e-02,  1.1659e-01,
         -2.8701e-02, -1.7889e-02,  1.4157e-01,  4.0818e-02,  5.6982e-02,
         -4.4979e-02, -5.1990e-02, -3.9126e-02, -6.7707e-02,  1.4470e-01,
         -3.3776e-02, -3.2846e-02,  1.1252e-01,  5.7349e-04,  1.2300e-02,
          5.3999e-02, -1.4965e-01, -9.2208e-02, -3.0675e-03,  4.8361e-02,
         -1.1533e-04, -2.1317e-02, -1.0769e-01,  4.9591e-02,  1.3808e-01,
         -1.2336e-01,  9.2501e-02,  1.1057e-01,  1.3246e-01, -1.4850e-01,
         -8.6625e-02, -1.3744e-01,  1.4057e-01, -1.9863e-02, -1.0035e-01,
         -1.4674e-01,  1.4563e-01, -1.4969e-01, -1.0977e-02,  1.4748e-01,
          1.3351e-01, -2.0423e-02,  1.8319e-02,  2.1764e-03, -1.1958e-01,
         -3.8048e-04,  8.9108e-02, -7.4886e-02,  1.1776e-01, -4.6885e-02,
         -4.5569e-02,  1.0364e-01,  8.2976e-02,  4.9946e-02, -9.3659e-02,
         -3.3275e-02, -1.3209e-01,  1.4653e-01,  4.5043e-02,  3.2259e-02,
         -1.0945e-01,  5.5349e-02, -9.3196e-02,  5.1350e-02,  9.7439e-02,
         -1.4320e-02,  1.4537e-01, -2.9693e-02, -1.2666e-01, -1.4797e-01,
         -6.6205e-02, -2.0427e-02,  1.4499e-01,  1.0231e-01,  1.0273e-04,
         -5.8834e-02, -6.8863e-02,  1.0242e-01, -8.8072e-03,  5.9410e-04,
         -1.4859e-01,  4.8467e-02,  5.1398e-02,  6.7590e-02, -9.6966e-02,
          4.1830e-03,  6.6081e-02,  2.8846e-02,  1.1295e-01,  5.2200e-02,
         -3.5701e-02, -2.0206e-02,  8.9137e-02,  9.0965e-02,  4.1715e-02,
         -2.1743e-02,  1.3166e-01,  1.0104e-04, -7.0953e-02, -1.9696e-02,
         -8.7868e-02,  4.0572e-02,  1.0518e-01, -2.6739e-02,  5.4915e-02,
          2.0840e-02, -2.3645e-02, -8.5692e-02, -8.1487e-02,  1.4340e-01,
         -1.4501e-01, -1.0563e-01,  9.4429e-02,  1.4619e-01,  3.0541e-02,
         -1.1296e-01,  1.0607e-01,  9.1633e-02,  4.7079e-03, -1.2226e-01,
          2.2467e-02,  9.5109e-02, -1.1509e-01, -4.5631e-02, -1.7413e-02,
         -5.5677e-02,  6.8115e-02, -9.6085e-02,  1.2119e-01,  7.1745e-02,
          7.9724e-02, -3.6677e-02,  3.8153e-02, -1.5163e-01,  5.1254e-02,
          1.5248e-01, -1.2599e-01, -1.2898e-01,  4.7107e-02, -1.0614e-01,
         -1.2727e-01,  1.3497e-01,  8.2705e-03, -6.3733e-03, -3.1990e-02,
         -1.4977e-01,  1.3300e-01, -1.1309e-01, -2.4016e-02, -1.2235e-01,
         -1.3319e-01,  2.1518e-02, -5.3582e-03, -4.8795e-02,  5.4103e-02,
          5.3221e-03, -5.3661e-02,  6.4269e-02,  1.3888e-01,  3.2528e-02,
         -1.0690e-02, -1.3416e-01, -4.2698e-02, -1.2797e-02, -1.3956e-01,
          1.4406e-01, -6.5786e-02, -2.5837e-02,  6.9500e-03,  8.6449e-02,
          1.4361e-01,  7.2802e-02, -1.1264e-01,  2.2987e-02, -6.5396e-02,
         -1.5006e-01,  1.0393e-01,  1.0673e-01, -8.1082e-02, -7.5340e-02,
          1.0487e-02, -9.1728e-02, -6.4551e-02,  1.3956e-03,  1.0956e-01,
         -1.1191e-01, -3.6893e-03,  4.3893e-02,  1.2157e-01, -1.2690e-01,
          5.5574e-02]], device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.1047,  0.0098, -0.0065,  ..., -0.1213,  0.0826, -0.0493],
        [-0.0258, -0.0266,  0.0887,  ...,  0.0368,  0.1002,  0.1193],
        [ 0.0464, -0.0882, -0.1218,  ..., -0.1064,  0.0481,  0.0499],
        ...,
        [ 0.0038,  0.1243, -0.0464,  ..., -0.1169, -0.0506, -0.0523],
        [-0.0563,  0.1023,  0.1110,  ...,  0.0989,  0.1019, -0.0259],
        [ 0.0347, -0.0518, -0.0756,  ..., -0.0664,  0.0318,  0.0134]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1047,  0.0098, -0.0065,  ..., -0.1213,  0.0826, -0.0493],
        [-0.0258, -0.0266,  0.0887,  ...,  0.0368,  0.1002,  0.1193],
        [ 0.0464, -0.0882, -0.1218,  ..., -0.1064,  0.0481,  0.0499],
        ...,
        [ 0.0038,  0.1243, -0.0464,  ..., -0.1169, -0.0506, -0.0523],
        [-0.0563,  0.1023,  0.1110,  ...,  0.0989,  0.1019, -0.0259],
        [ 0.0347, -0.0518, -0.0756,  ..., -0.0664,  0.0318,  0.0134]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.1681, -0.0328, -0.1735,  ...,  0.0536, -0.1253,  0.1382],
        [-0.0896, -0.0411, -0.0334,  ..., -0.1337,  0.0887, -0.1312],
        [-0.1281,  0.0248, -0.0645,  ...,  0.0637,  0.0168, -0.1714],
        ...,
        [ 0.0081, -0.0401,  0.0406,  ..., -0.0821, -0.1099, -0.1584],
        [ 0.1582,  0.0038, -0.1628,  ..., -0.1246,  0.1476, -0.0465],
        [ 0.0318,  0.0014,  0.0504,  ..., -0.0049,  0.0054,  0.0509]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1681, -0.0328, -0.1735,  ...,  0.0536, -0.1253,  0.1382],
        [-0.0896, -0.0411, -0.0334,  ..., -0.1337,  0.0887, -0.1312],
        [-0.1281,  0.0248, -0.0645,  ...,  0.0637,  0.0168, -0.1714],
        ...,
        [ 0.0081, -0.0401,  0.0406,  ..., -0.0821, -0.1099, -0.1584],
        [ 0.1582,  0.0038, -0.1628,  ..., -0.1246,  0.1476, -0.0465],
        [ 0.0318,  0.0014,  0.0504,  ..., -0.0049,  0.0054,  0.0509]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-2.0465e-01,  5.1368e-02, -1.5189e-01,  ...,  8.4644e-02,
         -1.1671e-02, -1.5965e-01],
        [ 9.6790e-02, -7.9822e-02,  5.2865e-02,  ...,  1.3399e-01,
          1.4860e-01, -1.5491e-01],
        [-1.0653e-01, -1.4787e-01, -1.1141e-01,  ..., -2.0024e-01,
         -2.0793e-01,  1.7353e-01],
        ...,
        [-1.1818e-01,  1.3947e-01, -1.5649e-04,  ..., -2.6465e-02,
          1.5417e-01, -1.2589e-01],
        [-1.1625e-01,  7.8104e-02, -1.9717e-01,  ...,  2.0579e-01,
         -9.4784e-02,  1.2405e-02],
        [-2.6868e-02,  1.2788e-01, -2.2476e-01,  ...,  2.1323e-01,
         -2.0302e-01,  1.4830e-01]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-2.0465e-01,  5.1368e-02, -1.5189e-01,  ...,  8.4644e-02,
         -1.1671e-02, -1.5965e-01],
        [ 9.6790e-02, -7.9822e-02,  5.2865e-02,  ...,  1.3399e-01,
          1.4860e-01, -1.5491e-01],
        [-1.0653e-01, -1.4787e-01, -1.1141e-01,  ..., -2.0024e-01,
         -2.0793e-01,  1.7353e-01],
        ...,
        [-1.1818e-01,  1.3947e-01, -1.5649e-04,  ..., -2.6465e-02,
          1.5417e-01, -1.2589e-01],
        [-1.1625e-01,  7.8104e-02, -1.9717e-01,  ...,  2.0579e-01,
         -9.4784e-02,  1.2405e-02],
        [-2.6868e-02,  1.2788e-01, -2.2476e-01,  ...,  2.1323e-01,
         -2.0302e-01,  1.4830e-01]], device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.1440],
        [ 0.1875],
        [-0.3930],
        [-0.1079],
        [ 0.2117],
        [-0.0992],
        [-0.2909],
        [ 0.1994],
        [-0.4225],
        [ 0.0444],
        [ 0.3589],
        [-0.3225],
        [ 0.0468],
        [-0.1277],
        [ 0.1848],
        [ 0.2991],
        [-0.1663],
        [-0.1822],
        [-0.0674],
        [-0.3116],
        [ 0.1916],
        [ 0.1333],
        [ 0.2694],
        [-0.3375],
        [-0.0249],
        [-0.1488],
        [-0.0283],
        [-0.0417],
        [ 0.1126],
        [-0.2120],
        [ 0.2412],
        [ 0.1448]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1440],
        [ 0.1875],
        [-0.3930],
        [-0.1079],
        [ 0.2117],
        [-0.0992],
        [-0.2909],
        [ 0.1994],
        [-0.4225],
        [ 0.0444],
        [ 0.3589],
        [-0.3225],
        [ 0.0468],
        [-0.1277],
        [ 0.1848],
        [ 0.2991],
        [-0.1663],
        [-0.1822],
        [-0.0674],
        [-0.3116],
        [ 0.1916],
        [ 0.1333],
        [ 0.2694],
        [-0.3375],
        [-0.0249],
        [-0.1488],
        [-0.0283],
        [-0.0417],
        [ 0.1126],
        [-0.2120],
        [ 0.2412],
        [ 0.1448]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-120.7653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.1456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0.1489, device='cuda:0')



h[100].sum tensor(-10.2001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.4284, device='cuda:0')



h[200].sum tensor(-5.1887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-5.3049, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(9476.3574, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(65256.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.1608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-131.7490, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17.3578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1.1172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1129],
        [-0.0796],
        [-0.0536],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-5225.4482, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.1129],
        [-0.0796],
        [-0.0536],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057, -0.0042,  0.0113,  ...,  0.0175,  0.0166, -0.0050],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(1128.0042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.3852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.3237, device='cuda:0')



h[100].sum tensor(81.2649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.3050, device='cuda:0')



h[200].sum tensor(96.7189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(99.1469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0000, 0.0427,  ..., 0.0662, 0.0628, 0.0000],
        [0.0177, 0.0000, 0.0352,  ..., 0.0544, 0.0517, 0.0000],
        [0.0041, 0.0000, 0.0082,  ..., 0.0127, 0.0121, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(104583.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.3215, 0.0890, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2755, 0.0763, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.2212, 0.0612, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(551349.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-893.2494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-322.9135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1267.3684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(88.5870, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[7.8845e-01],
        [8.5312e-01],
        [9.4138e-01],
        ...,
        [1.0274e-05],
        [1.7084e-05],
        [2.4421e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(54731.3477, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.1129],
        [-0.0796],
        [-0.0536],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2525, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.1971, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(54.9823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4687.4663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4321.2964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1270.5935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070, -0.0204, -0.0040,  ..., -0.0257, -0.0145, -0.0275],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0090, -0.0260, -0.0051,  ..., -0.0327, -0.0185, -0.0350],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(-345.4142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2525, device='cuda:0')



h[100].sum tensor(-63.6013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.1971, device='cuda:0')



h[200].sum tensor(29.2799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(54.9823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(52934.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0657, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0328],
        [0.1259, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0628],
        [0.1050, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0524],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(289950.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4955.0518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-635.3864, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1208.2981, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(262.3457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(0.0039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1935],
        [0.2410],
        [0.2635],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(33751.6445, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 61, in <module>
    plotevent(sitonsquare(result0[EvBTr - 10000]), 333, f'event number {EvBTr} passed through network before training \n color indicates time')
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/DataLoadBha.py", line 49, in sitonsquare
    sqevent[i, j + 144] = event[(wiresum[i]) + j + w]
IndexError: index 1 is out of bounds for dimension 0 with size 1

real	0m25.826s
user	0m18.443s
sys	0m5.063s
