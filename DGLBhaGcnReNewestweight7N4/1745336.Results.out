0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 12:15:53 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   22C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b8aed3f48e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.243s
user	0m2.718s
sys	0m1.090s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[12:16:18] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.4157],
        [-1.6519],
        [ 0.3862],
        ...,
        [-1.2803],
        [ 0.5230],
        [-0.6197]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-27.3843, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-9.7419e-04,  8.7099e-02,  6.1510e-02,  2.7586e-02, -8.5169e-02,
         -1.6451e-02,  1.0467e-01,  2.1548e-02,  1.3857e-01,  6.9796e-02,
         -2.4467e-02, -4.9726e-02,  3.6047e-02,  7.6587e-03,  9.9502e-02,
          1.1980e-01, -2.7263e-02,  1.2322e-01, -9.9286e-02,  1.1195e-01,
         -2.6527e-02,  5.7126e-02,  9.7903e-02,  7.3159e-02,  1.1992e-01,
          1.0629e-02, -1.1859e-01,  4.9782e-02,  2.6399e-02, -1.4990e-01,
          1.3325e-01, -1.3323e-01, -1.2689e-01,  1.2893e-01,  4.9862e-02,
         -4.5203e-02, -1.3745e-01, -1.1525e-01, -4.6989e-03, -1.9114e-02,
         -1.3126e-01, -1.0692e-01, -1.3820e-01, -8.6141e-02,  5.6818e-02,
         -1.4748e-01, -4.8477e-02,  8.2786e-02,  2.4152e-02,  5.4557e-02,
          5.7276e-02, -1.1737e-01,  9.6545e-02, -4.2519e-02,  4.6361e-02,
          2.5299e-02, -1.3435e-01, -6.2298e-03,  4.9409e-02, -4.5415e-02,
          4.9723e-02,  1.4051e-03,  7.2096e-02,  1.0724e-01,  2.5521e-02,
          4.2511e-02,  1.1970e-01,  1.4289e-01,  7.9557e-03,  3.7032e-02,
         -7.2209e-02, -6.7343e-02,  7.8135e-03,  9.0182e-02, -9.3318e-02,
         -1.1588e-01,  7.6098e-02, -1.1590e-01, -6.1671e-02,  8.8324e-02,
         -9.9631e-02,  6.9994e-02, -1.4836e-02,  1.4894e-01,  4.9841e-02,
         -9.0910e-03,  4.6373e-02, -4.5542e-02, -3.5174e-02,  1.4022e-01,
          8.7959e-02, -1.4363e-01, -1.0389e-01,  9.9401e-02,  2.8148e-02,
         -4.6057e-02, -5.5292e-02, -1.1915e-01,  1.1478e-01,  1.1334e-01,
          7.3634e-02, -1.3473e-01, -8.0183e-02, -1.7400e-02, -5.0004e-02,
          1.4115e-01,  7.3906e-02, -7.8774e-03, -1.2897e-02,  9.0126e-02,
          1.4096e-01, -4.4833e-02, -8.0696e-02, -2.7492e-02, -1.3823e-01,
         -6.0372e-02,  7.3100e-02,  1.0768e-01, -1.2323e-01,  7.8499e-02,
         -9.7726e-02, -8.2237e-02,  1.2763e-01,  3.8683e-02,  6.6490e-02,
          1.7034e-02, -1.0646e-01, -1.1083e-01,  1.0833e-01, -9.4091e-02,
          8.9121e-02, -1.5589e-02,  1.0000e-01,  3.1559e-02,  5.4021e-03,
         -1.4629e-01, -1.1666e-01, -1.3626e-01, -3.0013e-03,  6.5300e-02,
          9.6291e-02, -1.2569e-01, -3.0106e-02, -1.2776e-01,  6.1108e-02,
          1.0103e-01,  1.3657e-01,  6.3779e-02,  1.4403e-02,  9.2988e-02,
          4.6080e-02,  4.6655e-02, -9.9690e-02, -1.0727e-01, -9.1974e-02,
          1.4237e-01,  4.8466e-02,  6.6453e-02, -1.4861e-01, -2.4095e-02,
         -5.8219e-04,  9.7641e-02, -2.9129e-02,  7.2902e-02,  4.7510e-02,
          9.7444e-02,  9.6775e-02, -1.0742e-01, -8.5007e-02,  6.3558e-02,
          7.6151e-02, -8.6577e-02,  1.3448e-01, -1.0130e-01, -5.1299e-02,
         -4.8566e-02, -7.3602e-02,  8.5356e-03,  2.9295e-02, -1.0985e-01,
         -9.7236e-02,  1.5716e-02,  9.6078e-02,  1.3689e-01,  8.2569e-02,
         -7.7594e-02,  8.5917e-02,  7.3588e-02,  9.9540e-02, -1.3636e-01,
         -3.1293e-02,  1.3722e-01, -1.2379e-01, -6.0621e-02, -1.3022e-02,
          1.2897e-01,  1.1034e-01, -6.3354e-03,  1.3968e-01, -6.7794e-02,
          1.4446e-01,  2.9650e-02,  9.6099e-02,  2.6634e-02, -1.0377e-01,
         -1.4542e-01, -2.2823e-05, -1.3379e-01, -5.9512e-02,  8.5702e-02,
         -3.4318e-02, -4.7941e-02,  7.6739e-02, -1.1982e-01, -1.3925e-01,
         -1.3557e-01, -7.2358e-02,  5.3324e-03,  1.1705e-01, -9.3766e-02,
         -9.9003e-02,  1.0646e-02, -1.2595e-01,  7.6200e-03, -8.9576e-02,
          1.8945e-02,  5.8243e-02, -5.1202e-02, -4.7494e-03,  1.5578e-02,
         -2.6382e-02,  1.0780e-01, -1.1173e-01,  1.0071e-01, -6.0225e-02,
         -7.5415e-02,  9.7573e-02,  2.2702e-02,  4.4220e-02, -1.2925e-01,
         -3.3229e-02, -1.0989e-01,  6.8239e-02,  1.0639e-01,  6.2465e-02,
          7.6784e-02,  1.2079e-01,  8.8287e-02, -7.4843e-02, -8.7908e-02,
         -3.9485e-02, -1.1035e-01, -6.9428e-02,  2.0555e-03, -7.3379e-03,
         -1.4002e-01]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-9.7419e-04,  8.7099e-02,  6.1510e-02,  2.7586e-02, -8.5169e-02,
         -1.6451e-02,  1.0467e-01,  2.1548e-02,  1.3857e-01,  6.9796e-02,
         -2.4467e-02, -4.9726e-02,  3.6047e-02,  7.6587e-03,  9.9502e-02,
          1.1980e-01, -2.7263e-02,  1.2322e-01, -9.9286e-02,  1.1195e-01,
         -2.6527e-02,  5.7126e-02,  9.7903e-02,  7.3159e-02,  1.1992e-01,
          1.0629e-02, -1.1859e-01,  4.9782e-02,  2.6399e-02, -1.4990e-01,
          1.3325e-01, -1.3323e-01, -1.2689e-01,  1.2893e-01,  4.9862e-02,
         -4.5203e-02, -1.3745e-01, -1.1525e-01, -4.6989e-03, -1.9114e-02,
         -1.3126e-01, -1.0692e-01, -1.3820e-01, -8.6141e-02,  5.6818e-02,
         -1.4748e-01, -4.8477e-02,  8.2786e-02,  2.4152e-02,  5.4557e-02,
          5.7276e-02, -1.1737e-01,  9.6545e-02, -4.2519e-02,  4.6361e-02,
          2.5299e-02, -1.3435e-01, -6.2298e-03,  4.9409e-02, -4.5415e-02,
          4.9723e-02,  1.4051e-03,  7.2096e-02,  1.0724e-01,  2.5521e-02,
          4.2511e-02,  1.1970e-01,  1.4289e-01,  7.9557e-03,  3.7032e-02,
         -7.2209e-02, -6.7343e-02,  7.8135e-03,  9.0182e-02, -9.3318e-02,
         -1.1588e-01,  7.6098e-02, -1.1590e-01, -6.1671e-02,  8.8324e-02,
         -9.9631e-02,  6.9994e-02, -1.4836e-02,  1.4894e-01,  4.9841e-02,
         -9.0910e-03,  4.6373e-02, -4.5542e-02, -3.5174e-02,  1.4022e-01,
          8.7959e-02, -1.4363e-01, -1.0389e-01,  9.9401e-02,  2.8148e-02,
         -4.6057e-02, -5.5292e-02, -1.1915e-01,  1.1478e-01,  1.1334e-01,
          7.3634e-02, -1.3473e-01, -8.0183e-02, -1.7400e-02, -5.0004e-02,
          1.4115e-01,  7.3906e-02, -7.8774e-03, -1.2897e-02,  9.0126e-02,
          1.4096e-01, -4.4833e-02, -8.0696e-02, -2.7492e-02, -1.3823e-01,
         -6.0372e-02,  7.3100e-02,  1.0768e-01, -1.2323e-01,  7.8499e-02,
         -9.7726e-02, -8.2237e-02,  1.2763e-01,  3.8683e-02,  6.6490e-02,
          1.7034e-02, -1.0646e-01, -1.1083e-01,  1.0833e-01, -9.4091e-02,
          8.9121e-02, -1.5589e-02,  1.0000e-01,  3.1559e-02,  5.4021e-03,
         -1.4629e-01, -1.1666e-01, -1.3626e-01, -3.0013e-03,  6.5300e-02,
          9.6291e-02, -1.2569e-01, -3.0106e-02, -1.2776e-01,  6.1108e-02,
          1.0103e-01,  1.3657e-01,  6.3779e-02,  1.4403e-02,  9.2988e-02,
          4.6080e-02,  4.6655e-02, -9.9690e-02, -1.0727e-01, -9.1974e-02,
          1.4237e-01,  4.8466e-02,  6.6453e-02, -1.4861e-01, -2.4095e-02,
         -5.8219e-04,  9.7641e-02, -2.9129e-02,  7.2902e-02,  4.7510e-02,
          9.7444e-02,  9.6775e-02, -1.0742e-01, -8.5007e-02,  6.3558e-02,
          7.6151e-02, -8.6577e-02,  1.3448e-01, -1.0130e-01, -5.1299e-02,
         -4.8566e-02, -7.3602e-02,  8.5356e-03,  2.9295e-02, -1.0985e-01,
         -9.7236e-02,  1.5716e-02,  9.6078e-02,  1.3689e-01,  8.2569e-02,
         -7.7594e-02,  8.5917e-02,  7.3588e-02,  9.9540e-02, -1.3636e-01,
         -3.1293e-02,  1.3722e-01, -1.2379e-01, -6.0621e-02, -1.3022e-02,
          1.2897e-01,  1.1034e-01, -6.3354e-03,  1.3968e-01, -6.7794e-02,
          1.4446e-01,  2.9650e-02,  9.6099e-02,  2.6634e-02, -1.0377e-01,
         -1.4542e-01, -2.2823e-05, -1.3379e-01, -5.9512e-02,  8.5702e-02,
         -3.4318e-02, -4.7941e-02,  7.6739e-02, -1.1982e-01, -1.3925e-01,
         -1.3557e-01, -7.2358e-02,  5.3324e-03,  1.1705e-01, -9.3766e-02,
         -9.9003e-02,  1.0646e-02, -1.2595e-01,  7.6200e-03, -8.9576e-02,
          1.8945e-02,  5.8243e-02, -5.1202e-02, -4.7494e-03,  1.5578e-02,
         -2.6382e-02,  1.0780e-01, -1.1173e-01,  1.0071e-01, -6.0225e-02,
         -7.5415e-02,  9.7573e-02,  2.2702e-02,  4.4220e-02, -1.2925e-01,
         -3.3229e-02, -1.0989e-01,  6.8239e-02,  1.0639e-01,  6.2465e-02,
          7.6784e-02,  1.2079e-01,  8.8287e-02, -7.4843e-02, -8.7908e-02,
         -3.9485e-02, -1.1035e-01, -6.9428e-02,  2.0555e-03, -7.3379e-03,
         -1.4002e-01]], device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0367,  0.1158, -0.1055,  ...,  0.0386, -0.0180, -0.0170],
        [ 0.1245, -0.1020,  0.0064,  ..., -0.0570, -0.0039, -0.0752],
        [ 0.0448, -0.0549, -0.0348,  ...,  0.0836,  0.1226,  0.0675],
        ...,
        [-0.0250, -0.1186,  0.0621,  ...,  0.0634,  0.0860, -0.0368],
        [-0.0106,  0.0526, -0.1107,  ..., -0.1232,  0.0181, -0.0225],
        [ 0.0081,  0.1118, -0.0450,  ..., -0.0130, -0.0638,  0.0452]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0367,  0.1158, -0.1055,  ...,  0.0386, -0.0180, -0.0170],
        [ 0.1245, -0.1020,  0.0064,  ..., -0.0570, -0.0039, -0.0752],
        [ 0.0448, -0.0549, -0.0348,  ...,  0.0836,  0.1226,  0.0675],
        ...,
        [-0.0250, -0.1186,  0.0621,  ...,  0.0634,  0.0860, -0.0368],
        [-0.0106,  0.0526, -0.1107,  ..., -0.1232,  0.0181, -0.0225],
        [ 0.0081,  0.1118, -0.0450,  ..., -0.0130, -0.0638,  0.0452]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.1612,  0.0970, -0.1171,  ..., -0.0568, -0.1359,  0.0408],
        [ 0.0151,  0.1519,  0.0307,  ..., -0.1103, -0.1062, -0.1167],
        [ 0.1551,  0.0403,  0.1004,  ...,  0.0725, -0.0146,  0.0118],
        ...,
        [-0.1344, -0.0492,  0.1059,  ...,  0.1347,  0.0564,  0.1561],
        [-0.1708, -0.1579,  0.0927,  ...,  0.1299, -0.1466, -0.0513],
        [-0.1332,  0.1597, -0.1710,  ..., -0.0713,  0.0627,  0.0076]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1612,  0.0970, -0.1171,  ..., -0.0568, -0.1359,  0.0408],
        [ 0.0151,  0.1519,  0.0307,  ..., -0.1103, -0.1062, -0.1167],
        [ 0.1551,  0.0403,  0.1004,  ...,  0.0725, -0.0146,  0.0118],
        ...,
        [-0.1344, -0.0492,  0.1059,  ...,  0.1347,  0.0564,  0.1561],
        [-0.1708, -0.1579,  0.0927,  ...,  0.1299, -0.1466, -0.0513],
        [-0.1332,  0.1597, -0.1710,  ..., -0.0713,  0.0627,  0.0076]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.0057,  0.1088, -0.0982,  ..., -0.1422,  0.2121,  0.1511],
        [ 0.2459, -0.0482,  0.1088,  ..., -0.0631, -0.0654,  0.1207],
        [ 0.0724,  0.1077,  0.1786,  ...,  0.1417,  0.2356,  0.2445],
        ...,
        [-0.1358, -0.2372, -0.1116,  ...,  0.2096,  0.0507,  0.1690],
        [ 0.2442, -0.1152, -0.1752,  ..., -0.0030,  0.1412, -0.0428],
        [ 0.1400,  0.2369,  0.1882,  ...,  0.0484, -0.1806,  0.0056]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0057,  0.1088, -0.0982,  ..., -0.1422,  0.2121,  0.1511],
        [ 0.2459, -0.0482,  0.1088,  ..., -0.0631, -0.0654,  0.1207],
        [ 0.0724,  0.1077,  0.1786,  ...,  0.1417,  0.2356,  0.2445],
        ...,
        [-0.1358, -0.2372, -0.1116,  ...,  0.2096,  0.0507,  0.1690],
        [ 0.2442, -0.1152, -0.1752,  ..., -0.0030,  0.1412, -0.0428],
        [ 0.1400,  0.2369,  0.1882,  ...,  0.0484, -0.1806,  0.0056]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.0921],
        [-0.3985],
        [ 0.3214],
        [-0.2776],
        [-0.2356],
        [-0.3061],
        [-0.0097],
        [ 0.4019],
        [ 0.3910],
        [ 0.0727],
        [-0.0345],
        [ 0.1477],
        [ 0.0287],
        [ 0.3446],
        [ 0.1006],
        [ 0.3486],
        [ 0.1128],
        [-0.4065],
        [-0.4181],
        [-0.0495],
        [ 0.3501],
        [ 0.4060],
        [ 0.0156],
        [-0.1269],
        [ 0.4107],
        [-0.1580],
        [-0.0755],
        [-0.3975],
        [ 0.3599],
        [-0.1040],
        [ 0.1538],
        [-0.0160]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0921],
        [-0.3985],
        [ 0.3214],
        [-0.2776],
        [-0.2356],
        [-0.3061],
        [-0.0097],
        [ 0.4019],
        [ 0.3910],
        [ 0.0727],
        [-0.0345],
        [ 0.1477],
        [ 0.0287],
        [ 0.3446],
        [ 0.1006],
        [ 0.3486],
        [ 0.1128],
        [-0.4065],
        [-0.4181],
        [-0.0495],
        [ 0.3501],
        [ 0.4060],
        [ 0.0156],
        [-0.1269],
        [ 0.4107],
        [-0.1580],
        [-0.0755],
        [-0.3975],
        [ 0.3599],
        [-0.1040],
        [ 0.1538],
        [-0.0160]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(46.2099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.2876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(4.3836, device='cuda:0')



h[100].sum tensor(5.2239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3408, device='cuda:0')



h[200].sum tensor(10.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8431, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10288.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0103, 0.0000,  ..., 0.0000, 0.0100, 0.0165],
        [0.0023, 0.0048, 0.0000,  ..., 0.0000, 0.0047, 0.0078],
        [0.0006, 0.0014, 0.0000,  ..., 0.0000, 0.0013, 0.0022],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(51896.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(493.2858, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.4073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57.0270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.6395, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-124.9106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0395],
        [-0.0278],
        [-0.0187],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-1826.0593, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.0395],
        [-0.0278],
        [-0.0187],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0223,  0.0087, -0.0060,  ..., -0.0096, -0.0178, -0.0106],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-2754.0435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(146.3395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(150.0132, device='cuda:0')



h[100].sum tensor(-20.6232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-21.1410, device='cuda:0')



h[200].sum tensor(99.1495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(101.6386, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0841, 0.0328, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0692, 0.0270, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0162, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(89266.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1532,  ..., 0.0000, 0.0000, 0.2697],
        [0.0000, 0.0000, 0.1313,  ..., 0.0000, 0.0000, 0.2311],
        [0.0000, 0.0000, 0.1054,  ..., 0.0000, 0.0000, 0.1855],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(461255.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-619.2213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(693.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(48.7874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-490.7426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[2.8866e+00],
        [3.1235e+00],
        [3.4468e+00],
        ...,
        [3.7609e-05],
        [6.2529e-05],
        [8.9384e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(200383.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0395],
        [-0.0278],
        [-0.0187],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]]) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788)



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], grad_fn=<ToCopyBackward0>) 
g.edata[efet].sum tensor(731860., grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]]) 
g.ndata[nfet].sum tensor(548.4788)
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8704, grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(83.1904, device='cuda:0')



h[100].sum tensor(-1.6736, grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.7238, device='cuda:0')



h[200].sum tensor(166.0773, grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(56.3641, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2198, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3955, 0.4766],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4184],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150758., grad_fn=<SumBackward0>)



h2[0].sum tensor(0., grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 39, in <module>
    result1 = net(batcheddglgraph.to('cpu'), TraTen[10000:10010].reshape(10 * 6796, 1).to('cpu'))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 105, in forward
    print('\n(h1.sum(axis=0) * param0_2).sum() + bias0', (h1.sum(axis=0) * param0_2).sum() + bias0)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

real	0m28.001s
user	0m20.389s
sys	0m5.120s
