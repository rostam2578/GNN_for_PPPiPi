0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 13:43:35 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b00965c48e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.281s
user	0m2.629s
sys	0m1.299s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[13:43:59] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.5512],
        [ 0.6281],
        [-0.1528],
        ...,
        [ 0.5217],
        [-0.3470],
        [-1.3424]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-1.8491, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-1.0071e-01, -1.3967e-01,  8.5769e-02, -9.6893e-02,  1.5282e-02,
          1.2763e-01,  6.2452e-02,  1.4826e-01, -1.2932e-01,  4.5443e-02,
         -5.4025e-02, -9.3595e-02, -1.5021e-01,  1.4897e-01, -1.3410e-01,
          1.1739e-01,  1.3503e-01,  1.3579e-01, -9.2534e-03, -6.3721e-02,
          1.4719e-01, -1.0643e-01,  1.3037e-01, -7.0916e-03, -5.6952e-03,
         -1.2867e-01, -1.8323e-02,  1.2235e-01,  1.1236e-01, -1.3757e-02,
         -1.0514e-01,  7.7552e-03,  2.6613e-02,  7.7150e-02, -1.1435e-01,
          2.8744e-02, -1.0801e-01, -9.3181e-02, -8.8767e-02,  1.3640e-01,
          4.4421e-02,  9.1133e-02, -1.0970e-01, -5.8316e-02, -1.5270e-01,
          1.1359e-01, -8.9328e-02,  1.4676e-02, -6.4160e-02,  5.0172e-02,
         -4.6387e-02, -1.0190e-01,  5.7011e-02, -1.0232e-01, -1.1032e-01,
         -6.5310e-02, -1.3942e-01, -1.1244e-03, -1.1049e-01, -9.7792e-02,
         -3.3706e-02,  8.4827e-02,  1.4005e-01, -8.4828e-02, -2.6612e-05,
         -1.8013e-02,  1.8432e-02,  8.0789e-02, -1.2894e-01, -7.5401e-02,
          8.4046e-02, -4.2435e-02, -1.2970e-01,  6.6881e-02,  9.9228e-03,
          3.2941e-02,  5.5801e-02, -6.8810e-02,  1.1036e-01,  7.8590e-02,
         -1.1014e-01, -1.3726e-01, -1.1892e-01, -1.2974e-01,  1.3408e-01,
         -1.9870e-02,  7.5387e-02,  1.4921e-01,  1.3999e-01, -1.3820e-01,
          7.1990e-02, -7.0505e-02, -4.7552e-02,  7.6784e-02,  1.3925e-01,
         -1.3762e-01,  4.1974e-02, -2.5917e-02,  1.2645e-01,  8.0081e-02,
          2.5663e-02,  1.0253e-01,  7.8039e-02, -1.2924e-01, -5.7706e-02,
         -3.7729e-02,  7.0795e-02,  7.3149e-02, -1.1293e-01, -5.1888e-03,
         -7.2120e-02, -1.3962e-02, -1.0240e-01,  2.5655e-02,  1.8617e-02,
         -1.9248e-02, -7.8355e-02, -1.2018e-01,  6.8503e-02,  1.5995e-02,
         -7.9850e-02,  1.0649e-01, -6.1327e-02,  1.0025e-01,  9.9532e-02,
         -1.1746e-01, -1.1196e-01,  4.6684e-02, -7.1687e-02,  9.7075e-02,
         -5.8918e-02, -1.0337e-01, -1.6761e-02, -4.6858e-02,  1.2538e-01,
         -4.3412e-02, -8.0206e-02,  3.9520e-02,  3.3944e-02, -5.3584e-02,
          1.4240e-01, -1.3624e-01, -7.9529e-02, -1.0712e-01, -3.2755e-02,
         -2.7367e-02, -4.2803e-02,  7.9142e-02,  1.1654e-01, -1.5079e-01,
         -1.4952e-02, -9.7816e-02, -6.0756e-03,  1.2952e-01, -5.1248e-02,
          8.7060e-02, -5.8808e-02,  1.5236e-01, -9.6551e-02, -1.7518e-02,
         -4.8010e-02, -8.2040e-03, -5.1060e-02,  1.7760e-02,  4.8205e-02,
         -1.4317e-01,  1.3450e-01, -2.0174e-02,  6.6247e-02, -3.3638e-02,
         -1.4795e-01, -1.1324e-01,  6.9707e-02, -3.6345e-02, -5.8277e-03,
         -1.3639e-01,  1.3647e-01,  8.7360e-02, -1.2358e-02, -3.5089e-02,
         -1.4155e-01, -5.4285e-02, -9.5810e-02,  1.5232e-01,  1.4659e-01,
          1.1510e-01, -2.5503e-02,  8.9212e-02,  6.7049e-02, -1.3028e-01,
          5.3642e-02, -8.9320e-03,  7.4795e-02,  1.2718e-01,  1.3507e-01,
         -1.1586e-01, -1.5208e-02, -1.2101e-01, -8.1920e-02,  4.5442e-02,
         -1.0993e-01, -1.3727e-01,  1.0772e-01,  3.3320e-02,  2.8577e-02,
          8.8695e-02,  9.0453e-02, -9.9188e-02,  8.9036e-02, -1.0995e-01,
         -9.4715e-02, -1.1821e-01,  4.5036e-02,  1.3007e-01, -5.2596e-02,
         -5.1993e-02,  1.4838e-01, -1.3514e-01,  1.0677e-01, -1.3251e-01,
          5.6025e-02, -1.5836e-02,  1.2710e-01, -9.5759e-02,  1.2839e-01,
         -5.9181e-03, -8.7484e-02,  5.2889e-02,  1.4633e-01, -7.2512e-02,
          3.2609e-02,  7.3396e-02, -2.1378e-02,  9.4154e-03,  1.0268e-01,
          7.2023e-02, -1.0107e-01, -1.3017e-01,  1.7954e-04,  1.1734e-01,
         -7.0773e-02, -6.8322e-02,  7.2578e-02, -5.3111e-02, -3.7920e-02,
          4.2672e-03,  5.8960e-02,  1.3820e-01,  2.5017e-02,  4.1454e-02,
          3.0987e-02,  1.2776e-02, -1.5068e-01,  1.4242e-01,  8.3979e-02,
          1.0764e-01]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-1.0071e-01, -1.3967e-01,  8.5769e-02, -9.6893e-02,  1.5282e-02,
          1.2763e-01,  6.2452e-02,  1.4826e-01, -1.2932e-01,  4.5443e-02,
         -5.4025e-02, -9.3595e-02, -1.5021e-01,  1.4897e-01, -1.3410e-01,
          1.1739e-01,  1.3503e-01,  1.3579e-01, -9.2534e-03, -6.3721e-02,
          1.4719e-01, -1.0643e-01,  1.3037e-01, -7.0916e-03, -5.6952e-03,
         -1.2867e-01, -1.8323e-02,  1.2235e-01,  1.1236e-01, -1.3757e-02,
         -1.0514e-01,  7.7552e-03,  2.6613e-02,  7.7150e-02, -1.1435e-01,
          2.8744e-02, -1.0801e-01, -9.3181e-02, -8.8767e-02,  1.3640e-01,
          4.4421e-02,  9.1133e-02, -1.0970e-01, -5.8316e-02, -1.5270e-01,
          1.1359e-01, -8.9328e-02,  1.4676e-02, -6.4160e-02,  5.0172e-02,
         -4.6387e-02, -1.0190e-01,  5.7011e-02, -1.0232e-01, -1.1032e-01,
         -6.5310e-02, -1.3942e-01, -1.1244e-03, -1.1049e-01, -9.7792e-02,
         -3.3706e-02,  8.4827e-02,  1.4005e-01, -8.4828e-02, -2.6612e-05,
         -1.8013e-02,  1.8432e-02,  8.0789e-02, -1.2894e-01, -7.5401e-02,
          8.4046e-02, -4.2435e-02, -1.2970e-01,  6.6881e-02,  9.9228e-03,
          3.2941e-02,  5.5801e-02, -6.8810e-02,  1.1036e-01,  7.8590e-02,
         -1.1014e-01, -1.3726e-01, -1.1892e-01, -1.2974e-01,  1.3408e-01,
         -1.9870e-02,  7.5387e-02,  1.4921e-01,  1.3999e-01, -1.3820e-01,
          7.1990e-02, -7.0505e-02, -4.7552e-02,  7.6784e-02,  1.3925e-01,
         -1.3762e-01,  4.1974e-02, -2.5917e-02,  1.2645e-01,  8.0081e-02,
          2.5663e-02,  1.0253e-01,  7.8039e-02, -1.2924e-01, -5.7706e-02,
         -3.7729e-02,  7.0795e-02,  7.3149e-02, -1.1293e-01, -5.1888e-03,
         -7.2120e-02, -1.3962e-02, -1.0240e-01,  2.5655e-02,  1.8617e-02,
         -1.9248e-02, -7.8355e-02, -1.2018e-01,  6.8503e-02,  1.5995e-02,
         -7.9850e-02,  1.0649e-01, -6.1327e-02,  1.0025e-01,  9.9532e-02,
         -1.1746e-01, -1.1196e-01,  4.6684e-02, -7.1687e-02,  9.7075e-02,
         -5.8918e-02, -1.0337e-01, -1.6761e-02, -4.6858e-02,  1.2538e-01,
         -4.3412e-02, -8.0206e-02,  3.9520e-02,  3.3944e-02, -5.3584e-02,
          1.4240e-01, -1.3624e-01, -7.9529e-02, -1.0712e-01, -3.2755e-02,
         -2.7367e-02, -4.2803e-02,  7.9142e-02,  1.1654e-01, -1.5079e-01,
         -1.4952e-02, -9.7816e-02, -6.0756e-03,  1.2952e-01, -5.1248e-02,
          8.7060e-02, -5.8808e-02,  1.5236e-01, -9.6551e-02, -1.7518e-02,
         -4.8010e-02, -8.2040e-03, -5.1060e-02,  1.7760e-02,  4.8205e-02,
         -1.4317e-01,  1.3450e-01, -2.0174e-02,  6.6247e-02, -3.3638e-02,
         -1.4795e-01, -1.1324e-01,  6.9707e-02, -3.6345e-02, -5.8277e-03,
         -1.3639e-01,  1.3647e-01,  8.7360e-02, -1.2358e-02, -3.5089e-02,
         -1.4155e-01, -5.4285e-02, -9.5810e-02,  1.5232e-01,  1.4659e-01,
          1.1510e-01, -2.5503e-02,  8.9212e-02,  6.7049e-02, -1.3028e-01,
          5.3642e-02, -8.9320e-03,  7.4795e-02,  1.2718e-01,  1.3507e-01,
         -1.1586e-01, -1.5208e-02, -1.2101e-01, -8.1920e-02,  4.5442e-02,
         -1.0993e-01, -1.3727e-01,  1.0772e-01,  3.3320e-02,  2.8577e-02,
          8.8695e-02,  9.0453e-02, -9.9188e-02,  8.9036e-02, -1.0995e-01,
         -9.4715e-02, -1.1821e-01,  4.5036e-02,  1.3007e-01, -5.2596e-02,
         -5.1993e-02,  1.4838e-01, -1.3514e-01,  1.0677e-01, -1.3251e-01,
          5.6025e-02, -1.5836e-02,  1.2710e-01, -9.5759e-02,  1.2839e-01,
         -5.9181e-03, -8.7484e-02,  5.2889e-02,  1.4633e-01, -7.2512e-02,
          3.2609e-02,  7.3396e-02, -2.1378e-02,  9.4154e-03,  1.0268e-01,
          7.2023e-02, -1.0107e-01, -1.3017e-01,  1.7954e-04,  1.1734e-01,
         -7.0773e-02, -6.8322e-02,  7.2578e-02, -5.3111e-02, -3.7920e-02,
          4.2672e-03,  5.8960e-02,  1.3820e-01,  2.5017e-02,  4.1454e-02,
          3.0987e-02,  1.2776e-02, -1.5068e-01,  1.4242e-01,  8.3979e-02,
          1.0764e-01]], device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0281, -0.0557,  0.0494,  ..., -0.0337, -0.1025,  0.0517],
        [-0.1110, -0.0195, -0.0864,  ...,  0.0593, -0.1130,  0.0800],
        [ 0.0930, -0.0937,  0.0184,  ..., -0.0737,  0.0028,  0.1149],
        ...,
        [-0.0332, -0.0217,  0.0929,  ...,  0.0588,  0.0711, -0.0974],
        [-0.0530,  0.0412, -0.0719,  ...,  0.0049,  0.0987, -0.1013],
        [ 0.0699, -0.1153, -0.0315,  ...,  0.0181,  0.0986,  0.1057]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0281, -0.0557,  0.0494,  ..., -0.0337, -0.1025,  0.0517],
        [-0.1110, -0.0195, -0.0864,  ...,  0.0593, -0.1130,  0.0800],
        [ 0.0930, -0.0937,  0.0184,  ..., -0.0737,  0.0028,  0.1149],
        ...,
        [-0.0332, -0.0217,  0.0929,  ...,  0.0588,  0.0711, -0.0974],
        [-0.0530,  0.0412, -0.0719,  ...,  0.0049,  0.0987, -0.1013],
        [ 0.0699, -0.1153, -0.0315,  ...,  0.0181,  0.0986,  0.1057]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 1.6187e-01, -1.5564e-01,  8.7576e-02,  ...,  1.2215e-01,
         -5.3430e-02, -5.5216e-02],
        [-1.7048e-05,  1.2595e-01,  1.0482e-01,  ..., -1.3763e-02,
         -2.5192e-02, -1.4349e-01],
        [-4.8693e-02, -1.1313e-01,  1.4723e-01,  ..., -9.9409e-02,
         -1.5433e-01,  7.8397e-03],
        ...,
        [-5.9697e-02, -9.4030e-02,  1.4418e-01,  ...,  9.8509e-02,
          1.4799e-01, -9.7885e-02],
        [ 9.1402e-02,  7.1903e-02,  9.6084e-02,  ..., -2.9338e-02,
         -2.7346e-02, -1.0007e-01],
        [-6.9391e-02, -2.4256e-03,  5.4200e-02,  ...,  1.2739e-01,
          1.7558e-02,  1.2780e-01]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 1.6187e-01, -1.5564e-01,  8.7576e-02,  ...,  1.2215e-01,
         -5.3430e-02, -5.5216e-02],
        [-1.7048e-05,  1.2595e-01,  1.0482e-01,  ..., -1.3763e-02,
         -2.5192e-02, -1.4349e-01],
        [-4.8693e-02, -1.1313e-01,  1.4723e-01,  ..., -9.9409e-02,
         -1.5433e-01,  7.8397e-03],
        ...,
        [-5.9697e-02, -9.4030e-02,  1.4418e-01,  ...,  9.8509e-02,
          1.4799e-01, -9.7885e-02],
        [ 9.1402e-02,  7.1903e-02,  9.6084e-02,  ..., -2.9338e-02,
         -2.7346e-02, -1.0007e-01],
        [-6.9391e-02, -2.4256e-03,  5.4200e-02,  ...,  1.2739e-01,
          1.7558e-02,  1.2780e-01]], device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.1269,  0.2264,  0.0284,  ..., -0.2153,  0.1858,  0.1378],
        [-0.1361, -0.2338,  0.0830,  ..., -0.1666, -0.1340, -0.2354],
        [ 0.0182, -0.2444,  0.2422,  ...,  0.0076,  0.0009, -0.0316],
        ...,
        [ 0.0114,  0.2168,  0.0216,  ..., -0.2067,  0.0558, -0.2214],
        [ 0.0470,  0.2310,  0.1695,  ..., -0.1590,  0.0418,  0.0602],
        [-0.1997,  0.1064,  0.1572,  ...,  0.2013,  0.1109,  0.1802]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1269,  0.2264,  0.0284,  ..., -0.2153,  0.1858,  0.1378],
        [-0.1361, -0.2338,  0.0830,  ..., -0.1666, -0.1340, -0.2354],
        [ 0.0182, -0.2444,  0.2422,  ...,  0.0076,  0.0009, -0.0316],
        ...,
        [ 0.0114,  0.2168,  0.0216,  ..., -0.2067,  0.0558, -0.2214],
        [ 0.0470,  0.2310,  0.1695,  ..., -0.1590,  0.0418,  0.0602],
        [-0.1997,  0.1064,  0.1572,  ...,  0.2013,  0.1109,  0.1802]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.0579],
        [ 0.1697],
        [ 0.0932],
        [ 0.3875],
        [-0.3024],
        [ 0.2873],
        [ 0.3543],
        [-0.1824],
        [-0.3408],
        [ 0.3507],
        [ 0.3738],
        [ 0.3561],
        [-0.1396],
        [-0.1558],
        [ 0.3779],
        [-0.1970],
        [-0.1247],
        [ 0.4131],
        [ 0.0718],
        [ 0.1862],
        [ 0.4078],
        [ 0.3770],
        [-0.1184],
        [-0.0451],
        [-0.0699],
        [-0.2091],
        [ 0.2162],
        [-0.2313],
        [ 0.1825],
        [ 0.2017],
        [ 0.4192],
        [ 0.1493]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0579],
        [ 0.1697],
        [ 0.0932],
        [ 0.3875],
        [-0.3024],
        [ 0.2873],
        [ 0.3543],
        [-0.1824],
        [-0.3408],
        [ 0.3507],
        [ 0.3738],
        [ 0.3561],
        [-0.1396],
        [-0.1558],
        [ 0.3779],
        [-0.1970],
        [-0.1247],
        [ 0.4131],
        [ 0.0718],
        [ 0.1862],
        [ 0.4078],
        [ 0.3770],
        [-0.1184],
        [-0.0451],
        [-0.0699],
        [-0.2091],
        [ 0.2162],
        [-0.2313],
        [ 0.1825],
        [ 0.2017],
        [ 0.4192],
        [ 0.1493]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(96.4634, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.1315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.4031, device='cuda:0')



h[100].sum tensor(2.3333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3855, device='cuda:0')



h[200].sum tensor(10.3850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10542.5449, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0179, 0.0000,  ..., 0.0000, 0.0000, 0.0117],
        [0.0036, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0055],
        [0.0010, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(61332.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(784.8083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(49.9037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.7658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(308.6838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(19.5946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.2153],
        [0.1517],
        [0.1021],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(9962.7275, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[0.2153],
        [0.1517],
        [0.1021],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0176, -0.0123,  0.0154,  ...,  0.0156,  0.0105, -0.0187],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-832.7009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(115.9561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(118.8671, device='cuda:0')



h[100].sum tensor(-37.6141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-38.5584, device='cuda:0')



h[200].sum tensor(59.5189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(61.0131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0666, 0.0000, 0.0580,  ..., 0.0588, 0.0396, 0.0000],
        [0.0548, 0.0000, 0.0477,  ..., 0.0484, 0.0326, 0.0000],
        [0.0128, 0.0000, 0.0112,  ..., 0.0113, 0.0076, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(99369.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0209, 0.0000,  ..., 0.0000, 0.1173, 0.1126],
        [0.0000, 0.0179, 0.0000,  ..., 0.0000, 0.1005, 0.0965],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0807, 0.0775],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(582084.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-628.3080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1132.3726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9417.4131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(659.6152, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.9006e+00],
        [2.0567e+00],
        [2.2696e+00],
        ...,
        [2.4760e-05],
        [4.1166e-05],
        [5.8841e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(131947.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.2153],
        [0.1517],
        [0.1021],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.9182, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-21.3827, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4032.2795, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2650.5601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2941.1301, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0255, -0.0032,  0.0259,  ..., -0.0068, -0.0275,  0.0021],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0325, -0.0041,  0.0330,  ..., -0.0087, -0.0350,  0.0027],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(-504.1484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-70.6368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(65.9182, device='cuda:0')



h[100].sum tensor(22.9829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-21.3827, device='cuda:0')



h[200].sum tensor(10.8168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0212,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.1071,  ..., 0.0000, 0.0000, 0.0086],
        [0.0000, 0.0000, 0.0403,  ..., 0.0000, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(51141.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0253, 0.0319,  ..., 0.0000, 0.0000, 0.0664],
        [0.0000, 0.0485, 0.0611,  ..., 0.0000, 0.0000, 0.1274],
        [0.0000, 0.0405, 0.0510,  ..., 0.0000, 0.0000, 0.1062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(236885.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(89.2301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-429.0032, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(410.9776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4353],
        [-0.5423],
        [-0.5927],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-75934.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)
result1 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) result0 tensor([[-0.4353],
        [-0.5423],
        [-0.5927],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 61, in <module>
    plotevent(sitonsquare(result0[EvBTr - 10000].reshape(1, 6796)), 333, f'event number {EvBTr} passed through network before training \n color indicates time')
RuntimeError: shape '[1, 6796]' is invalid for input of size 1

real	0m25.286s
user	0m18.133s
sys	0m5.058s
