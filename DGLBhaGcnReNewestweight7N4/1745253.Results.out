0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 12:06:59 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   21C    P0    31W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b9c329df8e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.242s
user	0m2.717s
sys	0m1.196s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[12:07:23] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.3886],
        [ 2.5076],
        [-0.3517],
        ...,
        [ 1.0513],
        [ 1.0830],
        [ 0.1876]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-85.8939, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0354, -0.1292, -0.0360, -0.0449, -0.0773, -0.0118, -0.0859, -0.0029,
          0.0857, -0.0794, -0.1084, -0.0813,  0.0621,  0.0766, -0.1054,  0.1206,
          0.0184,  0.0261,  0.0813,  0.1006, -0.0976, -0.0250,  0.0524, -0.1490,
          0.0256, -0.0505,  0.0555, -0.0777, -0.0519,  0.0840, -0.0409,  0.1432,
          0.0122,  0.0796,  0.1283,  0.0924,  0.0714, -0.0566, -0.0177, -0.0891,
         -0.1288,  0.0966,  0.1245, -0.0632,  0.0103,  0.0027, -0.1224,  0.1473,
         -0.0903, -0.1228, -0.0804,  0.0307,  0.0717, -0.0485, -0.0382, -0.1421,
         -0.1467,  0.0802, -0.0986, -0.0617,  0.0299,  0.1307,  0.1298, -0.1034,
          0.0956, -0.0719, -0.1117, -0.0250, -0.1351, -0.0046,  0.0777,  0.0446,
          0.0337, -0.0431,  0.1447, -0.0682, -0.0570,  0.0130,  0.0783, -0.0313,
          0.1227, -0.1171, -0.0059, -0.1114, -0.0833,  0.0046, -0.0893, -0.1309,
          0.0123, -0.0006,  0.1446, -0.0857,  0.0794, -0.1107,  0.0014, -0.0697,
         -0.0048,  0.1264,  0.0131, -0.0714,  0.0620, -0.1164,  0.1039, -0.1451,
          0.1379,  0.1194, -0.0602, -0.0174, -0.1491,  0.1262, -0.0517,  0.1525,
          0.1005,  0.0541,  0.0681, -0.1259,  0.0073,  0.0634,  0.1345,  0.1300,
          0.0622,  0.1217, -0.0370,  0.1322,  0.0561,  0.0199,  0.0040, -0.0092,
          0.0422, -0.0172,  0.0682, -0.1384, -0.0386,  0.0317,  0.0622,  0.0827,
         -0.0399, -0.1433, -0.0526, -0.0215,  0.0023, -0.0859,  0.0615, -0.0395,
          0.0063, -0.1322,  0.0753,  0.0646, -0.0979, -0.0494, -0.0826,  0.0168,
         -0.1441, -0.0063,  0.1053, -0.1143,  0.0369, -0.1204, -0.0330,  0.0206,
         -0.0675, -0.1441,  0.0340,  0.0916, -0.0264, -0.0970, -0.0676,  0.0394,
         -0.1434,  0.0331, -0.0781, -0.0232, -0.0316,  0.1266, -0.0341,  0.1389,
         -0.0155, -0.1336, -0.0015, -0.0551, -0.0333, -0.1008, -0.0307,  0.1371,
          0.0745, -0.0926, -0.1102,  0.0776, -0.1228,  0.0904,  0.1000, -0.1358,
         -0.0595, -0.0788, -0.1348,  0.0358,  0.1428, -0.0323,  0.0302,  0.1078,
         -0.0710,  0.0182, -0.0898, -0.1319, -0.0350,  0.0438, -0.1339,  0.0526,
         -0.0025, -0.0488, -0.0158,  0.0242, -0.0029,  0.0698, -0.1523, -0.1175,
          0.0432,  0.1336, -0.0405,  0.0552,  0.1325, -0.0878, -0.0729,  0.1317,
         -0.0986, -0.0179,  0.1012,  0.0716,  0.1339,  0.1147, -0.1302, -0.1020,
          0.1262, -0.0371,  0.0745, -0.0685,  0.0492,  0.1511, -0.1005, -0.0282,
          0.1176,  0.0211,  0.0524,  0.1272, -0.0007,  0.0901, -0.0361,  0.0374,
         -0.0489,  0.0840,  0.0920,  0.0177,  0.0953, -0.0901,  0.0691,  0.1364]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0354, -0.1292, -0.0360, -0.0449, -0.0773, -0.0118, -0.0859, -0.0029,
          0.0857, -0.0794, -0.1084, -0.0813,  0.0621,  0.0766, -0.1054,  0.1206,
          0.0184,  0.0261,  0.0813,  0.1006, -0.0976, -0.0250,  0.0524, -0.1490,
          0.0256, -0.0505,  0.0555, -0.0777, -0.0519,  0.0840, -0.0409,  0.1432,
          0.0122,  0.0796,  0.1283,  0.0924,  0.0714, -0.0566, -0.0177, -0.0891,
         -0.1288,  0.0966,  0.1245, -0.0632,  0.0103,  0.0027, -0.1224,  0.1473,
         -0.0903, -0.1228, -0.0804,  0.0307,  0.0717, -0.0485, -0.0382, -0.1421,
         -0.1467,  0.0802, -0.0986, -0.0617,  0.0299,  0.1307,  0.1298, -0.1034,
          0.0956, -0.0719, -0.1117, -0.0250, -0.1351, -0.0046,  0.0777,  0.0446,
          0.0337, -0.0431,  0.1447, -0.0682, -0.0570,  0.0130,  0.0783, -0.0313,
          0.1227, -0.1171, -0.0059, -0.1114, -0.0833,  0.0046, -0.0893, -0.1309,
          0.0123, -0.0006,  0.1446, -0.0857,  0.0794, -0.1107,  0.0014, -0.0697,
         -0.0048,  0.1264,  0.0131, -0.0714,  0.0620, -0.1164,  0.1039, -0.1451,
          0.1379,  0.1194, -0.0602, -0.0174, -0.1491,  0.1262, -0.0517,  0.1525,
          0.1005,  0.0541,  0.0681, -0.1259,  0.0073,  0.0634,  0.1345,  0.1300,
          0.0622,  0.1217, -0.0370,  0.1322,  0.0561,  0.0199,  0.0040, -0.0092,
          0.0422, -0.0172,  0.0682, -0.1384, -0.0386,  0.0317,  0.0622,  0.0827,
         -0.0399, -0.1433, -0.0526, -0.0215,  0.0023, -0.0859,  0.0615, -0.0395,
          0.0063, -0.1322,  0.0753,  0.0646, -0.0979, -0.0494, -0.0826,  0.0168,
         -0.1441, -0.0063,  0.1053, -0.1143,  0.0369, -0.1204, -0.0330,  0.0206,
         -0.0675, -0.1441,  0.0340,  0.0916, -0.0264, -0.0970, -0.0676,  0.0394,
         -0.1434,  0.0331, -0.0781, -0.0232, -0.0316,  0.1266, -0.0341,  0.1389,
         -0.0155, -0.1336, -0.0015, -0.0551, -0.0333, -0.1008, -0.0307,  0.1371,
          0.0745, -0.0926, -0.1102,  0.0776, -0.1228,  0.0904,  0.1000, -0.1358,
         -0.0595, -0.0788, -0.1348,  0.0358,  0.1428, -0.0323,  0.0302,  0.1078,
         -0.0710,  0.0182, -0.0898, -0.1319, -0.0350,  0.0438, -0.1339,  0.0526,
         -0.0025, -0.0488, -0.0158,  0.0242, -0.0029,  0.0698, -0.1523, -0.1175,
          0.0432,  0.1336, -0.0405,  0.0552,  0.1325, -0.0878, -0.0729,  0.1317,
         -0.0986, -0.0179,  0.1012,  0.0716,  0.1339,  0.1147, -0.1302, -0.1020,
          0.1262, -0.0371,  0.0745, -0.0685,  0.0492,  0.1511, -0.1005, -0.0282,
          0.1176,  0.0211,  0.0524,  0.1272, -0.0007,  0.0901, -0.0361,  0.0374,
         -0.0489,  0.0840,  0.0920,  0.0177,  0.0953, -0.0901,  0.0691,  0.1364]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0192, -0.1239,  0.0376,  ..., -0.1138, -0.1100,  0.0383],
        [ 0.0984, -0.0526,  0.0347,  ..., -0.0233,  0.1196, -0.0166],
        [-0.0621,  0.0477, -0.0360,  ..., -0.1239, -0.0055, -0.0400],
        ...,
        [-0.0720,  0.0047, -0.0478,  ...,  0.0819, -0.1011,  0.1114],
        [ 0.0984, -0.0645,  0.0300,  ..., -0.1069, -0.0914, -0.0427],
        [ 0.1069,  0.0632,  0.1055,  ..., -0.0382, -0.0496,  0.1167]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0192, -0.1239,  0.0376,  ..., -0.1138, -0.1100,  0.0383],
        [ 0.0984, -0.0526,  0.0347,  ..., -0.0233,  0.1196, -0.0166],
        [-0.0621,  0.0477, -0.0360,  ..., -0.1239, -0.0055, -0.0400],
        ...,
        [-0.0720,  0.0047, -0.0478,  ...,  0.0819, -0.1011,  0.1114],
        [ 0.0984, -0.0645,  0.0300,  ..., -0.1069, -0.0914, -0.0427],
        [ 0.1069,  0.0632,  0.1055,  ..., -0.0382, -0.0496,  0.1167]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0624, -0.0696,  0.0494,  ..., -0.1632, -0.0877, -0.0808],
        [-0.1519,  0.1009, -0.0016,  ..., -0.1036,  0.1214, -0.1549],
        [ 0.1397,  0.1179, -0.0797,  ..., -0.0724,  0.0190,  0.0587],
        ...,
        [-0.1565, -0.0175, -0.0075,  ..., -0.1620, -0.1676, -0.1253],
        [-0.1233, -0.1097,  0.0953,  ...,  0.0295, -0.1177, -0.1145],
        [-0.1369, -0.0453,  0.0920,  ...,  0.1673, -0.1703, -0.0779]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0624, -0.0696,  0.0494,  ..., -0.1632, -0.0877, -0.0808],
        [-0.1519,  0.1009, -0.0016,  ..., -0.1036,  0.1214, -0.1549],
        [ 0.1397,  0.1179, -0.0797,  ..., -0.0724,  0.0190,  0.0587],
        ...,
        [-0.1565, -0.0175, -0.0075,  ..., -0.1620, -0.1676, -0.1253],
        [-0.1233, -0.1097,  0.0953,  ...,  0.0295, -0.1177, -0.1145],
        [-0.1369, -0.0453,  0.0920,  ...,  0.1673, -0.1703, -0.0779]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.0032,  0.1394,  0.0831,  ..., -0.0603,  0.1468, -0.2331],
        [-0.1481, -0.1992,  0.1157,  ..., -0.1739,  0.0838,  0.1107],
        [ 0.0175, -0.1761, -0.1868,  ...,  0.0439, -0.2141,  0.0144],
        ...,
        [-0.1557, -0.0151,  0.1574,  ...,  0.0424,  0.0027, -0.0780],
        [ 0.1244, -0.1451,  0.0502,  ..., -0.2433,  0.1636,  0.1872],
        [ 0.0630, -0.0631, -0.1037,  ...,  0.1525,  0.0055,  0.1507]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0032,  0.1394,  0.0831,  ..., -0.0603,  0.1468, -0.2331],
        [-0.1481, -0.1992,  0.1157,  ..., -0.1739,  0.0838,  0.1107],
        [ 0.0175, -0.1761, -0.1868,  ...,  0.0439, -0.2141,  0.0144],
        ...,
        [-0.1557, -0.0151,  0.1574,  ...,  0.0424,  0.0027, -0.0780],
        [ 0.1244, -0.1451,  0.0502,  ..., -0.2433,  0.1636,  0.1872],
        [ 0.0630, -0.0631, -0.1037,  ...,  0.1525,  0.0055,  0.1507]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.1980],
        [ 0.2344],
        [-0.1554],
        [ 0.2418],
        [-0.3454],
        [-0.0762],
        [ 0.3781],
        [ 0.1864],
        [ 0.0777],
        [-0.0159],
        [ 0.1145],
        [-0.1344],
        [ 0.0729],
        [-0.2048],
        [-0.3995],
        [ 0.1233],
        [ 0.4003],
        [ 0.1149],
        [ 0.1855],
        [-0.0816],
        [ 0.1509],
        [-0.0177],
        [ 0.4055],
        [ 0.2259],
        [ 0.1639],
        [-0.3647],
        [-0.0914],
        [ 0.2856],
        [ 0.0835],
        [ 0.1706],
        [-0.1087],
        [ 0.0764]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1980],
        [ 0.2344],
        [-0.1554],
        [ 0.2418],
        [-0.3454],
        [-0.0762],
        [ 0.3781],
        [ 0.1864],
        [ 0.0777],
        [-0.0159],
        [ 0.1145],
        [-0.1344],
        [ 0.0729],
        [-0.2048],
        [-0.3995],
        [ 0.1233],
        [ 0.4003],
        [ 0.1149],
        [ 0.1855],
        [-0.0816],
        [ 0.1509],
        [-0.0177],
        [ 0.4055],
        [ 0.2259],
        [ 0.1639],
        [-0.3647],
        [-0.0914],
        [ 0.2856],
        [ 0.0835],
        [ 0.1706],
        [-0.1087],
        [ 0.0764]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(47.6878, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.8983, device='cuda:0')



h[100].sum tensor(4.4332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5324, device='cuda:0')



h[200].sum tensor(2.7107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(2.7714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10505.6670, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0171, 0.0000, 0.0038,  ..., 0.0012, 0.0116, 0.0061],
        [0.0080, 0.0000, 0.0018,  ..., 0.0006, 0.0055, 0.0029],
        [0.0023, 0.0000, 0.0005,  ..., 0.0002, 0.0015, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(56795.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1732.9235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(110.1902, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1265.8147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(80.4830, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1358.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(86.3972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.2101],
        [0.1480],
        [0.0996],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(9723.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[0.2101],
        [0.1480],
        [0.0996],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0134,  0.0077, -0.0203,  ...,  0.0071,  0.0029, -0.0200],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-1114.2120, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-88.1990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-90.4132, device='cuda:0')



h[100].sum tensor(-26.2851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-26.9450, device='cuda:0')



h[200].sum tensor(60.4390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(61.9563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0292, 0.0000,  ..., 0.0267, 0.0109, 0.0000],
        [0.0000, 0.0240, 0.0000,  ..., 0.0219, 0.0089, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0051, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(91690.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0873, 0.0945,  ..., 0.2258, 0.1679, 0.0000],
        [0.0026, 0.0748, 0.0809,  ..., 0.1935, 0.1438, 0.0000],
        [0.0021, 0.0600, 0.0650,  ..., 0.1554, 0.1155, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(514314.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(152.8849, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(10.5723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11123.3643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(779.2766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5658.0479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(396.3126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2397e+00],
        [-1.3414e+00],
        [-1.4802e+00],
        ...,
        [-1.6154e-05],
        [-2.6857e-05],
        [-3.8388e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-86051.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.2101],
        [0.1480],
        [0.0996],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 39, in <module>
    result1 = net(batcheddglgraph, TraTen[10000:10010].reshape(10 * 6796, 1).to('cuda'))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 47, in forward
    h = self.conv1(g, in_fet)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 437, in forward
    rst = th.matmul(rst, weight)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_mm)

real	0m25.900s
user	0m18.408s
sys	0m5.025s
