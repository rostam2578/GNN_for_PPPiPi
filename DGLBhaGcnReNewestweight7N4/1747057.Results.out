0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 14:43:46 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0    33W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2aec6e6d48e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.185s
user	0m2.650s
sys	0m1.239s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[14:44:10] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.1207],
        [-0.1349],
        [-0.4444],
        ...,
        [-0.8589],
        [-0.2847],
        [ 0.0282]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-25.3125, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-1.3468e-01, -5.1112e-03, -1.3023e-01,  1.4978e-01,  1.1960e-01,
          6.4558e-02,  1.6689e-02,  2.7858e-02, -2.2777e-02, -8.6620e-02,
          5.8120e-02,  1.3728e-01, -1.2410e-01, -3.2371e-02,  5.8309e-02,
          8.0835e-02, -4.1389e-02, -8.6626e-02, -8.5999e-02,  1.1087e-01,
         -8.3711e-03, -1.0315e-01, -2.5170e-02, -9.0776e-02, -5.6752e-02,
          5.3647e-03, -1.3557e-01,  5.3515e-03,  1.7643e-02, -1.2894e-01,
         -6.8754e-03,  1.3730e-01, -6.4130e-02,  1.0792e-01,  1.0337e-04,
         -1.4694e-01,  1.3547e-01, -3.5647e-02, -6.7418e-02,  9.0523e-02,
         -1.0743e-01,  1.0820e-01, -1.8131e-02,  8.4912e-02, -8.9595e-02,
         -1.5003e-01,  1.3662e-01,  1.4427e-01,  4.2141e-02, -2.9462e-02,
          7.5668e-02,  5.4380e-02,  5.6524e-02,  8.1820e-02, -1.1897e-01,
         -1.0431e-01,  5.1891e-02, -1.5170e-01, -1.1088e-01, -7.6009e-02,
          7.1092e-02, -2.0310e-02,  2.1045e-02, -4.2610e-03, -9.2661e-03,
          1.3963e-01,  6.6740e-02,  8.9788e-02, -6.1561e-02,  1.2681e-01,
         -1.9583e-02,  9.9012e-03, -1.1397e-01, -1.2969e-01, -1.1015e-01,
         -1.4678e-01,  1.1854e-01,  9.2905e-02,  1.4112e-01,  8.3824e-02,
          1.4334e-01, -1.1506e-01, -5.0353e-02,  7.5023e-02,  9.1061e-02,
          1.1550e-01,  1.6260e-02,  7.5505e-02,  1.2187e-01,  7.2665e-02,
          2.3678e-02, -7.6020e-02, -1.1837e-01,  3.7845e-03,  2.1980e-02,
          4.4834e-02,  1.0448e-01, -1.0340e-01,  3.1549e-02,  5.9286e-03,
         -1.1523e-01, -8.9319e-02, -1.3783e-01, -9.4229e-02, -7.5147e-02,
         -1.2678e-01, -9.5199e-02, -1.2746e-01,  9.7290e-02,  4.2939e-02,
          4.0654e-02, -3.8323e-02,  1.3582e-01,  1.4748e-01,  3.5577e-03,
          1.0706e-01, -1.3382e-01,  1.3142e-01,  5.0431e-03,  1.1605e-01,
          9.4529e-02,  2.8763e-02, -9.1545e-04, -1.3678e-01, -3.8425e-02,
          3.9409e-02,  1.5273e-01,  4.6574e-02,  3.8145e-02,  1.5010e-01,
         -2.9982e-02, -1.0482e-01, -5.5377e-02, -1.4760e-01,  1.5179e-01,
         -1.2110e-01,  3.9874e-02,  1.0242e-01, -7.5067e-02, -9.0343e-02,
          3.1627e-02,  1.1357e-01,  1.3526e-01, -3.7603e-03,  7.2195e-02,
         -1.3823e-01, -4.2064e-02, -1.9195e-02, -3.2646e-02,  1.4554e-01,
          1.4459e-01, -7.8438e-02, -9.4885e-02, -1.0424e-02,  4.0101e-02,
         -1.0449e-01, -9.2063e-02,  1.4597e-01, -2.6129e-02,  1.5257e-01,
         -2.5482e-02, -1.3324e-01,  1.3996e-01, -3.6827e-02, -1.3244e-02,
         -7.0622e-02, -1.9799e-02, -1.2616e-01,  1.1343e-01, -1.2624e-01,
          7.1672e-02, -1.3100e-01, -5.4962e-02, -1.0994e-01,  2.3346e-02,
         -8.0946e-03, -1.3361e-01,  1.2992e-01, -9.3553e-02,  2.2039e-02,
         -1.0182e-01, -5.9515e-02, -2.8728e-02, -2.9999e-02, -1.2503e-02,
         -9.6514e-02,  8.1778e-02,  1.0470e-01, -2.2724e-02, -4.6886e-02,
          1.3400e-01,  1.3063e-01, -1.0010e-02, -1.3523e-01, -1.0936e-01,
          4.4085e-02,  6.5033e-02, -5.5733e-02,  3.7735e-03, -3.6044e-02,
         -6.8199e-02, -8.9156e-02, -9.7457e-02,  4.7553e-02,  1.3945e-01,
          3.7048e-03,  1.3299e-01,  7.3776e-02,  1.4323e-01,  1.1789e-01,
         -3.0434e-02, -2.3604e-02, -4.8905e-02, -9.6244e-02, -9.7343e-02,
         -1.7161e-02, -1.0658e-01,  7.2979e-02,  9.4226e-02,  4.1249e-02,
         -7.3720e-02,  2.1775e-02, -1.1537e-01,  4.7335e-02,  5.9440e-02,
          2.8314e-02, -7.5494e-02,  8.6215e-02,  2.8674e-02,  1.0477e-01,
          5.4400e-02, -9.7218e-02, -7.0008e-02, -8.4796e-03, -6.9916e-02,
         -1.4713e-01, -1.4572e-01,  1.2352e-01,  1.1827e-02,  9.7335e-02,
         -1.5172e-01, -4.5458e-02, -9.9141e-02,  1.2632e-01, -1.1860e-01,
          1.4747e-01, -8.6037e-02,  2.7241e-02, -1.4258e-01,  1.2608e-01,
         -1.0571e-01, -1.4774e-01, -9.1204e-02, -1.4349e-01, -3.6616e-02,
         -3.0049e-02]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-1.3468e-01, -5.1112e-03, -1.3023e-01,  1.4978e-01,  1.1960e-01,
          6.4558e-02,  1.6689e-02,  2.7858e-02, -2.2777e-02, -8.6620e-02,
          5.8120e-02,  1.3728e-01, -1.2410e-01, -3.2371e-02,  5.8309e-02,
          8.0835e-02, -4.1389e-02, -8.6626e-02, -8.5999e-02,  1.1087e-01,
         -8.3711e-03, -1.0315e-01, -2.5170e-02, -9.0776e-02, -5.6752e-02,
          5.3647e-03, -1.3557e-01,  5.3515e-03,  1.7643e-02, -1.2894e-01,
         -6.8754e-03,  1.3730e-01, -6.4130e-02,  1.0792e-01,  1.0337e-04,
         -1.4694e-01,  1.3547e-01, -3.5647e-02, -6.7418e-02,  9.0523e-02,
         -1.0743e-01,  1.0820e-01, -1.8131e-02,  8.4912e-02, -8.9595e-02,
         -1.5003e-01,  1.3662e-01,  1.4427e-01,  4.2141e-02, -2.9462e-02,
          7.5668e-02,  5.4380e-02,  5.6524e-02,  8.1820e-02, -1.1897e-01,
         -1.0431e-01,  5.1891e-02, -1.5170e-01, -1.1088e-01, -7.6009e-02,
          7.1092e-02, -2.0310e-02,  2.1045e-02, -4.2610e-03, -9.2661e-03,
          1.3963e-01,  6.6740e-02,  8.9788e-02, -6.1561e-02,  1.2681e-01,
         -1.9583e-02,  9.9012e-03, -1.1397e-01, -1.2969e-01, -1.1015e-01,
         -1.4678e-01,  1.1854e-01,  9.2905e-02,  1.4112e-01,  8.3824e-02,
          1.4334e-01, -1.1506e-01, -5.0353e-02,  7.5023e-02,  9.1061e-02,
          1.1550e-01,  1.6260e-02,  7.5505e-02,  1.2187e-01,  7.2665e-02,
          2.3678e-02, -7.6020e-02, -1.1837e-01,  3.7845e-03,  2.1980e-02,
          4.4834e-02,  1.0448e-01, -1.0340e-01,  3.1549e-02,  5.9286e-03,
         -1.1523e-01, -8.9319e-02, -1.3783e-01, -9.4229e-02, -7.5147e-02,
         -1.2678e-01, -9.5199e-02, -1.2746e-01,  9.7290e-02,  4.2939e-02,
          4.0654e-02, -3.8323e-02,  1.3582e-01,  1.4748e-01,  3.5577e-03,
          1.0706e-01, -1.3382e-01,  1.3142e-01,  5.0431e-03,  1.1605e-01,
          9.4529e-02,  2.8763e-02, -9.1545e-04, -1.3678e-01, -3.8425e-02,
          3.9409e-02,  1.5273e-01,  4.6574e-02,  3.8145e-02,  1.5010e-01,
         -2.9982e-02, -1.0482e-01, -5.5377e-02, -1.4760e-01,  1.5179e-01,
         -1.2110e-01,  3.9874e-02,  1.0242e-01, -7.5067e-02, -9.0343e-02,
          3.1627e-02,  1.1357e-01,  1.3526e-01, -3.7603e-03,  7.2195e-02,
         -1.3823e-01, -4.2064e-02, -1.9195e-02, -3.2646e-02,  1.4554e-01,
          1.4459e-01, -7.8438e-02, -9.4885e-02, -1.0424e-02,  4.0101e-02,
         -1.0449e-01, -9.2063e-02,  1.4597e-01, -2.6129e-02,  1.5257e-01,
         -2.5482e-02, -1.3324e-01,  1.3996e-01, -3.6827e-02, -1.3244e-02,
         -7.0622e-02, -1.9799e-02, -1.2616e-01,  1.1343e-01, -1.2624e-01,
          7.1672e-02, -1.3100e-01, -5.4962e-02, -1.0994e-01,  2.3346e-02,
         -8.0946e-03, -1.3361e-01,  1.2992e-01, -9.3553e-02,  2.2039e-02,
         -1.0182e-01, -5.9515e-02, -2.8728e-02, -2.9999e-02, -1.2503e-02,
         -9.6514e-02,  8.1778e-02,  1.0470e-01, -2.2724e-02, -4.6886e-02,
          1.3400e-01,  1.3063e-01, -1.0010e-02, -1.3523e-01, -1.0936e-01,
          4.4085e-02,  6.5033e-02, -5.5733e-02,  3.7735e-03, -3.6044e-02,
         -6.8199e-02, -8.9156e-02, -9.7457e-02,  4.7553e-02,  1.3945e-01,
          3.7048e-03,  1.3299e-01,  7.3776e-02,  1.4323e-01,  1.1789e-01,
         -3.0434e-02, -2.3604e-02, -4.8905e-02, -9.6244e-02, -9.7343e-02,
         -1.7161e-02, -1.0658e-01,  7.2979e-02,  9.4226e-02,  4.1249e-02,
         -7.3720e-02,  2.1775e-02, -1.1537e-01,  4.7335e-02,  5.9440e-02,
          2.8314e-02, -7.5494e-02,  8.6215e-02,  2.8674e-02,  1.0477e-01,
          5.4400e-02, -9.7218e-02, -7.0008e-02, -8.4796e-03, -6.9916e-02,
         -1.4713e-01, -1.4572e-01,  1.2352e-01,  1.1827e-02,  9.7335e-02,
         -1.5172e-01, -4.5458e-02, -9.9141e-02,  1.2632e-01, -1.1860e-01,
          1.4747e-01, -8.6037e-02,  2.7241e-02, -1.4258e-01,  1.2608e-01,
         -1.0571e-01, -1.4774e-01, -9.1204e-02, -1.4349e-01, -3.6616e-02,
         -3.0049e-02]], device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.0353, -0.0985, -0.0822,  ..., -0.1133, -0.0771, -0.0855],
        [ 0.0832,  0.0022,  0.0314,  ...,  0.0771, -0.0220,  0.0859],
        [ 0.0434, -0.0024, -0.0127,  ...,  0.1186,  0.1072,  0.0640],
        ...,
        [-0.1169,  0.1193,  0.0055,  ..., -0.0116, -0.0786, -0.0657],
        [ 0.0455,  0.0732,  0.0503,  ..., -0.1055,  0.0800,  0.0096],
        [ 0.0198,  0.0509,  0.0356,  ..., -0.0832,  0.0153, -0.0603]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0353, -0.0985, -0.0822,  ..., -0.1133, -0.0771, -0.0855],
        [ 0.0832,  0.0022,  0.0314,  ...,  0.0771, -0.0220,  0.0859],
        [ 0.0434, -0.0024, -0.0127,  ...,  0.1186,  0.1072,  0.0640],
        ...,
        [-0.1169,  0.1193,  0.0055,  ..., -0.0116, -0.0786, -0.0657],
        [ 0.0455,  0.0732,  0.0503,  ..., -0.1055,  0.0800,  0.0096],
        [ 0.0198,  0.0509,  0.0356,  ..., -0.0832,  0.0153, -0.0603]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[-0.0770,  0.1523,  0.1103,  ...,  0.1325,  0.1377, -0.1141],
        [ 0.1720, -0.0207,  0.1248,  ...,  0.0615,  0.1464,  0.1757],
        [ 0.0427,  0.1504,  0.0102,  ...,  0.1406,  0.0092, -0.0338],
        ...,
        [ 0.0878, -0.0047, -0.1434,  ..., -0.0376,  0.0973,  0.1481],
        [ 0.0700, -0.0044, -0.0362,  ...,  0.1330,  0.0038, -0.0330],
        [ 0.1642, -0.1387, -0.1037,  ...,  0.1617,  0.1195,  0.1540]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0770,  0.1523,  0.1103,  ...,  0.1325,  0.1377, -0.1141],
        [ 0.1720, -0.0207,  0.1248,  ...,  0.0615,  0.1464,  0.1757],
        [ 0.0427,  0.1504,  0.0102,  ...,  0.1406,  0.0092, -0.0338],
        ...,
        [ 0.0878, -0.0047, -0.1434,  ..., -0.0376,  0.0973,  0.1481],
        [ 0.0700, -0.0044, -0.0362,  ...,  0.1330,  0.0038, -0.0330],
        [ 0.1642, -0.1387, -0.1037,  ...,  0.1617,  0.1195,  0.1540]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.1687,  0.0724, -0.0592,  ...,  0.0726,  0.0139, -0.1723],
        [-0.1200, -0.0221,  0.1748,  ..., -0.2077, -0.1771,  0.0940],
        [ 0.0952, -0.0325, -0.1195,  ...,  0.0961, -0.0784,  0.2428],
        ...,
        [ 0.1483, -0.0883, -0.1354,  ..., -0.0492, -0.1454,  0.1804],
        [ 0.2141,  0.1297,  0.1399,  ..., -0.1471,  0.1363,  0.1311],
        [-0.0563,  0.1784,  0.1693,  ..., -0.1392,  0.1443,  0.0817]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.1687,  0.0724, -0.0592,  ...,  0.0726,  0.0139, -0.1723],
        [-0.1200, -0.0221,  0.1748,  ..., -0.2077, -0.1771,  0.0940],
        [ 0.0952, -0.0325, -0.1195,  ...,  0.0961, -0.0784,  0.2428],
        ...,
        [ 0.1483, -0.0883, -0.1354,  ..., -0.0492, -0.1454,  0.1804],
        [ 0.2141,  0.1297,  0.1399,  ..., -0.1471,  0.1363,  0.1311],
        [-0.0563,  0.1784,  0.1693,  ..., -0.1392,  0.1443,  0.0817]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.1106],
        [-0.0892],
        [ 0.1144],
        [ 0.3562],
        [ 0.4026],
        [ 0.1108],
        [-0.0200],
        [-0.2444],
        [-0.0158],
        [-0.3172],
        [-0.1510],
        [ 0.1136],
        [ 0.0543],
        [-0.3397],
        [-0.1610],
        [ 0.1857],
        [ 0.2581],
        [-0.2477],
        [ 0.1554],
        [-0.1644],
        [-0.0588],
        [ 0.2075],
        [ 0.0373],
        [-0.0591],
        [-0.0892],
        [ 0.0939],
        [ 0.3274],
        [ 0.2980],
        [ 0.2740],
        [-0.3151],
        [ 0.2339],
        [-0.2017]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1106],
        [-0.0892],
        [ 0.1144],
        [ 0.3562],
        [ 0.4026],
        [ 0.1108],
        [-0.0200],
        [-0.2444],
        [-0.0158],
        [-0.3172],
        [-0.1510],
        [ 0.1136],
        [ 0.0543],
        [-0.3397],
        [-0.1610],
        [ 0.1857],
        [ 0.2581],
        [-0.2477],
        [ 0.1554],
        [-0.1644],
        [-0.0588],
        [ 0.2075],
        [ 0.0373],
        [-0.0591],
        [-0.0892],
        [ 0.0939],
        [ 0.3274],
        [ 0.2980],
        [ 0.2740],
        [-0.3151],
        [ 0.2339],
        [-0.2017]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(63.6766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.8309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(2.8943, device='cuda:0')



h[100].sum tensor(-8.7245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.9198, device='cuda:0')



h[200].sum tensor(7.2887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10851.3027, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0271, 0.0000, 0.0162,  ..., 0.0055, 0.0025, 0.0000],
        [0.0127, 0.0000, 0.0076,  ..., 0.0026, 0.0012, 0.0000],
        [0.0036, 0.0000, 0.0021,  ..., 0.0007, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(71697.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2741.0437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(174.2827, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(547.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(34.8277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(806.1600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(51.2475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7652],
        [-0.5390],
        [-0.3627],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-35398.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.7652],
        [-0.5390],
        [-0.3627],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0148, -0.0066,  0.0024,  ...,  0.0086, -0.0223,  0.0223],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-921.5285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-97.5641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-100.0134, device='cuda:0')



h[100].sum tensor(-145.0813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-148.7235, device='cuda:0')



h[200].sum tensor(-9.9874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-10.2382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0090,  ..., 0.0325, 0.0000, 0.0841],
        [0.0000, 0.0000, 0.0074,  ..., 0.0267, 0.0000, 0.0692],
        [0.0000, 0.0000, 0.0017,  ..., 0.0063, 0.0000, 0.0162],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(97004.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1512, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0581],
        [0.1295, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0498],
        [0.1040, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0399],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(470702.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7482.1431, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(524.0923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1358.7539, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-28.7596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4221e+00],
        [-3.7029e+00],
        [-4.0861e+00],
        ...,
        [-4.4581e-05],
        [-7.4122e-05],
        [-1.0596e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(-237558.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.7652],
        [-0.5390],
        [-0.3627],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-55.4628, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.4752, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-5.6776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-943.1598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1315.4142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2984.2612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0151,  0.0067, -0.0205,  ...,  0.0230, -0.0092, -0.0219],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0192,  0.0085, -0.0260,  ...,  0.0292, -0.0116, -0.0279],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(-359.5891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(41.6624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-55.4628, device='cuda:0')



h[100].sum tensor(50.8809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.4752, device='cuda:0')



h[200].sum tensor(2.7557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-5.6776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0054, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        [0.0622, 0.0275, 0.0000,  ..., 0.0947, 0.0000, 0.0000],
        [0.0234, 0.0103, 0.0000,  ..., 0.0356, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(53885.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0990, 0.0000, 0.0675,  ..., 0.0092, 0.0303, 0.0000],
        [0.1898, 0.0000, 0.1294,  ..., 0.0177, 0.0582, 0.0000],
        [0.1582, 0.0000, 0.1079,  ..., 0.0147, 0.0485, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(295027., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7470.6689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(255.2786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-127.7481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(324.6158, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0717],
        [0.0893],
        [0.0976],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(12500.3008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)

result1 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape torch.Size([67960, 1]) 
result0 tensor([[0.0717],
        [0.0893],
        [0.0976],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result0.shape torch.Size([67960, 1])
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 80, in <module>
    plotevent(traingnnpppipi[EvBTr], 331, f'event number {EvBTr} with noise \n color indicates time')
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 48, in plotevent
    fig.colorbar(ax.matshow(event, aspect=2, vmin=0, vmax=1, extent=[0, 288, 0, 43], origin='lower')\
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/axes/_axes.py", line 7931, in matshow
    im = self.imshow(Z, **kw)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/__init__.py", line 1361, in inner
    return func(ax, *map(sanitize_sequence, args), **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/axes/_axes.py", line 5609, in imshow
    im.set_data(X)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/image.py", line 709, in set_data
    raise TypeError("Invalid shape {} for image data"
TypeError: Invalid shape (6796,) for image data

real	0m27.363s
user	0m20.156s
sys	0m5.062s
