0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Sep 19 14:48:59 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   23C    P0    32W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b5fb4d0f8e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.012s
user	0m2.594s
sys	0m0.981s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[14:49:23] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.3889],
        [-0.4510],
        [ 0.2605],
        ...,
        [ 0.7587],
        [-0.3758],
        [-0.5165]], device='cuda:0', requires_grad=True) 
node features sum: tensor(34.1689, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0101,  0.0134,  0.0064, -0.1205,  0.1173,  0.1058,  0.1001, -0.0988,
         -0.1358, -0.1401,  0.0096,  0.0579, -0.1442,  0.1306,  0.1475, -0.0614,
         -0.0903,  0.0067,  0.1117,  0.1213,  0.0554,  0.0439, -0.1267, -0.1092,
         -0.0277,  0.0298,  0.1044,  0.1196, -0.1507, -0.0756,  0.0868,  0.0446,
          0.0502,  0.1479, -0.1247,  0.0493,  0.0004, -0.0648,  0.0691,  0.0769,
         -0.0112,  0.1454, -0.0180,  0.0468,  0.1441,  0.0231, -0.1525, -0.0804,
          0.0962, -0.1218, -0.1207,  0.1165,  0.0718,  0.1524,  0.1430, -0.0966,
         -0.1328, -0.1203, -0.0922,  0.1071,  0.1106,  0.0849, -0.1147, -0.0741,
          0.0701, -0.0767, -0.1108, -0.1121, -0.1420,  0.0971,  0.0317, -0.0256,
         -0.1203, -0.0334,  0.1344,  0.0928,  0.1256,  0.0594, -0.1263, -0.0636,
         -0.0310, -0.0589, -0.1479,  0.0997, -0.0523, -0.0603, -0.1378,  0.0087,
          0.0520, -0.1381, -0.0029, -0.1150, -0.1485, -0.1369, -0.1207,  0.0472,
          0.0885, -0.0634, -0.1220, -0.0160,  0.0301, -0.1223,  0.0175,  0.0677,
          0.0319,  0.1433,  0.1092,  0.1194,  0.0521,  0.1052,  0.0340,  0.0925,
         -0.0610,  0.0962, -0.1389,  0.0161,  0.1446,  0.0628,  0.1047, -0.0046,
         -0.0490, -0.0188,  0.0965, -0.1063, -0.1022, -0.1331,  0.0770,  0.0936,
         -0.1100, -0.1507,  0.0767, -0.0170,  0.0691,  0.1524, -0.0046, -0.0513,
          0.0054, -0.1208,  0.0095,  0.1041, -0.0240,  0.0071,  0.0650, -0.1222,
          0.0397,  0.0520, -0.0228, -0.0534,  0.1382, -0.0733,  0.1454,  0.0763,
         -0.1427,  0.0807, -0.0582, -0.0095, -0.0576, -0.1290,  0.0432, -0.0153,
         -0.1051, -0.0679, -0.1355,  0.0784,  0.0799, -0.0528,  0.0954,  0.1209,
          0.0127, -0.0841,  0.0020, -0.0602,  0.0425, -0.0539, -0.0808, -0.0467,
         -0.0216, -0.0737, -0.1403,  0.0753, -0.0366,  0.1040, -0.1229,  0.1473,
          0.1245, -0.1115,  0.0234, -0.0093,  0.0957,  0.0202, -0.0143, -0.0070,
         -0.0471, -0.1476,  0.1512, -0.0589,  0.0145,  0.0097, -0.0166, -0.0878,
          0.1135, -0.1489,  0.1366, -0.0334,  0.1484,  0.0788, -0.1511,  0.1041,
          0.0385,  0.0880,  0.0492,  0.0993, -0.0688,  0.0150, -0.1418, -0.0043,
         -0.0296,  0.1277,  0.0302, -0.0605, -0.0524,  0.1514,  0.1071,  0.1492,
         -0.0364,  0.1172,  0.0865,  0.1110,  0.1026, -0.0344,  0.0797, -0.1286,
         -0.1224,  0.1180,  0.0012,  0.0453,  0.0524,  0.1368,  0.0260,  0.0373,
          0.1359, -0.1044, -0.0161,  0.0144, -0.0722, -0.0136, -0.0885, -0.0103,
          0.0714,  0.0292, -0.0412,  0.0663,  0.1092,  0.1146,  0.1457, -0.0260]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0101,  0.0134,  0.0064, -0.1205,  0.1173,  0.1058,  0.1001, -0.0988,
         -0.1358, -0.1401,  0.0096,  0.0579, -0.1442,  0.1306,  0.1475, -0.0614,
         -0.0903,  0.0067,  0.1117,  0.1213,  0.0554,  0.0439, -0.1267, -0.1092,
         -0.0277,  0.0298,  0.1044,  0.1196, -0.1507, -0.0756,  0.0868,  0.0446,
          0.0502,  0.1479, -0.1247,  0.0493,  0.0004, -0.0648,  0.0691,  0.0769,
         -0.0112,  0.1454, -0.0180,  0.0468,  0.1441,  0.0231, -0.1525, -0.0804,
          0.0962, -0.1218, -0.1207,  0.1165,  0.0718,  0.1524,  0.1430, -0.0966,
         -0.1328, -0.1203, -0.0922,  0.1071,  0.1106,  0.0849, -0.1147, -0.0741,
          0.0701, -0.0767, -0.1108, -0.1121, -0.1420,  0.0971,  0.0317, -0.0256,
         -0.1203, -0.0334,  0.1344,  0.0928,  0.1256,  0.0594, -0.1263, -0.0636,
         -0.0310, -0.0589, -0.1479,  0.0997, -0.0523, -0.0603, -0.1378,  0.0087,
          0.0520, -0.1381, -0.0029, -0.1150, -0.1485, -0.1369, -0.1207,  0.0472,
          0.0885, -0.0634, -0.1220, -0.0160,  0.0301, -0.1223,  0.0175,  0.0677,
          0.0319,  0.1433,  0.1092,  0.1194,  0.0521,  0.1052,  0.0340,  0.0925,
         -0.0610,  0.0962, -0.1389,  0.0161,  0.1446,  0.0628,  0.1047, -0.0046,
         -0.0490, -0.0188,  0.0965, -0.1063, -0.1022, -0.1331,  0.0770,  0.0936,
         -0.1100, -0.1507,  0.0767, -0.0170,  0.0691,  0.1524, -0.0046, -0.0513,
          0.0054, -0.1208,  0.0095,  0.1041, -0.0240,  0.0071,  0.0650, -0.1222,
          0.0397,  0.0520, -0.0228, -0.0534,  0.1382, -0.0733,  0.1454,  0.0763,
         -0.1427,  0.0807, -0.0582, -0.0095, -0.0576, -0.1290,  0.0432, -0.0153,
         -0.1051, -0.0679, -0.1355,  0.0784,  0.0799, -0.0528,  0.0954,  0.1209,
          0.0127, -0.0841,  0.0020, -0.0602,  0.0425, -0.0539, -0.0808, -0.0467,
         -0.0216, -0.0737, -0.1403,  0.0753, -0.0366,  0.1040, -0.1229,  0.1473,
          0.1245, -0.1115,  0.0234, -0.0093,  0.0957,  0.0202, -0.0143, -0.0070,
         -0.0471, -0.1476,  0.1512, -0.0589,  0.0145,  0.0097, -0.0166, -0.0878,
          0.1135, -0.1489,  0.1366, -0.0334,  0.1484,  0.0788, -0.1511,  0.1041,
          0.0385,  0.0880,  0.0492,  0.0993, -0.0688,  0.0150, -0.1418, -0.0043,
         -0.0296,  0.1277,  0.0302, -0.0605, -0.0524,  0.1514,  0.1071,  0.1492,
         -0.0364,  0.1172,  0.0865,  0.1110,  0.1026, -0.0344,  0.0797, -0.1286,
         -0.1224,  0.1180,  0.0012,  0.0453,  0.0524,  0.1368,  0.0260,  0.0373,
          0.1359, -0.1044, -0.0161,  0.0144, -0.0722, -0.0136, -0.0885, -0.0103,
          0.0714,  0.0292, -0.0412,  0.0663,  0.1092,  0.1146,  0.1457, -0.0260]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-0.1044, -0.0385,  0.0797,  ..., -0.0143, -0.0569,  0.0080],
        [-0.0129,  0.0278,  0.0477,  ..., -0.1129,  0.0417,  0.0134],
        [-0.1099,  0.1186, -0.1133,  ...,  0.0484, -0.0813,  0.0377],
        ...,
        [-0.0477, -0.0192, -0.1209,  ...,  0.0744, -0.0548, -0.0087],
        [-0.1180,  0.0773, -0.0455,  ..., -0.0005, -0.0195,  0.0838],
        [ 0.0695, -0.0438,  0.0941,  ..., -0.0212, -0.1030, -0.0715]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.1044, -0.0385,  0.0797,  ..., -0.0143, -0.0569,  0.0080],
        [-0.0129,  0.0278,  0.0477,  ..., -0.1129,  0.0417,  0.0134],
        [-0.1099,  0.1186, -0.1133,  ...,  0.0484, -0.0813,  0.0377],
        ...,
        [-0.0477, -0.0192, -0.1209,  ...,  0.0744, -0.0548, -0.0087],
        [-0.1180,  0.0773, -0.0455,  ..., -0.0005, -0.0195,  0.0838],
        [ 0.0695, -0.0438,  0.0941,  ..., -0.0212, -0.1030, -0.0715]],
       device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0781, -0.0580,  0.1271,  ..., -0.1114, -0.0457,  0.0632],
        [ 0.0529,  0.0324,  0.0425,  ..., -0.1156, -0.1018, -0.1260],
        [-0.0482,  0.1518, -0.1382,  ...,  0.0300,  0.0846, -0.0907],
        ...,
        [-0.0508, -0.0954, -0.0303,  ...,  0.0736, -0.1681, -0.1663],
        [ 0.0056,  0.1640, -0.1275,  ...,  0.1698, -0.1206, -0.0203],
        [ 0.0669,  0.1320,  0.0252,  ..., -0.0016, -0.1023,  0.0205]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0781, -0.0580,  0.1271,  ..., -0.1114, -0.0457,  0.0632],
        [ 0.0529,  0.0324,  0.0425,  ..., -0.1156, -0.1018, -0.1260],
        [-0.0482,  0.1518, -0.1382,  ...,  0.0300,  0.0846, -0.0907],
        ...,
        [-0.0508, -0.0954, -0.0303,  ...,  0.0736, -0.1681, -0.1663],
        [ 0.0056,  0.1640, -0.1275,  ...,  0.1698, -0.1206, -0.0203],
        [ 0.0669,  0.1320,  0.0252,  ..., -0.0016, -0.1023,  0.0205]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[-0.0636, -0.2009, -0.1164,  ...,  0.2177,  0.0205,  0.0721],
        [ 0.1804,  0.1941, -0.2478,  ...,  0.2278, -0.0557, -0.0411],
        [ 0.0276, -0.0849,  0.1005,  ...,  0.1338,  0.0953,  0.2434],
        ...,
        [ 0.0617, -0.1316,  0.1080,  ..., -0.1323,  0.1172,  0.0912],
        [ 0.1765,  0.0424,  0.1868,  ..., -0.0415, -0.1866, -0.0281],
        [-0.1908, -0.2292, -0.1422,  ..., -0.1952, -0.0093,  0.1787]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0636, -0.2009, -0.1164,  ...,  0.2177,  0.0205,  0.0721],
        [ 0.1804,  0.1941, -0.2478,  ...,  0.2278, -0.0557, -0.0411],
        [ 0.0276, -0.0849,  0.1005,  ...,  0.1338,  0.0953,  0.2434],
        ...,
        [ 0.0617, -0.1316,  0.1080,  ..., -0.1323,  0.1172,  0.0912],
        [ 0.1765,  0.0424,  0.1868,  ..., -0.0415, -0.1866, -0.0281],
        [-0.1908, -0.2292, -0.1422,  ..., -0.1952, -0.0093,  0.1787]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[-0.2807],
        [ 0.2643],
        [ 0.1990],
        [-0.1654],
        [-0.0507],
        [ 0.3512],
        [ 0.3438],
        [ 0.0161],
        [-0.3142],
        [ 0.1262],
        [-0.3607],
        [-0.0340],
        [ 0.0574],
        [ 0.4164],
        [-0.0218],
        [-0.0050],
        [-0.3421],
        [ 0.2353],
        [-0.0829],
        [ 0.2921],
        [-0.4155],
        [ 0.0724],
        [-0.0790],
        [ 0.1735],
        [ 0.2823],
        [ 0.2552],
        [ 0.0709],
        [-0.4258],
        [ 0.1257],
        [ 0.3562],
        [ 0.3687],
        [-0.3167]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.2807],
        [ 0.2643],
        [ 0.1990],
        [-0.1654],
        [-0.0507],
        [ 0.3512],
        [ 0.3438],
        [ 0.0161],
        [-0.3142],
        [ 0.1262],
        [-0.3607],
        [-0.0340],
        [ 0.0574],
        [ 0.4164],
        [-0.0218],
        [-0.0050],
        [-0.3421],
        [ 0.2353],
        [-0.0829],
        [ 0.2921],
        [-0.4155],
        [ 0.0724],
        [-0.0790],
        [ 0.1735],
        [ 0.2823],
        [ 0.2552],
        [ 0.0709],
        [-0.4258],
        [ 0.1257],
        [ 0.3562],
        [ 0.3687],
        [-0.3167]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-8.6705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2492, device='cuda:0')



h[100].sum tensor(-1.4918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.5252, device='cuda:0')



h[200].sum tensor(-4.1591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-4.2521, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(9206.5244, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0000, 0.0000,  ..., 0.0038, 0.0082, 0.0121],
        [0.0018, 0.0000, 0.0000,  ..., 0.0018, 0.0038, 0.0057],
        [0.0005, 0.0000, 0.0000,  ..., 0.0005, 0.0011, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(47537.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(387.0937, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.6078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-14.6037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(0.0453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1549],
        [-0.1092],
        [-0.0735],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-7167.1602, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.1549],
        [-0.1092],
        [-0.0735],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0163,  0.0165,  0.0083,  ..., -0.0057, -0.0025, -0.0115],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-1333.0879, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(106.8756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(109.5587, device='cuda:0')



h[100].sum tensor(-134.6106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-137.9899, device='cuda:0')



h[200].sum tensor(-12.6571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.9748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0614, 0.0623, 0.0313,  ..., 0.0000, 0.0000, 0.0000],
        [0.0505, 0.0512, 0.0258,  ..., 0.0000, 0.0000, 0.0000],
        [0.0118, 0.0120, 0.0060,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(100091.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0580, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.1930],
        [0.0497, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.1654],
        [0.0399, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.1328],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(520204.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2872.0168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(201.2407, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(28628.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2005.3152, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.8771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[2.2179e-01],
        [2.3997e-01],
        [2.6478e-01],
        ...,
        [2.8915e-06],
        [4.8057e-06],
        [6.8680e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(15390.9648, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.1549],
        [-0.1092],
        [-0.0735],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 




net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
batcheddglgraph Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
TraTen tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.3054, 0.5586, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0')



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.7562, device='cuda:0')



h[100].sum tensor(-1.6736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-76.5228, device='cuda:0')



h[200].sum tensor(166.0773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.1953, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2199, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3956, 0.4767],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4185],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1427.1102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79414.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(401.6857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(80.8540, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-3766810., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(548.4788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072, -0.0218,  0.0249,  ..., -0.0150, -0.0190,  0.0174],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0091, -0.0277,  0.0317,  ..., -0.0191, -0.0241,  0.0222],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(361.6071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(60.7562, device='cuda:0')



h[100].sum tensor(-36.9857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-76.5228, device='cuda:0')



h[200].sum tensor(74.4324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-7.1953, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0000, 0.0204,  ..., 0.0000, 0.0000, 0.0142],
        [0.0295, 0.0000, 0.1029,  ..., 0.0000, 0.0000, 0.0720],
        [0.0111, 0.0000, 0.0387,  ..., 0.0000, 0.0000, 0.0271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(60749.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.0868, 0.0871],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.1664, 0.1669],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.1387, 0.1392],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(335053.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(403.5995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10980.5420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(530.5568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7283.9053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-338.3651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0690],
        [-0.0860],
        [-0.0940],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([67960, 1]) 
h5.sum tensor(-12037.1982, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet].sum tensor(731860., device='cuda:0', grad_fn=<SumBackward0>)

result1 tensor([[-12.5551],
        [ -6.5943],
        [ -0.1104],
        ...,
        [-46.1807],
        [-45.9089],
        [-45.8005]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape torch.Size([67960, 1]) 
result0 tensor([[-0.0690],
        [-0.0860],
        [-0.0940],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result0.shape torch.Size([67960, 1])

Passing event 10007 from the network before training input tensor([0.6294, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0') 
result1: tensor([-3.0553], device='cuda:0', grad_fn=<SelectBackward0>) 
result1.shape: torch.Size([1])

Passing event 10007 from the network after training input tensor([0.6294, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0') 
result1: tensor([-0.0747], device='cuda:0', grad_fn=<SelectBackward0>) 
result1.shape: torch.Size([1])

real	0m31.508s
user	0m24.109s
sys	0m5.201s
