0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1160.71.1.el7.x86_64/extra/nvidia.ko.xz
firmware:       nvidia/515.65.01/gsp.bin
alias:          char-major-195-*
version:        515.65.01
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.9
srcversion:     8049D44E2C1B08F41E1B8A6
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        drm
vermagic:       3.10.0-1160.71.1.el7.x86_64 SMP mod_unload modversions 
signer:         DKMS module signing key
sig_key:        9B:9E:55:A9:86:D9:50:0B:6D:2D:9F:BA:A7:E6:45:39:D4:DD:5F:C6
sig_hashalgo:   sha512
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableGpuFirmwareLogs:int
parm:           NVreg_OpenRmEnableUnsupportedGpus:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_EnableDbgBreakpoint:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           NVreg_DmaRemapPeerMmio:int
parm:           rm_firmware_active:charp

nvidia-smi:
Sat Sep 17 11:51:38 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   20C    P0    31W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.358472704

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b5693f938e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m5.298s
user	0m2.712s
sys	0m1.215s
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''
[11:52:02] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch





 Loading data ... 



training set shape (80000, 6796) 
sum 8401300

target set shape (80000, 6796) 
sum 5574226

TraTen and TrvTen shape:
 torch.Size([80000, 6796]) torch.Size([80000, 6796])




 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.0821],
        [ 1.7533],
        [ 0.2028],
        ...,
        [ 1.8201],
        [ 1.1736],
        [-0.0155]], device='cuda:0', requires_grad=True) 
node features sum: tensor(150.3312, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14
Model name: DGLBhaGcnReNewestweight7N4

net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
) 
number of the free learnable parameters: 43777

parameters of the network:

name conv1.weight 
shape:
 torch.Size([1, 256]) 
grad:
 True 
date:
 tensor([[-0.0025,  0.1504, -0.0107,  0.0926, -0.0466, -0.0035, -0.0685,  0.1287,
         -0.0803, -0.0921, -0.1096, -0.0362,  0.0352, -0.0302,  0.0559, -0.1185,
         -0.0297, -0.1158, -0.0136, -0.1470,  0.0299,  0.0732, -0.0790,  0.0931,
         -0.1256,  0.0934,  0.0544,  0.1270,  0.1288,  0.0466, -0.1414,  0.0201,
         -0.1194, -0.1253, -0.1027, -0.1144,  0.0620, -0.1408, -0.0921, -0.0762,
          0.1333,  0.0994,  0.0783, -0.0353, -0.0735,  0.0040, -0.1257, -0.0900,
          0.1490, -0.0665, -0.0368,  0.1103, -0.1122,  0.0248, -0.0553, -0.1416,
         -0.0586, -0.0731, -0.0575, -0.0481, -0.0183,  0.0831,  0.1328, -0.1002,
          0.1374,  0.1286,  0.0768, -0.0725, -0.0994, -0.0148,  0.1178, -0.0979,
          0.0451, -0.1385,  0.0899, -0.0389,  0.1283, -0.0144,  0.0685, -0.0233,
          0.0443,  0.0901,  0.1522,  0.1104,  0.0054,  0.1494,  0.0105, -0.1109,
          0.1099, -0.1006,  0.0890, -0.0499, -0.1074, -0.0452,  0.0937, -0.0965,
          0.1457, -0.0163,  0.0713, -0.1432, -0.1159, -0.0110,  0.1235, -0.1260,
          0.1144, -0.0055,  0.1525, -0.0950, -0.0037,  0.0204,  0.1330,  0.1468,
         -0.0120, -0.0336, -0.1241, -0.1183,  0.1186,  0.1181,  0.1468, -0.0613,
          0.0447, -0.1273,  0.0870, -0.1066, -0.0376,  0.1076,  0.1288, -0.0094,
         -0.1364, -0.0860, -0.0221, -0.1363, -0.1257, -0.1000, -0.0254,  0.0558,
          0.0395, -0.0762, -0.0548, -0.1019,  0.1447,  0.0885, -0.1525,  0.0417,
          0.0271, -0.0568,  0.1393, -0.1143, -0.1342,  0.1316,  0.1001,  0.0004,
          0.1063, -0.0887,  0.1302, -0.1460, -0.0620,  0.1253, -0.0918, -0.0734,
          0.0096,  0.0663,  0.0593,  0.0158,  0.0213,  0.0087,  0.1411,  0.1493,
         -0.1397, -0.0893,  0.0747,  0.0757,  0.0582,  0.1277,  0.0156,  0.0844,
         -0.1082,  0.0314, -0.1337, -0.0556, -0.0029, -0.1402, -0.1464, -0.0506,
         -0.0603, -0.0123, -0.1318, -0.0280, -0.1004,  0.0070, -0.1185,  0.1241,
          0.1322, -0.1441, -0.0933,  0.1500, -0.1290,  0.0217,  0.0361,  0.0859,
         -0.0633,  0.0233, -0.1472,  0.1432,  0.1151, -0.0890,  0.0381,  0.0648,
         -0.1255,  0.1021, -0.0351,  0.0549, -0.1332, -0.0911, -0.0017, -0.0257,
         -0.1215, -0.1391,  0.1215, -0.0110,  0.1464,  0.1471,  0.1460,  0.0370,
          0.1445,  0.1404,  0.1391, -0.0435, -0.1475,  0.0413,  0.0794,  0.0713,
          0.0312, -0.1006,  0.1507,  0.1356,  0.1476, -0.0667, -0.1124,  0.1443,
         -0.0820,  0.1363, -0.0321, -0.0653, -0.1447, -0.0776,  0.0656,  0.0283,
         -0.0560, -0.0685,  0.0256, -0.1007, -0.0831,  0.0537, -0.0951,  0.1036]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-0.0025,  0.1504, -0.0107,  0.0926, -0.0466, -0.0035, -0.0685,  0.1287,
         -0.0803, -0.0921, -0.1096, -0.0362,  0.0352, -0.0302,  0.0559, -0.1185,
         -0.0297, -0.1158, -0.0136, -0.1470,  0.0299,  0.0732, -0.0790,  0.0931,
         -0.1256,  0.0934,  0.0544,  0.1270,  0.1288,  0.0466, -0.1414,  0.0201,
         -0.1194, -0.1253, -0.1027, -0.1144,  0.0620, -0.1408, -0.0921, -0.0762,
          0.1333,  0.0994,  0.0783, -0.0353, -0.0735,  0.0040, -0.1257, -0.0900,
          0.1490, -0.0665, -0.0368,  0.1103, -0.1122,  0.0248, -0.0553, -0.1416,
         -0.0586, -0.0731, -0.0575, -0.0481, -0.0183,  0.0831,  0.1328, -0.1002,
          0.1374,  0.1286,  0.0768, -0.0725, -0.0994, -0.0148,  0.1178, -0.0979,
          0.0451, -0.1385,  0.0899, -0.0389,  0.1283, -0.0144,  0.0685, -0.0233,
          0.0443,  0.0901,  0.1522,  0.1104,  0.0054,  0.1494,  0.0105, -0.1109,
          0.1099, -0.1006,  0.0890, -0.0499, -0.1074, -0.0452,  0.0937, -0.0965,
          0.1457, -0.0163,  0.0713, -0.1432, -0.1159, -0.0110,  0.1235, -0.1260,
          0.1144, -0.0055,  0.1525, -0.0950, -0.0037,  0.0204,  0.1330,  0.1468,
         -0.0120, -0.0336, -0.1241, -0.1183,  0.1186,  0.1181,  0.1468, -0.0613,
          0.0447, -0.1273,  0.0870, -0.1066, -0.0376,  0.1076,  0.1288, -0.0094,
         -0.1364, -0.0860, -0.0221, -0.1363, -0.1257, -0.1000, -0.0254,  0.0558,
          0.0395, -0.0762, -0.0548, -0.1019,  0.1447,  0.0885, -0.1525,  0.0417,
          0.0271, -0.0568,  0.1393, -0.1143, -0.1342,  0.1316,  0.1001,  0.0004,
          0.1063, -0.0887,  0.1302, -0.1460, -0.0620,  0.1253, -0.0918, -0.0734,
          0.0096,  0.0663,  0.0593,  0.0158,  0.0213,  0.0087,  0.1411,  0.1493,
         -0.1397, -0.0893,  0.0747,  0.0757,  0.0582,  0.1277,  0.0156,  0.0844,
         -0.1082,  0.0314, -0.1337, -0.0556, -0.0029, -0.1402, -0.1464, -0.0506,
         -0.0603, -0.0123, -0.1318, -0.0280, -0.1004,  0.0070, -0.1185,  0.1241,
          0.1322, -0.1441, -0.0933,  0.1500, -0.1290,  0.0217,  0.0361,  0.0859,
         -0.0633,  0.0233, -0.1472,  0.1432,  0.1151, -0.0890,  0.0381,  0.0648,
         -0.1255,  0.1021, -0.0351,  0.0549, -0.1332, -0.0911, -0.0017, -0.0257,
         -0.1215, -0.1391,  0.1215, -0.0110,  0.1464,  0.1471,  0.1460,  0.0370,
          0.1445,  0.1404,  0.1391, -0.0435, -0.1475,  0.0413,  0.0794,  0.0713,
          0.0312, -0.1006,  0.1507,  0.1356,  0.1476, -0.0667, -0.1124,  0.1443,
         -0.0820,  0.1363, -0.0321, -0.0653, -0.1447, -0.0776,  0.0656,  0.0283,
         -0.0560, -0.0685,  0.0256, -0.1007, -0.0831,  0.0537, -0.0951,  0.1036]],
       device='cuda:0', requires_grad=True)

name conv1.bias 
shape:
 torch.Size([256]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv2.weight 
shape:
 torch.Size([256, 128]) 
grad:
 True 
date:
 tensor([[-1.5280e-02, -8.5162e-02,  2.3534e-02,  ..., -1.2142e-01,
          1.6729e-02, -4.4052e-02],
        [ 3.5461e-02, -2.8095e-02,  4.5214e-02,  ..., -5.3698e-02,
         -1.1100e-01,  1.1168e-01],
        [-5.6871e-02, -8.2951e-02, -7.7777e-02,  ..., -1.1827e-04,
          1.2819e-02,  1.2438e-01],
        ...,
        [-1.2464e-01,  6.3677e-02,  1.2009e-01,  ...,  1.0424e-01,
          3.4151e-02, -5.8165e-02],
        [-5.2306e-02,  5.6506e-02,  1.6190e-02,  ..., -1.0663e-01,
          3.5236e-02, -2.4612e-02],
        [-9.7432e-03, -4.8880e-02,  3.9036e-02,  ...,  7.1240e-02,
         -4.4703e-05,  9.7729e-02]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[-1.5280e-02, -8.5162e-02,  2.3534e-02,  ..., -1.2142e-01,
          1.6729e-02, -4.4052e-02],
        [ 3.5461e-02, -2.8095e-02,  4.5214e-02,  ..., -5.3698e-02,
         -1.1100e-01,  1.1168e-01],
        [-5.6871e-02, -8.2951e-02, -7.7777e-02,  ..., -1.1827e-04,
          1.2819e-02,  1.2438e-01],
        ...,
        [-1.2464e-01,  6.3677e-02,  1.2009e-01,  ...,  1.0424e-01,
          3.4151e-02, -5.8165e-02],
        [-5.2306e-02,  5.6506e-02,  1.6190e-02,  ..., -1.0663e-01,
          3.5236e-02, -2.4612e-02],
        [-9.7432e-03, -4.8880e-02,  3.9036e-02,  ...,  7.1240e-02,
         -4.4703e-05,  9.7729e-02]], device='cuda:0', requires_grad=True)

name conv2.bias 
shape:
 torch.Size([128]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv3.weight 
shape:
 torch.Size([128, 64]) 
grad:
 True 
date:
 tensor([[ 0.0884,  0.0367, -0.0743,  ..., -0.1599,  0.0182, -0.1151],
        [ 0.0988,  0.1582,  0.1355,  ..., -0.1038, -0.0521,  0.0861],
        [-0.1087,  0.1507, -0.0818,  ...,  0.0656,  0.0140, -0.1760],
        ...,
        [-0.0664,  0.1209, -0.0106,  ...,  0.0435,  0.1007,  0.0025],
        [-0.0418, -0.0193, -0.1565,  ...,  0.0390, -0.0278,  0.1358],
        [-0.1707, -0.0101,  0.0817,  ..., -0.1587,  0.0898,  0.1525]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0884,  0.0367, -0.0743,  ..., -0.1599,  0.0182, -0.1151],
        [ 0.0988,  0.1582,  0.1355,  ..., -0.1038, -0.0521,  0.0861],
        [-0.1087,  0.1507, -0.0818,  ...,  0.0656,  0.0140, -0.1760],
        ...,
        [-0.0664,  0.1209, -0.0106,  ...,  0.0435,  0.1007,  0.0025],
        [-0.0418, -0.0193, -0.1565,  ...,  0.0390, -0.0278,  0.1358],
        [-0.1707, -0.0101,  0.0817,  ..., -0.1587,  0.0898,  0.1525]],
       device='cuda:0', requires_grad=True)

name conv3.bias 
shape:
 torch.Size([64]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)

name conv4.weight 
shape:
 torch.Size([64, 32]) 
grad:
 True 
date:
 tensor([[ 0.0474, -0.0528, -0.2370,  ...,  0.1506, -0.0686, -0.1378],
        [ 0.1596,  0.0984, -0.2291,  ..., -0.0974, -0.1622, -0.1272],
        [-0.0929, -0.0747,  0.1291,  ...,  0.0994,  0.1197, -0.2060],
        ...,
        [-0.0729, -0.1520, -0.2195,  ..., -0.0624,  0.0014, -0.0258],
        [ 0.2099, -0.1840, -0.1176,  ..., -0.2164, -0.1433, -0.1867],
        [ 0.1787, -0.1535, -0.0389,  ...,  0.1471,  0.1363, -0.0519]],
       device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0474, -0.0528, -0.2370,  ...,  0.1506, -0.0686, -0.1378],
        [ 0.1596,  0.0984, -0.2291,  ..., -0.0974, -0.1622, -0.1272],
        [-0.0929, -0.0747,  0.1291,  ...,  0.0994,  0.1197, -0.2060],
        ...,
        [-0.0729, -0.1520, -0.2195,  ..., -0.0624,  0.0014, -0.0258],
        [ 0.2099, -0.1840, -0.1176,  ..., -0.2164, -0.1433, -0.1867],
        [ 0.1787, -0.1535, -0.0389,  ...,  0.1471,  0.1363, -0.0519]],
       device='cuda:0', requires_grad=True)

name conv4.bias 
shape:
 torch.Size([32]) 
grad:
 True 
date:
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)

name conv5.weight 
shape:
 torch.Size([32, 1]) 
grad:
 True 
date:
 tensor([[ 0.0370],
        [-0.1180],
        [ 0.0400],
        [-0.2698],
        [ 0.2430],
        [ 0.1083],
        [ 0.3523],
        [-0.4134],
        [ 0.1431],
        [-0.2451],
        [ 0.2819],
        [ 0.1860],
        [ 0.2593],
        [-0.2439],
        [-0.1566],
        [-0.2909],
        [ 0.0540],
        [ 0.0484],
        [-0.1561],
        [-0.2199],
        [-0.1413],
        [-0.0088],
        [ 0.3186],
        [-0.0349],
        [-0.0810],
        [ 0.3690],
        [-0.4127],
        [ 0.2364],
        [ 0.3969],
        [ 0.1461],
        [-0.3797],
        [ 0.3099]], device='cuda:0') 
parameter:
 Parameter containing:
tensor([[ 0.0370],
        [-0.1180],
        [ 0.0400],
        [-0.2698],
        [ 0.2430],
        [ 0.1083],
        [ 0.3523],
        [-0.4134],
        [ 0.1431],
        [-0.2451],
        [ 0.2819],
        [ 0.1860],
        [ 0.2593],
        [-0.2439],
        [-0.1566],
        [-0.2909],
        [ 0.0540],
        [ 0.0484],
        [-0.1561],
        [-0.2199],
        [-0.1413],
        [-0.0088],
        [ 0.3186],
        [-0.0349],
        [-0.0810],
        [ 0.3690],
        [-0.4127],
        [ 0.2364],
        [ 0.3969],
        [ 0.1461],
        [-0.3797],
        [ 0.3099]], device='cuda:0', requires_grad=True)

name conv5.bias 
shape:
 torch.Size([1]) 
grad:
 True 
date:
 tensor([0.], device='cuda:0') 
parameter:
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(82.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(17.5102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.0097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(7.1666, device='cuda:0')



h[100].sum tensor(7.6790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8508, device='cuda:0')



h[200].sum tensor(1.8248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(1.8657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(9710.8096, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(64416.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.4367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-21.2622, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1221.3104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(77.6553, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2729],
        [-0.1922],
        [-0.1293],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-12622.8105, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 1007 from the network before training input tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0') 
result1: tensor([[-0.2729],
        [-0.1922],
        [-0.1293],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1])



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')



input graph: 
g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([135920, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(989.0452, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0118,  0.0222, -0.0048,  ..., -0.0040, -0.0016,  0.0100],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(-2967.8904, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(77.5523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(79.4992, device='cuda:0')



h[100].sum tensor(-62.9675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-64.5482, device='cuda:0')



h[200].sum tensor(-21.4238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0446, 0.0840, 0.0000,  ..., 0.0000, 0.0000, 0.0379],
        [0.0367, 0.0691, 0.0000,  ..., 0.0000, 0.0000, 0.0311],
        [0.0086, 0.0162, 0.0000,  ..., 0.0000, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([135920, 256]) 
h.sum tensor(83826.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4165, 0.2659, 0.1912,  ..., 0.0000, 0.0000, 0.4445],
        [0.3568, 0.2278, 0.1638,  ..., 0.0000, 0.0000, 0.3809],
        [0.2865, 0.1829, 0.1315,  ..., 0.0000, 0.0000, 0.3059],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([135920, 128]) 
h2.sum tensor(495713.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(20615.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1444.2106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3341.1155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(233.9662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(312.2835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(22.0079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=135920, num_edges=1463720,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.0946e+00],
        [1.1844e+00],
        [1.3069e+00],
        ...,
        [1.4258e-05],
        [2.3703e-05],
        [3.3876e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([135920, 1]) 
h5.sum tensor(75984.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1463720, 1]) 
g.edata[efet].sum tensor(1463720., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.2729],
        [-0.1922],
        [-0.1293],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/checkpoint_dir/90016284saved_checkpoint.tar



load_model True 
TraEvN 9001 
BatchSize 30.0 
EpochNum 6 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 






input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]]) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet].sum tensor(548.4788)



input graph: 
g Graph(num_nodes=67960, num_edges=731860,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([731860, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(731860., grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([67960, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7681],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]]) 
g.ndata[nfet].sum tensor(548.4788)
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1328e-04, -4.5823e-03,  1.3640e-02,  ...,  1.4443e-02,
         -2.4853e-22, -8.1416e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.4864e-03,  1.7536e-02,  ...,  1.8661e-02,
         -2.4853e-22, -8.1359e-04],
        ...,
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04],
        [-3.1328e-04, -4.9352e-03, -6.8306e-04,  ..., -1.0669e-03,
         -2.4853e-22, -8.1625e-04]], grad_fn=<AddBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(1305.8704, grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2904, grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.0866, device='cuda:0')



h[100].sum tensor(-1.6736, grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-35.7955, device='cuda:0')



h[200].sum tensor(166.0773, grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.1789, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0564,  ..., 0.0597, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0202,  ..., 0.0209, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([67960, 256]) 
h.sum tensor(237866.1250, grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.1356,  ..., 0.0000, 0.2198, 0.3187],
        [0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.3955, 0.4766],
        [0.0000, 0.0000, 0.0765,  ..., 0.0000, 0.3302, 0.4184],
        ...,
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438],
        [0.0000, 0.0000, 0.2059,  ..., 0.0000, 0.0278, 0.1438]],
       grad_fn=<ReluBackward0>) 
h2.shape torch.Size([67960, 128]) 
h2.sum tensor(2150758., grad_fn=<SumBackward0>)



h2[0].sum tensor(0., grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/./Results.py", line 38, in <module>
    result1 = net(batcheddglgraph, TraTen[10000:10010].reshape(10 * 6796, 1).to('cpu'))
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLBhaGcnReNewestweight7N4/ModelBha.py", line 105, in forward
    print('\n(h1.sum(axis=0) * param0_2).sum() + bias0', (h1.sum(axis=0) * param0_2).sum() + bias0)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!

real	0m28.177s
user	0m20.411s
sys	0m5.227s
